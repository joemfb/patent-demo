<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626488-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626488</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13441417</doc-number>
<date>20120406</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>20</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704  4</main-classification>
<further-classification>704  7</further-classification>
<further-classification>704  8</further-classification>
<further-classification>704  9</further-classification>
<further-classification>704 10</further-classification>
</classification-national>
<invention-title id="d2e51">Word alignment with bridge languages</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6131082</doc-number>
<kind>A</kind>
<name>Hargrave et al.</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  7</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6321189</doc-number>
<kind>B1</kind>
<name>Masuichi et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  7</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6321191</doc-number>
<kind>B1</kind>
<name>Kurahashi</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7454326</doc-number>
<kind>B2</kind>
<name>Marcu et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  2</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7620538</doc-number>
<kind>B2</kind>
<name>Marcu et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  2</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2004/0122656</doc-number>
<kind>A1</kind>
<name>Abir</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  4</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2005/0010421</doc-number>
<kind>A1</kind>
<name>Watanabe et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704277</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2005/0228640</doc-number>
<kind>A1</kind>
<name>Aue et al.</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  9</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2007/0203690</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  2</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>Ayan et al., &#x201c;A Maximum Entropy Approach to Combining Word Alignments,&#x201d; Proceedings of the Human Langauge Technology Conference of the North American Chapter of the ACL, New York, Jun. 2006, pp. 96-103.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>Bangalore et al., &#x201c;Bootstrapping Bilingual Data using Consensus Translation for a Multilingual Instant Messaging System,&#x201d; in Coling, 2002, Taipei, Taiwan, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>Brants et al., &#x201c;Large Lamguage Models in Machine Translation,&#x201d; in EMNLP, Prague, Czech Republic, 2007, pp. 858-867.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Filali et al., &#x201c;Leveraging Multiple Languages to Improve Statistical MT Word Alignments,&#x201d; IEEE Workshop on Automatic Speech Recognition and Understanding, San Juan, Puerto Rico, 2005, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Fraser et al., &#x201c;Measuring Word Alignment Quality for Statistical Machine Translation,&#x201d; Technical Report ISI-TR-7616, ISI/University of Southern Califiornia, 2006, pp. 293-303.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Ittycheriah et al., &#x201c;A Maximum Entropy Word Aligner for Arabic-English Machine Translation,&#x201d; Proceedings of Human Language Technology Conference and Conference on Impirical Methods in Natural Language Processing (HTL/EMNLP), Vancouver, Oct. 2005, pp. 89-96.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Koehn, &#x201c;Statistical Significance Tests for Machine Translation Evaluation,&#x201d; In EMNLP, Barcelona, Spain, 2004, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>Kumar et al., &#x201c;Minimum Bayes-Risk Decoding for Statistical Machine Translation,&#x201d; In HLTNAACL, Boston, MA, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>Macherey et al., &#x201c;An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems,&#x201d; in EMNLP, Prague, Czech Republic, 2007, pp. 986-995.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>Mann et al., &#x201c;Multipath Translation Lexicon Induction via Bridge Languages,&#x201d; in NAACL, Pittsburgh, PA, 2001, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Martin et al., &#x201c;Word Alignment for Languages with Scarce Resources,&#x201d; ACL Workshop on Building and Using Parallel Texts, Ann Arbor, MI, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Matusov et al., &#x201c;Computing Consensus Translation from Multiple Machine Translation Systems Using Enhanced Hypotheses Alignment,&#x201d; In EACL, Trento, Italy, 2006, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Moore, &#x201c;A Discriminative Framework for Bilingual Word Alignment,&#x201d; In EMNLP, Vancouver, Canada, 2005, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Och, &#x201c;Minimum Error Rate Training in Statistical Machine Translation,&#x201d; Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Jul. 2003, pp. 160-167.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Resnik et al., &#x201c;Creating a Parallel Corpus from the &#x201c;Book of 2000 tongues&#x201d;,&#x201d; Text Encoding Initative 10th Anniversary User Conference, Providence, RI, 1997, pp. 1-14.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Sim et al., &#x201c;Consensus Network Decoding for Statistical Machine Translation System Combination,&#x201d; IEEE International Conference on Acoustics, Speech, and Signal Processing, Honolulu, HI, 2007, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Simard, &#x201c;Text-Translation Alignment: Three Languages Are Better Than Two,&#x201d; EMNLP-VLC, College Park, MD, 1999, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Ueffing et al., &#x201c;Word-Level Confidence Estimation for Machine Translation using Phrase-Based Translation Models,&#x201d; EMNLP, Vancouver, Canada, 2005, pp. 763-770.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Borin, Lars, &#x201c;You'll Take the High Rad and I'll Take the Low Road: Using a Third Language to Improve Bilingual Word Alignment,&#x201d;. In COLING, 2000, pp. 97-103, Saarbrucken, Germany.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Brown et al., &#x201c;The Mathematics of Statistical Machine Translation: Parameter Estimation,&#x201d; Computational Linguistics, 1993, vol. 19, No. 2, pp. 263-311.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Deng et al., &#x201c;HMM Word and Phrase Alignment for Statistical Machine Translation,&#x201d; HLT-EMNLP 2005, Vancouver, Canada, 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Fraser et al., Semi-Supervised Training for Statistical Word Alignment. Proceedings of the 21st International COnference on Computational Linguistics and 44th Annual Meeting of the ACL, 2006, Sydney, Australia, pp. 769-776.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Matusov et al., Symmetric Word Alignments for Statistical Machine Translation. In Coling, 2004, Geneva, Switzerland, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Och et al., &#x201c;A Systematic Comparison of Various Statistical Alignment Models,&#x201d; Association for Computational Linguistics, 2003, vol. 29, No. 1, pp. 19-51.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Och et al., &#x201c;The Alignment Template Approach to Statistical Machine Translation,&#x201d; Association for Computational Linguistics, 2004, vol. 30, No. 4, pp. 417-449.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Vogel et al., &#x201c;HMM-Based Word Alignment in Statistical Translation,&#x201d; In COLING, 1996, Copenhagen, Denmark, pp. 836-841.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Schafer et al., &#x201c;Inducing Translation Lexicons via Diverse Similarity Measures and Bridge Languages,&#x201d; In CoNLL, 2002, Taipei, Taiwan, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Fiscus, &#x201c;A Post-Processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction&#x201d;, 1997, California.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Papineni, et al., &#x201c;Bleu: A Method for Automatic Evaluation of Machine Translation&#x201d;, 2002, Philadelphia.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704  4</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704  7- 10</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11781824</doc-number>
<date>20070723</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8185375</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13441417</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60920281</doc-number>
<date>20070326</date>
</document-id>
</us-provisional-application>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kumar</last-name>
<first-name>Shankar</first-name>
<address>
<city>New York</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Och</last-name>
<first-name>Franz Josef</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Macherey</last-name>
<first-name>Wolfgang</first-name>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Kumar</last-name>
<first-name>Shankar</first-name>
<address>
<city>New York</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Och</last-name>
<first-name>Franz Josef</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Macherey</last-name>
<first-name>Wolfgang</first-name>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Remarck Law Group PLC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Google Inc</orgname>
<role>02</role>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Cyr</last-name>
<first-name>Leonard Saint</first-name>
<department>2658</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Systems, methods, and computer program products are provided for statistical machine translation. In some implementations a method is provided. The method includes receiving multi-lingual parallel text associating a source language, a target language, and one or more bridge languages, determining an alignment between the source language and the target language using a first bridge language that is distinct from the source language and the target language, and using the determined alignment to generate a candidate translation of an input text in the source language to the target language.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="173.57mm" wi="146.90mm" file="US08626488-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="240.28mm" wi="192.11mm" file="US08626488-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="207.86mm" wi="149.35mm" file="US08626488-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="109.90mm" wi="159.60mm" file="US08626488-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="171.62mm" wi="190.25mm" file="US08626488-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="201.25mm" wi="169.08mm" file="US08626488-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 11/781,824 filed on Jul. 23, 2007 which claims the benefit under 35 U.S.C. &#xa7;119(e) of U.S. Provisional Patent Application No. 60/920,281, entitled Improving Word Alignment with Bridge Languages, to Shankar Kumar, Franz J. Och, and Wolfgang Macherey, filed on Mar. 26, 2007. The entire disclosures of each of the above applications are incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">This specification relates to statistical machine translation.</p>
<p id="p-0004" num="0003">Manual translation of text by a human operator can be time consuming and costly. One goal of machine translation is to automatically translate text in a source language to corresponding text in a target language. There are several different approaches to machine translation including example-based machine translation and statistical machine translation. Statistical machine translation attempts to identify a most probable translation in a target language given a particular input in a source language. For example, when translating a sentence from French to English, statistical machine translation identifies the most probable English sentence given the French sentence. This maximum likelihood translation can be written as:</p>
<p id="p-0005" num="0004">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <munder>
    <mi>argmax</mi>
    <mi>e</mi>
  </munder>
  <mo>&#x2062;</mo>
  <mrow>
    <mi>P</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mi>e</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>|</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>f</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
which describes the English sentence, e, out of all possible sentences, that provides the highest value for P(e|f). Additionally, Bayes Rule provides that:
</p>
<p id="p-0006" num="0005">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>P</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mi>e</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>|</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>f</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mfrac>
      <mrow>
        <mrow>
          <mi>P</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mi>e</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>&#x2062;</mo>
        <mrow>
          <mi>P</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>e</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mtext>|</mtext>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>f</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
      <mrow>
        <mi>P</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mi>f</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mfrac>
    <mo>.</mo>
  </mrow>
</mrow>
</math>
</maths>
<br/>
Using Bayes Rule, this most likely sentence can be re-written as:
</p>
<p id="p-0007" num="0006">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <mrow>
    <munder>
      <mi>argmax</mi>
      <mi>e</mi>
    </munder>
    <mo>&#x2062;</mo>
    <mrow>
      <mi>P</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>e</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>|</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>f</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <munder>
      <mi>argmax</mi>
      <mi>e</mi>
    </munder>
    <mo>&#x2062;</mo>
    <mrow>
      <mi>P</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mi>e</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>&#x2062;</mo>
    <mrow>
      <mrow>
        <mi>P</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mi>e</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mtext>|</mtext>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>f</mi>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
      <mo>.</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0008" num="0007">Consequently, the most likely e (i.e., the most likely English translation) is one that maximizes the product of the probability that e occurs and the probability that e would be translated into f (i.e., the probability that a given English sentence would be translated into the French sentence).</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0009" num="0008">Systems, methods, and computer program products are provided for statistical machine translation. In general, in one aspect, a method is provided. The method includes receiving multi-lingual parallel text associating a source language, a target language, and one or more bridge languages, determining an alignment between the source language and the target language using a first bridge language that is distinct from the source language and the target language, and using the determined alignment to generate a candidate translation of an input text in the source language to the target language. Other embodiments of this aspect include corresponding systems, apparatus, computer program products, and computer readable media.</p>
<p id="p-0010" num="0009">Implementations of the aspects can include one or more of the following features. Determining the alignment using the first bridge language can include determining a first alignment between the source language and the first bridge language, determining a second alignment between the bridge language and the target language, and determining the alignment between the source language and the target language using the first and second alignments. The aspects can further include determining a plurality of alignments between the source language and the target language using a plurality of respective bridge languages, using each of the determined plurality of alignments to generate respective candidate translations of the input text, and determining a consensus translation using the candidate translations.</p>
<p id="p-0011" num="0010">The aspects can further include determining a combined alignment using two or more alignments determined using two or more bridge languages and using the combined alignment to generate a candidate translation of the input text. Determining the alignment using the first bridge language can further include determining a first posterior probability matrix associated with an alignment from the source language to the bridge language, determining a second posterior probability matrix associated with an alignment from the bridge language to the target language, determining a third posterior probability matrix associated with an alignment from the source language to the target language as a function of the first posterior probability matrix and the second posterior probability matrix, and identifying a maximum alignment probability using the third posterior probability matrix.</p>
<p id="p-0012" num="0011">Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. The word alignment for a particular source and target language can be improved using bridge languages. In particular, when less parallel data is available for the source and target languages, bridge translations using languages in which there is a greater amount of parallel data between either source-bridge, target-bridge, or both can yield a better alignment. Improved alignment quality can lead to improved translation quality. More than one alignment between the source and target language sentences can be generated using different bridge languages.</p>
<p id="p-0013" num="0012">Translations generated using multiple bridge languages are uncorrelated. These provide a diverse pool of hypotheses for hypothesis combination, which in turn can produce a hypothesis with higher translation quality. Additionally, when the bridge language has the same amount of data as the source/target, alignment errors in the source-target language pair are generally not correlated to the alignment errors in source-bridge or bridge-target pairs. Consequently, source-target alignment errors can be corrected using source-bridge and bridge-target word alignments.</p>
<p id="p-0014" num="0013">The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of an example machine translation system.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an example of direct alignment between a source sentence and a translated sentence.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3</figref> is a flow chart illustrating an example method for providing translations using bridge languages.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> illustrates bridge alignments between a source sentence and a translated sentence.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram showing an example of translation using bridge languages.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 6</figref> is an example system for providing translations.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0021" num="0020">Like reference numbers and designations in the various drawings indicate like elements.</p>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of an example machine translation system <b>100</b>. Machine translation system includes a target corpus <b>102</b>, a language model <b>104</b>, a parallel corpus <b>106</b>, a translation model <b>108</b>, and a decoder <b>110</b>. Providing input text <b>112</b> to the translation system <b>100</b> produces translated text <b>114</b>.</p>
<p id="p-0023" num="0022">The target corpus <b>102</b> provides a collection of text in a target language (e.g., English), which is used to train the language model <b>104</b>. The target corpus <b>102</b> can include a number of different sources of text, including, e.g., web pages and news articles. In some implementations, the target corpus <b>102</b> includes text on the order of tens to hundreds of billions of words, or even more. One such corpus is the Linguistic Data Consortium (&#x201c;LDC&#x201d;) Web 1T 5-gram Version 1 corpus, LDC Catalog No.: DC2006T13, ISBN: 1-58563-397-6, contributed by Google Inc. This corpus uses approximately one trillion word tokens (including individual words, punctuation, and markers identifying a beginning and end of individual sentences) of text from publicly accessible Web pages.</p>
<p id="p-0024" num="0023">The language model <b>104</b> identifies the probability that a particular string (e.g., a phrase, sentence, or collection of sentences) in the source language occurs. Thus, for English, the language model <b>104</b> identifies the probability that a particular string in English occurs. To identify the probability of a particular string (e.g., sentence, phrase) occurring, the language model <b>104</b> calculates the number of times the string occurs in the target corpus <b>102</b> divided by the total number of strings in the target corpus <b>102</b>. For example, if the phrase &#x201c;The red wheelbarrow&#x201d; occurs 53,000 times in a corpus of 100,000,000 words, the probability equals:</p>
<p id="p-0025" num="0024">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>P</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mi>The</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>red</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>wheelbarrow</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mfrac>
      <mrow>
        <mstyle>
          <mrow>
            <mn>53</mn>
            <mo>,</mo>
            <mn>000</mn>
          </mrow>
        </mstyle>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
      </mrow>
      <mstyle>
        <mtext>100,000,000</mtext>
      </mstyle>
    </mfrac>
    <mo>=</mo>
    <mrow>
      <mn>0.00053</mn>
      <mo>.</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0026" num="0025">However, a number of possible strings will have a probability of zero since they are not found within the target corpus <b>102</b>. Therefore, in some implementations, the probability of a particular string is calculated as a function of the probabilities of sub-string components. One technique for representing sub-strings is by using n-grams. An n-gram is a sequence of n consecutive tokens. An n-gram has an order, which is the number of tokens in the n-gram. For example, a 1-gram (or unigram) includes one token; a 2-gram (or bigram) includes two tokens.</p>
<p id="p-0027" num="0026">An n-gram language model uses n-gram sub-strings to calculate the probability of a string. The probably of a given string can be calculated as a product of n-gram conditional probabilities. The conditional probability for a bigram, represented P(y|x), is the probability that word y follows word x. The conditional probabilities are generally determined empirically, according to relative frequencies in the target corpus <b>102</b>. In the example above, the probability of the word y given x is given by:</p>
<p id="p-0028" num="0027">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>P</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>y</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>|</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>x</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mfrac>
      <mrow>
        <mi>f</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mi>xy</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
      <mrow>
        <mi>f</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mfrac>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<br/>
where f (xy) is a frequency or a count of the occurrences of the string &#x201c;xy&#x201d; in the target corpus <b>102</b>.
</p>
<p id="p-0029" num="0028">The probability for the string can be determined as a product of conditional probabilities. For example, to calculate P(The red wheelbarrow) using a bigram language model, the n-gram language model calculates:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(The|&#x3c;start&#x3e;)<i>P</i>(red|The)<i>P</i>(wheelbarrow|red),<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where &#x3c;start&#x3e; is a marker representing the beginning of a sentence such that P(The|&#x3c;start&#x3e;) represents the probability that a sentence begins with &#x201c;The&#x201d;.
</p>
<p id="p-0030" num="0029">This string probability can be generalized to:</p>
<p id="p-0031" num="0030">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>P</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <msub>
          <mi>e</mi>
          <mn>1</mn>
        </msub>
        <mo>,</mo>
        <mi>&#x2026;</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>,</mo>
        <msub>
          <mi>e</mi>
          <mi>k</mi>
        </msub>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mrow>
      <mi>P</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <msub>
          <mi>e</mi>
          <mn>1</mn>
        </msub>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>&#x2062;</mo>
    <mrow>
      <mi>P</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <msub>
            <mi>e</mi>
            <mn>2</mn>
          </msub>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>|</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <msub>
            <mi>e</mi>
            <mn>1</mn>
          </msub>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>&#x2062;</mo>
    <mi>&#x2026;</mi>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.8em" height="0.8ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <munderover>
        <mo>&#x220f;</mo>
        <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mi>n</mi>
        </mrow>
        <mi>k</mi>
      </munderover>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mrow>
        <mi>P</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow>
              <msub>
                <mi>e</mi>
                <mi>i</mi>
              </msub>
              <mo>&#x2062;</mo>
              <mstyle>
                <mtext>|</mtext>
              </mstyle>
              <mo>&#x2062;</mo>
              <msub>
                <mi>e</mi>
                <mrow>
                  <mi>i</mi>
                  <mo>-</mo>
                  <mi>n</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
              </msub>
            </mrow>
            <mo>,</mo>
            <mi>&#x2026;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>,</mo>
            <msub>
              <mi>e</mi>
              <mrow>
                <mi>i</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
where (e<sub>1</sub>, . . . , e<sub>k</sub>) represent tokens in the string and n is the order of the largest n-gram allowed in the language model.
</p>
<p id="p-0032" num="0031">The parallel corpus <b>106</b> includes a collection of text in the source language (e.g., French) and a corresponding translation in one or more target languages (e.g., English). The parallel corpus <b>106</b> can include a number of different sources of text, including, e.g., web page and news article pairs where each pair includes text in the source language and the corresponding translated text in the target language. In another example, the parallel corpus <b>106</b> can include multi-lingual data. For example, United Nations proceedings are available which provide parallel translations in six languages.</p>
<p id="p-0033" num="0032">The translation model <b>108</b> identifies the conditional probability of a particular source language string given a particular target string. Thus, for an English target language and a French source language, the translation model <b>108</b> identifies the probability P(f|e) of a French string f given an English string e. Translation models can be generated in a number of different ways. In some implementations, a number of parameters are estimated in order to determine P(f|e). For example, a translation model can be defined according to four parameters: t, n, d, and p (e.g., IBM Model 3 described, for example, P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer, <i>The Mathematics of Statistical Machine Translation: Parameter Estimation</i>, Computational Linguistics 19(2), 1993, which is incorporated by reference).</p>
<p id="p-0034" num="0033">A translation parameter, t, provides a probability of producing a translated word from a target word, e.g., t(bonjour|hello). A fertility parameter, n, provides a probability that target word will produce n source words. For example, n(2|hello) represents the probability that the target word &#x201c;hello&#x201d; will produce exactly two French words. A distortion parameter, d, provides a probability associated with the position of a source word in a source sentence relative to the position of the corresponding target word in a target sentence. For example, d(3|5) represents the probability that the English word in position <b>5</b> of a sentence (e.g., the fifth word) will provide a French word in position <b>3</b> of a translated French sentence (e.g., the third word). Additionally, a parameter p provides a probability of the translation including a spurious word. A spurious word is a word that appears in the source language translation of a target language string that does not correspond to a target word in the target string.</p>
<p id="p-0035" num="0034">The values of the model parameters can be estimated directly if the words in the source and target sentence pairs are all aligned. The term &#x201c;alignment&#x201d; will be used to refer to a data structure that represents a word-for-word connection between source and target words (e.g., that &#x201c;hello&#x201d; in one sentence aligns with &#x201c;bonjour&#x201d;) in a pair of sentences. In some implementations, the alignment is simply a vector identifying positions of target words that various source words connect to. If the alignment is known, the parameter values can be estimated.</p>
<p id="p-0036" num="0035">For a collection of sentences, the sentence pairs (F, E) can be represented as (F, E)=((f<sub>1</sub>, e<sub>1</sub>), (f<sub>2</sub>, e<sub>2</sub>) . . . (f<sub>n</sub>, e<sub>n</sub>)). If A represents the alignment of all sentence pairs, then A=a<sub>1</sub>, a<sub>2</sub>, . . . , a<sub>n </sub>where n is the number of sentences in the collection. If there is conditional independence between sentences in the collection, the alignment probability of the collection can be written as a product of the alignment probabilities of individual sentences: P (A, E|F)=P(a<sub>1</sub>, e<sub>1</sub>|f<sub>1</sub>) P(a<sub>2</sub>, e<sub>2</sub>|f<sub>2</sub>) . . . P(a<sub>n</sub>, e<sub>n</sub>|f<sub>n</sub>). As a result of this factorization, the probability of a single sentence pair can be considered independently of the collection.</p>
<p id="p-0037" num="0036">There can be more than one possible alignment for a sentence pair. For example, a sentence pair could have two equally likely alignments. Consequently, a probability can be calculated for a particular alignment. The alignment probability defines the likelihood that words are aligned in a particular way. The probability of a particular alignment given a particular sentence pair can be written P(a|e, f), where:</p>
<p id="p-0038" num="0037">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>P</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mrow>
          <mi>a</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>|</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>e</mi>
        </mrow>
        <mo>,</mo>
        <mi>f</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>P</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>a</mi>
          <mo>,</mo>
          <mrow>
            <mi>f</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mtext>|</mtext>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>e</mi>
          </mrow>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mrow>
      <mi>P</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>f</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>|</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>e</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mfrac>
</mrow>
</math>
</maths>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>where,<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0039" num="0038">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>P</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>f</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mtext>|</mtext>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>e</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <munder>
        <mo>&#x2211;</mo>
        <mi>a</mi>
      </munder>
      <mo>&#x2062;</mo>
      <mrow>
        <mi>P</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mi>a</mi>
            <mo>,</mo>
            <mrow>
              <mi>f</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mtext>|</mtext>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>e</mi>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<br/>
and where the P(a, f|e) is summed over all alignments a and represents the joint probability of an alignment and a source sentence, given a target sentence. Alignment, a, is represented by a single letter; however, it represents a matrix whose dimension can vary. Specifically, a is a matrix random variable, a specific value of which refers to a matrix of associations (e.g., links) between a specific source sentence and a specific paired target sentence. Columns correspond to source word position and rows to target word positions. An additional row and column may be labeled with the null word, in cases where there is no correspondence. The elements in the matrix are zeroes and ones, indicating the presence or absence of an association between the corresponding source and target sentence words. Depending on the alignment model used, constraints may be imposed on where in the matrix links can occur (e.g., whether a source word can map to multiple target words, whether words can map to a null word, etc.)
</p>
<p id="p-0040" num="0039">Therefore, P(a|e, f) can be described in terms of P(a, f|e). However, P(a, f|e) can also be described as a function of the parameter values. Thus, if the parameter values are known, the alignment probabilities can be directly calculated.</p>
<p id="p-0041" num="0040">A particular alignment a refers to a specific alignment between a specific (f, e) pair, while P(a|e, f) is the posterior probability over possible alignments, again for the specific (f, e) pair. P(a|e, f) is described by parameters which can be estimated by some training procedure that iteratively learns the parameters by looping over a large number of (f, e) sentence pairs, using the current parameter values to achieve a better word alignment between each pair than was achieved in the previous iteration, then using that alignment to update the parameter values, then repeating. Additional details on alignment can be found, for example, in Franz Joseph Och and Hermann Ney, <i>A Systematic Comparison of Various Statistical Alignment Models, Computational Linguistics, </i>29(1): 9&#x2014;51, March 2003, which is incorporated by reference.</p>
<p id="p-0042" num="0041">Consequently, to calculate P(f|e), the translation system calculates P(a, f|e). However, to calculate P (a, f|e) the parameter values are needed. Additionally, to obtain the parameter values, the system determines P(a|e, f), but to do that P(a, f|e) is again needed. Thus, a technique is used to solve for both parameter values and alignment probabilities substantially simultaneously.</p>
<p id="p-0043" num="0042">An expectation-maximization (&#x201c;EM&#x201d;) algorithm can be used to estimate parameter values and alignment probabilities using an iterative process until local optimum values are determined. An EM algorithm calculates maximum likelihood estimates of variables in probabilistic models. An EM algorithm is a two-step process. An expectation step calculates an expectation of the likelihood by including the variable values as if they were observed. A maximization step calculates the maximum likelihood estimates by maximizing the expected likelihood calculated in the expectation step. The process iterates between expectation and maximization steps where the values for the variable calculated in the maximization step are used for a next expectation step. The term &#x201c;EM algorithm&#x201d; refers to a class of related algorithms: the expectation and maximization steps provide a roadmap for developing specific EM algorithms. In some implementations, other techniques are used to find maximum likelihood estimates other than the EM algorithm, for example, gradient descent or conjugate gradient techniques.</p>
<p id="p-0044" num="0043">Using a technique such as an EM algorithm, the translation model <b>108</b> is trained to determine a most likely parameter values and alignment probability for a given source and target language.</p>
<p id="p-0045" num="0044">The decoder <b>110</b> applies the language model <b>104</b> and the translation model <b>108</b> to a given string (e.g., from input text <b>112</b>) in order to produce a translated string (e.g., as translated text <b>114</b>). In particular, the decoder <b>110</b> translates an observed sentence, f, (e.g., a French sentence) by seeking the sentence, e, (e.g., an English sentence) that maximizes the product of P(e) determined by the language model <b>104</b> and P(f|e) determined by the translation model <b>108</b>.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an example of direct alignment <b>200</b> between a source sentence <b>201</b> and a translated sentence <b>202</b>. The source sentence <b>201</b> includes a sequence of four words. Similarly, the translated sentence <b>202</b> includes a sequence of four words. The number of words in the translated sentence <b>202</b> is equal to the number of words in the source sentence <b>201</b> for simplicity. However, the translated sentence <b>202</b> could have a greater or a fewer number of words. For example, a particular word in the source sentence could correspond to multiple words in the translated sentence. Alternatively, a particular word in the source sentence can correspond to no word in the translated sentence.</p>
<p id="p-0047" num="0046">The alignment <b>203</b> graphically illustrates word-to-word alignments between the source sentence <b>201</b> and the translated sentence <b>202</b>. For example, word S<b>1</b> of the source sentence <b>201</b> aligns directly to word T<b>1</b> of the translated sentence <b>202</b>. However, other words of the source sentence <b>201</b> align to translated words in different sentence positions. For example, word S<b>2</b> in the source sentence <b>201</b> aligns with word T<b>3</b> in position three of the translated sentence <b>202</b>. Thus, the alignment illustrates that a translated sentence can have a different word ordering than the source sentence.</p>
<p id="p-0048" num="0047">Alignments can be determined according to a particular alignment model. The alignment model introduces a hidden alignment variable, a=a<sub>l</sub><sup>J</sup>. The alignment is a vector that specifies a mapping between source and target words within a string. An alignment term of a<sub>j</sub>=i indicates that the jth source word is linked to the ith target word. The alignment model assigns a probability, P(f, a|e), to the source sentence (e.g., in French) and alignment conditioned on the target sentence (e.g., in English). Given a French-English sentence-pair (f, e), a most likely word alignment is represented by:</p>
<p id="p-0049" num="0048">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mrow>
  <mover>
    <mi>a</mi>
    <mo>^</mo>
  </mover>
  <mo>=</mo>
  <mrow>
    <munder>
      <mi>argmax</mi>
      <mi>a</mi>
    </munder>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.3em" height="0.3ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <mrow>
        <mi>P</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mi>f</mi>
            <mo>,</mo>
            <mrow>
              <mi>a</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mtext>|</mtext>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>e</mi>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
      <mo>.</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0050" num="0049">Where the ^ notation indicates an alignment that is the most likely alignment for the sentence pair. In some implementations, an alternative technique, referred to as Maximum A-Posteriori (&#x201c;MAP&#x201d;) estimation is used to find the most likely word alignment. MAP provides a statistical technique for estimating an unobserved quantity (e.g., alignment) given empirical data (e.g., source and target language pairs) as posterior probability matrices.</p>
<p id="p-0051" num="0050">For a given alignment model, posterior probabilities can be computed as:</p>
<p id="p-0052" num="0051">
<maths id="MATH-US-00010" num="00010">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>P</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mrow>
            <msub>
              <mi>a</mi>
              <mi>j</mi>
            </msub>
            <mo>=</mo>
            <mrow>
              <mi>i</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mtext>|</mtext>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>e</mi>
            </mrow>
          </mrow>
          <mo>,</mo>
          <mi>f</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <munder>
        <mo>&#x2211;</mo>
        <mi>a</mi>
      </munder>
      <mo>&#x2062;</mo>
      <mrow>
        <mrow>
          <mi>P</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mrow>
                <mi>a</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mtext>|</mtext>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>f</mi>
              </mrow>
              <mo>,</mo>
              <mi>e</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>&#x2062;</mo>
        <mrow>
          <mi>&#x3b4;</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>i</mi>
              <mo>,</mo>
              <msub>
                <mi>a</mi>
                <mi>j</mi>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<br/>
where i=&#x3b5;{0, 1, . . . , I} and I is the length of the target sentence of the sentence pair. Additionally, &#x3b5; is the Dirac-delta function where &#x3b4;(a, b)=1 if a=b and &#x3b4; (a, b)=0 otherwise. The posterior probabilities provide the conditional probability of a particular alignment assigned according to the source and target language data. The assignment a<sub>j</sub>=0 corresponds to a NULL (i.e., empty) alignment. This means that the source words do not align to any target words. The notation (a<sub>j</sub>=i|e, f) indicates that a<sub>j </sub>can take on multiple values. The probability (a<sub>j</sub>=i|e, f) identifies the probability that a<sub>j </sub>takes on some particular value i, where i can be varied depending on e, f. The posterior probabilities form a matrix of size (I+1)&#xd7;J, where entries along each column sum to one.
</p>
<p id="p-0053" num="0052">The MAP alignment for each source position, j&#x3b5;{1, 2, . . . , J}, is calculated according to the most likely posterior probability as:</p>
<p id="p-0054" num="0053">
<maths id="MATH-US-00011" num="00011">
<math overflow="scroll">
<mrow>
  <mrow>
    <msub>
      <mi>a</mi>
      <mi>MAP</mi>
    </msub>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mi>j</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <munder>
      <mi>argmax</mi>
      <mi>i</mi>
    </munder>
    <mo>&#x2062;</mo>
    <mstyle>
      <mspace width="0.3em" height="0.3ex"/>
    </mstyle>
    <mo>&#x2062;</mo>
    <mrow>
      <mrow>
        <mi>P</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow>
              <msub>
                <mi>a</mi>
                <mi>j</mi>
              </msub>
              <mo>=</mo>
              <mrow>
                <mi>i</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mtext>|</mtext>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>e</mi>
              </mrow>
            </mrow>
            <mo>,</mo>
            <mi>f</mi>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
      <mo>.</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0055" num="0054">An example of MAP estimation is described in E. Matusov, R. Zens, and H. Ney. &#x201c;Symmetric Word Alignments for Statistical Machine Translation,&#x201d; <i>COLING </i>2004, Geneva, Switzerland, which is incorporated by reference.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 3</figref> is a flow chart illustrating an example method <b>300</b> for providing translations using bridge languages. For convenience the method <b>300</b> will be described with reference to a system that performs the method <b>300</b>.</p>
<p id="p-0057" num="0056">The system identifies <b>302</b> a source and a target language. For example, when translating from French to English, the source language is French and the target language is English. The system determines <b>304</b> the direct alignment between sentence pairs of the source and target languages. Thus, for a translation from French to English, a particular probability for the alignment a<sup>FE </sup>is determined. The alignment can be determined directly as described above. Alternatively, the system can receive a particular alignment that was separately determined and supplied, e.g., to the translation system <b>100</b>.</p>
<p id="p-0058" num="0057">The system determines <b>306</b> an alignment between the source and target languages using one or more bridge languages. If a sentence includes corresponding translations in the source, target, and at least one other language, the system determines an alignment for the source and target languages as a function of the bridge languages.</p>
<p id="p-0059" num="0058">For example, for a source language F (French), a target language E (English) and a bridge language G (German), the corresponding sentences can be represented as f=f<sub>l</sub><sup>J</sup>, e=e<sub>l</sub><sup>J</sup>, and g=e<sub>l</sub><sup>K</sup>. A posterior probability estimate for a sentence-pair in FE: (f, e) is obtained using the posterior probability estimates for sentence pairs in FG: (f, g) and GE: (g, e). The word alignments for each sentence-pair are represented as a<sup>FE</sup>, a<sup>FG </sup>and a<sup>GE </sup>respectively. The notation a<sup>FE</sup>, for example, indicates that the alignment maps a position in F to a position in E. In other words, the alignment maps a particular word in the source language F to a position in a target language translation E. The posterior probability estimate for FE is expressed as a sum over all possible translations g in G and hidden alignments FG as follows:</p>
<p id="p-0060" num="0059">
<maths id="MATH-US-00012" num="00012">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>P</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mrow>
          <msubsup>
            <mi>a</mi>
            <mi>j</mi>
            <mi>FE</mi>
          </msubsup>
          <mo>=</mo>
          <mrow>
            <mi>i</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mtext>|</mtext>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>e</mi>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mi>f</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <munder>
      <mo>&#x2211;</mo>
      <mrow>
        <mi>g</mi>
        <mo>,</mo>
        <mi>k</mi>
      </mrow>
    </munder>
    <mo>&#x2062;</mo>
    <mrow>
      <mrow>
        <mo>{</mo>
        <mrow>
          <mrow>
            <mi>P</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>g</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mtext>|</mtext>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mi>e</mi>
                </mrow>
                <mo>,</mo>
                <mi>f</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>&#x2062;</mo>
          <mrow>
            <mi>P</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <msubsup>
                    <mi>a</mi>
                    <mi>j</mi>
                    <mi>FG</mi>
                  </msubsup>
                  <mo>=</mo>
                  <mrow>
                    <mi>k</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mtext>|</mtext>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>g</mi>
                  </mrow>
                </mrow>
                <mo>,</mo>
                <mi>e</mi>
                <mo>,</mo>
                <mi>f</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>&#x2062;</mo>
          <mrow>
            <mi>P</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <msubsup>
                    <mi>a</mi>
                    <mi>j</mi>
                    <mi>FE</mi>
                  </msubsup>
                  <mo>=</mo>
                  <mrow>
                    <mrow>
                      <mi>i</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mtext>|</mtext>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <msubsup>
                        <mi>a</mi>
                        <mi>j</mi>
                        <mi>FG</mi>
                      </msubsup>
                    </mrow>
                    <mo>=</mo>
                    <mi>k</mi>
                  </mrow>
                </mrow>
                <mo>,</mo>
                <mi>g</mi>
                <mo>,</mo>
                <mi>e</mi>
                <mo>,</mo>
                <mi>f</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>}</mo>
      </mrow>
      <mo>.</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0061" num="0060">In some implementations, the above expression is simplified according to particular assumptions. For example, it can be assumed that there is exactly one translation g in the bridge language G that corresponds to the sentence pair (f, e). Additionally, it can also be assumed that the alignments in FG do not depend on E. Given these assumptions, the posterior probability FE can be expressed in terms of posterior probabilities for GF and EG as:</p>
<p id="p-0062" num="0061">
<maths id="MATH-US-00013" num="00013">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>P</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mrow>
          <msubsup>
            <mi>a</mi>
            <mi>j</mi>
            <mi>FE</mi>
          </msubsup>
          <mo>=</mo>
          <mrow>
            <mi>i</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mtext>|</mtext>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>e</mi>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mi>f</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <munderover>
      <mo>&#x2211;</mo>
      <mrow>
        <mi>k</mi>
        <mo>=</mo>
        <mn>0</mn>
      </mrow>
      <mi>K</mi>
    </munderover>
    <mo>&#x2062;</mo>
    <mrow>
      <mi>P</mi>
      <mo>&#x2062;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mrow>
            <msubsup>
              <mi>a</mi>
              <mi>j</mi>
              <mi>FE</mi>
            </msubsup>
            <mo>=</mo>
            <mrow>
              <mi>k</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mtext>|</mtext>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>g</mi>
            </mrow>
          </mrow>
          <mo>,</mo>
          <mi>f</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
      <mo>&#x2062;</mo>
      <mi>P</mi>
      <mo>&#x2062;</mo>
      <mrow>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow>
              <msubsup>
                <mi>a</mi>
                <mi>j</mi>
                <mi>FE</mi>
              </msubsup>
              <mo>=</mo>
              <mrow>
                <mi>i</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mtext>|</mtext>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>g</mi>
              </mrow>
            </mrow>
            <mo>,</mo>
            <mi>e</mi>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>.</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0063" num="0062">As a result of the above expression, the posterior probability matrix for FE can be obtained using a simple matrix multiplication of posterior probability matrices for GE and FG. The posterior probability for a<sup>GE </sup>forms an I&#xd7;K matrix and the posterior probability for a<sup>FG </sup>forms a K&#xd7;J matrix, which, when multiplied, forms an I&#xd7;J matrix for a<sup>FE</sup>. In some implementations, a column is prepended to the GE matrix corresponding to the case where k=0. This probability, P(a<sub>k</sub><sup>GE</sup>=1) when k=0 is not assigned by the alignment model and is used to account for the probability of an empty word alignment. The number of empty word alignments between a given source and target language can vary depending on the bridge language. Consequently, the empty word alignment can be controlled by specifying a value for a parameter &#x3b5; when i=0 and (1&#x2212;&#x3b5;)/i when i&#x3b5;{1, 2, . . . , I}. The parameter &#x3b5; controls a number of empty alignments. A higher value allows more empty alignments. The value of c can be specified, e.g., to have a value of 0.5.</p>
<p id="p-0064" num="0063">The resulting posterior probability can be maximized (e.g., using a<sub>MAP</sub>) in order to determine the most likely alignment probability.</p>
<p id="p-0065" num="0064">The alignment for the source and target language as a posterior probability matrix FE can be obtained as a function of a number of different bridge languages in a similar manner as described above for the German bridge language G. For example, for a French source language and an English target language, bridge languages of German, Spanish, Russian, and Chinese can each be used such that a particular alignment from French to English can be determined as a function of each particular bridge language.</p>
<p id="p-0066" num="0065">The system optionally determines <b>308</b> a combined alignment between the source and target languages using more than one bridge language. The word alignment posterior probabilities described above can be used to generate a combined word alignment using multiple bridge languages.</p>
<p id="p-0067" num="0066">Specifically, if translations exist in N bridge languages G<sub>1</sub>, G<sub>2</sub>, . . . , G<sub>N</sub>, a posterior probability matrix for FE can be generated using each of the bridge languages. Additionally, the posterior probability matrix for FE can be generated directly without using a bridge language. The N+1 posterior matrices can be combined as:</p>
<p id="p-0068" num="0067">
<maths id="MATH-US-00014" num="00014">
<math overflow="scroll">
<mrow>
  <mrow>
    <mrow>
      <mi>P</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mrow>
            <msubsup>
              <mi>a</mi>
              <mi>j</mi>
              <mi>FE</mi>
            </msubsup>
            <mo>=</mo>
            <mrow>
              <mi>i</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mtext>|</mtext>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>e</mi>
            </mrow>
          </mrow>
          <mo>,</mo>
          <mi>f</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <munderover>
        <mo>&#x2211;</mo>
        <mrow>
          <mi>l</mi>
          <mo>=</mo>
          <mn>0</mn>
        </mrow>
        <mi>N</mi>
      </munderover>
      <mo>&#x2062;</mo>
      <mrow>
        <mrow>
          <mi>P</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>B</mi>
              <mo>=</mo>
              <msub>
                <mi>G</mi>
                <mi>l</mi>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>&#x2062;</mo>
        <mi>P</mi>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow>
              <msubsup>
                <mi>a</mi>
                <mi>j</mi>
                <mi>FE</mi>
              </msubsup>
              <mo>=</mo>
              <mrow>
                <mi>i</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mtext>|</mtext>
                </mstyle>
                <mo>&#x2062;</mo>
                <msub>
                  <mi>G</mi>
                  <mi>l</mi>
                </msub>
              </mrow>
            </mrow>
            <mo>,</mo>
            <mi>e</mi>
            <mo>,</mo>
            <mi>f</mi>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<br/>
where the variable B indicates the bridge language, B &#x3b5;{G<sub>0</sub>, G<sub>1</sub>, . . . , G<sub>N</sub>), with G<sub>0 </sub>representing a case where no bridge language is used and G<sub>1</sub>, . . . , G<sub>N </sub>representing particular languages e.g., German, Spanish, Chinese. Additionally, P(a<sub>j</sub><sup>FE</sup>=i|G<sub>l</sub>,e,f) is the posterior probability where the bridge language B=G<sub>I</sub>. The probabilities P(B=G<sub>l</sub>) sum to one over l &#x3b5;{0, 1, 2, . . . , N} and represent the prior probability of bridge language l. In some implementations, a uniform prior probability is used. For example, P(B=G<sub>l</sub>)=1/(N+1). In some implementations, different bridge languages can be associated with particular weights, which are indicative of the quality of the alignment produced given one bridge language over another.
</p>
<p id="p-0069" num="0068">Using the above formula for combining posterior probabilities, the system interpolates the posterior probability matrices and then calculates the corresponding MAP word alignment.</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram illustrating bridge alignments <b>400</b> between a source sentence and a translated sentence. In <figref idref="DRAWINGS">FIG. 4</figref>, alignments between a French source language <b>402</b> and an English target language <b>404</b> are shown. A direct alignment a<sup>FE </sup>provides the word alignment directly from French to English. Additionally, bridge languages are used to form alignments a<sup>FE </sup>as a function of the corresponding bridge languages. In particular, <figref idref="DRAWINGS">FIG. 4</figref> shows bridge languages Russian <b>406</b>, Spanish <b>408</b>, German <b>410</b>, and Chinese <b>412</b>.</p>
<p id="p-0071" num="0070">As shown in <figref idref="DRAWINGS">FIG. 3</figref>, the system uses <b>310</b> each alignment to determine candidate translations. Thus, for an input sentence in a source language, a number of candidate translated sentences in the target language can be generated according to each alignment. For example, a first candidate can be generated using the direct alignment (e.g., for a given input string a candidate string can be generated based using the translation model according to the direct alignment and language model). One or more additional candidates can be generated using alignments derived from a particular bridge language. Additionally, in some implementations, a combined alignment is generated and also used to generate a candidate translation.</p>
<p id="p-0072" num="0071">The system determines <b>312</b> a consensus translation using the candidate translations. The various candidate translations are assessed to determine a consensus translation. For example, given a set of translations, the system selects the translation that has the highest consensus (under BLEU score) with respect to the candidate translations. Consequently, a single translation is produced from the various candidates, which represents a best translation of the source sentence into the target sentence. An example technique for selecting a consensus translation from candidate translations is described in W. Macherey and F. Och &#x201c;An Empirical Study on Computing Consensus Translations From Multiple Machine Translation Systems,&#x201d; <i>EMNLP</i>, Prague, Czech Republic, 2007, which is incorporated by reference. Additionally, a general technique for selecting from candidate hypothesis in speech recognition is described in J. G. Fiscus, &#x201c;A Post-Processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER),&#x201d; <i>Proceedings IEEE Workshop on Automatic Speech Recognition and Understanding</i>, pages 347-352, Santa Barbara, Calif. 1997.</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram illustrating an example of a translation system <b>500</b> using bridge languages. In <figref idref="DRAWINGS">FIG. 5</figref>, a French input sentence <b>502</b> is used by the translation system <b>500</b> to produce an English consensus output sentence <b>504</b>. The translation system <b>500</b> using a number of different French-English alignments to generate a number of different English output candidates <b>506</b><i>a</i>-<i>f</i>. For example, a<sup>FGE </sup>represents an alignment using a German bridge language and a<sup>FCE </sup>represents an alignment using a combined bridge alignments. Each of the English output candidates <b>506</b> are input into a consensus engine <b>508</b>. The consensus engine selects a best output sentence as the English consensus output sentence <b>504</b>.</p>
<p id="p-0074" num="0073">In some implementations, training can identify particular bridge languages that provide better alignment results than other bridge languages. For example, when translating from an Arabic source language to an English target, alignments can be tested and it can be determined that Spanish provides the best alignment for Arabic-English while Chinese provides the worst alignment results. Alignment quality can be measured, for example, in terms of precision, recall, and alignment error rate.</p>
<p id="p-0075" num="0074">Translation quality can be assessed for the bridge languages, for example, by calculating Bilingual Evaluation Understudy (&#x201c;BLEU&#x201d;) scores for translation results using each bridge alignment relative to a direct alignment between a source and a target language. For example, the resulting translation can be compared with a known translation to derive a BLEU score representing the quality of the translation. BLEU scores are described, for example, in Papineni, K., Roukos, S., Ward, T., and Zhu, W. J. (2002). &#x201c;BLEU: A Method for Automatic Evaluation of Machine Translation&#x201d; in <i>ACL</i>-2002: 40<i>th Annual meeting of the Association for Computational Linguistics </i>pp. 311-318, which is incorporated by reference. In some implementations, particular bridge languages or combinations of bridge languages can be identified as providing improved translation quality for a given source and target language over the translation quality of a direct alignment.</p>
<p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. 6</figref> is an example system <b>600</b> for providing translations. A data processing apparatus <b>610</b> can include hardware/firmware, an operating system and one or more programs, including translation program <b>620</b>. The translation program <b>620</b> operates, in conjunction with the data processing apparatus <b>610</b>, to effect the operations described in this specification. Thus, the translation program <b>620</b>, in combination with one or more processors and computer-readable media (e.g., memory), represents one or more structural components in the system <b>600</b>.</p>
<p id="p-0077" num="0076">The translation program <b>620</b> can be a translation processing application, or a portion. As used here, an application is a computer program that the user perceives as a distinct computer tool used for a defined purpose. An application can be built entirely into the operating system (OS) of the data processing apparatus <b>610</b>, or an application can have different components located in different locations (e.g., one portion in the OS or kernel mode, one portion in the user mode, and one portion in a remote server), and an application can be built on a runtime library serving as a software platform of the apparatus <b>610</b>. Moreover, application processing can be distributed over a network <b>680</b> using one or more processors <b>690</b>.</p>
<p id="p-0078" num="0077">The data processing apparatus <b>610</b> includes one or more processors <b>630</b> and at least one computer-readable medium <b>640</b> (e.g., random access memory, storage device, etc.). The data processing apparatus <b>610</b> can also include a communication interface <b>650</b>, one or more user interface devices <b>660</b>, and one or more additional devices <b>670</b>. The user interface devices <b>660</b> can include display screens, keyboards, mouse, stylus, or any combination thereof.</p>
<p id="p-0079" num="0078">Once programmed, the data processing apparatus <b>610</b> is operable to identify bridge alignments between a source and a target language using one or more bridge languages. Additionally, one or more of the bridge alignments can be used to perform a translation between the source and the target languages.</p>
<p id="p-0080" num="0079">Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a tangible program carrier for execution by, or to control the operation of, data processing apparatus. The tangible program carrier can be a propagated signal or a computer-readable medium. The propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a computer. The computer-readable medium can be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them.</p>
<p id="p-0081" num="0080">The term &#x201c;data processing apparatus&#x201d; encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.</p>
<p id="p-0082" num="0081">A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.</p>
<p id="p-0083" num="0082">The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).</p>
<p id="p-0084" num="0083">Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, to name just a few.</p>
<p id="p-0085" num="0084">Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.</p>
<p id="p-0086" num="0085">To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.</p>
<p id="p-0087" num="0086">Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described is this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (&#x201c;LAN&#x201d;) and a wide area network (&#x201c;WAN&#x201d;), e.g., the Internet.</p>
<p id="p-0088" num="0087">The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p>
<p id="p-0089" num="0088">While this specification contains many specifics, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.</p>
<p id="p-0090" num="0089">Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.</p>
<p id="p-0091" num="0090">Particular embodiments of the subject matter described in this specification have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In certain implementations, multitasking and parallel processing may be advantageous.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08626488-20140107-M00001.NB">
<img id="EMI-M00001" he="5.67mm" wi="76.20mm" file="US08626488-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08626488-20140107-M00002.NB">
<img id="EMI-M00002" he="7.45mm" wi="76.20mm" file="US08626488-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08626488-20140107-M00003.NB">
<img id="EMI-M00003" he="5.67mm" wi="76.20mm" file="US08626488-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08626488-20140107-M00004.NB">
<img id="EMI-M00004" he="8.13mm" wi="76.20mm" file="US08626488-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08626488-20140107-M00005.NB">
<img id="EMI-M00005" he="6.69mm" wi="76.20mm" file="US08626488-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08626488-20140107-M00006.NB">
<img id="EMI-M00006" he="8.81mm" wi="76.20mm" file="US08626488-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08626488-20140107-M00007.NB">
<img id="EMI-M00007" he="7.79mm" wi="76.20mm" file="US08626488-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US08626488-20140107-M00008.NB">
<img id="EMI-M00008" he="7.03mm" wi="76.20mm" file="US08626488-20140107-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US08626488-20140107-M00009.NB">
<img id="EMI-M00009" he="5.67mm" wi="76.20mm" file="US08626488-20140107-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00010" nb-file="US08626488-20140107-M00010.NB">
<img id="EMI-M00010" he="7.03mm" wi="76.20mm" file="US08626488-20140107-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00011" nb-file="US08626488-20140107-M00011.NB">
<img id="EMI-M00011" he="5.67mm" wi="76.20mm" file="US08626488-20140107-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00012" nb-file="US08626488-20140107-M00012.NB">
<img id="EMI-M00012" he="12.02mm" wi="76.20mm" file="US08626488-20140107-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00013" nb-file="US08626488-20140107-M00013.NB">
<img id="EMI-M00013" he="8.81mm" wi="76.20mm" file="US08626488-20140107-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00014" nb-file="US08626488-20140107-M00014.NB">
<img id="EMI-M00014" he="8.81mm" wi="76.20mm" file="US08626488-20140107-M00014.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method comprising:
<claim-text>receiving multi-lingual parallel text associating a source language, a target language, and one or more bridge languages;</claim-text>
<claim-text>determining a direct alignment for the parallel text between the source language and the target language, the direct alignment representing a connection between words in the source language and the target language in the parallel text;</claim-text>
<claim-text>determining, using one or more computing devices, a first bridge-based alignment for the parallel text between the source language and the target language using a first bridge language that is distinct from the source language and the target language, wherein the first bridge-based alignment is determined based in part on a first probability for a first alignment for the parallel text from the source language to the first bridge language and a second probability for a second alignment for the parallel text from the first bridge language to the target language;</claim-text>
<claim-text>using the direct alignment for the parallel text between the source language and the target language to generate a first candidate translation of an input text from the source language to the target language;</claim-text>
<claim-text>using the first bridge-based alignment to generate a second candidate translation of the input text from the source language to the target language; and</claim-text>
<claim-text>determining a consensus translation based on the first and second candidate translations.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the first bridge-based alignment using the first bridge language includes:
<claim-text>determining the first alignment for the parallel text between the source language and the first bridge language;</claim-text>
<claim-text>determining the second alignment for the parallel text between the first bridge language and the target language; and</claim-text>
<claim-text>determining the first bridge-based alignment for the parallel text between the source language and the target language using the first and second alignments.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>determining a plurality of bridge-based alignments for the parallel text between the source language and the target language using a plurality of respective bridge languages, the first bridge-based alignment being one of the plurality of bridge-based alignments; and</claim-text>
<claim-text>using each of the determined plurality of bridge-based alignments to generate respective candidate translations of the input text, the second candidate translation being one of the respective candidate translations.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the connection between words in the source language and the target language in the parallel text includes a connection between each specific source word in the source language and at least one of: (i) a specific target word in the target language, (ii) a plurality of target words in the target language, and (iii) no word in the target language.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>determining a combined alignment using two or more bridge-based alignments using two or more bridge languages; and</claim-text>
<claim-text>using the combined alignment to generate a third candidate translation of the input text.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein determining the combined alignment includes:
<claim-text>determining a specific alignment for each of the two or more bridge languages;</claim-text>
<claim-text>associating each specific alignment with a weight based on its respective bridge language, the source language and the target language; and</claim-text>
<claim-text>combining the specific alignments based on the weights to obtain the combined alignment.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein each of the weights is indicative of a quality of the specific alignment between the source language and the target language for its respective bridge language.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A method comprising:
<claim-text>receiving, at one or computing devices, a direct alignment between a source language and a target language based on multi-lingual parallel text, the multi-lingual parallel text associating the source language, the target language, and one or more bridge languages, the direct alignment representing a connection between words in the source language and the target language in the parallel text;</claim-text>
<claim-text>receiving, at the one or more computing devices, a first bridge-based alignment for the parallel text between the source language and the target language using a first bridge language that is distinct from the source language and the target language, wherein the first bridge-based alignment is based in part on a first probability for a first alignment for the parallel text from the source language to the first bridge language and a second probability for a second alignment for the parallel text from the first bridge language to the target language;</claim-text>
<claim-text>receiving, at the one or more computing devices, an input text in the source language to be translated to the target language;</claim-text>
<claim-text>using the direct alignment for the parallel text between the source language and the target language to generate a first candidate translation of the input text from the source language to the target language;</claim-text>
<claim-text>using the first bridge-based alignment to generate a second candidate translation of the input text from the source language to the target language; and</claim-text>
<claim-text>determining, at the one or more computing devices, a consensus translation based on the first and second candidate translations.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:
<claim-text>receiving a plurality of bridge-based alignments for the parallel text between the source language and the target language based on a plurality of respective bridge languages, the first bridge-based alignment being one of the plurality of bridge-based alignments; and</claim-text>
<claim-text>using each of the plurality of bridge-based alignments to generate respective candidate translations of the input text, the second candidate translation being one of the respective candidate translations.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the connection between words in the source language and the target language in the parallel text includes a connection between each specific source word in the source language and at least one of: (i) a specific target word in the target language, (ii) a plurality of target words in the target language, and (iii) no word in the target language.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:
<claim-text>determining a combined alignment using two or more bridge-based alignments based on two or more bridge languages; and</claim-text>
<claim-text>using the combined alignment to generate a third candidate translation of the input text.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein determining the combined alignment includes:
<claim-text>Receiving a specific alignment for each of the two or more bridge languages;</claim-text>
<claim-text>associating each specific alignment with a weight based on its respective bridge language, the source language and the target language; and</claim-text>
<claim-text>combining the specific alignments based on the weights to obtain the combined alignment.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein each of the weights is indicative of a quality of the specific alignment between the source language and the target language for its respective bridge language.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A system comprising:
<claim-text>one or more computing devices operable to perform operations including:</claim-text>
<claim-text>receiving multi-lingual parallel text associating a source language, a target language, and one or more bridge languages;</claim-text>
<claim-text>determining a direct alignment for the parallel text between the source language and the target language, the direct alignment representing a connection between words in the source language and the target language in the parallel text;</claim-text>
<claim-text>determining a first bridge-based alignment for the parallel text between the source language and the target language using a first bridge language that is distinct from the source language and the target language, wherein the first bridge-based alignment is determined based in part on a first probability for a first alignment for the parallel text from the source language to the first bridge language and a second probability for a second alignment for the parallel text from the first bridge language to the target language;</claim-text>
<claim-text>using the direct alignment for the parallel text between the source language and the target language to generate a first candidate translation of an input text from the source language to the target language;</claim-text>
<claim-text>using the first bridge-based alignment to generate a second candidate translation of the input text from the source language to the target language; and</claim-text>
<claim-text>determining a consensus translation based on the first and second candidate translations.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein determining the first bridge-based alignment using the first bridge language includes:
<claim-text>determining the first alignment for the parallel text between the source language and the first bridge language;</claim-text>
<claim-text>determining the second alignment for the parallel text between the first bridge language and the target language; and</claim-text>
<claim-text>determining the first bridge-based alignment for the parallel text between the source language and the target language using the first and second alignments.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further operable to perform operations comprising:
<claim-text>determining a plurality of bridge-based alignments for the parallel text between the source language and the target language using a plurality of respective bridge languages, the first bridge-based alignment being one of the plurality of bridge-based alignments; and</claim-text>
<claim-text>using each of the determined plurality of bridge-based alignments to generate respective candidate translations of the input text, the second candidate translation being one of the respective candidate translations.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the connection between words in the source language and the target language in the parallel text includes a connection between each specific source word in the source language and at least one of: (i) a specific target word in the target language, (ii) a plurality of target words in the target language, and (iii) no word in the target language.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further operable to perform operations comprising:
<claim-text>determining a combined alignment using two or more bridge-based alignments using two or more bridge languages; and</claim-text>
<claim-text>using the combined alignment to generate a third candidate translation of the input text.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein determining the combined alignment includes:
<claim-text>determining a specific alignment for each of the two or more bridge languages;</claim-text>
<claim-text>associating each specific alignment with a weight based on its respective bridge language, the source language and the target language; and</claim-text>
<claim-text>combining the specific alignments based on the weights to obtain the combined alignment.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein each of the weights is indicative of a quality of the specific alignment between the source language and the target language for its respective bridge language.</claim-text>
</claim>
</claims>
</us-patent-grant>
