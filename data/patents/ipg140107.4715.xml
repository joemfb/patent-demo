<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625808-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625808</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11865632</doc-number>
<date>20071001</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<us-term-of-grant>
<us-term-extension>1425</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>R</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>381  1</main-classification>
<further-classification>381 22</further-classification>
</classification-national>
<invention-title id="d2e53">Methods and apparatuses for encoding and decoding object-based audio signals</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>3882280</doc-number>
<kind>A</kind>
<name>Goutmann</name>
<date>19750500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5583962</doc-number>
<kind>A</kind>
<name>Davis et al.</name>
<date>19961200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6849794</doc-number>
<kind>B1</kind>
<name>Lau et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7006636</doc-number>
<kind>B2</kind>
<name>Baumgarte et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7116787</doc-number>
<kind>B2</kind>
<name>Faller</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>381 17</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2003/0026441</doc-number>
<kind>A1</kind>
<name>Faller</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2003/0167173</doc-number>
<kind>A1</kind>
<name>Levy et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2003/0187663</doc-number>
<kind>A1</kind>
<name>Truman et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2003/0236583</doc-number>
<kind>A1</kind>
<name>Baumgarte et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2005/0120870</doc-number>
<kind>A1</kind>
<name>Ludwig</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2005/0157883</doc-number>
<kind>A1</kind>
<name>Herre et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2005/0180579</doc-number>
<kind>A1</kind>
<name>Baumgarte et al.</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2006/0016735</doc-number>
<kind>A1</kind>
<name>Ito et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2006/0085200</doc-number>
<kind>A1</kind>
<name>Allamanche et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2007/0236858</doc-number>
<kind>A1</kind>
<name>Disch et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2007/0291951</doc-number>
<kind>A1</kind>
<name>Faller</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2008/0130904</doc-number>
<kind>A1</kind>
<name>Faller</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2008/0167880</doc-number>
<kind>A1</kind>
<name>Seo et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2009/0028360</doc-number>
<kind>A1</kind>
<name>Griesinger</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2009/0043591</doc-number>
<kind>A1</kind>
<name>Breebaart et al.</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2009/0067634</doc-number>
<kind>A1</kind>
<name>Oh et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2009/0129601</doc-number>
<kind>A1</kind>
<name>Ojala et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>CA</country>
<doc-number>2 597 746</doc-number>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>CN</country>
<doc-number>1503572</doc-number>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>CN</country>
<doc-number>1783728</doc-number>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>EP</country>
<doc-number>0 857 375</doc-number>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>EP</country>
<doc-number>1 278 184</doc-number>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>EP</country>
<doc-number>1691348</doc-number>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>EP</country>
<doc-number>2 038 878</doc-number>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>IT</country>
<doc-number>TO950869</doc-number>
<date>19970400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>JP</country>
<doc-number>2000-156038</doc-number>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>JP</country>
<doc-number>2001-028800</doc-number>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>JP</country>
<doc-number>2003-186500</doc-number>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>JP</country>
<doc-number>2004-064363</doc-number>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>JP</country>
<doc-number>2006-517356</doc-number>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>JP</country>
<doc-number>2008-522244</doc-number>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>JP</country>
<doc-number>2008-537833</doc-number>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>JP</country>
<doc-number>2009-518725</doc-number>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>JP</country>
<doc-number>2009-527954</doc-number>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>RU</country>
<doc-number>2121718</doc-number>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>RU</country>
<doc-number>2002126217</doc-number>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>RU</country>
<doc-number>2004133032</doc-number>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>RU</country>
<doc-number>2005104123</doc-number>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>RU</country>
<doc-number>2005135648</doc-number>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>WO</country>
<doc-number>97/15983</doc-number>
<date>19970500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>WO</country>
<doc-number>03/090208</doc-number>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>WO</country>
<doc-number>2003/090208</doc-number>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>WO</country>
<doc-number>2005/101370</doc-number>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>WO</country>
<doc-number>WO 2006-003891</doc-number>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>WO</country>
<doc-number>2006/016735</doc-number>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>WO</country>
<doc-number>2006/048203</doc-number>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>WO</country>
<doc-number>2006/060279</doc-number>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>WO</country>
<doc-number>2006/089685</doc-number>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>WO</country>
<doc-number>WO 2006-089570</doc-number>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>WO</country>
<doc-number>2007/004828</doc-number>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>WO</country>
<doc-number>2007/004830</doc-number>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>WO</country>
<doc-number>2007/089131</doc-number>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00058">
<othercit>Office Action, Canadian Appln. No. 2 645 909, dated Dec. 29, 2010, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00059">
<othercit>&#x201c;Call for Proposals on Spatial Audio Object Coding.&#x201d; ITU Study Group 16&#x2014;Video Coding Experts Group&#x2014;ISO/IEC MPEG &#x26; ITU-T VGEG(ISO/IEC JTC1/SC29/WG11 &#x26; ITU-T SG16 Q6) No. N8853, Feb. 19, 2007, 18 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00060">
<othercit>&#x201c;Draft Call for Proposals on Spatial Audio Object Coding,&#x201d; ITU Study Group 16&#x2014;Video Coding Experts Group&#x2014;ISO/IEC MPEG &#x26; ITU-T VGEG(ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6) No. N8639, Oct. 27, 2006, 16 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00061">
<othercit>Summons to Attend Oral Proceedings, European Appln. No. 07833115.4, dated Apr. 6, 2011, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00062">
<othercit>&#x201c;Concepts of Object-Oriented Spatial Audio Coding&#x201d;, (Jul. 21, 2006), 8 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00063">
<othercit>Supp. European Search Report for Application No. EP 07 83 3115, dated Jul. 24, 2009, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00064">
<othercit>Supp. European Search Report for Application No. EP 07 83 3116, dated Jul. 28, 2009, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00065">
<othercit>Faller, C. and Baumgarte, F., (2003) Binaural Cue Coding&#x2014;Part II: Schemes and Applications, IEEE Transactions on Speech and Audio Processing, 11(6):520-531.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00066">
<othercit>Herre, J. and Disch, S., (2007) &#x201c;New Concepts in Parametric Coding of Spatial Audio: From Sac to Saoc&#x201d;, IEEE pp. 1894-1897.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00067">
<othercit>Villemoes et al., (2006) &#x201c;MPEG Surround: The Forthcoming ISO Standard for Spatial Audio Coding&#x201d;, Proceedings of the International AES Conference pp. 1-18.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00068">
<othercit>Office Action, U.S. Appl. No. 11/865,671, mailed Aug. 27, 2010, 16 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00069">
<othercit>Notice of Allowance, Russian Appln. No. 2009116275, mailed Aug. 5, 2010, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00070">
<othercit>Notice of Allowance, Russian Appln. No. 2009116279, mailed Aug. 5, 2010, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00071">
<othercit>Office Action, U.S. Appl. No. 11/865,663, dated Nov. 8, 2010, 5 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00072">
<othercit>Baumgarte, et al., &#x201c;Binaural Cue Coding&#x2014;Part I: Psychoacoustic Fundamentals and Design Principles&#x201d;, IEEE Transactions on Speech and Audio processing, vol. 11, No. 6, Nov. 2003, pp. 509-519.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00073">
<othercit>Office Action, U.S. Appl. No. 11/865,679, dated Oct. 27, 2010, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00074">
<othercit>Faller et al., &#x201c;Efficient Representation of Spatial Audio Using Parameterization&#x201d;, IEEE Workshop on the Applications of Signal Processing to Audio and Acoustics, Oct. 20-24, 2001, pp. W2001-1-W2001-4.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00075">
<othercit>Office Action, Japanese Appln. No. 2009-530280, dated Sep. 27, 2010, 10 pages with English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00076">
<othercit>Supplementary European Search Report, dated Oct. 19, 2009, corresponding to European Application No. EP 07834266.4, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00077">
<othercit>Herre J et al: &#x201c;The Reference Model Architecture, for Mpeg Spatial Audio Coding&#x201d; Audio Engineering Society Convention Paper, New York, NY, US May 28, 2005, pp. 1-13, XP009059973.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00078">
<othercit>Joint Video Team: &#x201c;Concepts of Object-Oriented Spatial Audio Coding&#x201d; Joint Video Team (JVT) of ISO/IEC MPEG &#x26; ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6), No. N8329, Jul. 21, 2006, XP030014821.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00079">
<othercit>Engdegard J et al: &#x201c;Spatial Audio Object Coding (SAOC)&#x2014;The Upcoming MPEG Standard on Parametric Object Based Audio Coding&#x201d; 124th AES Convention, Audio Engineering Society, Paper 7377, May 17, 2008 &#x2dc; May 20, 2008, pp. 1-15, XP002541458.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00080">
<othercit>Summons to Attend Oral Proceedings, European Appln. No. 07833112.1, dated May 30, 2011, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00081">
<othercit>Notice of Allowance, Russian Application No. 2009116256, mailed Jun. 16, 2010, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00082">
<othercit>Faller, &#x201c;Parametric Joint-Coding of Audio Sources,&#x201d; <i>Audio Engineering Society 120 Convention</i>, May 20-23, 2006, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00083">
<othercit>Scheirer et al., &#x201c;AudioBIFS: The MPEG-4 Standard for Effects Processing,&#x201d; <i>Workshop on Digital Audio Effects Processing </i>(<i>DAFX'98</i>), Nov. 1992, 9 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00084">
<othercit>Office Action from Korean Application No. 10-2008-7026605, dated Jul. 30, 2010, 9 pages (English language translation included).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00085">
<othercit>Faller, &#x201c;Parametric Coding of Spatial Audio Effects,&#x201d; Oct. 5, 2004, Chapter 5.4, pp. 84-90.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00086">
<othercit>Notice of Allowance, Russian Appln. No. 2010141971, dated Jan. 16, 2012, 14 pages with English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00087">
<othercit>Engdeg&#xe5;rd et al., &#x201c;CT/Fraunhofer IIS/Philips Submission to the SAOC CfP,&#x201d; 1. AVC Meeting, Nov. 13, 1990-Nov. 16, 1990, The Hague, (CCITT SGXVExpert Group for ATM Video Coding), No. M14696, Jun. 27, 2007, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00088">
<othercit>Oral Proceedings Communication, European Appln. No. 07833118.8, dated Oct. 17, 2011, 31 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00089">
<othercit>Herre et al., &#x201c;Thoughts on an SAOC Architecture,&#x201d; ITU Study Group 16&#x2014;Video Coding Experts Group&#x2014;ISO/IEC MPEG &#x26; ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q6), No. M13935, Oct. 18, 2006, 9 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00090">
<othercit>Office Action, Canadian Appln. No. 2,645,910, dated May 23, 2012, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00091">
<othercit>Notice of Allowance in Russian Application No. 2010140328, dated Dec. 4, 2012, 16 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00092">
<othercit>US Office Action in U.S. Appl. No. 13/022,585, dated Jun. 18, 2013, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00093">
<othercit>International Search Report, PCT Appln. No. PCT/KR2007/004800, Jan. 16, 2008, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00094">
<othercit>International Search Report, PCT Appln. No. PCT/KR2007/004801, Jan. 28, 2008, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00095">
<othercit>International Search Report, PCT Appln. No. PCT/KR2007/004803, Jan. 25, 2008, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00096">
<othercit>International Search Report, PCT Appln. No. PCT/KR2007/005969, Mar. 31, 2008, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00097">
<othercit>International Search Report, PCT Appln. No. PCT/KR2008/000883, Jun. 18, 2008, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00098">
<othercit>Breebaart, J. et al., &#x201c;MPEG Spatial Audio Coding/MPEG Surround: Overview and Current Status&#x201d;, Audio Engineering Society Convention Paper, Oct. 2005, New York, 17 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00099">
<othercit>Breebaart, J. et al., &#x201c;Multi-Channel Goes Mobile: MPEG Surround Binaural Rendering&#x201d;, AES 29<sup>th </sup>International Conference, Sep. 2006, 13 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00100">
<othercit>Faller, C., &#x201c;Coding of Spatial Audio Compatible with Different Playback Formats&#x201d;, Audio Engineering Society Convention Paper, 117<sup>th </sup>Convention, Oct. 2004, SF, 12 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00101">
<othercit>International Search Report based on International Application No. PCT/KR2007/004800, dated Jan. 16, 2008, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00102">
<othercit>International Search Report based on International Application No. PCT/KR2007/004803, dated Jan. 25, 2008, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00103">
<othercit>International Search Report based on International Application No. PCT/KR2007/004801, dated Jan. 28, 2008, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00104">
<othercit>International Search Report based on International Application No. PCT/KR2007/005969, dated Mar. 31, 2008, 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00105">
<othercit>International Search Report based on International Application No. PCT/KR2008/000883, dated Jun. 18, 2008, 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00106">
<othercit>Moon, H. et al., &#x201c;A Multi-Channel Audio Compression Method with Virtual Source Location Information for MPEG-4 SAC&#x201d;, IEEE Transactions on Consumer Electronics, 2005, 7 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00107">
<othercit>Scheirer E. et al., &#x201c;Audio BIFS: Describing Audio Scenes with the MPEG-4 Multimedia Standard&#x201d;, IEEE Transactions on Multimedia, vol. 1, No. 3, Sep. 1999, 14 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>10</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>381  1</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381 17</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>381 22</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>700 94</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704500</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704501</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704502</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704503</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704504</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>17</number-of-drawing-sheets>
<number-of-figures>21</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60848293</doc-number>
<date>20060929</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60829800</doc-number>
<date>20061017</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60863303</doc-number>
<date>20061027</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60860823</doc-number>
<date>20061124</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60880714</doc-number>
<date>20070117</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60880942</doc-number>
<date>20070118</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60948373</doc-number>
<date>20070706</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20090164221</doc-number>
<kind>A1</kind>
<date>20090625</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Dong Soo</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Pang</last-name>
<first-name>Hee Suk</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lim</last-name>
<first-name>Jae Hyun</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Yoon</last-name>
<first-name>Sung Yong</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Hyun Kook</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Dong Soo</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Pang</last-name>
<first-name>Hee Suk</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Lim</last-name>
<first-name>Jae Hyun</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Yoon</last-name>
<first-name>Sung Yong</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Hyun Kook</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fish &#x26; Richardson P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>LG Elecronics Inc.</orgname>
<role>03</role>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Nguyen</last-name>
<first-name>Duc</first-name>
<department>2651</department>
</primary-examiner>
<assistant-examiner>
<last-name>Blair</last-name>
<first-name>Kile</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Provided are an audio encoding method and apparatus and an audio decoding method and apparatus in which audio signals can be encoded or decoded so that sound images can be localized at any desired position for each object audio signal. The audio decoding method includes extracting a downmix signal and object-based side information from an audio signal; generating channel-based side information based on object-based side information and control information for rendering the downmix signal; processing the downmix signal using a decorrelated channel signal; and generating a multi-channel audio signal using the processed downmix signal and the channel-based side information.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="148.08mm" wi="221.91mm" file="US08625808-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="254.00mm" wi="124.88mm" orientation="landscape" file="US08625808-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="234.10mm" wi="179.92mm" orientation="landscape" file="US08625808-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="223.52mm" wi="118.87mm" orientation="landscape" file="US08625808-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="261.87mm" wi="187.79mm" file="US08625808-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="234.10mm" wi="190.67mm" file="US08625808-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="196.17mm" wi="175.85mm" file="US08625808-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="183.73mm" wi="180.00mm" file="US08625808-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="221.66mm" wi="147.15mm" orientation="landscape" file="US08625808-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="236.47mm" wi="161.04mm" orientation="landscape" file="US08625808-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="244.77mm" wi="164.25mm" orientation="landscape" file="US08625808-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="248.84mm" wi="143.93mm" orientation="landscape" file="US08625808-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="265.01mm" wi="144.61mm" orientation="landscape" file="US08625808-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="248.41mm" wi="173.91mm" orientation="landscape" file="US08625808-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="264.67mm" wi="165.18mm" orientation="landscape" file="US08625808-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="223.94mm" wi="181.78mm" orientation="landscape" file="US08625808-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="217.42mm" wi="171.62mm" file="US08625808-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="254.93mm" wi="169.84mm" orientation="landscape" file="US08625808-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application claims the benefit of priority from U.S. Provisional Patent Application</p>
<p id="p-0003" num="0002">No. 60/848,293, for &#x201c;Effective Coding Method for Applying Spatial Audio Object Coding and Sound Image Panning,&#x201d; filed Sep. 29, 2006, which application is incorporated by reference herein in its entirety.</p>
<p id="p-0004" num="0003">This application claims the benefit of priority from U.S. Provisional Patent Application No. 60/829,800, for &#x201c;Method for Coding Audio Signal Based on Object Signal,&#x201d; filed Oct. 17, 2006, which application is incorporated by reference herein in its entirety.</p>
<p id="p-0005" num="0004">This application claims the benefit of priority from U.S. Provisional Patent Application No. 60/863,303, for &#x201c;Effective Coding Method for Applying Spatial Audio Object Coding,&#x201d; filed Oct. 27, 2006, which application is incorporated by reference herein in its entirety.</p>
<p id="p-0006" num="0005">This application claims the benefit of priority from U.S. Provisional Patent Application No. 60/860,823, filed Nov. 24, 2006, which application is incorporated by reference herein in its entirety.</p>
<p id="p-0007" num="0006">This application claims the benefit of priority from U.S. Provisional Patent Application No. 60/880,714, filed Jan. 17, 2007, which application is incorporated by reference herein in its entirety.</p>
<p id="p-0008" num="0007">This application claims the benefit of priority from U.S. Provisional Patent Application No. 60/880,942, filed Jan. 18, 2007, which application is incorporated by reference herein in its entirety.</p>
<p id="p-0009" num="0008">This application claims the benefit of priority from U.S. Provisional Patent Application No. 60/948,373, filed Jul. 6, 2007, which application is incorporated by reference herein in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0010" num="0009">1. Field of the Invention</p>
<p id="p-0011" num="0010">The present invention relates to an audio encoding method and apparatus and an audio decoding method and apparatus in which sound images can be localized at any desired position for each object audio signal.</p>
<p id="p-0012" num="0011">2. Description of the Related Art</p>
<p id="p-0013" num="0012">In general, in multi-channel audio encoding and decoding techniques, a number of channel signals of a multi-channel signal are downmixed into fewer channel signals, side information regarding the original channel signals is transmitted, and a multi-channel signal having as many channels as the original multi-channel signal is restored.</p>
<p id="p-0014" num="0013">Object-based audio encoding and decoding techniques are basically similar to multi-channel audio encoding and decoding techniques in terms of downmixing several sound sources into fewer sound source signals and transmitting side information regarding the original sound sources. However, in object-based audio encoding and decoding techniques, object signals, which are basic elements (e.g., the sound of a musical instrument or a human voice) of a channel signal, are treated the same as channel signals in multi-channel audio encoding and decoding techniques and can thus be coded.</p>
<p id="p-0015" num="0014">In other words, in object-based audio encoding and decoding techniques, each object signal is deemed the entity to be coded. In this regard, object-based audio encoding and decoding techniques are different from multi-channel audio encoding and decoding techniques in which a multi-channel audio coding operation is performed simply based on inter-channel information regardless of the number of elements of a channel signal to be coded.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0016" num="0015">The present invention provides an audio encoding method and apparatus and an audio decoding method and apparatus in which audio signals can be encoded or decoded so that sound images can be localized at any desired position for each object audio signal.</p>
<p id="p-0017" num="0016">According to an aspect of the present invention, there is provided an audio decoding method including extracting a downmix signal and object-based side information from an audio signal; generating channel-based side information based on object-based side information and control information for rendering the downmix signal; processing the downmix signal using a decorrelated channel signal; and generating a multi-channel audio signal using the processed downmix signal and the channel-based side information.</p>
<p id="p-0018" num="0017">According to another aspect of the present invention, there is provided an audio decoding apparatus including a demultiplexer which extracts a downmix signal and object-based side information from an audio signal; a parameter converter which generates channel-based side information based on object-based side information and control information for rendering the downmix signal; a downmix processor which modifies the downmix signal by decorrelated downmix signal if the downmix signal is a stereo downmix signal; and a multi-channel decoder which generates a multi-channel audio signal using a modified downmix signal obtained by the downmix processor and the channel-based side information.</p>
<p id="p-0019" num="0018">According to another aspect of the present invention, there is provided an audio decoding method including extracting a downmix signal and object-based side information from an audio signal; generating channel-based side information and one or more processing parameters based on object-based side information and control information for rendering the downmix signal; generating a multi-channel audio signal using the downmix signal and the channel-based side information; and modifying the multi-channel audio signal using the processing parameters.</p>
<p id="p-0020" num="0019">According to another aspect of the present invention, there is provided an audio decoding apparatus including a demultiplexer which extracts a downmix signal and object-based side information from an audio signal; a parameter converter which generates channel-based side information and one or more processing parameters based on object-based side information and control information for rendering the downmix signal; a multi-channel decoder which generates a multi-channel audio signal using the downmix signal and the channel-based side information; and a channel processor which modifies the multi-channel audio signal using the processing parameters.</p>
<p id="p-0021" num="0020">According to another aspect of the present invention, there is provided a computer-readable recording medium having recorded thereon an audio decoding method including extracting a downmix signal and object-based side information from an audio signal; generating channel-based side information based on object-based side information and control information for rendering the downmix signal; processing the downmix signal using a decorrelated channel signal; and generating a multi-channel audio signal using the processed downmix signal obtained by the swapping and the channel-based side information.</p>
<p id="p-0022" num="0021">According to another aspect of the present invention, there is provided a computer-readable recording medium having recorded thereon an audio decoding method including extracting a downmix signal and object-based side information from an audio signal; generating channel-based side information and one or more processing parameters based on object-based side information and control information for rendering the downmix signal; generating a multi-channel audio signal using the downmix signal and the channel-based side information; and modifying the multi-channel audio signal using the processing parameters.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0023" num="0022">The present invention will become more fully understood from the detailed description given hereinbelow and the accompanying drawings, which are given by illustration only, and thus are not limitative of the present invention, and wherein:</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a typical object-based audio encoding/decoding system;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of an audio decoding apparatus according to a first embodiment of the present invention;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram of an audio decoding apparatus according to a second embodiment of the present invention;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 4</figref> is a graph for explaining the influence of an amplitude difference and a time difference, which are independent from each other, on the localization of sound images;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 5</figref> is a graph of functions regarding the correspondence between amplitude differences and time differences which are required to localize sound images at a predetermined position;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 6</figref> illustrates the format of control data including harmonic information;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram of an audio decoding apparatus according to a third embodiment of the present invention;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 8</figref> is a block diagram of an artistic downmix gains (ADG) module that can be used in the audio decoding apparatus illustrated in <figref idref="DRAWINGS">FIG. 7</figref>;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram of an audio decoding apparatus according to a fourth embodiment of the present invention;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 10</figref> is a block diagram of an audio decoding apparatus according to a fifth embodiment of the present invention;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 11</figref> is a block diagram of an audio decoding apparatus according to a sixth embodiment of the present invention;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 12</figref> is a block diagram of an audio decoding apparatus according to a seventh embodiment of the present invention;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 13</figref> is a block diagram of an audio decoding apparatus according to an eighth embodiment of the present invention;</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 14</figref> is a diagram for explaining the application of three-dimensional (3D) information to a frame by the audio decoding apparatus illustrated in <figref idref="DRAWINGS">FIG. 13</figref>;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 15</figref> is a block diagram of an audio decoding apparatus according to a ninth embodiment of the present invention;</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 16</figref> is a block diagram of an audio decoding apparatus according to a tenth embodiment of the present invention;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIGS. 17 through 19</figref> are diagrams for explaining an audio decoding method according to an embodiment of the present invention; and</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 20</figref> is a block diagram of an audio encoding apparatus according to an embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DESCRIPTION OF THE PREFERRED EMBODIMENT</heading>
<p id="p-0042" num="0041">The present invention will hereinafter be described in detail with reference to the accompanying drawings in which exemplary embodiments of the invention are shown.</p>
<p id="p-0043" num="0042">An audio encoding method and apparatus and an audio decoding method and apparatus according to the present invention may be applied to object-based audio processing operations, but the present invention is not restricted to this. In other words, the audio encoding method and apparatus and the audio decoding method and apparatus may be applied to various signal processing operations other than object-based audio processing operations.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a typical object-based audio encoding/decoding system. In general, audio signals input to an object-based audio encoding apparatus do not correspond to channels of a multi-channel signal but are independent object signals. In this regard, an object-based audio encoding apparatus is differentiated from a multi-channel audio encoding apparatus to which channel signals of a multi-channel signal are input.</p>
<p id="p-0045" num="0044">For example, channel signals such as a front left channel signal and a front right channel signal of a 5.1-channel signal may be input to a multi-channel audio signal, whereas object audio signals such as a human voice or the sound of a musical instrument (e.g., the sound of a violin or a piano) which are smaller entities than channel signals may be input to an object-based audio encoding apparatus.</p>
<p id="p-0046" num="0045">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, the object-based audio encoding/decoding system includes an object-based audio encoding apparatus and an object-based audio decoding apparatus. The object-based audio encoding apparatus includes an object encoder <b>100</b>, and the object-based audio decoding apparatus includes an object decoder <b>111</b> and a renderer <b>113</b>.</p>
<p id="p-0047" num="0046">The object encoder <b>100</b> receives N object audio signals, and generates an object-based downmix signal with one or more channels and side information including a number of pieces of information extracted from the N object audio signals such as energy difference, phase difference, and correlation value. The side information and the object-based downmix signal are incorporated into a single bitstream, and the bitstream is transmitted to the object-based decoding apparatus.</p>
<p id="p-0048" num="0047">The side information may include a flag indicating whether to perform channel-based audio coding or object-based audio coding, and thus, it may be determined whether to perform channel-based audio coding or object-based audio coding based on the flag of the side information. The side information may also include envelope information, grouping information, silent period information, and delay information regarding object signals. The side information may also include object level differences information, inter-object cross correlation information, downmix gain information, downmix channel level difference information, and absolute object energy information.</p>
<p id="p-0049" num="0048">The object decoder <b>111</b> receives the object-based downmix signal and the side information from the object-based audio encoding apparatus, and restores object signals having similar properties to those of the N object audio signals based on the object-based downmix signal and the side information. The object signals generated by the object decoder <b>111</b> have not yet been allocated to any position in a multi-channel space. Thus, the renderer <b>113</b> allocates each of the object signals generated by the object decoder <b>111</b> to a predetermined position in a multi-channel space and determines the levels of the object signals so that the object signals can be reproduced from respective corresponding positions designated by the renderer <b>113</b> with respective corresponding levels determined by the renderer <b>113</b>. Control information regarding each of the object signals generated by the object decoder <b>111</b> may vary over time, and thus, the spatial positions and the levels of the object signals generated by the object decoder <b>111</b> may vary according to the control information.</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of an audio decoding apparatus <b>120</b> according to a first embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 2</figref>, the audio decoding apparatus <b>120</b> includes an object decoder <b>121</b>, a renderer <b>123</b>, and a parameter converter <b>125</b>. The audio decoding apparatus <b>120</b> may also include a demultiplexer (not shown) which extracts a downmix signal and side information from a bitstream input thereto, and this will apply to all audio decoding apparatuses according to other embodiments of the present invention.</p>
<p id="p-0051" num="0050">The object decoder <b>121</b> generates a number of object signals based on a downmix signal and modified side information provided by the parameter converter <b>125</b>. The renderer <b>123</b> allocates each of the object signals generated by the object decoder <b>121</b> to a predetermined position in a multi-channel space and determines the levels of the object signals generated by the object decoder <b>121</b> according to control information. The parameter converter <b>125</b> generates the modified side information by combining the side information and the control information. Then, the parameter converter <b>125</b> transmits the modified side information to the object decoder <b>121</b>.</p>
<p id="p-0052" num="0051">The object decoder <b>121</b> may be able to perform adaptive decoding by analyzing the control information in the modified side information.</p>
<p id="p-0053" num="0052">For example, if the control information indicates that a first object signal and a second object signal are allocated to the same position in a multi-channel space and have the same level, a typical audio decoding apparatus may decode the first and second object signals separately, and then arrange them in a multi-channel space through a mixing/rendering operation.</p>
<p id="p-0054" num="0053">On the other hand, the object decoder <b>121</b> of the audio decoding apparatus <b>120</b> learns from the control information in the modified side information that the first and second object signals are allocated to the same position in a multi-channel space and have the same level as if they were a single sound source. Accordingly, the object decoder <b>121</b> decodes the first and second object signals by treating them as a single sound source without decoding them separately. As a result, the complexity of decoding decreases. In addition, due to a decrease in the number of sound sources that need to be processed, the complexity of mixing/rendering also decreases.</p>
<p id="p-0055" num="0054">The audio decoding apparatus <b>120</b> may be effectively used in the situation when the number of object signals is greater than the number of output channels because a plurality of object signals are highly likely to be allocated to the same spatial position.</p>
<p id="p-0056" num="0055">Alternatively, the audio decoding apparatus <b>120</b> may be used in the situation when the first object signal and the second object signal are allocated to the same position in a multi-channel space but have different levels. In this case, the audio decoding apparatus <b>120</b> decode the first and second object signals by treating the first and second object signals as a single, instead of decoding the first and second object signals separately and transmitting the decoded first and second object signals to the renderer <b>123</b>. More specifically, the object decoder <b>121</b> may obtain information regarding the difference between the levels of the first and second object signals from the control information in the modified side information, and decode the first and second object signals based on the obtained information. As a result, even if the first and second object signals have different levels, the first and second object signals can be decoded as if they were a single sound source.</p>
<p id="p-0057" num="0056">Still alternatively, the object decoder <b>121</b> may adjust the levels of the object signals generated by the object decoder <b>121</b> according to the control information. Then, the object decoder <b>121</b> may decode the object signals whose levels are adjusted. Accordingly, the renderer <b>123</b> does not need to adjust the levels of the decoded object signals provided by the object decoder <b>121</b> but simply arranges the decoded object signals provided by the object decoder <b>121</b> in a multi-channel space. In short, since the object decoder <b>121</b> adjusts the levels of the object signals generated by the object decoder <b>121</b> according to the control information, the renderer <b>123</b> can readily arrange the object signals generated by the object decoder <b>121</b> in a multi-channel space without the need to additionally adjust the levels of the object signals generated by the object decoder <b>121</b>. Therefore, it is possible to reduce the complexity of mixing/rendering.</p>
<p id="p-0058" num="0057">According to the embodiment of <figref idref="DRAWINGS">FIG. 2</figref>, the object decoder of the audio decoding apparatus <b>120</b> can adaptively perform a decoding operation through the analysis of the control information, thereby reducing the complexity of decoding and the complexity of mixing/rendering. A combination of the above-described methods performed by the audio decoding apparatus <b>120</b> may be used.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram of an audio decoding apparatus <b>130</b> according to a second embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 3</figref>, the audio decoding apparatus <b>130</b> includes an object decoder <b>131</b> and a renderer <b>133</b>. The audio decoding apparatus <b>130</b> is characterized by providing side information not only to the object decoder <b>131</b> but also to the renderer <b>133</b>.</p>
<p id="p-0060" num="0059">The audio decoding apparatus <b>130</b> may effectively perform a decoding operation even when there is an object signal corresponding to a silent period. For example, second through fourth object signals may correspond to a music play period during which a musical instrument is played, and a first object signal may correspond to a silent period during which an accompaniment is played. In this case, information indicating which of a plurality of object signals corresponds to a silent period may be included in side information, and the side information may be provided to the renderer <b>133</b> as well as to the object decoder <b>131</b>.</p>
<p id="p-0061" num="0060">The object decoder <b>131</b> may minimize the complexity of decoding by not decoding an object signal corresponding to a silent period. The object decoder <b>131</b> sets an object signal corresponding to a value of 0 and transmits the level of the object signal to the renderer <b>133</b>. In general, object signals having a value of 0 are treated the same as object signals having a value, other than 0, and are thus subjected to a mixing/rendering operation.</p>
<p id="p-0062" num="0061">On the other hand, the audio decoding apparatus <b>130</b> transmits side information including information indicating which of a plurality of object signals corresponds to a silent period to the renderer <b>133</b> and can thus prevent an object signal corresponding to a silent period from being subjected to a mixing/rendering operation performed by the renderer <b>133</b>. Therefore, the audio decoding apparatus <b>130</b> can prevent an unnecessary increase in the complexity of mixing/rendering.</p>
<p id="p-0063" num="0062">The renderer <b>133</b> may use mixing parameter information which is included in control information to localize a sound image of each object signal at a stereo scene. The mixing parameter information may include amplitude information only or both amplitude information and time information. The mixing parameter information affects not only the localization of stereo sound images but also the psychoacoustic perception of a spatial sound quality by a user.</p>
<p id="p-0064" num="0063">For example, upon comparing two sound images which are generated using a time panning method and an amplitude panning method, respectively, and reproduced at the same location using a 2-channel stereo speaker, it is recognized that the amplitude panning method can contribute to a precise localization of sound images, and that the time panning method can provide natural sounds with a profound feeling of space. Thus, if the renderer <b>133</b> only uses the amplitude panning method to arrange object signals in a multi-channel space, the renderer <b>133</b> may be able to precisely localize each sound image, but may not be able to provide as profound a feeling of sound as when using the time panning method. Users may sometime prefer a precise localization of sound images to a profound feeling of sound or vice versa according to the type of sound sources.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIGS. 4(</figref><i>a</i>) and <b>4</b>(<i>b</i>) explains the influence of intensity (amplitude difference) and a time difference on the localization of sound images as performed in the reproduction of signals with a 2-channel stereo speaker. Referring to <figref idref="DRAWINGS">FIGS. 4(</figref><i>a</i>) and <b>4</b>(<i>b</i>), a sound image may be localized at a predetermined angle according to an amplitude difference and a time difference which are independent from each other. For example, an amplitude difference of about 8 dB or a time difference of about 0.5 ms, which is equivalent to the amplitude difference of 8 dB, may be used in order to localize a sound image at an angle of 20&#xb0;. Therefore, even if only an amplitude difference is provided as mixing parameter information, it is possible to obtain various sounds with different properties by converting the amplitude difference into a time difference which is equivalent to the amplitude difference during the localization of sound images.</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 5</figref> illustrates functions regarding the correspondence between amplitude differences and time differences which are required to localize sound images at angles of 10&#xb0;, 20&#xb0;, and 30&#xb0;. The function illustrated in <figref idref="DRAWINGS">FIG. 5</figref> may be obtained based on <figref idref="DRAWINGS">FIGS. 4(</figref><i>a</i>) and <b>4</b>(<i>b</i>). Referring to <figref idref="DRAWINGS">FIG. 5</figref>, various amplitude difference-time difference combinations may be provided for localizing a sound image at a predetermined position. For example, assume that an amplitude difference of 8 dB is provided as mixing parameter information in order to localize a sound image at an angle of 20&#xb0;. According to the function illustrated in <figref idref="DRAWINGS">FIG. 5</figref>, a sound image can also be localized at the angle of 20&#xb0; using the combination of an amplitude difference of 3 dB and a time difference of 0.3 ms. In this case, not only amplitude difference information but also time difference information may be provided as mixing parameter information, thereby enhancing the feeling of space.</p>
<p id="p-0067" num="0066">Therefore, in order to generate sounds with properties desired by a user during a mixing/rendering operation, mixing parameter information may be appropriately converted so that whichever of amplitude panning and time panning suits the user can be performed. That is, if mixing parameter information only includes amplitude difference information and the user wishes for sounds with a profound feeling of space, the amplitude difference information may be converted into time difference information equivalent to the amplitude difference information with reference to psychoacoustic data. Alternatively, if the user wishes for both sounds with a profound feeling of space and a precise localization of sound images, the amplitude difference information may be converted into the combination of amplitude difference information and time difference information equivalent to the original amplitude information.</p>
<p id="p-0068" num="0067">Alternatively, if mixing parameter information only includes time difference information and a user prefers a precise localization of sound images, the time difference information may be converted into amplitude difference information equivalent to the time difference information, or may be converted into the combination of amplitude difference information and time difference information which can satisfy the user's preference by enhancing both the precision of localization of sound images and the feeling of space.</p>
<p id="p-0069" num="0068">Still alternatively, if mixing parameter information includes both amplitude difference information and time difference information and a user prefers a precise localization of sound images, the combination of the amplitude difference information and the time difference information may be converted into amplitude difference information equivalent to the combination of the original amplitude difference information and the time difference information. On the other hand, if mixing parameter information includes both amplitude difference information and time difference information and a user prefers the enhancement of the feeling of space, the combination of the amplitude difference information and the time difference information may be converted into time difference information equivalent the combination of the amplitude difference information and the original time difference information.</p>
<p id="p-0070" num="0069">Referring to <figref idref="DRAWINGS">FIG. 6</figref>, control information may include mixing/rendering information and harmonic information regarding one or more object signals. The harmonic information may include at least one of pitch information, fundamental frequency information, and dominant frequency band information regarding one or more object signals, and descriptions of the energy and spectrum of each sub-band of each of the object signals.</p>
<p id="p-0071" num="0070">The harmonic information may be used to process an object signal during a rendering operation because the resolution of a renderer which performs its operation in units of sub-bands is insufficient.</p>
<p id="p-0072" num="0071">If the harmonic information includes pitch information regarding one or more object signals, the gain of each of the object signals may be adjusted by attenuating or strengthening a predetermined frequency domain using a comb filter or an inverse comb filter. For example, if one of a plurality of object signals is a vocal signal, the object signals may be used as a karaoke by attenuating only the vocal signal. Alternatively, if the harmonic information includes dominant frequency domain information regarding one or more object signals, a process of attenuating or strengthening a dominant frequency domain may be performed. Still alternatively, if the harmonic information includes spectrum information regarding one or more object signals, the gain of each of the object signals may be controlled by performing attenuation or enforcement without being restricted by any sub-band boundaries.</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 7</figref> is a block diagram of an audio decoding apparatus <b>140</b> according to another embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 7</figref>, the audio decoding apparatus <b>140</b> uses a multi-channel decoder <b>141</b>, instead of an object decoder and a renderer, and decodes a number of object signals after the object signals are appropriately arranged in a multi-channel space.</p>
<p id="p-0074" num="0073">More specifically, the audio decoding apparatus <b>140</b> includes the multi-channel decoder <b>141</b> and a parameter converter <b>145</b>. The multi-channel decoder <b>141</b> generates a multi-channel signal whose object signals have already been arranged in a multi-channel space based on a down-mix signal and spatial parameter information, which is channel-based side information provided by the parameter converter <b>145</b>. The parameter converter <b>145</b> analyzes side information and control information transmitted by an audio encoding apparatus (not shown), and generates the spatial parameter information based on the result of the analysis. More specifically, the parameter converter <b>145</b> generates the spatial parameter information by combining the side information and the control information which includes playback setup information and mixing information. That is, the parameter conversion <b>145</b> performs the conversion of the combination of the side information and the control information to spatial data corresponding to a One-To-Two (OTT) box or a Two-To-Three (TTT) box.</p>
<p id="p-0075" num="0074">The audio decoding apparatus <b>140</b> may perform a multi-channel decoding operation into which an object-based decoding operation and a mixing/rendering operation are incorporated and may thus skip the decoding of each object signal. Therefore, it is possible to reduce the complexity of decoding and/or mixing/rendering.</p>
<p id="p-0076" num="0075">For example, when there are 10 object signals and a multi-channel signal obtained based on the 10 object signals is to be reproduced by a 5.1 channel speaker reproduction system, a typical object-based audio decoding apparatus generates decoded signals respectively corresponding the 10 object signals based on a down-mix signal and side information and then generates a 5.1 channel signal by appropriately arranging the 10 object signals in a multi-channel space so that the object signals can become suitable for a 5.1 channel speaker environment. However, it is inefficient to generate 10 object signals during the generation of a 5.1 channel signal, and this problem becomes more severe as the difference between the number of object signals and the number of channels of a multi-channel signal to be generated increases.</p>
<p id="p-0077" num="0076">On the other hand, according to the embodiment of <figref idref="DRAWINGS">FIG. 7</figref>, the audio decoding apparatus <b>140</b> generates spatial parameter information suitable for a 5.1-channel signal based on side information and control information, and provides the spatial parameter information and a downmix signal to the multi-channel decoder <b>141</b>. Then, the multi-channel decoder <b>141</b> generates a 5.1 channel signal based on the spatial parameter information and the downmix signal. In other words, when the number of channels to be output is 5.1 channels, the audio decoding apparatus <b>140</b> can readily generate a 5.1-channel signal based on a downmix signal without the need to generate 10 object signals and is thus more efficient than a conventional audio decoding apparatus in terms of complexity.</p>
<p id="p-0078" num="0077">The audio decoding apparatus <b>140</b> is deemed efficient when the amount of computation required to calculates spatial parameter information corresponding to each of an OTT box and a TTT box through the analysis of side information and control information transmitted by an audio encoding apparatus is less than the amount of computation required to perform a mixing/rendering operation after the decoding of each object signal.</p>
<p id="p-0079" num="0078">The audio decoding apparatus <b>140</b> may be obtained simply by adding a module for generating spatial parameter information through the analysis of side information and control information to a typical multi-channel audio decoding apparatus, and may thus maintain the compatibility with a typical multi-channel audio decoding apparatus. Also, the audio decoding apparatus <b>140</b> can improve the quality of sound using existing tools of a typical multi-channel audio decoding apparatus such as an envelope shaper, a sub-band temporal processing (STP) tool, and a decorrelator. Given all this, it is concluded that all the advantages of a typical multi-channel audio decoding method can be readily applied to an object-audio decoding method.</p>
<p id="p-0080" num="0079">Spatial parameter information transmitted to the multi-channel decoder <b>141</b> by the parameter converter <b>145</b> may have been compressed so as to be suitable for being transmitted. Alternatively, the spatial parameter information may have the same format as that of data transmitted by a typical multi-channel encoding apparatus. That is, the spatial parameter information may have been subjected to a Huffman decoding operation or a pilot decoding operation and may thus be transmitted to each module as uncompressed spatial cue data. The former is suitable for transmitting the spatial parameter information to a multi-channel audio decoding apparatus in a remote place, and the later is convenient because there is no need for a multi-channel audio decoding apparatus to convert compressed spatial cue data into uncompressed spatial cue data that can readily be used in a decoding operation.</p>
<p id="p-0081" num="0080">The configuration of spatial parameter information based on the analysis of side information and control information may cause a delay between a downmix signal and the spatial parameter information. In order to address this, an additional buffer may be provided either for a downmix signal or for spatial parameter information so that the downmix signal and the spatial parameter information can be synchronized with each other. These methods, however, are inconvenient because of the requirement to provide an additional buffer. Alternatively, side information may be transmitted ahead of a downmix signal in consideration of the possibility of occurrence of a delay between a downmix signal and spatial parameter information. In this case, spatial parameter information obtained by combining the side information and control information does not need to be adjusted but can readily be used.</p>
<p id="p-0082" num="0081">If a plurality of object signals of a downmix signal have different levels, an artistic downmix gains (ADG) module which can directly compensate for the downmix signal may determine the relative levels of the object signals, and each of the object signals may be allocated to a predetermined position in a multi-channel space using spatial cue data such as channel level difference information, inter-channel correlation (ICC) information, and channel prediction coefficient (CPC) information.</p>
<p id="p-0083" num="0082">For example, if control information indicates that a predetermined object signal is to be allocated to a predetermined position in a multi-channel space and has a higher level than other object signals, a typical multi-channel decoder may calculate the difference between the energies of channels of a downmix signal, and divide the downmix signal into a number of output channels based on the results of the calculation. However, a typical multi-channel decoder cannot increase or reduce the volume of a certain sound in a downmix signal. In other words, a typical multi-channel decoder simply distributes a downmix signal to a number of output channels and thus cannot increase or reduce the volume of a sound in the downmix signal.</p>
<p id="p-0084" num="0083">It is relatively easy to allocate each of a number of object signals of a downmix signal generated by an object encoder to a predetermined position in a multi-channel space according to control information. However, special techniques are required to increase or reduce the amplitude of a predetermined object signal. In other words, if a downmix signal generated by an object encoder is used as it is, it is difficult to reduce the amplitude of each object signal of the downmix signal.</p>
<p id="p-0085" num="0084">Therefore, according to an embodiment of the present invention, the relative amplitudes of object signals may be varied according to control information using an ADG module <b>147</b> illustrated in <figref idref="DRAWINGS">FIG. 8</figref>. More specifically, the amplitude of any one of a plurality of object signals of a downmix signal transmitted by an object encoder may be increased or reduced using the ADG module <b>147</b>. A downmix signal obtained by compensation performed by the ADG module <b>147</b> may be subjected to multi-channel decoding.</p>
<p id="p-0086" num="0085">If the relative amplitudes of object signals of a downmix signal are appropriately adjusted using the ADG module <b>147</b>, it is possible to perform object decoding using a typical multi-channel decoder. If a downmix signal generated by an object encoder is a mono or stereo signal or a multi-channel signal with three or more channels, the downmix signal may be processed by the ADG module <b>147</b>. If a downmix signal generated by an object encoder has two or more channels and a predetermined object signal that needs to be adjusted by the ADG module <b>147</b> only exists in one of the channels of the downmix signal, the ADG module <b>147</b> may be applied only to the channel including the predetermined object signal, instead of being applied to all the channels of the downmix signal. A downmix signal processed by the ADG module <b>147</b> in the above-described manner may be readily processed using a typical multi-channel decoder without the need to modify the structure of the multi-channel decoder.</p>
<p id="p-0087" num="0086">Even when a final output signal is not a multi-channel signal that can be reproduced by a multi-channel speaker but is a binaural signal, the ADG module <b>147</b> may be used to adjust the relative amplitudes of object signals of the final output signal.</p>
<p id="p-0088" num="0087">Alternatively to the use of the ADG module <b>147</b>, gain information specifying a gain value to be applied to each object signal may be included in control information during the generation of a number of object signals. For this, the structure of a typical multi-channel decoder may be modified. Even though requiring a modification to the structure of an existing multi-channel decoder, this method is convenient in terms of reducing the complexity of decoding by applying a gain value to each object signal during a decoding operation without the need to calculate ADG and to compensate for each object signal.</p>
<p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. 9</figref> is a block diagram of an audio decoding apparatus <b>150</b> according to a fourth embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 9</figref>, the audio decoding apparatus <b>150</b> is characterized by generating a binaural signal.</p>
<p id="p-0090" num="0089">More specifically, the audio decoding apparatus <b>150</b> includes a multi-channel binaural decoder <b>151</b>, a first parameter converter <b>157</b>, and a second parameter converter <b>159</b>.</p>
<p id="p-0091" num="0090">The second parameter converter <b>159</b> analyzes side information and control information which are provided by an audio encoding apparatus, and configures spatial parameter information based on the result of the analysis. The first parameter converter <b>157</b> configures binaural parameter information, which can be used by the multi-channel binaural decoder <b>151</b>, by adding three-dimensional (3D) information such as head-related transfer function (HRTF) parameters to the spatial parameter information. The multi-channel binaural decoder <b>151</b> generates a virtual three-dimensional (3D) signal by applying the virtual 3D parameter information to a downmix signal.</p>
<p id="p-0092" num="0091">The first parameter converter <b>157</b> and the second parameter converter <b>159</b> may be replaced by a single module, i.e., a parameter conversion module <b>155</b> which receives the side information, the control information, and the HRTF parameters and configures the binaural parameter information based on the side information, the control information, and the HRTF parameters.</p>
<p id="p-0093" num="0092">Conventionally, in order to generate a binaural signal for the reproduction of a downmix signal including 10 object signals with a headphone, an object signal must generate 10 decoded signals respectively corresponding to the 10 object signals based on the downmix signal and side information. Thereafter, a renderer allocates each of the 10 object signals to a predetermined position in a multi-channel space with reference to control information so as to suit a 5-channel speaker environment. Thereafter, the renderer generates a 5-channel signal that can be reproduced using a 5-channel speaker. Thereafter, the renderer applies HRTF parameters to the 5-channel signal, thereby generating a 2-channel signal. In short, the above-mentioned conventional audio decoding method includes reproducing 10 object signals, converting the 10 object signals into a 5-channel signal, and generating a 2-channel signal based on the 5-channel signal, and is thus inefficient.</p>
<p id="p-0094" num="0093">On the other hand, the audio decoding apparatus <b>150</b> can readily generate a binaural signal that can be reproduced using a headphone based on object audio signals. In addition, the audio decoding apparatus <b>150</b> configures spatial parameter information through the analysis of side information and control information, and can thus generate a binaural signal using a typical multi-channel binaural decoder. Moreover, the audio decoding apparatus <b>150</b> still can use a typical multi-channel binaural decoder even when being equipped with an incorporated parameter converter which receives side information, control information, and HRTF parameters and configures binaural parameter information based on the side information, the control information, and the HRTF parameters.</p>
<p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. 10</figref> is a block diagram of an audio decoding apparatus <b>160</b> according to a fifth embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 10</figref>, the audio decoding apparatus <b>160</b> includes a downmix processor <b>161</b>, a multi-channel decoder <b>163</b>, and a parameter converter <b>165</b>. The downmix processor <b>161</b> and the parameter converter <b>163</b> may be replaced by a single module <b>167</b>.</p>
<p id="p-0096" num="0095">The parameter converter <b>165</b> generates spatial parameter information, which can be used by the multi-channel decoder <b>163</b>, and parameter information, which can be used by the downmix processor <b>161</b>. The downmix processor <b>161</b> performs a pre-processing operation on a downmix signal, and transmits a downmix signal resulting from the pre-processing operation to the multi-channel decoder <b>163</b>. The multi-channel decoder <b>163</b> performs a decoding operation on the downmix signal transmitted by the downmix processor <b>161</b>, thereby outputting a stereo signal, a binaural stereo signal or a multi-channel signal. Examples of the pre-processing operation performed by the downmix processor <b>161</b> include the modification or conversion of a downmix signal in a time domain or a frequency domain using filtering.</p>
<p id="p-0097" num="0096">If a downmix signal input to the audio decoding apparatus <b>160</b> is a stereo signal, the downmix signal may have be subjected to downmix preprocessing performed by the downmix processor <b>161</b> before being input to the multi-channel decoder <b>163</b> because the multi-channel decoder <b>163</b> cannot map a component of the downmix signal corresponding to a left channel, which is one of multiple channels, to a right channel, which is another of the multiple channels. Therefore, in order to shift the position of an object signal classified into the left channel to the direction of the right channel, the downmix signal input to the audio decoding apparatus <b>160</b> may be preprocessed by the downmix processor <b>161</b>, and the preprocessed downmix signal may be input to the multi-channel decoder <b>163</b>.</p>
<p id="p-0098" num="0097">The preprocessing of a stereo downmix signal may be performed based on preprocessing information obtained from side information and from control information.</p>
<p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. 11</figref> is a block diagram of an audio decoding apparatus <b>170</b> according to a sixth embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 11</figref>, the audio decoding apparatus <b>170</b> includes a multi-channel decoder <b>171</b>, a channel processor <b>173</b>, and a parameter converter <b>175</b>.</p>
<p id="p-0100" num="0099">The parameter converter <b>175</b> generates spatial parameter information, which can be used by the multi-channel decoder <b>173</b>, and parameter information, which can be used by the channel processor <b>173</b>. The channel processor <b>173</b> performs a post-processing operation on a signal output by the multi-channel decoder <b>173</b>. Examples of the signal output by the multi-channel decoder <b>173</b> include a stereo signal, a binaural stereo signal and a multi-channel signal.</p>
<p id="p-0101" num="0100">Examples of the post-processing operation performed by the post processor <b>173</b> include the modification and conversion of each channel or all channels of an output signal. For example, if side information includes fundamental frequency information regarding a predetermined object signal, the channel processor <b>173</b> may remove harmonic components from the predetermined object signal with reference to the fundamental frequency information. A multi-channel audio decoding method may not be efficient enough to be used in a karaoke system. However, if fundamental frequency information regarding vocal object signals is included in side information and harmonic components of the vocal object signals are removed during a post-processing operation, it is possible to realize a high-performance karaoke system using the embodiment of <figref idref="DRAWINGS">FIG. 11</figref>. The embodiment of <figref idref="DRAWINGS">FIG. 11</figref> may also be applied to object signals, other than vocal object signals. For example, it is possible to remove the sound of a predetermined musical instrument using the embodiment of <figref idref="DRAWINGS">FIG. 11</figref>. Also, it is possible to amplify predetermined harmonic components using fundamental frequency information regarding object signals using the embodiment of <figref idref="DRAWINGS">FIG. 11</figref>.</p>
<p id="p-0102" num="0101">The channel processor <b>173</b> may perform additional effect processing on a downmix signal. Alternatively, the channel processor <b>173</b> may add a signal obtained by the additional effect processing to a signal output by the multi-channel decoder <b>171</b>. The channel processor <b>173</b> may change the spectrum of an object or modify a downmix signal whenever necessary. If it is not appropriate to directly perform an effect processing operation such as reverberation on a downmix signal and to transmit a signal obtained by the effect processing operation to the multi-channel decoder <b>171</b>, the downmix processor <b>173</b> may add the signal obtained by the effect processing operation to the output of the multi-channel decoder <b>171</b>, instead of performing effect processing on the downmix signal.</p>
<p id="p-0103" num="0102">The audio decoding apparatus <b>170</b> may be designed to include not only the channel processor <b>173</b> but also a downmix processor. In this case, the downmix processor may be disposed in front of the multi-channel decoder <b>173</b>, and the channel processor <b>173</b> may be disposed behind the multi-channel decoder <b>173</b>.</p>
<p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. 12</figref> is a block diagram of an audio decoding apparatus <b>210</b> according to a seventh embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 12</figref>, the audio decoding apparatus <b>210</b> uses a multi-channel decoder <b>213</b>, instead of an object decoder.</p>
<p id="p-0105" num="0104">More specifically, the audio decoding apparatus <b>210</b> includes the multi-channel decoder <b>213</b>, a transcoder <b>215</b>, a renderer <b>217</b>, and a 3D information database <b>217</b>.</p>
<p id="p-0106" num="0105">The renderer <b>217</b> determines the 3D positions of a plurality of object signals based on 3D information corresponding to index data included in control information. The transcoder <b>215</b> generates channel-based side information by synthesizing position information regarding a number of object audio signals to which 3D information is applied by the renderer <b>217</b>. The multi-channel decoder <b>213</b> outputs a 3D signal by applying the channel-based side information to a down-mix signal</p>
<p id="p-0107" num="0106">A head-related transfer function (HRTF) may be used as the 3D information. An HRTF is a transfer function which describes the transmission of sound waves between a sound source at an arbitrary position and the eardrum, and returns a value that varies according to the direction and altitude of the sound source. If a signal with no directivity is filtered using the HRTF, the signal may be heard as if it were reproduced from a certain direction.</p>
<p id="p-0108" num="0107">When an input bitstream is received, the audio decoding apparatus <b>210</b> extracts an object-based downmix signal and object-based parameter information from the input bitstream using a demultiplexer (not shown). Then, the renderer <b>217</b> extracts index data from control information, which is used to determine the positions of a plurality of object audio signals, and withdraws 3D information corresponding to the extracted index data from the 3D information database <b>219</b>.</p>
<p id="p-0109" num="0108">More specifically, mixing parameter information, which is included in control information that is used by the audio decoding apparatus <b>210</b>, may include not only level information but also index data necessary for searching for 3D information. The mixing parameter information may also include time information regarding the time difference between channels, position information and one or more parameters obtained by appropriately combining the level information and the time information.</p>
<p id="p-0110" num="0109">The position of an object audio signal may be determined initially according to default mixing parameter information, and may be changed later by applying 3D information corresponding to a position desired by a user to the object audio signal. Alternatively, if the user wishes to apply a 3D effect only to several object audio signals, level information and time information regarding other object audio signals to which the user wishes not to apply a 3D effect may be used as mixing parameter information.</p>
<p id="p-0111" num="0110">The transcoder <b>217</b> generates channel-based side information regarding M channels by synthesizing object-based parameter information regarding N object signals transmitted by an audio encoding apparatus and position information of a number of object signals to which 3D information such as an HRTF is applied by the renderer <b>217</b>.</p>
<p id="p-0112" num="0111">The multi-channel decoder <b>213</b> generates an audio signal based on a downmix signal and the channel-based side information provided by the transcoder <b>217</b>, and generates a 3D multi-channel signal by performing a 3D rendering operation using 3D information included in the channel-based side information.</p>
<p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. 13</figref> is a block diagram of an audio decoding apparatus <b>220</b> according to a eighth embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 13</figref>, the audio decoding apparatus <b>220</b> is different from the audio decoding apparatus <b>210</b> illustrated in <figref idref="DRAWINGS">FIG. 12</figref> in that a transcoder <b>225</b> transmits channel-based side information and 3D information separately to a multi-channel decoder <b>223</b>. In other words, the transcoder <b>225</b> of the audio decoding apparatus <b>220</b> obtains channel-based side information regarding M channels from object-based parameter information regarding N object signals and transmits the channel-based side information and 3D information, which is applied to each of the N object signals, to the multi-channel decoder <b>223</b>, whereas the transcoder <b>217</b> of the audio decoding apparatus <b>210</b> transmits channel-based side information including 3D information to the multi-channel decoder <b>213</b>.</p>
<p id="p-0114" num="0113">Referring to <figref idref="DRAWINGS">FIG. 14</figref>, channel-based side information and 3D information may include a plurality of frame indexes. Thus, the multi-channel decoder <b>223</b> may synchronize the channel-based side information and the 3D information with reference to the frame indexes of each of the channel-based side information and the 3D information, and may thus apply 3D information to a frame of a bitstream corresponding to the 3D information. For example, 3D information having index <b>2</b> may be applied at the beginning of frame <b>2</b> having index <b>2</b>.</p>
<p id="p-0115" num="0114">Since channel-based side information and 3D information both includes frame indexes, it is possible to effectively determine a temporal position of the channel-based side information to which the 3D information is to be applied, even if the 3D information is updated over time. In other words, the transcoder <b>225</b> includes 3D information and a number of frame indexes in channel-based side information, and thus, the multi-channel decoder <b>223</b> can easily synchronize the channel-based side information and the 3D information.</p>
<p id="p-0116" num="0115">The downmix processor <b>231</b>, transcoder <b>235</b>, renderer <b>237</b> and the 3D information database may be replaced by a single module <b>239</b>.</p>
<p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. 15</figref> is a block diagram of an audio decoding apparatus <b>230</b> according to a ninth embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 15</figref>, the audio decoding apparatus <b>230</b> is differentiated from the audio decoding apparatus <b>220</b> illustrated in <figref idref="DRAWINGS">FIG. 14</figref> by further including a downmix processor <b>231</b>.</p>
<p id="p-0118" num="0117">More specifically, the audio decoding apparatus <b>230</b> includes a transcoder <b>235</b>, a renderer <b>237</b>, a 3D information database <b>239</b>, a multi-channel decoder <b>233</b>, and the downmix processor <b>231</b>. The transcoder <b>235</b>, the renderer <b>237</b>, the 3D information database <b>239</b>, and the multi-channel decoder <b>233</b> are the same as their respective counterparts illustrated in <figref idref="DRAWINGS">FIG. 14</figref>. The downmix processor <b>231</b> performs a pre-processing operation on a stereo downmix signal for position adjustment. The 3D information database <b>239</b> may be incorporated with the renderer <b>237</b>. A module for applying a predetermined effect to a downmix signal may also be provided in the audio decoding apparatus <b>230</b>.</p>
<p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. 16</figref> illustrates a block diagram of an audio decoding apparatus <b>240</b> according to a tenth embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 16</figref>, the audio decoding apparatus <b>240</b> is differentiated from the audio decoding apparatus <b>230</b> illustrated in <figref idref="DRAWINGS">FIG. 15</figref> by including a multi-point control unit combiner <b>241</b>.</p>
<p id="p-0120" num="0119">That is, the audio decoding apparatus <b>240</b>, like the audio decoding apparatus <b>230</b>, includes a downmix processor <b>243</b>, a multi-channel decoder <b>244</b>, a transcoder <b>245</b>, a renderer <b>247</b>, and a 3D information database <b>249</b>. The multi-point control unit combiner <b>241</b> combines a plurality of bitstreams obtained by object-based encoding, thereby obtaining a single bitstream. For example, when a first bitstream for a first audio signal and a second bitstream for a second audio signal are input, the multi-point control unit combiner <b>241</b> extracts a first downmix signal from the first bitstream, extracts a second downmix signal from the second bitstream and generates a third downmix signal by combining the first and second downmix signals. In addition, the multi-point control unit combiner <b>241</b> extracts first object-based side information from the first bitstream, extract second object-based side information from the second bitstream, and generates third object-based side information by combining the first object-based side information and the second object-based side information. Thereafter, the multi-point control unit combiner <b>241</b> generates a bitstream by combining the third downmix signal and the third object-based side information and outputs the generated bitstream.</p>
<p id="p-0121" num="0120">Therefore, according to the tenth embodiment of the present invention, it is possible to efficiently process even signals transmitted by two or more communication partners compared to the case of encoding or decoding each object signal.</p>
<p id="p-0122" num="0121">In order for the multi-point control unit combiner <b>241</b> to incorporate a plurality of downmix signals, which are respectively extracted from a plurality of bitstreams and are associated with different compression codecs, into a single downmix signal, the downmix signals may need to be converted into pulse code modulation (PCM) signals or signals in a predetermined frequency domain according to the types of the compression codecs of the downmix signals, the PCM signals or the signals obtained by the conversion may need to be combined together, and a signal obtained by the combination may need to be converted using a predetermined compression codec. In this case, a delay may occur according to whether the downmix signals are incorporated into a PCM signal or into a signal in the predetermined frequency domain. The delay, however, may not be able to be properly estimated by a decoder. Therefore, the delay may need to be included in a bitstream and transmitted along with the bitstream. The delay may indicate the number of delay samples in a PCM signal or the number of delay samples in the predetermined frequency domain.</p>
<p id="p-0123" num="0122">During an object-based audio coding operation, a considerable number of input signals may sometimes need to be processed compared to the number of input signals generally processed during a typical multi-channel coding operation (e.g., a 5.1-channel or 7.1-channel coding operation). Therefore, an object-based audio coding method requires much higher bitrates than a typical channel-based multi-channel audio coding method. However, since an object-based audio coding method involves the processing of object signals which are smaller than channel signals, it is possible to generate dynamic output signals using an object-based audio coding method.</p>
<p id="p-0124" num="0123">An audio encoding method according to an embodiment of the present invention will hereinafter be described in detail with reference to <figref idref="DRAWINGS">FIGS. 17 through 20</figref>.</p>
<p id="p-0125" num="0124">In an object-based audio encoding method, object signals may be defined to represent individual sounds such as the voice of a human or the sound of a musical instrument. Alternatively, sounds having similar characteristics such as the sounds of stringed musical instruments (e.g., a violin, a viola, and a cello), sounds belonging to the same frequency band, or sounds classified into the same category according to the directions and angles of their sound sources, may be grouped together, and defined by the same object signals. Still alternatively, object signals may be defined using the combination of the above-described methods.</p>
<p id="p-0126" num="0125">A number of object signals may be transmitted as a downmix signal and side information. During the creation of information to be transmitted, the energy or power of a downmix signal or each of a plurality of object signals of the downmix signal is calculated originally for the purpose of detecting the envelope of the downmix signal. The results of the calculation may be used to transmit the object signals or the downmix signal or to calculate the ratio of the levels of the object signals.</p>
<p id="p-0127" num="0126">A linear predictive coding (LPC) algorithm may be used to lower bitrates. More specifically, a number of LPC coefficients which represent the envelope of a signal are generated through the analysis of the signal, and the LPC coefficients are transmitted, instead of transmitting envelop information regarding the signal. This method is efficient in terms of bitrates. However, since the LPC coefficients are very likely to be discrepant from the actual envelope of the signal, this method requires an addition process such as error correction. In short, a method that involves transmitting envelop information of a signal can guarantee a high quality of sound, but results in a considerable increase in the amount of information that needs to be transmitted. On the other hand, a method that involves the use of LPC coefficients can reduce the amount of information that needs to be transmitted, but requires an additional process such as error correction and results in a decrease in the quality of sound.</p>
<p id="p-0128" num="0127">According to an embodiment of the present invention, a combination of these methods may be used. In other words, the envelope of a signal may be represented by the energy or power of the signal or an index value or another value such as an LPC coefficient corresponding to the energy or power of the signal.</p>
<p id="p-0129" num="0128">Envelope information regarding a signal may be obtained in units of temporal sections or frequency sections. More specifically, referring to <figref idref="DRAWINGS">FIG. 17</figref>, envelope information regarding a signal may be obtained in units of frames. Alternatively, if a signal is represented by a frequency band structure using a filter bank such as a quadrature mirror filter (QMF) bank, envelope information regarding a signal may be obtained in units of frequency sub-bands, frequency sub-band partitions which are smaller entities than frequency sub-bands, groups of frequency sub-bands or groups of frequency sub-band partitions. Still alternatively, a combination of the frame-based method, the frequency sub-band-based method, and the frequency sub-band partition-based method may be used within the scope of the present invention.</p>
<p id="p-0130" num="0129">Still alternatively, given that low-frequency components of a signal generally have more information than high-frequency components of the signal, envelop information regarding low-frequency components of a signal may be transmitted as it is, whereas envelop information regarding high-frequency components of the signal may be represented by LPC coefficients or other values and the LPC coefficients or the other values may be transmitted instead of the envelop information regarding the high-frequency components of the signal. However, low-frequency components of a signal may not necessarily have more information than high-frequency components of the signal. Therefore, the above-described method must be flexibly applied according to the circumstances.</p>
<p id="p-0131" num="0130">According to an embodiment of the present invention, envelope information or index data corresponding to a portion (hereinafter referred to as the dominant portion) of a signal that appears dominant on a time/frequency axis may be transmitted, and none of envelope information and index data corresponding to a non-dominant portion of the signal may be transmitted. Alternatively, values (e.g., LPC coefficients) that represent the energy and power of the dominant portion of the signal may be transmitted, and no such values corresponding to the non-dominant portion of the signal may be transmitted. Still alternatively, envelope information or index data corresponding to the dominant portion of the signal may be transmitted, and values that represent the energy or power of the non-dominant portion of the signal may be transmitted. Still alternatively, information only regarding the dominant portion of the signal may be transmitted so that the non-dominant portion of the signal can be estimated based on the information regarding the dominant portion of the signal. Still alternatively, a combination of the above-described methods may be used.</p>
<p id="p-0132" num="0131">For example, referring to <figref idref="DRAWINGS">FIG. 18</figref>, if a signal is divided into a dominant period and a non-dominant period, information regarding the signal may be transmitted in four different manners, as indicated by (a) through (d).</p>
<p id="p-0133" num="0132">In order to transmit a number of object signals as the combination of a downmix signal and side information, the downmix signal needs to be divided into a plurality of elements as part of a decoding operation, for example, in consideration of the ratio of the levels of the object signals. In order to guarantee independence between the elements of the downmix signal, a decorrelation operation needs to be additionally performed.</p>
<p id="p-0134" num="0133">Object signals which are the units of coding in an object-based coding method have more independence than channel signals which are the units of coding in a multi-channel coding method. In other words, a channel signal includes a number of object signals, and thus needs to be decorrelated. On the other hand, object signals are independent from one another, and thus, channel separation may be easily performed simply using the characteristics of the object signals without a requirement of a decorrelation operation.</p>
<p id="p-0135" num="0134">More specifically, referring to <figref idref="DRAWINGS">FIG. 19</figref>, object signals A, B, and C take turns to appear dominant on a frequency axis. In this case, there is no need to divide a downmix signal into a number of signals according to the ratio of the levels of the object signals A, B, and C and to perform decorrelation. Instead, information regarding the dominant periods of the object signals A, B, and C may be transmitted, or a gain value may be applied to each frequency component of each of the object signals A, B, and C, thereby skipping decorrelation. Therefore, it is possible to reduce the amount of computation and to reduce the bitrate by the amount that would have otherwise been required by side information necessary for decorrelation.</p>
<p id="p-0136" num="0135">In short, in order to skip decorrelation, which is performed so as to guarantee independence among a number of signals obtained by dividing a downmix signal according to the ratio of the ratios of object signals of the downmix signal, information regarding a frequency domain including each object signal may be transmitted as side information. Alternatively, different gain values may be applied to a dominant period during which each object signal appears dominant and a non-dominant period during which each object signal appears less dominant, and thus, information regarding the dominant period may be mainly provided as side information. Still alternatively, the information regarding the dominant period may be transmitted as side information, and no information regarding the non-dominant period may be transmitted. Still alternatively, a combination of the above-described methods which are alternatives to a decorrelation method may be used.</p>
<p id="p-0137" num="0136">The above-described methods which are alternatives to a decorrelation method may be applied to all object signals or only to some object signals with easily distinguishable dominant periods. Also, the above-described methods which are alternatives to a decorrelation method may be variably applied in units of frames.</p>
<p id="p-0138" num="0137">The encoding of object audio signals using a residual signal will hereinafter be described in detail.</p>
<p id="p-0139" num="0138">In general, in an object-based audio coding method, a number of object signals are encoded, and the results of the encoding are transmitted as the combination of a downmix signal and side information. Then, a number of object signals are restored from the downmix signal through decoding according to the side information, and the restored object signals are appropriately mixed, for example, at the request of a user according to control information, thereby generating a final channel signal. An object-based audio coding method generally aims to freely vary an output channel signal according to control information with the aid of a mixer. However, an object-based audio coding method may also be used to generate a channel output in a predefined manner regardless of control information.</p>
<p id="p-0140" num="0139">For this, side information may include not only information necessary to obtain a number of object signals from a downmix signal but also mixing parameter information necessary to generate a channel signal. Thus, it is possible to generate a final channel output signal without the aid of a mixer. In this case, such an algorithm as residual coding may be used to improve the quality of sound.</p>
<p id="p-0141" num="0140">A typical residual coding method includes coding a signal and coding the error between the coded signal and the original signal, i.e., a residual signal. During a decoding operation, the coded signal is decoded while compensating for the error between the coded signal and the original signal, thereby restoring a signal that is as similar to the original signal as possible. Since the error between the coded signal and the original signal is generally inconsiderable, it is possible to reduce the amount of information additionally necessary to perform residual coding.</p>
<p id="p-0142" num="0141">If a final channel output of a decoder is fixed, not only mixing parameter information necessary for generating a final channel signal but also residual coding information may be provided as side information. In this case, it is possible to improve the quality of sound.</p>
<p id="p-0143" num="0142"><figref idref="DRAWINGS">FIG. 20</figref> is a block diagram of an audio encoding apparatus <b>310</b> according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 20</figref>, the audio encoding apparatus <b>310</b> is characterized by using a residual signal.</p>
<p id="p-0144" num="0143">More specifically, the audio encoding apparatus <b>310</b> includes an encoder <b>311</b>, a decoder <b>313</b>, a first mixer <b>315</b>, a second mixer <b>319</b>, an adder <b>317</b> and a bitstream generator <b>321</b>.</p>
<p id="p-0145" num="0144">The first mixer <b>315</b> performs a mixing operation on an original signal, and the second mixer <b>319</b> performs a mixing operation on a signal obtained by performing an encoding operation and then a decoding operation on the original signal. The adder <b>317</b> calculates a residual signal between a signal output by the first mixer <b>315</b> and a signal output by the second mixer <b>319</b>. The bitstream generator <b>321</b> adds the residual signal to side information and transmits the result of the addition. In this manner, it is possible to enhance the quality of sound.</p>
<p id="p-0146" num="0145">The calculation of a residual signal may be applied to all portions of a signal or only for low-frequency portions of a signal. Alternatively, the calculation of a residual signal may be variably applied only to frequency domains including dominant signals on a frame-by-frame basis. Still alternatively, a combination of the above-described methods may be used.</p>
<p id="p-0147" num="0146">Since the amount of side information including residual signal information is much greater than the amount of side information including no residual signal information, the calculation of a residual signal may be applied only to some portions of a signal that directly affect the quality of sound, thereby preventing an excessive increase in bitrate.</p>
<p id="p-0148" num="0147">The present invention can be realized as computer-readable code written on a computer-readable recording medium. The computer-readable recording medium may be any type of recording device in which data is stored in a computer-readable manner. Examples of the computer-readable recording medium include a ROM, a RAM, a CD-ROM, a magnetic tape, a floppy disc, an optical data storage, and a carrier wave (e.g., data transmission through the Internet). The computer-readable recording medium can be distributed over a plurality of computer systems connected to a network so that computer-readable code is written thereto and executed therefrom in a decentralized manner. Functional programs, code, and code segments needed for realizing the present invention can be easily construed by one of ordinary skill in the art.</p>
<p id="p-0149" num="0148">As described above, according to the present invention, sound images are localized for each object audio signal by benefiting from the advantages of object-based audio encoding and decoding methods. Thus, it is possible to offer more realistic sounds through the reproduction of object audio signals. In addition, the present invention may be applied to interactive games, and may thus provide a user with a more realistic virtual reality experience.</p>
<p id="p-0150" num="0149">While the present invention has been particularly shown and described with reference to exemplary embodiments thereof, it will be understood by those of ordinary skill in the art that various changes in form and details may be made therein without departing from the spirit and scope of the present invention as defined by the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An audio decoding method comprising:
<claim-text>receiving, by an audio decoding apparatus, a downmix signal comprising at least one object signal, and object-based side information generated when the at least one object signal is downmixed into the downmix signal, wherein the downmix signal is a stereo signal consisting of a left channel signal and a right channel signal;</claim-text>
<claim-text>receiving, by an audio decoding apparatus, control information for controlling position or level of at least one object signal;</claim-text>
<claim-text>generating, by an audio decoding apparatus, channel-based side information based on the object-based side information and the control information;</claim-text>
<claim-text>generating, by an audio decoding apparatus, parameter information using the object-based side information and the control information;</claim-text>
<claim-text>generating, by an audio decoding apparatus, a processed downmix signal by modifying the downmix signal using the parameter information wherein a position or level of at least one object signal is controlled based at least in part on the control information, and wherein the processed downmix signal is a stereo signal consisting of a left channel signal and a right channel signal; and</claim-text>
<claim-text>generating, by an audio decoding apparatus, a multi-channel audio signal using the processed downmix signal and the channel-based side information</claim-text>
<claim-text>wherein:</claim-text>
<claim-text>the downmix signal is modified in a frequency domain,</claim-text>
<claim-text>a number of channels of the processed downmix signal is equal to a number of channels of the downmix signal,</claim-text>
<claim-text>a number of channels of the multi-channel audio signal is larger than the number of channels of the processed downmix signal, and</claim-text>
<claim-text>the channel-based side information is used to generate a multi-channel signal from a mono signal or a stereo signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The audio decoding method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the downmix signal is modified by performing at least one of level adjustment, sound image processing and effect addition on the downmix signal.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The audio decoding method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processed downmix signal is generated using a decorrelated channel signal.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where in the audio decoding apparatus processes a plurality of object signals as a single sound source according to the position or level specified in the control information.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The audio decoding method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object-based side information comprises at least one of object level difference information, inter object cross correlation information, downmix gain information, downmix channel level difference information, and absolute object energy information.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The audio decoding method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the channel-based side information comprises at least one of channel level differences information, inter-channel correlation information, and channel prediction coefficient information.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. An audio decoding apparatus comprising:
<claim-text>a demultiplexer receiving a downmix signal comprising at least one object signal, and object-based side information generated when the at least one object signal is downmixed into the downmix signal, wherein the downmix signal is a stereo signal consisting of a left channel signal and a right channel signal;</claim-text>
<claim-text>a parameter converter configured to:</claim-text>
<claim-text>receive control information for controlling position or level of at least one object signal,</claim-text>
<claim-text>generate parameter information using the object based side information and the control information, and,</claim-text>
<claim-text>generate channel-based side information based on the object-based side information and the control information;</claim-text>
<claim-text>a downmix processor generating a processed downmix signal by modifying the downmix signal using the parameter information wherein a position or level of at least one object signal is controlled based at least in part on the control information, and wherein the processed downmix signal is a stereo signal consisting of a left channel signal and a right channel signal; and</claim-text>
<claim-text>a multi-channel decoder generating a multi-channel audio signal using the processed downmix signal and the channel-based side information,</claim-text>
<claim-text>wherein:</claim-text>
<claim-text>the downmix signal is modified in a frequency domain,</claim-text>
<claim-text>a number of channels of the processed downmix signal is equal to a number of channels of the downmix signal,</claim-text>
<claim-text>a number of channels of the multi-channel audio signal is larger than the number of channels of the processed downmix signal, and</claim-text>
<claim-text>the channel-based side information is used to generate a multi-channel signal from a mono signal or a stereo signal.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The audio decoding apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the downmix signal is modified by performing at least one of level adjustment, sound image processing and effect addition on the downmix signal.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The audio decoding apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the processed downmix signal is generated using a decorrelated channel signal.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The audio decoding apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, where in the audio decoding apparatus processes a plurality of object signals as a single sound source according to the position or level specified in the control information. </claim-text>
</claim>
</claims>
</us-patent-grant>
