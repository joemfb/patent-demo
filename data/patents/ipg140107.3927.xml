<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624995-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624995</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11802225</doc-number>
<date>20070521</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>KR</country>
<doc-number>10-2006-0047751</doc-number>
<date>20060526</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>1460</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>9</main-group>
<subgroup>73</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>222</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>3482231</main-classification>
<further-classification>348370</further-classification>
</classification-national>
<invention-title id="d2e71">Automatic white balancing method, medium, and system</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5448502</doc-number>
<kind>A</kind>
<name>Kindo et al.</name>
<date>19950900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382165</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5530474</doc-number>
<kind>A</kind>
<name>Takei</name>
<date>19960600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482241</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5684359</doc-number>
<kind>A</kind>
<name>Yano et al.</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>313487</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6359651</doc-number>
<kind>B1</kind>
<name>Yokonuma</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348370</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6504952</doc-number>
<kind>B1</kind>
<name>Takemura et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382167</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6665434</doc-number>
<kind>B1</kind>
<name>Yamaguchi</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382162</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6788813</doc-number>
<kind>B2</kind>
<name>Cooper</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382167</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7352895</doc-number>
<kind>B2</kind>
<name>Speigle et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382162</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7728880</doc-number>
<kind>B2</kind>
<name>Hung et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2003/0020826</doc-number>
<kind>A1</kind>
<name>Kehtarnavaz et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348362</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2006/0078182</doc-number>
<kind>A1</kind>
<name>Zwirn et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2006/0176379</doc-number>
<kind>A1</kind>
<name>Hyodo</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2007/0025718</doc-number>
<kind>A1</kind>
<name>Mori et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396155</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>JP</country>
<doc-number>05-068258</doc-number>
<date>19930300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>JP</country>
<doc-number>05-083728</doc-number>
<date>19930400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>JP</country>
<doc-number>2000-102030</doc-number>
<date>20000400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>JP</country>
<doc-number>2002-290988</doc-number>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>JP</country>
<doc-number>2005-33609</doc-number>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>JP</country>
<doc-number>2005-109930</doc-number>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>JP</country>
<doc-number>2005-236375</doc-number>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>JP</country>
<doc-number>2006-033158</doc-number>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Japanese Office Action dated Sep. 4, 2009, issued during examination of corresponding Japanese Patent Application No. 2007-120869.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>31</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>3482211</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482231</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348362-368</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3481131</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482241</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348370</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348371</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>19</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20070285530</doc-number>
<kind>A1</kind>
<date>20071213</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Sung-su</first-name>
<address>
<city>Yongin-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kang</last-name>
<first-name>Byoung-ho</first-name>
<address>
<city>Yongin-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Seong-deok</first-name>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Sung-su</first-name>
<address>
<city>Yongin-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Kang</last-name>
<first-name>Byoung-ho</first-name>
<address>
<city>Yongin-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Seong-deok</first-name>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Staas &#x26; Halsey LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Samsung Electronics Co., Ltd.</orgname>
<role>03</role>
<address>
<city>Suwon-Si</city>
<country>KR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Velez</last-name>
<first-name>Roberto</first-name>
<department>2662</department>
</primary-examiner>
<assistant-examiner>
<last-name>Le</last-name>
<first-name>Tuan</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A white balancing detecting method, medium, and system. The white balancing method includes setting an illuminant detection region of an input image in accordance with an exposure integration time indicative of a collected amount of light when the image is taken, and detecting an illuminant by using data contained in the illuminant detection region in a color gamut of the image.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="78.15mm" wi="173.23mm" file="US08624995-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="250.53mm" wi="175.68mm" file="US08624995-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="194.06mm" wi="96.44mm" file="US08624995-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="173.74mm" wi="131.23mm" orientation="landscape" file="US08624995-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="206.67mm" wi="95.93mm" file="US08624995-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="217.76mm" wi="109.90mm" file="US08624995-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="222.25mm" wi="104.99mm" file="US08624995-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="181.19mm" wi="164.25mm" file="US08624995-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="118.03mm" wi="105.41mm" file="US08624995-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="179.66mm" wi="109.39mm" file="US08624995-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="227.16mm" wi="153.16mm" orientation="landscape" file="US08624995-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="128.78mm" wi="109.56mm" file="US08624995-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="206.25mm" wi="96.77mm" file="US08624995-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is based on and claims priority from Korean Patent Application No. 10-2006-0047751, filed on May 26, 2006 in the Korean Intellectual Property Office, the disclosure of which is incorporated herein in its entirety by reference.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">One or more embodiments of the present invention relate to a color reproduction technology, and more particularly, to a white balancing method, medium, and system.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">Though natural Light is typically thought of as being white, in actuality the light may have an abundance of one or more wavelengths resulting in the overall light having a peculiar color called a color temperature, expressed in Kevin (K). In general, since the human beings' visual ability automatically adjusts for such minor discrepancies, the human beings' cognitive difference for the colors is very insignificant even though light of a particular color temperature may be illuminated. However, image pick-up devices, such as a camera or a camcorder, sense colors, in which color temperatures are reflected, as they are. Accordingly, if an illuminant is changed, images taken by the image pick-up device are tingled with different colors.</p>
<p id="p-0007" num="0006">For example, since the color temperature of sunlight around noon on a sunny day is considered to be high, the image taken by an image pick-up device will appear bluish on the whole. By contrast, since the color temperature of the sunlight just after sunrise or just before sunset is considered to be low, the image taken by the image pick-up device will appear reddish on the whole.</p>
<p id="p-0008" num="0007">An auto white balancing (AWB) technique proposed to solve this problem compensates for distortions of the color tone of the image if the image is deflected toward any one of red (R), green (G), and blue (B) components depending upon its color temperature.</p>
<p id="p-0009" num="0008">In one example, an image pick-up device discussed in Japanese Patent Unexamined Publication No. 2002-290988, divides an object into a plurality of regions, detects chromaticity in every region having a luminance higher than a threshold value, and calculates a gain value to perform white balancing based on the detected chromaticity.</p>
<p id="p-0010" num="0009">However, such white balancing techniques have problems in that it is difficult to perform a consistent color reproduction in accordance with the color or dimension of an object existing in the image even though the image is taken under the same light source or illuminant.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0011" num="0010">Accordingly, the present invention has been made to solve such above-mentioned problems, with an aspect of one or more embodiments of the present invention being to improve the performance of color reproduction through a more stable illuminant estimation.</p>
<p id="p-0012" num="0011">Additional aspects and/or advantages of the invention will be set forth in part in the description which follows and, in part, will be apparent from the description, or may be learned by practice of the invention.</p>
<p id="p-0013" num="0012">To achieve the above and/or other aspects and advantages, embodiments of the present invention include a method with white balancing, including setting an illuminant detection region of an image based on an exposure integration time indicative of an amount of light collected for the image when the image was captured, and detecting an illuminant of the image by using data relative to the illuminant detection region in a color gamut of the image.</p>
<p id="p-0014" num="0013">To achieve the above and/or other aspects and advantages, embodiments of the present invention include at least one medium including computer readable code to control at least one processing element to implement one or more embodiments of the present invention.</p>
<p id="p-0015" num="0014">To achieve the above and/or other aspects and advantages, embodiments of the present invention include a system, including a setting unit to set an illuminant detection region of an image based on an exposure integration time indicative of an amount of light collected for the image when the image was captured, and a detection unit to detect an illuminant of the image by using data relative to the illuminant detection region in a color gamut of the image.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0016" num="0015">These and/or other aspects and advantages of the invention will become apparent and more readily appreciated from the following description of the embodiments, taken in conjunction with the accompanying drawings of which:</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a white balancing system, according to an embodiment of the present invention;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a white balancing method, according to an embodiment of the present invention;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 3</figref> illustrates an illuminant detection region, according to an embodiment of the present invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a candidate region that can be set as an illuminant detection region, according to an embodiment of the present invention;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIGS. 5A through 5C</figref> are illustrations explaining a setting of an illuminant detection region, according to an embodiment of the present invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. 6A through 6E</figref> are illustrations explaining an obtaining of variation of color gamut and a central point, according to an embodiment of the present invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a detection unit, according to an embodiment of the present invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an operation of a division unit, according to an embodiment of the present invention;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an operation of a comparative value determination unit, according to an embodiment of the present invention;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 10</figref> is an illustration explaining the inconsistency between the illuminant distribution probability and a reference illuminant locus axis;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 11</figref> illustrates a correcting of an illuminant, according to an embodiment of the present invention; and</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIGS. 12A and 12B</figref> are illustrations explaining a correcting of an illuminant, according to an embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0029" num="0028">Reference will now be made in detail to embodiments of the present invention, examples of which are illustrated in the accompanying drawings, wherein like reference numerals refer to the like elements throughout. Embodiments are described below to explain the present invention by referring to the figures.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a white balancing system <b>100</b>, according to an embodiment of the present invention. The white balancing system <b>100</b> may include a setting unit <b>110</b>, a detection unit <b>120</b>, a stabilization unit <b>130</b>, and a white balancing unit <b>140</b>, for example. In differing embodiments, the white balancing system may be an image processing system such as a digital still camera, a digital video camcorder, and others, for example.</p>
<p id="p-0031" num="0030">In addition, white balancing operations illustrated in <figref idref="DRAWINGS">FIG. 2</figref> will be described with reference to the white balancing system <b>100</b>, noting that the reference to the white balancing system <b>100</b> is merely used as an example for instructive purposes. The setting unit <b>110</b> may set an illuminant detection region for an input image, for example, in operation S<b>210</b>, e.g., by selectively using an exposure integration time (EIT) as a reference to determine the illuminant detection region. The EIT means a time required for collecting light when photographing/taking an image. However, the EIT is not limited to a temporal element, and may be determined by other information which can predict a collected amount of the light when the image is photographed/taken. More specifically, for example, the EIT may be determined by exposure information such as a shutter speed or an aperture value.</p>
<p id="p-0032" num="0031">The EIT may also be provided together with an image to be input either at the time of the image photographing of the image or stored with or for the image for subsequent correction. For example, as the digital still camera attaches the exposure information at the time of the photographing of the image, such as shutter speed or aperture value, to the photographed image as additional data, the EIT may be attached to the image file, or imbedded in the image, as the additional data.</p>
<p id="p-0033" num="0032">Meanwhile, the illuminant detection region represents a range of data to be used to detect an illuminant of the image, an example of which is shown in <figref idref="DRAWINGS">FIG. 3</figref>. The detection unit <b>120</b> may detect the illuminant by using data to be contained in an illuminant detection region <b>320</b>, which may be set by the setting unit <b>110</b>, in a color gamut <b>310</b> of the image, for example, in operation S<b>220</b>.</p>
<p id="p-0034" num="0033">The illuminant detected by the detection unit <b>120</b> may reflect distorted information, e.g., in accordance with a deviation between devices or an amount of data sampled for detecting the illuminant, such that the stabilization unit <b>130</b> may further correct the detected illuminant so as to correct the distorted information, for example, in operation S<b>230</b>.</p>
<p id="p-0035" num="0034">The white balancing unit <b>140</b> may further perform white balancing on the image by use of the corrected illuminant, for example, in operation S<b>240</b>. Since there are diverse known techniques for performing the white balancing on the image, the detailed description thereof will be omitted herein.</p>
<p id="p-0036" num="0035">The example operation of setting the illuminant detection region for the input image can be performed by the setting unit <b>110</b> in <figref idref="DRAWINGS">FIG. 1</figref>, and can correspond to the operation S<b>210</b> of <figref idref="DRAWINGS">FIG. 2</figref>, for example, noting that alternative operations and units for accomplishing the same are equally available.</p>
<p id="p-0037" num="0036">As described above, the setting unit <b>110</b> may set the illuminant detection region associated with the EIT. In an embodiment, by statistically analyzing the relationship between the EIT consumed when the image is taken and the point, in which the illuminant exists, in the color gamut of the taken image, any of the candidate regions <b>410</b> through <b>440</b> having a high possibility that the illuminant exists may be previously set in accordance with the EIT, as shown in <figref idref="DRAWINGS">FIG. 4</figref>. For example, if the EIT of the input image belongs to a range of 0 to N1, the setting unit <b>110</b> may set the first candidate region <b>410</b> as the illuminant detection region. Meanwhile, if the EIT of the input image belongs to a range of N1 to N2, the setting unit <b>110</b> may set the second candidate region <b>420</b> as the illuminant detection region. Other candidate regions <b>430</b> and <b>440</b> may be set as the illuminant detection region associated with the EIT of the input image.</p>
<p id="p-0038" num="0037">According to the embodiment shown in <figref idref="DRAWINGS">FIG. 4</figref>, the candidate regions <b>410</b> through <b>440</b> may be fixed on chromaticity coordinates, and any one of the fixed candidate regions determined as the illuminant detection region. However, embodiments of the present invention are not limited thereto. For example, an alternative embodiment may be implemented which performs modeling of the point, in which the illuminant of the image exists, on the chromaticity coordinates in accordance with the EIT, and variably sets the illuminant detection region in accordance with the EIT of the input image. Such an embodiment will now be described in detail with reference to <figref idref="DRAWINGS">FIGS. 5A through 5C</figref>.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 5A</figref> illustrates chromatic values of illuminants of respective images associated with respective EITs. Such information may be obtained through previous experiments or during operation of a respective camera device or white balancing system.</p>
<p id="p-0040" num="0039">From such information, a modeling of a median chromaticity locus of the illuminant associated with the EIT can be performed. To this end, an average chromatic value of the illuminants corresponding to each EIT can be calculated, and a trend line <b>510</b> of the points representing each average chromatic value can be obtained. The obtained trend line can further be projected on the chromaticity coordinates to obtain the median chromaticity locus <b>520</b> of the illuminant, as shown in <figref idref="DRAWINGS">FIG. 5B</figref>.</p>
<p id="p-0041" num="0040">The illustrated median chromaticity locus <b>520</b> of the illuminant associated with any particular EIT may not be previously set, and the setting unit <b>110</b> can obtain the central illuminant point corresponding to the EIT of the image to be input from the median chromaticity locus <b>520</b> of the illuminant.</p>
<p id="p-0042" num="0041">In an embodiment, if the central illuminant point is obtained, the setting unit <b>110</b> may then set a given range as the illuminant detection region <b>540</b> based on the central illuminant point <b>530</b>, as shown in <figref idref="DRAWINGS">FIG. 5C</figref>. In one embodiment, the illuminant detection region <b>540</b> can be set from the central illuminant point <b>530</b> to the first threshold distance <b>550</b> in the median chromaticity locus <b>520</b> of the illuminant, and to the second threshold distance <b>560</b> in a direction perpendicular to the median chromaticity locus <b>520</b> of the illuminant. Herein, the first threshold distance and the second threshold distance may be set in accordance with the tendency of the chromaticity distribution of the illuminant associated with the EIT, which again may be previously determined by experiment or previous operation.</p>
<p id="p-0043" num="0042">The first threshold distance and the second threshold distance may be determined dynamically in accordance with the EIT. For example, if it is assumed that when the EIT is low, the chromaticity distributed range of the illuminant is narrow, while when the EIT is high, the chromaticity distributed range of the illuminant is wide, at least one of the first threshold distance and the second threshold distance may be altered in accordance with the EIT, so as to reflect this observed tendency. Here, alternate tendencies may also be observed depending on embodiment.</p>
<p id="p-0044" num="0043">According to one embodiment, the setting unit <b>110</b> may additionally use a variance of the color gamut of the image to be input and the central point of the color gamut, so as to set the illuminant detection region, as shown in <figref idref="DRAWINGS">FIG. 6A</figref>.</p>
<p id="p-0045" num="0044">In order to obtain the variance of the color gamut, the setting unit <b>110</b> may select the illustrated threshold number of data <b>610</b>-<b>1</b> through <b>610</b>-<b>4</b> in near order from four reference points O, P, Q, and R on the chromaticity coordinates in the color gamut of the image to be input.</p>
<p id="p-0046" num="0045">The four reference points, according to an embodiment of the present invention, include, as shown in <figref idref="DRAWINGS">FIG. 6B</figref>, an origin O (0,0) of the chromaticity coordinates, a point P (Cr-max, 0) indicative of the maximum Cr value, Cr-max, which can be possessed by a general image on a Cr-axis, a point Q (Cb-max, 0) indicative of the maximum Cb value, Cb-max, which can be possessed by a general image on a Cb-axis, and a coordinate point R (Cr-max, Cb-max) indicative of the maximum Cr value and the maximum Cb value.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 6C</figref> illustrates four such reference points, according to alternative embodiment of the present invention, with two reference points O (0,0) and R(Cr-max, Cb-max) among four reference points being similar to those of the embodiment in <figref idref="DRAWINGS">FIG. 6B</figref>. However, the other two reference points P and Q are cross points formed by the axes of two chromaticity coordinates and an extension of the reference illuminant locus <b>620</b>. The reference illuminant locus <b>620</b> means a trend line based on the chromaticity of diverse types of standard illuminants (e.g., D65, D50, CWF (Cool White Fluorescent), A, Horizon, and others) which are proper to characteristics of a device (e.g., a digital still camera including a white balancing system <b>100</b>) capturing an image.</p>
<p id="p-0048" num="0047">As illustrated in <figref idref="DRAWINGS">FIG. 6D</figref>, if the threshold number of data is selected in near order from each reference point, the setting unit <b>110</b> may determine edge points <b>630</b>-<b>1</b>, <b>630</b>-<b>2</b>, <b>630</b>-<b>3</b>, and <b>630</b>-<b>4</b> having the average chromatic value of the data every selected data through each reference point.</p>
<p id="p-0049" num="0048">After that, as illustrated in <figref idref="DRAWINGS">FIG. 6E</figref>, the setting unit <b>110</b> may calculate a distance between the edge points derived from the diagonal reference points among four reference points. That is, the setting unit <b>110</b> may calculate a distance <b>640</b> (referred to as a color gamut height) between the edge point <b>630</b>-<b>1</b> derived from the reference point O and the edge point <b>630</b>-<b>4</b> derived from the reference point R, and a distance <b>650</b> (referred to as a color gamut width) between the edge point <b>630</b>-<b>2</b> derived from the reference point P and the edge point <b>630</b>-<b>3</b> derived from the reference point Q.</p>
<p id="p-0050" num="0049">Then, the setting unit <b>110</b> may determine whether the color gamut height <b>540</b> and the color gamut width <b>650</b> exist in a given threshold range, respectively. If the color gamut height <b>540</b> and the color gamut width <b>650</b> satisfy the given threshold range, the setting unit <b>110</b> may use the illuminant detection region determined in accordance with the EIT as it is, since it may be considered that the input image has a normal color distribution. However, if the color gamut height <b>540</b> and the color gamut width <b>650</b> are found to be outside of the threshold range, it can be understood that the input image has an abnormal color distribution, since the color gamut of the input image is excessively wider or narrower than a normal case. In this instance, where the color gamut height <b>540</b> and the color gamut width <b>650</b> are outside of the threshold range, if a portion of the color gamut of the input image were to be determined to be the illuminant detection region in accordance with the EIT, there is a high possibility that the correct illuminant would not be detected. Accordingly, the setting unit <b>110</b> can set the color gamut of the input image as the illuminant detection region irrespective of the EIT, such as through conventional techniques, after such a detection that the color gamut height or the color gamut width is out of the threshold range.</p>
<p id="p-0051" num="0050">Since the variance of the color gamut indicates the uniformity of the color gamut, embodiments of the present invention are not limited to the above described methods of calculating the variance of the color gamut. For example, the setting unit <b>110</b> may select the threshold number of data in the color gamut of the input image in near order from four reference points, and predict the variance of the color gamut by use of the distance between the points having the mean chromaticity of the selected data. Alternate methods are also available.</p>
<p id="p-0052" num="0051">Returning to <figref idref="DRAWINGS">FIG. 6E</figref>, in another embodiment, the central point of the color gamut can be determined as a cross point <b>660</b> of a segment representing the color gamut height and the color gamut width. The setting unit <b>110</b> may determine whether the use of the illuminant detection region determined in accordance with the EIT is appropriate through the central point <b>660</b> of the color gamut.</p>
<p id="p-0053" num="0052">For example, modeling can be performed of the point, on which the illuminant of the image exists, on the chromaticity coordinates in accordance with the central point <b>660</b> of the color gamut, similar to the method of modeling the point, on which the illuminant of the image exists, on the chromaticity coordinates in accordance with the EIT. That is, the point, on which the illuminant of the image can exist, may be set as a desired number of regions on the chromaticity coordinates in accordance with the central point <b>660</b> of the color gamut.</p>
<p id="p-0054" num="0053">Then, it can be determined whether the use of the illuminant detection region determined by the EIT is appropriate, through whether the region on the chromaticity coordinates corresponding to the central point <b>660</b> of the color gamut of the input image overlaps the illuminant detection region determined in accordance with the EIT. If the region on the chromaticity coordinates corresponding to the central point <b>660</b> of the color gamut is identical or sufficiently similar to the illuminant detection region determined by the EIT, the illuminant detection region determined by the EIT can be used as it is. However, if the region on the chromaticity coordinates corresponding to the central point <b>660</b> of the color gamut is not identical or sufficiently similar to the illuminant detection region determined by the EIT, the color gamut of the input image can be set as the illuminant detection region irrespective of the EIT.</p>
<p id="p-0055" num="0054">An operation of detecting the illuminant may, thus, be performed by the detection unit <b>120</b> in <figref idref="DRAWINGS">FIG. 1</figref>, for example, and may correspond to operation S<b>220</b> in <figref idref="DRAWINGS">FIG. 2</figref>, also for example, noting that alternative operations and units for accomplishing the same are equally available. The detection unit <b>120</b> may include, as shown in <figref idref="DRAWINGS">FIG. 7</figref>, a division unit <b>710</b>, a comparative value determination unit <b>720</b>, and an illuminant estimation unit <b>730</b>, for example.</p>
<p id="p-0056" num="0055">The division unit <b>710</b> may divide the data contained in the illuminant detection region, e.g., as set by the setting unit <b>110</b> in the color gamut of an input image, into two groups on the basis of luminance. <figref idref="DRAWINGS">FIG. 8</figref> illustrates an example operation of the division unit <b>710</b>, noting that alternative operations and units for accomplishing the same are equally available.</p>
<p id="p-0057" num="0056">In reference to <figref idref="DRAWINGS">FIG. 8</figref>, an average luminance value and a mean luminance value of the data contained in the illuminant detection region may be calculated, in operation S<b>810</b>. Threshold number of data (referred to as superior luminance data) may be further sorted in order of high luminance value among luminance distribution of the data contained in the illuminant detection region, and the same number of data as that of the superior luminance data in order of low luminance value. In an embodiment, the average luminance value Y<sub>thresh </sub>may be calculated from a mean between an average value of the luminance values of the upper luminance data and an average value of the luminance values of lower luminance data. In addition, the mean luminance value Y<sub>avg </sub>may be calculated by an average of the luminance values of all data contained in the illuminant detection region.</p>
<p id="p-0058" num="0057">Then, a threshold luminance value may be calculated to be used to divide the data in the illuminant detection region into two groups by using the average luminance value and the mean luminance value, in operation S<b>820</b>. The threshold luminance value may be determined by a weighted sum of the average luminance value of the data contained in the illuminant detection region and the mean luminance value thereof, which may be expressed by the below Equation 1, for example.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Y</i><sub>thresh</sub><i>=k&#xb7;Y</i><sub>median</sub>+(1&#x2212;<i>k</i>)&#xb7;<i>Y</i><sub>avg</sub>&#x2003;&#x2003;Equation 1:<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0059" num="0058">Here, Y<sub>thresh </sub>is a threshold luminance value to be calculated, Y<sub>median </sub>is a median luminance value, and Y<sub>avg </sub>is an average luminance value. In addition, k may be a weighted value of 0 or 1, for example.</p>
<p id="p-0060" num="0059">If the threshold luminance value is calculated, the data in the illuminant detection region may be divided into two groups on the basis of the threshold luminance value, in operation S<b>830</b>. For example, the data having a luminance more than a threshold luminance value among the data in the illuminant detection region may be classified into the first group, and the data having a luminance less than a threshold luminance value may be classified into the second group.</p>
<p id="p-0061" num="0060">The comparative value determination unit <b>720</b> in <figref idref="DRAWINGS">FIG. 7</figref> may set a comparative illuminant to be a standard of determining the illuminant. <figref idref="DRAWINGS">FIG. 9</figref> illustrates an example operation of the comparative value determination unit <b>720</b>, noting that alternative operations and units for accomplishing the same are equally available.</p>
<p id="p-0062" num="0061">In reference to <figref idref="DRAWINGS">FIG. 9</figref>, an average may be calculated of an average chromatic value of the data contained in the illuminant detection region and a mean chromatic value thereof, in operation S<b>910</b>. Here, for example, the average chromatic value may be calculated by the average of the chromatic values of the data contained in the illuminant detection region, and the median chromatic value may be calculated by a chromatic value of the center point of the illuminant detection region.</p>
<p id="p-0063" num="0062">Then, a weighted average may be calculated of the average chromatic value and the median chromatic value, in operation S<b>920</b>, as expressed by the below Equation 2, for example.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Ch</i><sub>w</sub><i>=m&#xb7;Ch</i><sub>avg</sub>+(1<i>&#x2212;m</i>)&#xb7;<i>Ch</i><sub>median</sub>&#x2003;&#x2003;Equation 2:<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0064" num="0063">Here, Ch<sub>w </sub>is a weighted average to be calculated, Ch<sub>avg </sub>is an average chromatic value, and Ch<sub>median </sub>is a median chromatic value. In addition, m may be a weighted value of 0 or 1, for example.</p>
<p id="p-0065" num="0064">An average may be calculated of chromatic values of the data contained in each of two divided groups, e.g., as divided by the division unit <b>710</b>, in operation S<b>930</b>. Hereinafter, the average of the chromatic values of the data contained in the first group will be referred to as the first average, and the average of the chromatic values of the data contained in the second group will be referred to as the second average.</p>
<p id="p-0066" num="0065">A difference value may further be calculated between the first average and the second average, in operation S<b>940</b>. Next, a comparative illuminant may be set by using the weighted average Ch<sub>w</sub>, e.g., calculated in the operation S<b>920</b>, the difference value, e.g., calculated in the operation S<b>940</b>, and a standard illuminant (e.g., D65, D50, CWF, A, Horizon, and the others) of a device providing a corresponding image frame (e.g., a digital still camera comprising the white balancing system <b>100</b>) as an input value, in operation S<b>950</b>. In order to obtain the comparative illuminant, the below Equation 3 may be used, for example.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i><sub>ref</sub>(<i>r,b</i>)=<i>F</i><sub>2</sub>(<i>F</i><sub>1</sub>(<i>Ch</i><sub>w</sub><i>,DEV</i><sub>w</sub>),<i>Ch</i><sub>dist</sub>)&#x2003;&#x2003;Equation 3:<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0067" num="0066">Here, W<sub>ref</sub>(r,b) is a chromatic value of the comparative illuminant, Ch<sub>w </sub>is a weighted average calculated in operation S<b>920</b>, for example, DEV<sub>w </sub>is a standard illuminant, and Ch<sub>dist </sub>is a difference value between the first average and the second average calculated in the operation S<b>940</b>, for example. In addition, F1 may be a quadratic correlation function, and F2 may be a linear correlation function.</p>
<p id="p-0068" num="0067">The correlation function F1 may be a function reflecting a correlation between the standard illuminants and Ch<sub>w </sub>under the standard illuminant, and a substantial comparative estimating function to estimate the point of the illuminant in the image, for example. The correlation function F2 may be a modeling function considering Ch<sub>dist </sub>in the standard illuminant locus function, and a function to compensate a performance of a comparative illuminant estimation of the correlation function F1, for example.</p>
<p id="p-0069" num="0068">The order of functions F1 and F2 can be varied, and F1 and F2 may be set as a quadratic function and a linear function, respectively, as one example of optimizing its complexity. A concrete embodiment of F1 and F2 can be easily understood through the below Equations 4 through 6, for example.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3a3;(|<i>DEV</i><sub>w</sub><i>&#x2212;&#x3b1;*Ch</i><sub>w</sub><sup>2</sup><i>&#x2212;&#x3b2;*Ch</i><sub>w</sub>&#x2212;&#x3b3;|)&#x2245;0&#x2003;&#x2003;Equation 4:<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>F</i><sub>1</sub><i>=&#x3b1;*Ch</i><sub>w</sub><sup>2</sup><i>+&#x3b2;*Ch</i><sub>w</sub>+&#x3b3;&#x2003;&#x2003;Equation 5:<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>F</i><sub>2</sub>=&#x3b8;*(<i>F</i><sub>1</sub>(<i>Ch</i><sub>w</sub><i>,DEV</i><sub>w</sub>)&#xb1;<i>Ch</i><sub>dist</sub>)+&#x3b6;&#x2003;&#x2003;Equation 6:<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0070" num="0069">Here, in Equations 4 through 6, &#x3b1;, &#x3b2;, &#x3b3;, &#x3b8;, and &#x3b6; are a certain real number, and may be determined as a proper value based on experiment results. For example, &#x3b1;, &#x3b2;, and &#x3b3; may preferably exist as in the relation in Equation 4, noting that alternative embodiments are equally available.</p>
<p id="p-0071" num="0070">Referring again to <figref idref="DRAWINGS">FIG. 7</figref>, in identifying the initial illuminant, the illuminant estimation unit <b>730</b> may determine the point, in which a chromatic difference between the comparative illuminant, e.g., as determined in the above operation S<b>950</b>, and any one of the first average and the second average, e.g., as calculated in the above operation S<b>940</b>, as the initial illuminant. That is, the illuminant estimation unit <b>730</b> may calculate a chromatic difference (referred to as first chromatic difference) between the first average and the comparative illuminant, and a chromatic difference (referred to as second chromatic difference) between the second average and the comparative illuminant, and compare the first chromatic difference and the second chromatic difference. If the first chromatic difference is smaller than the second chromatic difference, the first average may be determined to be the initial illuminant, while if the second chromatic difference is smaller than the first chromatic difference, the second average may be determined to be the initial illuminant.</p>
<p id="p-0072" num="0071">With reference to <figref idref="DRAWINGS">FIG. 10</figref>, distorted information may be reflected in the illuminant detected by the detection unit <b>120</b> in accordance with the deflection between devices providing the image or an amount of data sampled to detect the illuminant. For example, if the illuminant is detected according to the data contained in the illuminant detection region, the position of the illuminant detection region may be determined to be in the specified range in accordance with the EIT, but the amount of data or the chromaticity information to be input into the illuminant detection region may be varied depending upon the chromaticity variation of the device providing the image. In this instance, since the illustrated probability distribution <b>1020</b> in which the illuminant information exists in the illuminant detection region <b>1010</b> may not conform to the reference illuminant locus axis <b>1030</b>, it becomes desirable to provide a new illuminant locus axis <b>1040</b> by adjusting the reference illuminant locus axis <b>1030</b>.</p>
<p id="p-0073" num="0072">In order to compensate this distortion phenomenon, the stabilization unit <b>130</b>, for example, may stabilize the initial illuminant detected by the detection unit <b>120</b> based on the reference illuminant locus and the average chromatic value of data input into the illuminant detection region in the color gamut of the input image. <figref idref="DRAWINGS">FIG. 11</figref> illustrates an example operation of the stabilization unit <b>130</b>, noting that alternative operations and units for accomplishing the same are equally available.</p>
<p id="p-0074" num="0073">In reference to <figref idref="DRAWINGS">FIG. 11</figref>, the stabilization unit <b>130</b> may calculate a weighted average point between the average chromatic value of data input into the illuminant detection region in the color gamut of the image and the chromatic value of the initial illuminant, in operation S<b>1110</b>. The weighted average may be expressed by the below Equation 7, for example.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i><sub>avg</sub><i>=N&#xb7;W</i><sub>i</sub>+(1&#x2212;<i>N</i>)&#xb7;<i>Ch</i><sub>avg</sub>&#x2003;&#x2003;Equation 7:<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0075" num="0074">Here, W<sub>avg </sub>is a weighted value to be calculated, W<sub>i </sub>is a chromatic value of the initial illuminant, e.g., as determined by the illuminant estimation unit <b>730</b>, and Ch<sub>avg </sub>is an average of chromatic values of the data contained in the illuminant detection region. Ch<sub>avg </sub>is also used in Equation 2. In addition, N may be a weighted value of 0 or 1, for example.</p>
<p id="p-0076" num="0075">As shown in <figref idref="DRAWINGS">FIG. 12A</figref>, a new illuminant locus <b>1230</b> which contains a weighted average point <b>1210</b> and is parallel with the reference illuminant locus <b>1220</b> may then be set, in operation S<b>1120</b>.</p>
<p id="p-0077" num="0076">Further, as shown in <figref idref="DRAWINGS">FIG. 12B</figref>, a point <b>1250</b> in which the initial illuminant <b>1240</b> is projected on the new illuminant locus <b>1230</b> in a vertical direction may be determined, in operation S<b>1130</b>.</p>
<p id="p-0078" num="0077">In embodiments of the present invention, the term &#x201c;unit&#x201d; indicating a respective component of the white balancing system <b>100</b>, for example, may be constructed as a module, for example. Here, the term &#x201c;module&#x201d;, as used herein, means, but is not limited to, a software and/or hardware component, such as a Field Programmable Gate Array (FPGA) or Application Specific Integrated Circuit (ASIC), which performs certain tasks. A module may advantageously be configured to reside on the addressable storage medium and configured to execute on one or more processors. Thus, a module may include, by way of example, components, such as software components, object-oriented software components, class components and task components, processes, functions, attributes, procedures, subroutines, segments of program code, drivers, firmware, microcode, circuitry, data, databases, data structures, tables, arrays, and variables. The operation provided for in the components and modules may be combined into fewer components and modules or further separated into additional components and modules.</p>
<p id="p-0079" num="0078">In addition to the above described embodiments, embodiments of the present invention can also be implemented through computer readable code/instructions in/on a medium, e.g., a computer readable medium, to control at least one processing element to implement any above described embodiment. The medium can correspond to any medium/media permitting the storing and/or transmission of the computer readable code.</p>
<p id="p-0080" num="0079">The computer readable code can be recorded/transferred on a medium in a variety of ways, with examples of the medium including recording media, such as magnetic storage media (e.g., ROM, floppy disks, hard disks, etc.) and optical recording media (e.g., CD-ROMs, or DVDs), and transmission media such as carrier waves, as well as through the Internet, for example. Thus, the medium may further be a signal, such as a resultant signal or bitstream, according to embodiments of the present invention. The media may also be a distributed network, so that the computer readable code is stored/transferred and executed in a distributed fashion. Still further, as only an example, the processing element could include a processor or a computer processor, and processing elements may be distributed and/or included in a single device.</p>
<p id="p-0081" num="0080">In accordance with the above description, one or more embodiments of the present invention include a white balancing method, medium, and system, where the color reproducing performance can be improved through more stabilized illuminant estimation.</p>
<p id="p-0082" num="0081">Although a few embodiments of the present invention have been shown and described, it would be appreciated by those skilled in the art that changes may be made in these embodiments without departing from the principles and spirit of the invention, the scope of which is defined in the claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method with white balancing, comprising:
<claim-text>setting an illuminant detection region within a chrominance space of an image based on an exposure integration time (EIT) indicative of an amount of light collected for the image when the image was captured, the EIT being used to determine a range of the illuminant detection region; and</claim-text>
<claim-text>detecting an illuminant of the image, by using data relative to the set illuminant detection region, within a color gamut of the image corresponding to the set illuminant detection region.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the setting of the illuminant detection region comprises determining a candidate region which corresponds to the exposure integration time, from among a plurality of predefined candidate regions set in accordance with respective exposure integration times, as the illuminant detection region.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the setting of the illuminant detection region comprises determining a given range, as the illuminant detection region, from a point corresponding to the exposure integration time in a specified locus on predefined chromaticity coordinates set based on a degree of illuminant distribution according to respective exposure integration times.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the given range is variable depending upon the exposure integration time.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the setting of the illuminant detection region further comprises determining the illuminant detection region by using at least one of a variance of the color gamut of the image, obtained using reference points in the chrominance space, and a center point of the color gamut.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the determining of the illuminant detection region comprises adjusting the illuminant detection region based on whether the variance of the color gamut satisfies a specified threshold range.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the detecting of the illuminant comprises:
<claim-text>dividing the data relative to the illuminant detection region into a first group and a second group in accordance with luminance distribution of the data relative to the illuminant detection region in the color gamut of the image; and</claim-text>
<claim-text>determining at least one of a first average luminance value of data relative to the first group and a second average luminance value of data relative to the second group as the illuminant.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the dividing comprises dividing the data relative to the illuminant detection region into the first group and the second group based on a threshold luminance value derived from a weighted sum of an average luminance value of the data relative to the illuminant detection region and a median luminance value of the illuminant detection region.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the determining of the at least one of the first average luminance value and the second average luminance value comprises determining any one of the first average luminance value and the second average luminance value as the illuminant, in which when a chromatic difference between the first average luminance value and a given comparative illuminant and a chromatic difference between the second average luminance value and the given comparative illuminant are compared, based on a respective lowest chromatic difference.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the comparative illuminant is obtained from a correlation function using at least one among a weighted average value between an average chromatic value of the data relative to the illuminant detection region and a median chromatic value of the illuminant detection region, a difference value between the first average luminance value and the second average luminance value, and standard illuminant information of a system which captured the image, as an input value.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising compensating the detected illuminant.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the compensating comprises modifying the detected illuminant by setting a point, in which the detected illuminant is perpendicularly reflected on a given illuminant locus, as the detected illuminant.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the given illuminant locus is parallel to a trend line of a plurality of standard illuminants associated with a characteristic of a system which captured the image, and has a weighted average point of a chromatic value of the detected illuminant and an average chromatic value of the data relative to the set illuminant detection region.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising performing white balancing on the image based on the determined illuminant.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. At least one non-transitory medium comprising computer readable code to control at least one processing element to implement the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A method with white balancing, comprising:
<claim-text>setting an illuminant detection region within a chrominance space of an image based on an exposure integration time indicative of an amount of light collected for the image when the image was captured; and</claim-text>
<claim-text>detecting an illuminant of the image, by using data relative to the set illuminant detection region, within a color gamut of the image corresponding to the set illuminant detection region,</claim-text>
<claim-text>wherein the detecting of the illuminant comprises:</claim-text>
<claim-text>dividing the data relative to the illuminant detection region into a first group and a second group in accordance with luminance distribution of the data relative to the illuminant detection region in the color gamut of the image; and</claim-text>
<claim-text>determining at least one of a first average luminance value of data relative to the first group and a second average luminance value of data relative to the second group as the illuminant,</claim-text>
<claim-text>wherein the determining of the at least one of the first average luminance value and the second average luminance value comprises determining any one of the first average luminance value and the second average luminance value as the illuminant, in which when a chromatic difference between the first average luminance value and a given comparative illuminant and a chromatic difference between the second average luminance value and the given comparative illuminant are compared, based on a respective lowest chromatic difference,</claim-text>
<claim-text>wherein the comparative illuminant is obtained from a correlation function using at least one among a weighted average value between an average chromatic value of the data relative to the illuminant detection region and a median chromatic value of the illuminant detection region, a difference value between the first average luminance value and the second average luminance value, and standard illuminant information of a system which captured the image, as an input value,</claim-text>
<claim-text>wherein the correlation function is a linear correlation function using a resultant value of a sub-correlation function and the difference value as an input value, and the sub-correlation function is a quadratic correlation function using the weighted average value and the standard illuminant information as an input value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A system, comprising:
<claim-text>a setting unit to set an illuminant detection region within a chrominance space of an image based on an exposure integration time (EIT) indicative of an amount of light collected for the image when the image was captured, the EIT being used to determine a range of the illuminant detection region; and</claim-text>
<claim-text>a detection unit to detect an illuminant of the image, by using data relative to the set illuminant detection region, within a color gamut of the image corresponding to the set illuminant detection region.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the setting unit determines a candidate region which corresponds to the exposure integration time, from among a plurality of predefined candidate regions set in accordance with respective exposure integration times, as the illuminant detection region.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref> wherein the setting unit determines a given range, as the illuminant detection region, from a point corresponding to the exposure integration time in a specified locus on predefined chromaticity coordinates set based on a degree of illuminant distribution according to respective exposure integration times.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the given range is variable depending upon the exposure integration time.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the setting unit determines the illuminant detection region by using at least one of a variance of the color gamut of the image, obtained using reference points in the chrominance space, and a center point of the color gamut.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the setting unit adjusts the illuminant detection region according to whether the variance of the color gamut satisfies a specified threshold range.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the setting unit comprises:
<claim-text>a division unit to divide the data relative to the illuminant detection region into a first group and a second group in accordance with luminance distribution of the data relative to the illuminant detection region in the color gamut of the image; and</claim-text>
<claim-text>an illuminant determination unit to determine at least one of a first average luminance value of data relative to the first group and a second average luminance value of the data relative to the second group as the illuminant.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the division unit divides the data relative to the illuminant detection region into the first group and the second group based on a threshold luminance value derived from a weighted sum of an average luminance value of the data relative to the illuminant detection region and a median luminance value of the illuminant detection region.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the illuminant determination unit determines any one of the first average luminance value and the second average luminance value as the illuminant, in which when a chromatic difference between the first average luminance value and a given comparative illuminant and a chromatic difference between the second average luminance value and the given comparative illuminant are compared, based on a respective lowest chromatic difference.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the comparative illuminant is obtained from a correlation function using at least one among a weighted average value between an average chromatic value of the data relative to the illuminant detection region and a median chromatic value of the illuminant detection region, a difference value between the first average luminance value and the second average luminance value, and standard illuminant information of a system which captured the image, as an input value.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising a stabilization unit compensating the detected illuminant.</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the stabilization unit modifies the detected illuminant by setting a point, in which the detected illuminant is perpendicularly reflected on a given illuminant locus, as the detected illuminant.</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the given illuminant locus is parallel to a trend line of a plurality of standard illuminants associated with a characteristic of a system which captured the image, and, has a weighted average point of a chromatic value of the detected illuminant and an average chromatic value of the data relative to the illuminant detection region.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising a white balancing unit which performs white balancing on the image based on the detected illuminant.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. A system, comprising:
<claim-text>a setting unit to set an illuminant detection region within a chrominance space of an image based on an exposure integration time indicative of an amount of light collected for the image when the image was captured; and</claim-text>
<claim-text>a detection unit to detect an illuminant of the image, by using data relative to the set illuminant detection region, within a color gamut of the image corresponding to the set illuminant detection region,</claim-text>
<claim-text>wherein the setting unit comprises:</claim-text>
<claim-text>a division unit to divide the data relative to the illuminant detection region into a first group and a second group in accordance with luminance distribution of the data relative to the illuminant detection region in the color gamut of the image; and</claim-text>
<claim-text>an illuminant determination unit to determine at least one of a first average luminance value of data relative to the first group and a second average luminance value of the data relative to the second group as the illuminant,</claim-text>
<claim-text>wherein the illuminant determination unit determines any one of the first average luminance value and the second average luminance value as the illuminant, in which when a chromatic difference between the first average luminance value and a given comparative illuminant and a chromatic difference between the second average luminance value and the given comparative illuminant are compared, based on a respective lowest chromatic difference,</claim-text>
<claim-text>wherein the comparative illuminant is obtained from a correlation function using at least one among a weighted average value between an average chromatic value of the data relative to the illuminant detection region and a median chromatic value of the illuminant detection region, a difference value between the first average luminance value and the second average luminance value, and standard illuminant information of a system which captured the image, as an input value,</claim-text>
<claim-text>wherein the correlation function is a linear correlation function using a resultant value of a sub-correlation function and the difference value as an input value, and the sub-correlation function is a quadratic correlation function using the weighted average value and the standard illuminant information as an input value. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
