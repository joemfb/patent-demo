<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624717-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624717</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12862950</doc-number>
<date>20100825</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>682</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>B</section>
<class>60</class>
<subclass>Q</subclass>
<main-group>1</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>340435</main-classification>
<further-classification>340436</further-classification>
<further-classification>3405731</further-classification>
<further-classification>340576</further-classification>
</classification-national>
<invention-title id="d2e53">Image processor, storage medium storing an image processing program and vehicle-mounted terminal</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>EP</country>
<doc-number>0 962 906</doc-number>
<kind>A2</kind>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>JP</country>
<doc-number>11-353597</doc-number>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>JP</country>
<doc-number>2001-43494</doc-number>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>JP</country>
<doc-number>2001-101566</doc-number>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>JP</country>
<doc-number>2001-155297</doc-number>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>JP</country>
<doc-number>2005-178623</doc-number>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-cpc-text>B60K 35/00</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>JP</country>
<doc-number>2007-69777</doc-number>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>JP</country>
<doc-number>2007-102691</doc-number>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00009">
<othercit>International Search Report for PCT/JP2008/053273, mailed Apr. 22, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>11</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>16</number-of-drawing-sheets>
<number-of-figures>19</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>PCT/JP2008/053273</doc-number>
<date>20080226</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12862950</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100315214</doc-number>
<kind>A1</kind>
<date>20101216</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Yano</last-name>
<first-name>Katsutoshi</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kawai</last-name>
<first-name>Jun</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Yamada</last-name>
<first-name>Hiroshi</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Yano</last-name>
<first-name>Katsutoshi</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Kawai</last-name>
<first-name>Jun</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Yamada</last-name>
<first-name>Hiroshi</first-name>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Staas &#x26; Halsey LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Fujitsu Limited</orgname>
<role>03</role>
<address>
<city>Kawasaki</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Ghayour</last-name>
<first-name>Mohammad</first-name>
<department>2687</department>
</primary-examiner>
<assistant-examiner>
<last-name>Wilson</last-name>
<first-name>Brian</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An image processor (<b>3</b>) includes: an image input unit (<b>7</b>) that obtains an image captured by an imaging device (<b>2</b>) installed on a vehicle; a vehicle information input unit (<b>8</b>) that obtains the distance between the vehicle and a road junction or curve from a navigation system (<b>5</b>); a recording unit (<b>15</b>) in which imaging information and driver information are pre-stored; a magnifying object recognition unit (<b>9</b>) that recognizes a certain magnifying object in the obtained captured image; and a composing unit (<b>11</b>) that produces, when a magnifying object is recognized, a composite image of an magnified image and the captured image. The composing unit (<b>11</b>) uses the distance obtained by the vehicle information input unit (<b>8</b>) as well as the imaging information and the driver information stored in the recording unit (<b>15</b>) to calculate an area in the captured image that is not an image of a blind spot for the driver, and produces the composite image such that the magnified image is superimposed on the non-blind spot area. This allows the driver to perceive information on the blind spot area and on the magnified image through a single action.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="226.31mm" wi="159.26mm" file="US08624717-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="245.62mm" wi="148.42mm" orientation="landscape" file="US08624717-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="208.28mm" wi="160.19mm" file="US08624717-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="93.64mm" wi="122.85mm" file="US08624717-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="190.84mm" wi="160.70mm" orientation="landscape" file="US08624717-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="179.07mm" wi="125.39mm" orientation="landscape" file="US08624717-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="211.33mm" wi="165.27mm" orientation="landscape" file="US08624717-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="215.98mm" wi="152.99mm" file="US08624717-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="235.37mm" wi="176.53mm" file="US08624717-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="197.53mm" wi="138.18mm" file="US08624717-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="174.50mm" wi="117.69mm" file="US08624717-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="215.48mm" wi="138.68mm" file="US08624717-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="245.62mm" wi="147.40mm" file="US08624717-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="235.88mm" wi="168.32mm" file="US08624717-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="233.34mm" wi="154.01mm" file="US08624717-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="231.82mm" wi="154.01mm" file="US08624717-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="247.14mm" wi="151.47mm" file="US08624717-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application is a continuation based upon and claiming the benefit of priority of the prior International Patent Application No. PCT/JP2008/053273, filed on Feb. 26, 2008, the entire contents of which are incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD</heading>
<p id="p-0003" num="0002">The present invention relates to an image processor that processes an image captured by a vehicle-mounted camera, etc. to make the image displayable to a user.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">In recent years, systems using vehicle-mounted cameras (e.g., Blind Corner Monitor: BCM) have been introduced for the purpose of preventing collision accidents, etc., upon entry into out-of-sight intersections, T junctions, etc.</p>
<p id="p-0005" num="0004">In BCM, for example, a blind spot area not directly observable to a driver from the driver seat is captured by a camera installed in the front end or the like of the vehicle and the area is displayed on a vehicle-mounted monitor. With BCM, it is possible to visually assist the driver.</p>
<p id="p-0006" num="0005">However, when a vehicle enters a road at an angle other than 90&#xb0;, such as entering an intersection where the intersecting roads are not perpendicular to each other, road conditions on the left and right sides of the vehicle may not be simultaneously displayed on the BCM vehicle-mounted monitor. That is, there may arise blind spots that do not appear even on the BCM monitor. Further, in some cases, utility poles, pedestrians, etc., may become shielding objects, whereby the driver may not be able to grasp the left and right conditions even by looking at the image on the BCM monitor. In such a case, the driver may not be aware of an approaching vehicle, meaning that road safety may not be confirmed to a sufficient degree with BCM alone.</p>
<p id="p-0007" num="0006">Meanwhile, as a means for preventing accidents, mirrors (corner mirrors) that reflect information on areas that are blind spots for drivers are placed at intersections.</p>
<p id="p-0008" num="0007">An obstacle detection system using such mirrors installed on roadsides has been proposed (see Japanese Laid-open Patent Publication No. 2007-69777 (Patent document 1), for example). In this system, infrared light is irradiated from a vehicle to a reflector installed on a roadside and the presence or absence of a dangerous object is determined based on an image produced from the infrared light reflected on the reflector. When a dangerous object is present, the system notifies the driver as such.</p>
<p id="p-0009" num="0008">Further, there has also been proposed a system in which a corner mirror is identified from an image captured by a camera installed in the front end of a vehicle and a magnified image of the corner mirror is displayed on a HUD (Head-Up Display) (see Japanese Laid-open Patent Publication No. 2007-102691 (Patent document 2), for example).
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0009">Patent document 1: Japanese Laid-open Patent Publication No. 2007-69777</li>
    <li id="ul0001-0002" num="0010">Patent document 2: Japanese Laid-open Patent Publication No. 2007-102691</li>
</ul>
</p>
<p id="p-0010" num="0011">However, depending on conditions such as the size, shape and direction of each corner mirror and road width, areas (blind spots) that may not be captured even by corner mirrors may arise. For this reason, drivers may not be able to confirm road safety to a sufficient degree with corner mirrors and their magnified image alone.</p>
<p id="p-0011" num="0012">As described above, since the situation in blind spot areas changes momentarily, it is necessary for drivers to check at all times both the BCM image and corner mirrors installed on roads or an image thereof. For example, immediately before entering an intersection, the driver may make movements such as taking a look at mirrors placed on the road to check information on blind spot areas captured by the mirrors and taking a look at the monitor to check information on blind spot areas captured by a BCM camera. Such actions are a burden on drivers driving vehicles.</p>
<p id="p-0012" num="0013">That is, even if the environment for presenting blind spot areas and a magnified image to drivers through BCM and corner mirrors is put into place, when the drivers cannot check information on the blind spots and on the magnified image through a single action (action such as checking the monitor), the benefits of the information are halved.</p>
<p id="p-0013" num="0014">For this reason, a mechanism that allows drivers to perceive the information on both the blind spot areas and the magnified image through a single action and to make full use of the both information is desired.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0014" num="0015">According to an aspect of the invention, the image processor includes: an image input unit that obtains an image captured by an imaging device installed on a vehicle; a vehicle information input unit that obtains a distance between the vehicle and a road junction or curve in a traveling direction of the vehicle based on information received from a vehicle-mounted device installed in the vehicle; a recording unit in which imaging information indicating properties of the imaging device and driver information regarding a visual field of a driver of the vehicle are stored; a magnifying object recognition unit that recognizes a certain magnifying object in the captured image; and a composing unit that produces, when the magnifying object recognition unit recognizes the certain magnifying object, a composite image of a magnified image of the magnifying object and the captured image. The composing unit uses the distance obtained by the vehicle information input unit as well as the imaging information and the driver information stored in the recording unit to determine an area in the captured image that is not an image of a blind spot for the driver, and produces the composite image such that the magnified image is superimposed on the non-blind spot area.</p>
<p id="p-0015" num="0016">The object and advantages of the invention will be realized and attained by means of the elements and combinations particularly pointed out in the claims. It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are not restrictive of the invention, as claimed.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWING(S)</heading>
<p id="p-0016" num="0017"><figref idref="DRAWINGS">FIG. 1</figref> is a functional block diagram illustrating a configuration of an entire vehicle-mounted system including an image processor.</p>
<p id="p-0017" num="0018"><figref idref="DRAWINGS">FIG. 2A</figref> is a simplified perspective view illustrating a vehicle <b>10</b> equipped with the vehicle-mounted system.</p>
<p id="p-0018" num="0019"><figref idref="DRAWINGS">FIG. 2B</figref> is a diagram illustrating a horizontal area to be captured by a vehicle-mounted camera.</p>
<p id="p-0019" num="0020"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram illustrating an exemplary image captured by the vehicle-mounted camera.</p>
<p id="p-0020" num="0021"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram illustrating an example when the vehicle enters a road at an angel other than 90&#xb0;.</p>
<p id="p-0021" num="0022"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram illustrating an example when the vehicle enters an intersection at which a shielding object is present on the left side.</p>
<p id="p-0022" num="0023"><figref idref="DRAWINGS">FIG. 6</figref> is a conceptual diagram illustrating an area checkable to a driver with the use of road mirrors.</p>
<p id="p-0023" num="0024"><figref idref="DRAWINGS">FIG. 7A</figref> is a diagram illustrating an exemplary image inputted from the camera to the image processor.</p>
<p id="p-0024" num="0025"><figref idref="DRAWINGS">FIG. 7B</figref> is a diagram illustrating an exemplary composite image produced by the image processor.</p>
<p id="p-0025" num="0026"><figref idref="DRAWINGS">FIG. 8</figref> is a flowchart illustrating an exemplary operation of the image processor.</p>
<p id="p-0026" num="0027"><figref idref="DRAWINGS">FIG. 9</figref> is a top view illustrating an example of distances L, l<sub>0</sub>, intersection position K<b>1</b>, etc.</p>
<p id="p-0027" num="0028"><figref idref="DRAWINGS">FIG. 10</figref> is a flowchart illustrating exemplary processing performed by the composition control unit <b>14</b>.</p>
<p id="p-0028" num="0029"><figref idref="DRAWINGS">FIG. 11</figref> is a top view illustrating exemplary imaging area and visual field area present when the vehicle is about to enter an intersection.</p>
<p id="p-0029" num="0030"><figref idref="DRAWINGS">FIG. 12</figref> is a diagram illustrating an image captured by the camera in the example of <figref idref="DRAWINGS">FIG. 11</figref> made to fit an image display area of a monitor.</p>
<p id="p-0030" num="0031"><figref idref="DRAWINGS">FIG. 13</figref> is a diagram illustrating an example when a composite image including a magnified image with a magnified size is displayed.</p>
<p id="p-0031" num="0032"><figref idref="DRAWINGS">FIG. 14</figref> is a top view illustrating exemplary imaging area and visual field area when the vehicle is about to enter an intersection.</p>
<p id="p-0032" num="0033"><figref idref="DRAWINGS">FIG. 15A</figref> is a top view illustrating a situation in which the vehicle enters a road at an angle other than 90&#xb0;.</p>
<p id="p-0033" num="0034"><figref idref="DRAWINGS">FIG. 15B</figref> is a magnified view of the vehicle-mounted camera <b>2</b> portion.</p>
<p id="p-0034" num="0035"><figref idref="DRAWINGS">FIG. 16</figref> is a top view illustrating a situation in which the vehicle heads for a curve.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENT(S)</heading>
<p id="p-0035" num="0036">In the configuration described above, with the use of the distance between the vehicle and the road junction or curve in the traveling direction of the vehicle as well as the properties of the imaging device and the information regarding the visual field of the driver, the composing unit may determine an area in the image captured by the imaging device that is not an image of a blind spot for the driver (i.e., non-blind spot area). Consequently, the composing unit may produce a composite image in which a magnified image of a magnifying object is superimposed on the non-blind spot area in the captured image. When the composite image is displayed, the driver may check the image of blind spots in the captured image and the magnified image of the magnifying object simultaneously simply by glancing at the composite image. In other words, the driver may check both the blind spots captured by the imaging device and the magnifying object through a single action (act of looking at the displayed composite image).</p>
<p id="p-0036" num="0037">According to the present invention, it allows a drivers to recognize information on blind spot areas and on a magnified image through a single action.</p>
<p id="p-0037" num="0038">In one embodiment of the invention, the composing unit may use the distance between the vehicle and the junction or the curve and the imaging information to calculate an imaging area to be captured by the imaging device in the vicinity of a position of the junction or the curve, use the distance between the vehicle and the junction or the curve and the driver information to calculate a visual field area to be in the visual field of the driver in the vicinity of the position of the junction or the curve, and use a positional relationship between the imaging area and the visual field to determine a position of the non-blind spot area in the captured image.</p>
<p id="p-0038" num="0039">As described above, the composing unit calculates both the imaging area of imaging device and the visual field area of the driver at a position based on the junction or curve position. The positional relationship between the two areas seems to correspond to the positional relationship between the captured image and the non-blind spot area. Consequently, with the use of the positional relationship between the imaging area and the visual field area, the composing unit may calculate the non-blind spot area in the captured image with precision.</p>
<p id="p-0039" num="0040">In one embodiment of the invention, the composing unit may use the distance obtained by the vehicle information input unit to determine a magnification level of the magnified image.</p>
<p id="p-0040" num="0041">Consequently, the magnification level of the magnified image of road mirrors may be adjusted in accordance with the distance between the vehicle and the junction or curve. As a result, a composite image that allows the driver to check the road mirrors and blind spots in the captured image more easily is produced.</p>
<p id="p-0041" num="0042">In one embodiment of the invention, the image input unit may further obtain a horizontal rudder angle of the imaging device at a time of capturing the captured image, and the composing unit may use the horizontal rudder angle to calculate the imaging area.</p>
<p id="p-0042" num="0043">Here, the horizontal rudder angle of the imaging device refers to an amount expressed by a rotating angle of the optical axis of the imaging device installed on the vehicle from a certain position when the optical axis is rotated about the axis of the vertical direction from the certain position within the horizontal plane. Consequently, even with an image captured by an imaging device by rotating the optical axis of the imaging device in the horizontal direction, the composing unit may calculate the non-blind spot area appropriately in accordance with the amount of rotation.</p>
<p id="p-0043" num="0044">In one embodiment of the invention, the vehicle information input unit may further obtain a value representing a curvature of the curve in the traveling direction of the vehicle, and the composing unit uses the curvature to determine the magnification level of the magnified image.</p>
<p id="p-0044" num="0045">Consequently, the magnification level of the magnified image of road mirrors may be adjusted in accordance with the curvature of a curve in the traveling direction of the vehicle. As a result, a composite image including a magnified image of road mirrors having an appropriate size according to the curvature of a curve is produced.</p>
<p id="p-0045" num="0046">In one embodiment of the invention, the vehicle information input unit may further obtain information indicating a frequency of occurrence of accidents at the junction or the curve, and the composing unit may use the frequency of occurrence of accidents to determine the magnification level of the magnified image.</p>
<p id="p-0046" num="0047">Consequently, the magnification level of the magnified image of road mirrors may be adjusted according to the frequency of occurrence of accidents at a junction or curve in the traveling direction of the vehicle. As a result, a composite image including a magnified image of road mirrors having an appropriate size according to the degree of risk at a junction or curve is produced.</p>
<p id="p-0047" num="0048">In one embodiment of the invention, the image processor may work in conjunction with a navigation system installed in the vehicle, and the vehicle information input unit may obtain the distance between the vehicle and the road junction or curve in the traveling direction of the vehicle from the navigation system installed in the vehicle. By configuring the image processor to be able to work in conjunction with the navigation system in this way, it is possible to perform processing in an efficient manner.</p>
<p id="p-0048" num="0049">In one embodiment of the invention, the vehicle information input unit may further obtains information indicating presence or absence of the magnifying object at the junction or the curve from the navigation system, and the magnifying object recognition unit may recognize a certain magnifying object in the obtained captured image when the magnifying object is present at the junction or the curve.</p>
<p id="p-0049" num="0050">As described above, by determining whether to recognize the magnifying object or not based on information indicating the presence or absence of the magnifying object at the junction or curve, the recognition may be performed precisely and efficiently.</p>
<p id="p-0050" num="0051">In one embodiment of the invention, when the distance becomes equal to or less than a certain distance, the magnifying object recognition unit may recognize a certain magnifying object in the obtained captured image.</p>
<p id="p-0051" num="0052">According to an aspect of the invention, an image processing method executed by a computer includes: obtaining an image captured by an imaging device installed on a vehicle; obtaining a distance between the vehicle and a road junction or curve in a traveling direction of the vehicle based on information from a vehicle-mounted device installed in the vehicle; reading and obtaining imaging information indicating properties of the imaging device and driver information regarding a visual field of a driver of the vehicle stored in a recording unit accessible to the computer; recognizing a certain magnifying object in the captured image; and producing, when the certain magnifying object is recognized in the magnifying object recognition, a composite image of a magnified image of the magnifying object and the captured image. In the production of the composite image, the obtained distance as well as the obtained imaging information and driver information are used to determine a non-blind spot area in the captured image that does not include an image of a blind spot for the driver, and the composite image is produced such that the magnified image is superimposed on the non-blind spot area.</p>
<p id="p-0052" num="0053">According to an aspect of the invention, a storage medium storing an image processing program that causes a computer to perform processing of: obtaining an image captured by an imaging device installed on a vehicle; obtaining a distance between the vehicle and a road junction or curve in a traveling direction of the vehicle based on information from a vehicle-mounted device installed in the vehicle; obtaining imaging information indicating properties of the imaging device and driver information regarding a visual field of a driver of the vehicle stored in a recording unit accessible to the computer; recognizing a certain magnifying object in the captured image; and producing, when the certain magnifying object is recognized in the magnifying object recognition, a composite image of a magnified image of the magnifying object and the captured image. In the production of the composite image, the obtained distance as well as the obtained imaging information and driver information are used to determine a blind-spot area in the captured image that does not include an image of a blind spot for the driver, and the composite image is produced such that the magnified image is superimposed on the non-blind spot area.</p>
<p id="p-0053" num="0054">According to an aspect of the invention, a vehicle-mounted terminal is capable of working in conjunction with an imaging device installed on a vehicle, and the vehicle-mounted terminal includes: a navigation system having a function of identifying a position of the vehicle and including a map data recording unit in which road information including a position of each road junction or curve is stored; an image input unit that obtains an image captured by the imaging device; a vehicle information input unit that obtains a distance between the vehicle and a road junction or curve in a traveling direction of the vehicle with the use of the position of the vehicle identified by the navigation system and the road information; a recording unit in which imaging information indicating properties of the imaging device and driver information regarding a visual field of a driver of the vehicle are stored; a magnifying object recognition unit that recognizes a certain magnifying object in the captured image; a composing unit that produces, when the magnifying object recognition unit recognizes the certain magnifying object, a composite image of a magnified image of the magnifying object and the captured image; and a display unit that displays the composite image produced by the composing unit.</p>
<p id="p-0054" num="0055">Hereinafter, one embodiment of the present invention will be described with reference to the drawings.</p>
<p id="p-0055" num="0056">&#x3c;Overview of Configuration of Vehicle-Mounted System&#x3e;</p>
<p id="p-0056" num="0057"><figref idref="DRAWINGS">FIG. 1</figref> is a functional block diagram illustrating a configuration of an overall vehicle-mounted system, including an image processor according to this embodiment. A vehicle-mounted system <b>1</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref> is a system installed in a vehicle and the system includes a camera (imaging device) <b>2</b>, an image processor <b>3</b>, a GPS antenna <b>4</b>, a navigation system (navigation system unit) <b>5</b> and a monitor (display unit) <b>6</b>. The camera <b>2</b> is placed at a position from which a front view from the vehicle may be captured. The image processor <b>3</b> receives and processes an image of the front view captured by the camera <b>2</b> and outputs the processed image to the monitor <b>6</b>.</p>
<p id="p-0057" num="0058">The GPS antenna <b>4</b> receives radio waves from a plurality of GPS artificial satellites (GPS satellites). The navigation system <b>5</b> measures the current position of the vehicle based on the radio waves received by the GPS antenna <b>4</b>. The navigation system <b>5</b> produces navigation information using the current position and map data pre-stored in a map data recording unit <b>17</b> and displays the produced information on the monitor <b>6</b>. In addition to a road map (including information on road widths and positions of junctions and curves), the map data also includes data on a variety of facilities, landmarks and the like. The map data is used by the navigation system <b>5</b> in displaying the current position, route search and route guidance, for example.</p>
<p id="p-0058" num="0059">The image processor <b>3</b> receives information on intersections or curves in the traveling direction, the current position of the vehicle and the like from the navigation system <b>5</b> and use them in image processing. The image processor <b>3</b> includes an image input unit <b>7</b>, a vehicle information input unit <b>8</b>, a magnifying object recognition unit <b>9</b>, a composing unit <b>11</b>, an output unit <b>15</b> and a recording unit <b>16</b>. The composing unit <b>11</b> includes a magnifying unit <b>12</b>, a superimposition unit <b>13</b> and a composition control unit <b>14</b>. Details on the inner workings of the image processor <b>3</b> will be described later.</p>
<p id="p-0059" num="0060">&#x3c;Mounting Example on Vehicle&#x3e;</p>
<p id="p-0060" num="0061"><figref idref="DRAWINGS">FIGS. 2A and 2B</figref> illustrate an exemplary configuration of the vehicle-mounted system <b>1</b> when the system is mounted on a vehicle. <figref idref="DRAWINGS">FIG. 2A</figref> is a perspective view illustrating a schematic configuration of a vehicle <b>10</b> on which the vehicle-mounted system <b>1</b> is mounted. The camera <b>2</b> is installed in the front end of the vehicle <b>10</b> and is connected to a housing (vehicle-mounted terminal) <b>18</b>. The navigation system <b>5</b>, the image processor <b>3</b> and the monitor <b>6</b> are integrated to form the housing <b>18</b>. The monitor <b>6</b> is formed at a position that may be observed by a driver H<b>1</b> in the vehicle <b>10</b>.</p>
<p id="p-0061" num="0062">For example, the housing <b>18</b> includes a computer including a CPU, recording media (RAM, ROM, HDD, etc.), a display, a power circuit, bus lines for connecting these components, and the like. The navigation system <b>5</b> as well as the image input unit <b>7</b>, the vehicle information input unit <b>8</b>, the magnifying object recognition unit <b>9</b>, the composing unit <b>11</b> and the output unit <b>15</b> of the image processor <b>3</b> are each functionally implemented via execution of a certain program by the CPU. The program for implementing each of the functions and a recording medium in which the program is stored are also one example of the embodiment of the present invention. Herein, the recording medium is non-transitory tangible medium and dose not include transitory medium such as propagating signal per se. Further, the recording unit <b>16</b> and the map data recording unit <b>17</b> are implemented via the recording medium included in the computer.</p>
<p id="p-0062" num="0063">Note the example illustrated in <figref idref="DRAWINGS">FIG. 2A</figref> does not limit forms in which the vehicle-mounted system <b>1</b> is mounted on the vehicle. For example, the navigation system <b>5</b> and the monitor <b>6</b> may form a single housing, and the image processor <b>3</b> may be installed as an ECU (Electronic Control Unit) to be connected to the camera <b>2</b> and the housing. Further, the image processor <b>3</b> may be formed with such a chip as a 1394 controller LSI. Further, the monitor <b>6</b> may be formed with an instrumental panel, an HUD or the like. In <figref idref="DRAWINGS">FIG. 2A</figref>, a range between two lines m<b>1</b> and n<b>1</b> indicates an area to be captured by the camera <b>2</b> in the vertical direction.</p>
<p id="p-0063" num="0064"><figref idref="DRAWINGS">FIG. 2B</figref> is a diagram illustrating an area to be captured by the camera <b>2</b> of the vehicle <b>10</b> in the vertical direction when the vehicle <b>10</b> is about to enter an intersection. In <figref idref="DRAWINGS">FIG. 2B</figref>, an area between two lines p<b>2</b> and n<b>2</b> indicates a horizontal area to be captured by the camera <b>2</b>. In the example illustrated in <figref idref="DRAWINGS">FIG. 2B</figref>, an angle &#x3b1; between the two lines p<b>2</b> and n<b>2</b> is a little less than 180&#xb0; (hereinafter, an area expressed in angle, which is captured by the camera and displayed on the monitor <b>6</b>, such as the angle &#x3b1;, will be referred to as a monitorable angle). Here, the monitorable angle &#x3b1; of the camera <b>2</b> is close to 180&#xb0;. Thus, when the front end of the vehicle <b>10</b> enters the intersection, an image of the left and right sides of the road perpendicular to the traveling direction of the vehicle <b>10</b> is to be captured by the camera <b>2</b> and displayed on the monitor <b>6</b>. Further, since the camera is positioned in the front end of the vehicle <b>10</b>, the driver H<b>1</b> may check blind spots ahead of the vehicle <b>10</b> right away when the vehicle <b>10</b> moves forward.</p>
<p id="p-0064" num="0065"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram illustrating an exemplary image captured by the camera <b>2</b> of the vehicle <b>10</b> that is about to enter an intersection. In the image illustrated in <figref idref="DRAWINGS">FIG. 3</figref>, areas that are blind spots for the driver H<b>1</b> are in a right end portion AR and a left end portion AL.</p>
<p id="p-0065" num="0066">In the example illustrated in <figref idref="DRAWINGS">FIG. 2B</figref>, the left and right sides of the vehicle <b>10</b> are captured by a single camera whose horizontal monitorable angle is close to 180&#xb0;. Alternatively, two cameras each having a monitorable angle of about 90&#xb0; may be installed on the vehicle <b>10</b> to face the left and right directions, respectively. Further, by placing three cameras in the front end of the vehicle in the front, left and right directions, the monitorable range may be increased to more than 180&#xb0;. The number of cameras to be installed is not particularly limited and is determined in view of the cost, image viewability and the like as needed.</p>
<p id="p-0066" num="0067">Further, the camera installation position is not limited to the position illustrated in <figref idref="DRAWINGS">FIGS. 2A and 2B</figref>. For example, the camera may also be installed on the hood, rearview mirror, door mirror, etc., of the vehicle <b>10</b>. Further, in order to capture a rearview image when the vehicle <b>10</b> moves backward, the camera may be installed on the rear of the vehicle <b>10</b>. In this embodiment, a case of capturing a front view image by a single camera will be described as an example.</p>
<p id="p-0067" num="0068">&#x3c;Exemplary Output Image from Image Processor&#x3e;</p>
<p id="p-0068" num="0069">When the vehicle <b>10</b> enters an intersection or T junction, left and right conditions may not be captured by the camera <b>2</b> in some cases. For example, as illustrated in <figref idref="DRAWINGS">FIG. 4</figref>, when the vehicle <b>10</b> enters a road at an angle other than 90&#xb0; and there is a fence <b>21</b> on the right side of the vehicle <b>10</b>, not the road conditions in the right direction but the fence <b>21</b> appears in an image captured by the camera <b>2</b> at the right end. That is, because a vehicle <b>22</b><i>a </i>approaching from the right side is overshadowed by the fence <b>21</b>, the vehicle <b>22</b><i>a </i>does not appear in the image captured by the camera <b>2</b>. Also, a vehicle <b>22</b><i>b </i>approaching from the left side does not appear in the image captured by the camera <b>2</b> unless the vehicle <b>22</b><i>b </i>enters into the monitorable angle of the camera <b>2</b>.</p>
<p id="p-0069" num="0070">Further, as another example, as illustrated in <figref idref="DRAWINGS">FIG. 5</figref>, also when the vehicle <b>10</b> enters an intersection at which a shielding object <b>23</b> (e.g., a utility pole or pedestrian) is present on the left side, a vehicle <b>22</b><i>c </i>approaching from the left side does not appear in an image captured by the camera <b>2</b> because the vehicle <b>22</b><i>c </i>is overshadowed by the shielding object <b>23</b>. In this way, there are situations where road conditions of the left and right sides may not be captured by the camera <b>2</b>.</p>
<p id="p-0070" num="0071">Meanwhile, the driver H<b>1</b> may look at a road mirror (commonly known as a curve mirror) placed at an intersection, T junction or curve to check road conditions that are not directly observable to the driver. <figref idref="DRAWINGS">FIG. 6</figref> is a conceptual diagram illustrating an area where the driver H<b>1</b> may check with the use of road mirrors when the vehicle <b>10</b> is about to enter a T junction. In the example illustrated in <figref idref="DRAWINGS">FIG. 6</figref>, two road mirrors <b>24</b><i>a</i>, <b>24</b><i>b </i>are placed at the T junction. A range between lines p<b>3</b> and n<b>3</b> indicates an area where the driver H<b>1</b> of the vehicle may check by taking a direct look. The driver H<b>1</b> may look at the road mirror <b>24</b><i>a </i>to check a range between lines p<b>4</b> and n<b>4</b> and look at the road mirror <b>24</b><i>b </i>to check a range between lines p<b>5</b> and n<b>5</b>.</p>
<p id="p-0071" num="0072">In the example illustrated in <figref idref="DRAWINGS">FIG. 6</figref>, areas S<b>1</b> as blind spots for the driver H<b>1</b> on the left and right sides are captured by the camera <b>2</b> and are displayed on the monitor <b>6</b>. In this case, the driver H<b>1</b> may look at the road mirrors and the monitor <b>6</b> to check left and right conditions that are not directly observable to the driver H<b>1</b>. The image processor <b>3</b> according to this embodiment provides an image that allows the driver H<b>1</b> to check both the blind spot areas S<b>1</b> and the areas reflected on the road mirrors <b>24</b><i>a</i>, <b>24</b><i>b </i>in a situation as illustrated in <figref idref="DRAWINGS">FIG. 6</figref> by simply looking at the monitor <b>6</b>, for example.</p>
<p id="p-0072" num="0073"><figref idref="DRAWINGS">FIG. 7A</figref> is a diagram illustrating an exemplary image inputted from the camera <b>2</b> to the image processor <b>3</b>. The image processor <b>3</b> produce a composite image by superimposing on the original image a magnified image of a road mirror portion A<b>1</b> in the image illustrated in <figref idref="DRAWINGS">FIG. 7A</figref>. <figref idref="DRAWINGS">FIG. 7B</figref> is a diagram illustrating an exemplary composite image produced by the image processor <b>3</b>. As illustrated in <figref idref="DRAWINGS">FIG. 7B</figref>, the image processor <b>3</b> may produce an image in which a magnified image is superimposed on a non-blind spot area (area that is not included in the blind spot areas S<b>1</b>) in the original image (<figref idref="DRAWINGS">FIG. 7A</figref>).</p>
<p id="p-0073" num="0074">In this way, by superimposing a magnified image of road mirrors on the non-blind spot area for the driver H<b>1</b> in the original captured image and displaying the composite image, the overall blind spot area for the driver H<b>1</b> may be displayed in a single screen. Hereinafter, exemplary configuration and operation of the image processor <b>3</b> capable of performing such image processing will be described in detail.</p>
<p id="p-0074" num="0075">&#x3c;Configuration of Image Processor <b>3</b>&#x3e;</p>
<p id="p-0075" num="0076">In the image processor <b>3</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, the image input unit <b>7</b> obtains image data of a captured image from the camera <b>2</b> and makes the data accessible to the magnifying object recognition unit <b>9</b> and the composing unit <b>11</b>. The image input unit <b>7</b> subjects the image signals received from the camera <b>2</b> to A/D conversion and other necessary conversion processing and records the signals frame by frame in a recording medium accessible to the magnifying object recognition unit <b>9</b> and the composing unit <b>11</b>. The image input unit <b>7</b> may also receive image data that has already been subjected to necessary conversion processing, such as A/D conversion.</p>
<p id="p-0076" num="0077">The magnifying object recognition unit <b>9</b> reads the image data obtained by the image input unit <b>7</b> frame by frame and determines the presence or absence of an area that may be recognized as a magnifying object (a road mirror in this case) in each frame of the image. When an area that may be recognized as a road mirror is present, the magnifying object recognition unit <b>9</b> extracts the data of the area and passes the data to the magnifying unit <b>12</b>. The magnifying unit <b>12</b> magnifies an image of the area to produce a magnified image.</p>
<p id="p-0077" num="0078">With the use of known image recognition techniques, the magnifying object recognition unit <b>9</b> may recognize a road mirror portion in the image. As an example, first, the magnifying object recognition unit <b>9</b> uses a Laplacian filter to extract an edge portion in the image. Then, the magnifying object recognition unit <b>9</b> matches image data forming the edge portion with pre-stored feature quantity data of a road mirror (e.g., a template of a standard road mirror) and calculates the correlation value. An area in which the correlation value is larger than a threshold value may be determined as a road mirror area.</p>
<p id="p-0078" num="0079">Further, templates of objects that could be easily misidentified as a road mirror, such as road signs, may be pre-stored as feature quantity data. In this case, the magnifying object recognition unit <b>9</b> may be configured not to recognize an object as a road mirror when the correlation value between such templates and image data forming the edge portion is larger than a threshold value. As a result, the precision of the recognition improves. Note that the road mirror recognition is not limited to the example described above.</p>
<p id="p-0079" num="0080">As described above, magnifying objects recognized by the magnifying object recognition unit <b>9</b> are objects that need to be magnified and presented to the driver. That is, objects that provide the driver with information beneficial to the driver in driving vehicle are predefined as magnifying objects. In the example described above, the pre-stored feature quantity data defines road mirrors as magnifying objects. Magnifying objects are not limited to road mirrors and road signs, guideboards and road surface markings (letters (e.g., &#x201c;STOP&#x201d;) and arrows painted on a road surface) may also be defined as magnifying objects.</p>
<p id="p-0080" num="0081">The magnifying unit <b>12</b> magnifies the image of the road mirror area according to a value representing a magnification level received from the composition control unit <b>14</b>. The magnifying unit <b>12</b> passes the magnified image to the superimposition unit <b>13</b>.</p>
<p id="p-0081" num="0082">The superimposition unit <b>13</b> superimposes the received magnified image on the frames (hereinafter referred to as the original frames) corresponding to the image data of the captured image obtained by the image input unit <b>7</b> to produce a composite image. When performing superimposition, the superimposition unit <b>13</b> obtains from the composition control unit <b>14</b> information indicating the superimposition position of the magnified image in the original frames, and superimposes the magnified image on the original frames based on the obtained information.</p>
<p id="p-0082" num="0083">In this way, the composite image produced by the superimposition unit <b>13</b> is outputted to the monitor <b>6</b> via the output unit <b>15</b>. Frames in which the magnifying object recognition unit <b>9</b> has not found a mirror portion are sent to the output unit <b>15</b> without being processed by the superimposition unit <b>13</b> and are displayed on the monitor <b>6</b>.</p>
<p id="p-0083" num="0084">In the recording unit <b>16</b>, imaging information indicating the properties of the camera <b>2</b> installed on the vehicle <b>10</b> and driver information regarding the visual field of the driver H<b>1</b> of the vehicle <b>10</b> are pre-stored. The imaging information includes information used in determining an area to be captured by the camera <b>2</b>. For example, the imaging information includes such information as the monitorable angle, angle of view, lens properties of the camera <b>2</b> and the installation position of the camera <b>2</b> on the vehicle <b>10</b>.</p>
<p id="p-0084" num="0085">Further, the driver information includes information that enables to estimate the visual field of the driver sitting in the driver seat of the vehicle <b>10</b>. For example, the driver information includes such information as the position of the driver's eyes in the vehicle <b>10</b> and the visual field properties (e.g., effective visual field) of the driver. The driver information is not limited to pre-stored fixed values. For example, the vehicle information input unit <b>8</b> may receive information on the visual field of the driver from a vehicle-mounted device (not shown) for monitoring the driver's eye movements and store the information in the recording unit <b>16</b> as a piece of the driver information.</p>
<p id="p-0085" num="0086">The vehicle information input unit <b>8</b> obtains the current position of the vehicle <b>10</b> and the immediate junction position in the traveling direction of the vehicle <b>10</b> from the navigation system <b>5</b>, calculates the distance between the vehicle <b>10</b> and the junction (hereinafter referred to as distance L) and notifies the composition control unit <b>14</b> of the calculated distance.</p>
<p id="p-0086" num="0087">The way to obtain the distance L between the vehicle <b>10</b> and the junction is not limited to one described above. The vehicle information input unit <b>8</b> may obtain the distance L based on data received from a vehicle-mounted device capable of gathering information for determining the current position. The vehicle-mounted device is not limited to a particular device. For example, the vehicle information input unit <b>8</b> may receive data indicating the distance L between the vehicle <b>10</b> and the junction from the navigation system <b>5</b> or may calculate the distance L using radio waves received by the GPS antenna <b>4</b> and the map data stored in the map data recording unit <b>17</b>. Further, in addition to or in place of the distance between the vehicle <b>10</b> and the junction, the vehicle information input unit <b>8</b> may obtain the distance between the vehicle and the curve in the traveling direction. Further, the vehicle information input unit <b>8</b> may further use information received from a vehicle speed sensor, a vehicle direction sensor, etc., (all of which are not shown) to determine the current position of the vehicle <b>10</b>.</p>
<p id="p-0087" num="0088">The composition control unit <b>14</b> uses the distance L between the vehicle <b>10</b> and the junction as well as the imaging information and the driver information stored in the recording unit <b>16</b> to calculate the magnification level of the road mirror area in the image and the position at which the magnified image is superimposed, and notifies the magnifying unit <b>12</b> and the superimposition unit <b>13</b> of the results, respectively. Here, the composition control unit <b>14</b> calculates an image area in the original frames of the captured image, area directly observable to the driver H<b>1</b> (=a non-blind spot area that does not include an image of a blind spot for the driver H<b>1</b>), and calculates the superimposition position so that the magnified image is superimposed on the non-blind spot area.</p>
<p id="p-0088" num="0089">For example, the composition control unit <b>14</b> uses the distance L and the imaging information to calculate an area to be captured by the camera <b>2</b> (imaging area) in the vicinity of a position apart from the vehicle <b>10</b> by the distance L. Further, the composition control unit <b>14</b> uses the distance L and the driver information to calculate an area to be in the visual field of the driver (visual field area) in the vicinity of the position apart from the vehicle <b>10</b> by the distance L. And the composition control unit <b>14</b> uses the positional relationship between the imaging area and the visual field area to calculate the non-blind spot area in the original frames of the captured image. The composition control unit <b>14</b> may calculate the non-blind spot area such that the positional relationship of the imaging area with the visual field area corresponds to the positional relationship of the non-blind spot area with the original frames of the captured image. Note that the way to calculate the non-blind spot area is not limited to one described above. The composition control unit <b>14</b> may determine the non-blind spot area for the driver H<b>1</b> using the properties of the camera <b>2</b>, road shape, intersection position, vehicle position, etc.</p>
<p id="p-0089" num="0090">Further, the composition control unit <b>14</b> may use information inputted by the vehicle information input unit <b>8</b> to control the operation of the image input unit <b>7</b> or the magnifying object recognition unit <b>9</b>. For example, the composition control unit <b>14</b> may control the image input unit <b>7</b> and the magnifying object recognition unit <b>9</b> to operate only when the distance L is smaller than a certain distance. As a result, the magnifying object recognition unit <b>9</b> performs the road mirror recognition every time the vehicle <b>10</b> approaches an intersection.</p>
<p id="p-0090" num="0091">Further, it is not necessary to perform the road mirror recognition every time the vehicle <b>10</b> approaches an intersection. For example, by pre-including in the map data of the navigation system <b>5</b> information indicating the presence or absence of a magnifying object (a road mirror in this case) at each junction, the vehicle information input unit <b>8</b> may be configured to obtained the pre-included information. In this case, the composition control unit <b>14</b> may control the magnifying object recognition unit <b>9</b> to perform the road mirror recognition when the vehicle <b>10</b> approaches a junction with a road mirror. Also, the magnifying object recognition unit <b>9</b> may directly receive the pre-included information from the vehicle information input unit <b>8</b> to determine whether to perform the road mirror recognition or not.</p>
<p id="p-0091" num="0092">&#x3c;Exemplary Operation of Image Processor <b>3</b>&#x3e;</p>
<p id="p-0092" num="0093">Next, an exemplary operation of the image processor <b>3</b> will be described. <figref idref="DRAWINGS">FIG. 8</figref> is a flowchart illustrating an exemplary operation of the image processor <b>3</b>. In the exemplary operation illustrated in <figref idref="DRAWINGS">FIG. 8</figref>, first, the image processor <b>3</b> initializes &#x201c;Flag&#x201d; (set Flag to 0) indicating whether to output a composite image or not (Op<b>1</b>).</p>
<p id="p-0093" num="0094">Then, the vehicle information input unit <b>8</b> obtains the current position of the vehicle <b>10</b> and a position K<b>1</b> of the closest intersection in the traveling direction of the vehicle <b>10</b> from the navigation system <b>5</b> (Op<b>2</b>). For example, the current position information and the intersection position K<b>1</b> are each expressed in latitude and longitude. The vehicle information input unit <b>8</b> calculates the distance L between the current position of the vehicle <b>10</b> and the intersection position K<b>1</b> (Op<b>3</b>). The composition control unit <b>14</b> is notified of the distance L.</p>
<p id="p-0094" num="0095">When the distance L is smaller than a threshold value l<sub>0</sub>, in other words, when the vehicle <b>10</b> is in the vicinity of the intersection (L&#x3c;l<sub>0</sub>: Yes at Op<b>4</b>) and has not passed the intersection (L&#x2267;0: Yes at Op<b>5</b>), the composition control unit <b>14</b> causes the image input unit <b>7</b> to obtain an image captured by the camera <b>2</b> (Op<b>9</b>).</p>
<p id="p-0095" num="0096"><figref idref="DRAWINGS">FIG. 9</figref> is a top view illustrating an example of positional relationships among the distances L, l<sub>0</sub>, the intersection position K<b>1</b>, etc. In the example illustrated in <figref idref="DRAWINGS">FIG. 9</figref>, the intersection position K<b>1</b> is set at the center of the intersection (the intersection point of lines (central lines) passing through the center of the intersecting roads). The vehicle <b>10</b> is traveling towards the intersection position K<b>1</b>. Here, the distance l<sub>0 </sub>is the sum of a distance l<sub>1 </sub>determined by the width of the road (road width A) into which the vehicle <b>10</b> is about to enter and a certain fixed value l<sub>2</sub>. The distance l<sub>1 </sub>is the distance between the intersection position and the roadside, and may be determined by l<sub>1</sub>=A/2, for example. In this way, it is possible to calculate the threshold value l<sub>0 </sub>used in determining the time period over which an image is captured by the camera <b>2</b> with the addition of the value based on the road width A to the fixed value performed by the composition control unit <b>14</b>.</p>
<p id="p-0096" num="0097">At Op<b>9</b> in <figref idref="DRAWINGS">FIG. 8</figref>, the image input unit <b>7</b> obtains a single frame of the captured image, for example. Although an example where a single frame of the image is obtained will be described in the following, the image input unit <b>7</b> may obtain a plurality of frames at Op<b>9</b> and each of the frames may be subjected to a process at Op<b>10</b>, which will be described later.</p>
<p id="p-0097" num="0098">When the vehicle <b>10</b> is not in the vicinity of the intersection (No at Op<b>4</b>) or has already passed the intersection (No at Op<b>8</b>), the composition control unit <b>14</b> interprets Flag without obtaining the image captured by the camera <b>14</b> (Op<b>5</b>). When Flag is 0 (&#x201c;Flag=0&#x201d;) (No at Op<b>5</b>), the process at Op <b>2</b> is performed again. When Flag is 1 (&#x201c;Flag=1&#x201d;) (Yes at Op<b>5</b>), the composition control unit <b>14</b> instructs the magnifying unit <b>12</b> and the superimposition unit <b>13</b> to end the image superimposition (Op<b>6</b>) and sets Flag to 0 (&#x201c;Flag=0&#x201d;) (Op<b>7</b>). Thereafter, the process at Op<b>2</b> is performed again.</p>
<p id="p-0098" num="0099">When the vehicle <b>10</b> arrives at the point apart from the intersection position by the certain distance l<sub>0</sub>, due to the processes at Op<b>4</b> to Op<b>9</b>, the image input unit <b>7</b> starts obtaining the image captured by the camera <b>2</b> and stops obtaining the image captured by the camera <b>2</b> when the vehicle <b>10</b> passes the intersection position.</p>
<p id="p-0099" num="0100">When the image input unit <b>7</b> obtains a frame of the image captured by the camera <b>2</b> (original frame) at Op<b>9</b>, the magnifying object recognition unit <b>9</b> extracts from the original frame an area that may be recognized as a road mirror (road mirror area) (Op<b>10</b>). When the magnifying object recognition unit <b>9</b> extracts the road mirror area (Yes at Op<b>11</b>), the composition control unit <b>14</b> calculates the magnification level of the road mirror area and the position at which a magnified image is superimposed on the original frame (Op<b>13</b>). At Op<b>13</b>, the composition control unit <b>14</b> uses the distance L calculated at Op<b>3</b> as well as the imaging information and the driver information stored in the recording unit <b>16</b> to calculate the magnification level and the superimposition position. In so doing, the composition control unit <b>14</b> calculates the superimposition position such that the magnified image is superimposed on the area in the original frame of the image that is directly observable to the driver H<b>1</b> (non-blind spot area). Details on the process at Op<b>13</b> will be described later.</p>
<p id="p-0100" num="0101">Base on the magnification level calculated at Op<b>13</b>, the magnifying unit <b>12</b> produces a magnified image of the road mirror area extracted at Op<b>10</b> (Op<b>14</b>). The superimposition unit <b>13</b> produces a composite image in which the magnified image is superimposed on the original frame based on the superimposition position calculated at Op<b>13</b> (Op<b>15</b>). Then, the output unit <b>15</b> produces display data which is processed such that the composite image may be displayed on the monitor <b>6</b> (Op<b>16</b>) and outputs the data to the monitor <b>6</b> (Op<b>17</b>). As a result, the image in which the magnified image of the road mirrors is superimposed on the non-blind spot area in the image captured by the camera <b>2</b> is displayed on the monitor <b>6</b>. When the composite image is displayed on the monitor <b>6</b>, Flag is set to 1 (&#x201c;Flag=1&#x201d;) (Op<b>18</b>).</p>
<p id="p-0101" num="0102">Thereafter, Op<b>2</b> is performed again. As a result of the processes illustrated in <figref idref="DRAWINGS">FIG. 8</figref>, the image captured by the camera <b>2</b> is displayed on the monitor <b>6</b> during the period between the arrival of the vehicle <b>10</b> at the point apart from the intersection position by the distance l<sub>0 </sub>and the passage of the intersection position. At that time, when a road mirror is in an image captured by the camera <b>2</b>, an image of the road mirror is magnified and displayed in the non-blind spot area in the captured image. By simply glancing at the image on the monitor <b>6</b>, the driver H<b>1</b> may check both the conditions of the blind spot area reflected on the road mirror and the conditions of the blind spot area captured by the camera <b>2</b>.</p>
<p id="p-0102" num="0103">&#x3c;Exemplary Calculation of Magnification Level and Superimposition Position&#x3e;</p>
<p id="p-0103" num="0104">An exemplary calculation of the magnification level and the superimposition position at Op<b>13</b> in <figref idref="DRAWINGS">FIG. 8</figref> will be described. <figref idref="DRAWINGS">FIG. 10</figref> is a flowchart illustrating an example of the process performed by the composition control unit <b>14</b> at Op<b>13</b>. In <figref idref="DRAWINGS">FIG. 10</figref>, the composition control unit <b>14</b> retrieves the camera properties of the camera <b>2</b> from the recording unit <b>16</b> as the imaging information (Op<b>21</b>). For example, the camera properties include the installation position, angle of view and lens properties of the camera <b>2</b>.</p>
<p id="p-0104" num="0105">Further, the composition control unit <b>14</b> obtains the view position and effective visual field of the driver from the recording unit <b>16</b> as the driver information (Op<b>22</b>). Normally, in a case of the vehicle <b>10</b>, the view position of the driver may be determined based on the driver seat position. Thus, the driver seat position may be pre-stored in the recording unit <b>16</b> as the data indicating the view position of the driver.</p>
<p id="p-0105" num="0106">The composition control unit <b>14</b> uses the camera properties obtained at Op<b>21</b> and the distance L to calculate the imaging area captured by the camera <b>2</b> in the vicinity of the intersection position (Op<b>22</b>). Furthermore, the composition control unit <b>14</b> uses the driver information obtained at Op<b>22</b> and the distance L to calculate the visual field area directly observable to the driver in the vicinity of the intersection position (Op<b>23</b>).</p>
<p id="p-0106" num="0107">Hereinafter, an exemplary calculation at Op<b>22</b> and Op<b>23</b> will be described with reference to <figref idref="DRAWINGS">FIG. 11</figref>. As the imaging area and the visual field area in the vicinity of the intersection, here, a case of calculating those on a line passing through a horizontal plane including the intersection position K<b>1</b> and perpendicular to the traveling direction of the vehicle will be described. Note that the imaging area and the visual field area in the vicinity of an intersection are not limited to those in this example. For example, the imaging area and the visual field area on a line passing through or a plane including a position advanced from the intersection position K<b>1</b> by &#xbd; of the road width A may be calculated.</p>
<p id="p-0107" num="0108"><figref idref="DRAWINGS">FIG. 11</figref> is a top view illustrating exemplary imaging area and visual field area when the vehicle <b>10</b> is about to enter an intersection. In the example illustrated in <figref idref="DRAWINGS">FIG. 11</figref>, the position of the camera <b>2</b> on the vehicle <b>10</b> corresponds to the current position of the vehicle <b>10</b> and the intersection position K<b>1</b> is on a line extending from the current position to the traveling direction of the vehicle <b>10</b>.</p>
<p id="p-0108" num="0109">In <figref idref="DRAWINGS">FIG. 11</figref>, a range between lines p<b>6</b> and n<b>6</b> is an area to be captured by the camera <b>2</b> in the horizontal direction. The angle &#x201c;&#x3b1;&#x201d; between the lines p<b>6</b> and n<b>6</b> is the monitorable angle of the camera <b>2</b>. The value of angle of view of the camera <b>2</b> included in the camera properties may be used as the value of &#x201c;&#x3b1;&#x201d;. Further, the composition control unit <b>14</b> may calculate the value of &#x201c;&#x3b1;&#x201d; based on the angle of view and lens properties of the camera <b>2</b>. Alternatively, the value of &#x201c;&#x3b1;&#x201d; may be pre-stored in the recording unit <b>16</b> as a fixed value.</p>
<p id="p-0109" num="0110">Further, a range between lines p<b>7</b> and n<b>7</b> is an area that is directly observable to the driver H<b>1</b>. An angle &#x201c;&#x3b2;&#x201d; between the lines p<b>7</b> and n<b>7</b> is the effective visual field of the driver H<b>1</b>. For example, the value of &#x201c;&#x3b2;&#x201d; may be pre-stored in the recording unit <b>16</b> as a fixed value. The effective human visual field is a visual field in which an object can be captured simply by eye movements and a target object may be perceived in the noise. Since the normal effective visual field of humans is about 15&#xb0; in left and right eyes, this value may be stored as the value of the angle &#x201c;&#x3b2;&#x201d;.</p>
<p id="p-0110" num="0111">Further, l<sub>m </sub>denotes the distance between the installation position of the camera <b>2</b> and the driver H<b>1</b> in the traveling direction and n denotes the distance between the installation position of the camera <b>2</b> and the driver H<b>1</b> in the direction perpendicular to the traveling direction. These distances l<sub>m </sub>and n may be determined from the installation position of the camera <b>2</b> obtained at Op<b>21</b> and the view position of the driver obtained at Op<b>22</b>. Alternatively, these distances l<sub>m </sub>and n may be pre-stored in the recording unit <b>16</b>.</p>
<p id="p-0111" num="0112">On a line q, line passing through a horizontal plane including the intersection position K<b>1</b> and perpendicular to the traveling direction of the vehicle, the area captured by the camera <b>2</b> is from the intersection point of the lines q and p<b>6</b> to the intersection point of the lines q and n<b>6</b>. Further, the area that is directly observable to the driver H<b>1</b> is from the intersection point of the lines q and p<b>7</b> to the intersection point of the lines q and n<b>7</b>.</p>
<p id="p-0112" num="0113">At Op<b>22</b>, the composition control unit <b>14</b> calculates &#xbd; of the length of the area captured by the camera <b>2</b> on the line q (=m<b>1</b>) as the imaging area. In this calculation, the monitorable angle &#x3b1; and the distance L between the vehicle <b>10</b> and the intersection position are used. Specifically, the value of m<b>1</b> may be calculated as expressed by the following equations (1) and (2).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>tan(&#x3b1;/2)=<i>m</i>1/<i>L</i>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>m</i>1<i>=L</i>&#xd7;tan(&#x3b1;/2)&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0113" num="0114">Further, at Op<b>23</b>, the composition control unit <b>14</b> calculates &#xbd; of the length of the area directly observable to the driver H<b>1</b> on the line q (=m<b>2</b>) as the visual field area. In this calculation, the distance L, the angle &#x3b2; and the distance l<sub>m </sub>between the camera <b>2</b> and the driver H<b>1</b> in the traveling direction are used. Specifically, the value of m<b>2</b> may be calculated as expressed by the following equations (3) and (4).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>tan(&#x3b2;/2)=<i>m</i>2/(<i>L+l</i><sub>m</sub>)&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>m</i>2=(<i>L+l</i><sub>m</sub>)&#xd7;tan(&#x3b2;/2)&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0114" num="0115">The composition control unit <b>14</b> uses m<b>1</b> and m<b>2</b> to calculate the positional relationship between the imaging area and the visual field area. Specifically, in the example illustrated in <figref idref="DRAWINGS">FIG. 11</figref>, the composition control unit <b>14</b> calculates a distance X between the intersection point of the lines q and p<b>6</b> and the intersection point of the lines q and p<b>7</b>. The distance X is an exemplary value representing the positional relationship between the imaging area and the visual field area, and it is from the left end of the imaging area to the left end of the visual field area. The value of X may be calculated by the following equation (5) when l<sub>0</sub>&#x3c;L, equation (6) when l<sub>1</sub>&#x3c;L&#x2266;l<sub>0 </sub>and equation (7) when 0&#x3c;L&#x2266;l<sub>1</sub>. In this way, by changing the ways to calculate the value of X in accordance with the value of the distance L between the vehicle <b>10</b> and the intersection position K<b>1</b>, it is possible to calculate the superimposition position appropriately in accordance with the distance L.</p>
<p id="p-0115" num="0116">When l<sub>0</sub>&#x2266;L:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>X=</i>0&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0116" num="0117">When l<sub>1</sub>&#x3c;L&#x2266;l<sub>0</sub>:</p>
<p id="p-0117" num="0118">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mi>X</mi>
              <mo>=</mo>
              <mrow>
                <mrow>
                  <mi>m</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>1</mn>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mrow>
                      <mi>m</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <mn>2</mn>
                    </mrow>
                    <mo>-</mo>
                    <mi>n</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mo>=</mo>
              <mrow>
                <mrow>
                  <mo>{</mo>
                  <mrow>
                    <mi>L</mi>
                    <mo>&#xd7;</mo>
                    <mrow>
                      <mi>tan</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>&#x3b1;</mi>
                          <mo>/</mo>
                          <mn>2</mn>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>}</mo>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <mo>{</mo>
                  <mrow>
                    <mrow>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>L</mi>
                          <mo>+</mo>
                          <msub>
                            <mn>1</mn>
                            <mi>m</mi>
                          </msub>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mo>&#xd7;</mo>
                      <mrow>
                        <mi>tan</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>&#x3b2;</mi>
                            <mo>/</mo>
                            <mn>2</mn>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                    <mo>-</mo>
                    <mi>n</mi>
                  </mrow>
                  <mo>}</mo>
                </mrow>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>6</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0118" num="0119">When 0&#x3c;L&#x2266;l<sub>1</sub>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>X={l</i><sub>1</sub>&#xd7;tan(&#x3b1;/2)}&#x2212;{(<i>l</i><sub>1</sub><i>+l</i><sub>m</sub>)&#xd7;tan(&#x3b2;/2)&#x2212;<i>n}</i>&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0119" num="0120">With the use of m<b>1</b>, m<b>2</b> and X calculated in this way, the composition control unit <b>14</b> calculates a value representing the non-blind spot area in the image captured by the camera <b>2</b> (Op<b>25</b> in <figref idref="DRAWINGS">FIG. 10</figref>). For example, the composition control unit <b>14</b> calculates the left end position of a portion in the captured image where the visual field of the driver H<b>1</b> is shown (non-blind spot area).</p>
<p id="p-0120" num="0121">Specifically, the composition control unit <b>14</b> uses the value representing the imaging area (m<b>1</b>), the value representing the visual field area (m<b>2</b>) and the distance X between the left end of the imaging area and the left end of the visual field area to calculate the left end position of the non-blind spot area in the image captured by the camera <b>2</b>. An example of this calculation will be described with reference to <figref idref="DRAWINGS">FIG. 12</figref>.</p>
<p id="p-0121" num="0122"><figref idref="DRAWINGS">FIG. 12</figref> is a diagram illustrating an image captured by the camera <b>2</b> in the example of <figref idref="DRAWINGS">FIG. 11</figref>, which is fit into the display area of the monitor. In the example illustrated in <figref idref="DRAWINGS">FIG. 12</figref>, the width of the imaging area on the line q (2&#xd7;m<b>1</b>) corresponds to a width W<b>1</b> of a captured image G<b>1</b>. Here, it is assumed that the width W<b>1</b> of the captured image G<b>1</b> is the width of the image display area of the monitor <b>6</b>. For example, the width W<b>1</b> is pre-stored in the recording unit <b>16</b> as a fixed value.</p>
<p id="p-0122" num="0123">As illustrated in <figref idref="DRAWINGS">FIG. 12</figref>, it is possible to assume that the positional relationship between the imaging area and the visual field area on the line q corresponds to the positional relationship between the overall captured image G<b>1</b> and the non-blind spot area in the image. In other words, it is possible to assume that the relationship between the width W<b>1</b> of the captured image G<b>1</b> and a length X<sub>PIX </sub>between the left end of the captured image G<b>1</b> and the left end of the non-blind spot area corresponds to the relationship between the width of the imaging area (2&#xd7;m<b>1</b>) and the distance X. If that is the case, the following equation (8) holds true.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(2<i>&#xd7;m</i>1):<i>W</i>1=<i>X:X</i><sub>PIX</sub>&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0123" num="0124">X<sub>PIX </sub>may be calculated by the following equation (9).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>PIX</sub>=(<i>X&#xd7;W</i>1)/(2<i>&#xd7;m</i>1)&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0124" num="0125">The composition control unit <b>14</b> calculates the value of X<sub>PIX </sub>as the value representing the non-blind spot area (Op<b>25</b> in <figref idref="DRAWINGS">FIG. 10</figref>) and further notifies the superimposition unit <b>13</b> of X<sub>PIX </sub>as the value directly representing the superimposition position (Op<b>26</b>). Note that the value representing the superimposition position is not limited to X<sub>PIX</sub>. For example, the composition control unit <b>14</b> may determine the left end and right end positions of the visual field area on the line q and set the position in the captured image corresponding to the midpoint of the two ends as the superimposition position.</p>
<p id="p-0125" num="0126">The method for calculating the non-blind spot area described above is based upon the premise that the monitorable angle &#x3b1; of the camera <b>2</b> is smaller than 180&#xb0;. When the angle of view of the camera <b>2</b> is 180&#xb0; or more, for example, by subtracting 1&#xb0; each from 180&#xb0; on the left and right to set a to 178&#xb0;, the non-blind spot area may be calculated using the above-described calculation method.</p>
<p id="p-0126" num="0127">The area captured by the camera <b>2</b> becomes larger as the value of &#x3b1; increases. This results in a decrease in the proportion of the image of a road mirror portion to the captured image, making the road mirror portion difficult to see. For this reason, by pre-storing in the recording unit <b>16</b> the maximum monitorable angle &#x3b1;<sub>max </sub>(=threshold value) of the camera <b>2</b> at which road mirrors are recognized, it is possible to set an image of the area within the maximum monitorable angle &#x3b1;<sub>max </sub>as the captured image even when the angle of view of the camera <b>2</b> is larger than the maximum monitorable angle &#x3b1;<sub>max</sub>. As a result, even when the angle of view of the camera <b>2</b> is 180&#xb0; or more, the non-blind spot area may be calculated using the calculation method described above.</p>
<p id="p-0127" num="0128">Next, at Op<b>27</b>, the composition control unit <b>14</b> calculates the magnification size of the magnified image (the size after magnification). Here, as illustrated in <figref idref="DRAWINGS">FIGS. 7A and 7B</figref>, it is assumed that W<b>1</b> and H<b>1</b> denote the width and height of the image display area of the monitor <b>6</b>, in other words, the width and height of the captured image, respectively, and W<b>2</b> and H<b>2</b> denote the width and height of the magnified image, respectively. The values of W<b>1</b> and H<b>1</b> are pre-stored in the recording unit <b>16</b>, for example. The values of W<b>2</b> and H<b>2</b> may be calculated respectively by the following equations (10) and (11) when l<sub>0</sub>&#x3c;L, equations (12) and (13) when l<sub>1</sub>&#x3c;L&#x2266;l<sub>0 </sub>and equations (14) and (15) when 0&#x3c;L&#x2266;l<sub>0</sub>. In this way, by changing the ways to calculate the magnification size (W<b>2</b>, H<b>2</b>) in accordance with the value of the distance L between the vehicle <b>10</b> and the intersection position K<b>1</b>, it is possible to calculate the magnification size appropriately in accordance with the distance L.</p>
<p id="p-0128" num="0129">When l<sub>0</sub>&#x3c;L:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i>2=0&#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i>2=0&#x2003;&#x2003;(11)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0129" num="0130">When l<sub>1</sub>&#x3c;L&#x2266;l<sub>0</sub>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i>2<i>=W</i>1&#x2212;(<i>a&#xd7;L</i>)&#x2003;&#x2003;(12)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i>2<i>=H</i>1&#x2212;(<i>b&#xd7;L</i>)&#x2003;&#x2003;(13)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0130" num="0131">When 0&#x3c;L&#x2266;l<sub>1</sub>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i>2=<i>W</i>1&#x2212;(<i>a&#xd7;l</i><sub>1</sub>)&#x2003;&#x2003;(14)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i>2=<i>H</i>1&#x2212;(<i>b&#xd7;l</i><sub>1</sub>)&#x2003;&#x2003;(15)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0131" num="0132">In the equations (12) to (15), coefficients a, b are constants and the values of the coefficients a, b are appropriately determined based on the camera properties and the road width B, for example. Specifically, the coefficient a may be determined with the use of a table in which value combinations of the horizontal angle of view of the camera <b>2</b> (horizontal angle of view VX) and the road width B and values of the coefficient a corresponding to the combinations are pre-stored. Table 1 below is an example of the table in which the values of the coefficient a corresponding to the value combinations of the horizontal angle of view VX and the road width B are stored.</p>
<p id="p-0132" num="0133">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="70pt" align="left"/>
<colspec colname="2" colwidth="147pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="2" rowsep="1">TABLE 1</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Road width B</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="6">
<colspec colname="1" colwidth="70pt" align="left"/>
<colspec colname="2" colwidth="28pt" align="left"/>
<colspec colname="3" colwidth="35pt" align="left"/>
<colspec colname="4" colwidth="14pt" align="left"/>
<colspec colname="5" colwidth="42pt" align="left"/>
<colspec colname="6" colwidth="28pt" align="left"/>
<tbody valign="top">
<row>
<entry>Horizontal angle</entry>
<entry/>
<entry>B<sub>1 </sub>&#x2266; B &#x3c;</entry>
<entry/>
<entry>B<sub>m&#x2212;1</sub> &#x2266; B &#x3c;</entry>
<entry/>
</row>
<row>
<entry>of view VX</entry>
<entry>B &#x3c; B<sub>1</sub></entry>
<entry>B<sub>2</sub></entry>
<entry>. . .</entry>
<entry>B<sub>m</sub></entry>
<entry>B<sub>m </sub>&#x2266; B</entry>
</row>
<row>
<entry namest="1" nameend="6" align="center" rowsep="1"/>
</row>
<row>
<entry>VX &#x3c; VX<sub>1</sub></entry>
<entry>a<sub>1&#x2212;1</sub></entry>
<entry>a<sub>1&#x2212;2</sub></entry>
<entry>. . .</entry>
<entry>a<sub>1&#x2212;(m&#x2212;1)</sub></entry>
<entry>a<sub>1&#x2212;m</sub></entry>
</row>
<row>
<entry>VX<sub>1 </sub>&#x2266; VX &#x3c; VX<sub>2</sub></entry>
<entry>a<sub>2&#x2212;1</sub></entry>
<entry>a<sub>2&#x2212;2</sub></entry>
<entry>. . .</entry>
<entry>a<sub>2&#x2212;(m&#x2212;1)</sub></entry>
<entry>a<sub>2&#x2212;m</sub></entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>VX<sub>n&#x2212;1</sub> &#x2266; VX &#x3c;</entry>
<entry>a<sub>(n&#x2212;1)&#x2212;1</sub></entry>
<entry>a<sub>(n&#x2212;1)&#x2212;2</sub></entry>
<entry>. . .</entry>
<entry>a<sub>(n&#x2212;1)&#x2212;(m&#x2212;1)</sub></entry>
<entry>a<sub>(n&#x2212;1)&#x2212;m</sub></entry>
</row>
<row>
<entry>VX<sub>n</sub>(180&#xb0;)</entry>
<entry/>
<entry/>
<entry/>
<entry/>
<entry/>
</row>
<row>
<entry>VX<sub>n</sub>(180&#xb0;) &#x2266; VX</entry>
<entry>a<sub>n&#x2212;1</sub></entry>
<entry>a<sub>n&#x2212;2</sub></entry>
<entry>. . .</entry>
<entry>a<sub>n&#x2212;(m&#x2212;1)</sub></entry>
<entry>a<sub>n&#x2212;m</sub></entry>
</row>
<row>
<entry namest="1" nameend="6" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0133" num="0134">Similarly, the coefficient b may be determined with the use of a table in which value combinations of the vertical angle of view of the camera <b>2</b> (vertical angle of view VZ) and the road width B and the values of the coefficient b corresponding to the combinations are pre-stored. Table 2 below is an example of the table in which the values of the coefficient b corresponding to the value combinations of the vertical angle of view VZ and the road width B are stored.</p>
<p id="p-0134" num="0135">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="63pt" align="left"/>
<colspec colname="2" colwidth="154pt" align="center"/>
<thead>
<row>
<entry namest="1" nameend="2" rowsep="1">TABLE 2</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry namest="1" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>Road width B</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="6">
<colspec colname="1" colwidth="63pt" align="left"/>
<colspec colname="2" colwidth="28pt" align="left"/>
<colspec colname="3" colwidth="42pt" align="left"/>
<colspec colname="4" colwidth="14pt" align="left"/>
<colspec colname="5" colwidth="42pt" align="left"/>
<colspec colname="6" colwidth="28pt" align="left"/>
<tbody valign="top">
<row>
<entry>Vertical angle</entry>
<entry/>
<entry>B<sub>1 </sub>&#x2266; B &#x3c;</entry>
<entry/>
<entry>B<sub>m&#x2212;1</sub> &#x2266; B &#x3c;</entry>
<entry/>
</row>
<row>
<entry>of view VZ</entry>
<entry>B &#x3c; B<sub>1</sub></entry>
<entry>B<sub>2</sub></entry>
<entry>. . .</entry>
<entry>B<sub>m</sub></entry>
<entry>B<sub>m </sub>&#x2266; B</entry>
</row>
<row>
<entry namest="1" nameend="6" align="center" rowsep="1"/>
</row>
<row>
<entry>VZ &#x3c; VZ<sub>1</sub></entry>
<entry>b<sub>1&#x2212;1</sub></entry>
<entry>b<sub>1&#x2212;2</sub></entry>
<entry>. . .</entry>
<entry>b<sub>1&#x2212;(m&#x2212;1)</sub></entry>
<entry>b<sub>1&#x2212;m</sub></entry>
</row>
<row>
<entry>VZ<sub>1 </sub>&#x2266; VZ &#x3c; VZ<sub>2</sub></entry>
<entry>b<sub>2&#x2212;1</sub></entry>
<entry>b<sub>2&#x2212;2</sub></entry>
<entry>. . .</entry>
<entry>b<sub>2&#x2212;(m&#x2212;1)</sub></entry>
<entry>b<sub>2&#x2212;m</sub></entry>
</row>
<row>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
<entry>. . .</entry>
</row>
<row>
<entry>VZ<sub>n&#x2212;1</sub> &#x2266; VZ &#x3c;</entry>
<entry>b<sub>(n&#x2212;1)&#x2212;1</sub></entry>
<entry>b<sub>(n&#x2212;1)&#x2212;2</sub></entry>
<entry>. . .</entry>
<entry>b<sub>(n&#x2212;1)&#x2212;(m&#x2212;1)</sub></entry>
<entry>b<sub>(n&#x2212;1)&#x2212;m</sub></entry>
</row>
<row>
<entry>VZ<sub>n</sub>(180&#xb0;)</entry>
<entry/>
<entry/>
<entry/>
<entry/>
<entry/>
</row>
<row>
<entry>VZ<sub>n</sub>(180&#xb0;) &#x2266; VZ</entry>
<entry>b<sub>n&#x2212;1</sub></entry>
<entry>b<sub>n&#x2212;2</sub></entry>
<entry>. . .</entry>
<entry>b<sub>n&#x2212;(m&#x2212;1)</sub></entry>
<entry>b<sub>n&#x2212;m</sub></entry>
</row>
<row>
<entry namest="1" nameend="6" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0135" num="0136">Note that the ways to determine the coefficients a, b are not limited to those in the example described above. For example, in addition to the road width, horizontal angle of view and vertical angle of view, the height and width of an image sent out from the camera may be added to the conditions for determining the coefficients a, b. Further, it is not necessary to include the road width B in the conditions. Further, the magnification size may be determined by using the value representing the non-blind spot area calculated at Op<b>25</b>. For example, the size of the magnified image may be set to fit into the non-blind spot area.</p>
<p id="p-0136" num="0137">In this way, the magnifying unit <b>12</b> is notified of the magnification size (W<b>2</b>, H<b>2</b>) calculated at Op<b>27</b>. The magnifying unit <b>12</b> magnifies the road mirror area in the original frames of the captured image such that the road mirror area is magnified to have the magnification size (W<b>2</b>, H<b>2</b>).</p>
<p id="p-0137" num="0138">As described above with reference to <figref idref="DRAWINGS">FIGS. 10 to 12</figref>, the composition control unit <b>14</b> uses the imaging information on the camera <b>2</b> and the distance L to calculate the imaging area, and uses the driver information and the distance L to calculate the visual field area. Then, the composition control unit <b>14</b> uses the value representing the relationship between the installation position of the camera <b>2</b> and the position of the driver H<b>1</b> to determine the positional relationship between the imaging area and the visual field area. With the use of this positional relationship, the composition control unit <b>14</b> may calculate the value representing the non-blind spot area.</p>
<p id="p-0138" num="0139">&#x3c;Exemplary Display Image&#x3e;</p>
<p id="p-0139" num="0140"><figref idref="DRAWINGS">FIG. 13</figref> is a diagram illustrating an example where a composite image including the magnified image having the magnification size is displayed on the monitor <b>6</b>. The example in <figref idref="DRAWINGS">FIG. 13</figref> illustrates exemplary screens each displayed on the monitor <b>6</b> when the vehicle <b>10</b> passes each position in the course of entry into and passage of an intersection.</p>
<p id="p-0140" num="0141">When the vehicle <b>10</b> has not arrived within a range apart from the intersection position K<b>1</b> by the threshold value l<sub>0 </sub>(when l<sub>0</sub>&#x3c;L), a screen D<b>1</b> is displayed on the monitor <b>6</b>. When the vehicle <b>10</b> enters within the range apart from the intersection position K<b>1</b> by l<sub>0</sub>, a screen D<b>2</b> is displayed. In the screen D<b>2</b>, the road mirror portion is magnified and is superimposed on the original captured image. Thereafter, until the vehicle <b>10</b> enters the intersecting road (period over which l<sub>1</sub>&#x3c;L&#x2266;l<sub>0 </sub>holds), the magnified image of the road mirrors becomes larger in size as the vehicle <b>10</b> moves closer to the intersection (screen D<b>3</b>). That is, the magnified image with the width W<b>2</b> and height H<b>2</b> calculated by the equations (12) and (13) is displayed. Then, during the period between the entry of the vehicle <b>10</b> into the intersecting road and arrival at the intersection position K<b>1</b> (period over which 0&#x3c;L&#x2266;l<sub>1 </sub>holds), the magnified image is displayed in fixed size (W<b>2</b>, H<b>2</b> of the equations (14), (15)) (screen D<b>4</b>). And when the vehicle <b>10</b> passes the intersection position K<b>1</b> (0&#x3e;L), the magnified image is not shown as in the screen D<b>5</b>.</p>
<p id="p-0141" num="0142">As described above, because of the screen transitions illustrated in <figref idref="DRAWINGS">FIG. 13</figref>, the magnified image of road mirrors is superimposed on the original captured image and is displayed in the size and at the position that allow the driver H<b>1</b> to check the image easily in accordance with the conditions of the vehicle <b>10</b>. In particular, road conditions on the left and right sides that are not directly observable to the driver H<b>1</b> are displayed in a single screen immediately after the entry of the vehicle into the intersection (immediately after L=l<sub>1 </sub>holds). Thus, by simply glancing at the monitor <b>6</b>, the driver H<b>1</b> may check the situation in the blind spots reflected on the road mirrors and the situation in the blind spots captured by the camera <b>2</b>.</p>
<p id="p-0142" num="0143">Note that the screen transitions described above are one example and are not limited to one described above. For example, when L&#x3c;0 and l<sub>0</sub>&#x3c;L hold, an image captured by the camera <b>2</b> may not be displayed.</p>
<p id="p-0143" num="0144">As described above, the driver H<b>1</b> may check the situation in blind spot areas captured by both the camera <b>2</b> and road mirrors through a single action (action of checking the situation in the blind spot areas on the monitor <b>6</b>). As a result, the burden on the driver H<b>1</b> is reduced and the situation that changes momentarily may be recognized with certainty.</p>
<p id="p-0144" num="0145">Further, according to the image processor <b>3</b>, by detecting the road mirror area from the image captured by the camera <b>2</b>, magnifying the area and superimposing the magnified area on the non-blind spot area in the original captured image, it is possible to display the composite image in a single screen. Consequently, conditions of a bicycle, pedestrian or vehicle present in an area that cannot be captured by the camera <b>2</b> or road mirrors alone may be presented to the driver H<b>1</b>. As a result, the number of collision accidents upon entry into intersections, accidents making up the majority of traffic accidents, may be expected to decline.</p>
<p id="p-0145" num="0146">&#x3c;Modified Example of Calculation of Superimposition Position&#x3e;</p>
<p id="p-0146" num="0147">Here, a description will be given of a modified example of calculation of the imaging area (m<b>1</b>), the visual field area (m<b>2</b>) and the value (X) representing the positional relationship between the two areas, which have been described with reference to <figref idref="DRAWINGS">FIG. 11</figref>. In the example illustrated in <figref idref="DRAWINGS">FIG. 11</figref>, the optical axis of the camera <b>2</b> and the viewing direction of the driver H<b>1</b> (direction of eyes) are substantially parallel with each other and they are also parallel with the traveling direction of the vehicle <b>10</b>. In contrast, in this modified example, the optical axis of the camera <b>2</b> and the viewing direction of the driver H<b>1</b> are not parallel with each other. <figref idref="DRAWINGS">FIG. 14</figref> is a top view illustrating exemplary imaging area and visual field area in this modified example when the vehicle <b>10</b> is about to enter an intersection. Variables L, m<b>1</b>, lm, n, &#x3b1; and &#x3b2; illustrated in <figref idref="DRAWINGS">FIG. 14</figref> are the same as the variables illustrated in <figref idref="DRAWINGS">FIG. 11</figref>. In the example illustrated in <figref idref="DRAWINGS">FIG. 14</figref>, the direction of the driver H<b>1</b>'s eyes is shifted from the traveling direction by an angle &#x3b3; on the horizontal plane. Here, a range between lines p<b>8</b> and n<b>8</b> is an area that is directly observable to the driver H<b>1</b>. The value of the angle &#x3b3; may be pre-stored in the recording unit <b>16</b> or may be set by the driver H<b>1</b> through the navigation system <b>5</b>, for example. Further, information regarding the angle &#x3b3; may be obtained from a vehicle-mounted device (not shown) for monitoring the driver H<b>1</b>'s eye movements through the vehicle information input unit <b>8</b>.</p>
<p id="p-0147" num="0148">Similarly to the example illustrated in <figref idref="DRAWINGS">FIG. 11</figref>, in the example illustrated in <figref idref="DRAWINGS">FIG. 14</figref>, &#xbd; of the length of the area captured by the camera <b>2</b> on the line q (=m<b>1</b>) may be calculated by the equation 2.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>m</i>1=<i>L</i>&#xd7;tan(&#x3b1;/2)&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0148" num="0149">In this modified example, it is assumed that the distance between an intersection point F<b>1</b> of the line q and a line extending from the driver H<b>1</b> in the traveling direction and the point of left end of the area directly observable to the driver H<b>1</b> (intersection point of the lines p<b>8</b> and q) is m<b>3</b>. The composition control unit <b>14</b> calculates the value of m<b>3</b> as the value of the visual field area of the driver H<b>1</b>. Since the angle between a line connecting the point F<b>1</b> and the driver H<b>1</b> and the line p<b>8</b> is (&#x3b3;+&#x3b2;/2), the following equation (16) holds true. Consequently, m<b>3</b> may be calculated by the following equation (17).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>tan(&#x3b3;+&#x3b2;/2)=<i>m</i>3/(<i>L+l</i><sub>m</sub>)&#x2003;&#x2003;(16)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>m</i>3=(<i>L+l</i><sub>m</sub>)&#xd7;tan(&#x3b3;+&#x3b2;/2)&#x2003;&#x2003;(17)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0149" num="0150">The composition control unit <b>14</b> uses m<b>1</b> and m<b>3</b> to calculate the positional relationship between the imaging area and the visual field area. Specifically, the composition control unit <b>14</b> calculates a distance X<b>2</b> between the intersection point of the lines q and p<b>6</b> and the intersection point of the lines q and p<b>8</b>. This is the distance between the left end of the imaging area and the left end of the visual field area. The value of X<b>2</b> may be calculated by the following equations (18) to (20).</p>
<p id="p-0150" num="0151">When l<sub>0</sub>&#x3c;L:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i>2=0&#x2003;&#x2003;(18)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0151" num="0152">When l<sub>1</sub>&#x3c;L&#x2266;l<sub>0</sub>:</p>
<p id="p-0152" num="0153">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mi>X</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mn>2</mn>
              </mrow>
              <mo>=</mo>
              <mrow>
                <mrow>
                  <mi>m</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mn>1</mn>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mrow>
                      <mi>m</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.3em" height="0.3ex"/>
                      </mstyle>
                      <mo>&#x2062;</mo>
                      <mn>3</mn>
                    </mrow>
                    <mo>-</mo>
                    <mi>n</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mo>=</mo>
              <mrow>
                <mrow>
                  <mo>{</mo>
                  <mrow>
                    <mi>L</mi>
                    <mo>&#xd7;</mo>
                    <mrow>
                      <mi>tan</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>&#x3b1;</mi>
                          <mo>/</mo>
                          <mn>2</mn>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>}</mo>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <mo>{</mo>
                  <mrow>
                    <mrow>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>L</mi>
                          <mo>+</mo>
                          <msub>
                            <mn>1</mn>
                            <mi>m</mi>
                          </msub>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mo>&#xd7;</mo>
                      <mrow>
                        <mi>tan</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>y</mi>
                            <mo>+</mo>
                            <mrow>
                              <mi>&#x3b2;</mi>
                              <mo>/</mo>
                              <mn>2</mn>
                            </mrow>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                    <mo>-</mo>
                    <mi>n</mi>
                  </mrow>
                  <mo>}</mo>
                </mrow>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>19</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0153" num="0154">When 0&#x3c;L&#x2266;l<sub>1</sub>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i>2={<i>l</i><sub>1</sub>&#xd7;tan(&#x3b1;/2)}&#x2212;{(<i>l</i><sub>1</sub><i>+l</i><sub>m</sub>)&#xd7;tan(&#x3b3;+&#x3b2;/2)&#x2212;<i>n}</i>&#x2003;&#x2003;(20)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0154" num="0155">With the use of m<b>1</b>, m<b>3</b> and X<b>2</b> calculated in this way, the composition control unit <b>14</b> calculates the value representing the non-blind spot area in the image captured by the camera <b>2</b>. For example, when it is assumed that the relationship between the width W<b>1</b> of the captured image and a length X<sub>PIX-2 </sub>between the left end of the captured image and the left end of the non-blind spot area corresponds to the relationship between the width (2&#xd7;m<b>1</b>) of the imaging area and the distance X<b>2</b>, the following equation (21) holds true. Consequently, X<sub>PIX-2 </sub>may be calculated by the following equation (22).
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(2&#xd7;<i>m</i>1):<i>W</i>1=<i>X</i>2<i>:X</i><sub>PIX-2</sub>&#x2003;&#x2003;(21)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>PIX-2</sub>=(<i>X</i>2&#xd7;<i>W</i>1)/(2&#xd7;<i>m</i>1)&#x2003;&#x2003;(22)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0155" num="0156">The composition control unit <b>14</b> may calculate the value of X<sub>PIX-2 </sub>as the value representing the non-blind spot area and notify the superimposition unit <b>13</b> of the value of X<sub>PIX-2 </sub>as the value representing the superimposition position. As a result, the output unit <b>15</b> may produce a composite image in which the magnified image is superimposed on the non-blind spot area in the captured image and output the composite image to the monitor <b>6</b>.</p>
<p id="p-0156" num="0157">In this modified example, the case where the eye direction of the driver H<b>1</b> is shifted from the traveling direction towards the left side has been described. By using the calculation method in this modified example, it is possible to calculate the non-blind spot area even when the optical axis of the camera <b>2</b> is rotated by a certain angle (horizontal rudder angle) on the horizontal plane.</p>
<p id="p-0157" num="0158"><figref idref="DRAWINGS">FIGS. 15A and 15B</figref> illustrate an example where the optical axis of the camera <b>2</b> is rotated by a certain rudder angle. <figref idref="DRAWINGS">FIG. 15A</figref> is a top view illustrating an example where the vehicle <b>10</b> is about to enter a road at an angle other than 90&#xb0;. At the junction (T junction) illustrated in <figref idref="DRAWINGS">FIG. 15A</figref>, two roads merge at an angle &#x3bb;. Here, as an example, it is assumed that the intersection point of center lines of these two merging roads is a junction point K<b>2</b>. B denotes the width of the road on which the vehicle <b>10</b> is traveling and A<b>2</b> denotes the width of the road to which the vehicle is about to enter. L denotes the distance between the vehicle <b>10</b> and the junction position K<b>2</b>, l<sub>1 </sub>denotes the distance between the junction position K<b>2</b> and the roadside, <b>12</b> denotes a threshold value for determining the staring point to display the magnified image and l<sub>0 </sub>denotes the distance between the junction position K<b>2</b> and the starting point to display the magnified image. The distance l<sub>1 </sub>is calculated by l<sub>1</sub>=(A<b>2</b>/2)cos &#x3bb;+(B/2)tan &#x3bb;, for example. The road information such as the widths A<b>2</b> and B, the angle &#x3bb; at which the two roads merge and the junction position K<b>2</b> may be obtained from the navigation system <b>5</b> through the vehicle information input unit <b>8</b>.</p>
<p id="p-0158" num="0159"><figref idref="DRAWINGS">FIG. 15B</figref> is a magnified view illustrating the camera <b>2</b> portion of the vehicle <b>10</b>. In <figref idref="DRAWINGS">FIG. 15B</figref>, the Y axis indicates the traveling direction of the vehicle <b>10</b> and a line J indicates the direction of optical axis of the camera <b>2</b> on the horizontal plane. On the horizontal plane, the optical axis J of the camera <b>2</b> is rotated from the Y axis in an anti-clockwise direction by an angle &#x3b5;. In other words, it could be said that the horizontal rudder angle of the camera <b>2</b> is &#x3b5;.</p>
<p id="p-0159" num="0160">Here, the rudder angle of the camera <b>2</b> may be controlled by other ECU mounted on the vehicle (e.g., a camera controlling ECU (not shown)). As an example, this ECU may control the horizontal rudder angle &#x3b5; of the camera <b>2</b> in accordance with the angle &#x3bb; between the road on which the vehicle <b>10</b> is traveling and the road into which the vehicle <b>10</b> is about to enter. For example, the horizontal rudder angle &#x3b5; of the camera <b>2</b> is controlled such that the optical axis becomes perpendicular to the road into which the vehicle is about to enter.</p>
<p id="p-0160" num="0161">For example, the image input unit <b>7</b> may obtain the information indicating the horizontal rudder angle &#x3b5; of the camera <b>2</b> from the ECU as one of the camera properties. By receiving the horizontal rudder angle &#x3b5; of the camera <b>2</b> from the image input unit <b>7</b>, the composition control unit <b>14</b> may use the angle to calculate the non-blind spot area.</p>
<p id="p-0161" num="0162">The composition control unit <b>14</b> calculates the non-blind spot area by calculating the imaging area when the camera <b>2</b> is rotated by the rudder angle &#x3b5; and determining the positional relationship between the calculated imaging area and the visual field area. Similarly to the calculation method used in calculating the visual field area when the direction of the driver H<b>1</b>'s eyes is rotated by the angle &#x3b3; described above, the composition control unit <b>14</b> may calculate the imaging area when the camera <b>2</b> is rotated by the horizontal rudder angle &#x3b5;. Further, with the use of the equations (10) to (15), the composition control unit <b>14</b> may calculate the size (W<b>2</b>, H<b>2</b>) of the magnified image appropriately in accordance with the distance L.</p>
<p id="p-0162" num="0163">As described above, even when the optical axis of the camera <b>2</b> is rotated by the certain horizontal rudder angle on the horizontal plane, by calculating the non-blind spot area, it is possible to determine the superimposition position and the magnification size of the magnified image.</p>
<p id="p-0163" num="0164">&#x3c;Calculation of Superimposition Position and Magnification Level when Vehicle <b>10</b> is Heading for Curve&#x3e;</p>
<p id="p-0164" num="0165">Another modified example of the operation of the composition control unit <b>14</b> will be described. Here, a description will be given of an exemplary calculation performed by the composition control unit <b>14</b> when the vehicle <b>10</b> is heading for a curve. <figref idref="DRAWINGS">FIG. 16</figref> is a top view illustrating an exemplary situation in which the vehicle <b>10</b> is heading for a curve.</p>
<p id="p-0165" num="0166"><figref idref="DRAWINGS">FIG. 16</figref> illustrates a situation in which the vehicle <b>10</b> is about to approach a curve with a radius of curvature R. As an example, it is assumed that a curve position K<b>3</b> is an intersection point of the center line of the road and a line bisecting the rotating angle (<b>0</b>) of the curve. A denotes the width of the road on which the vehicle <b>10</b> is traveling, L denotes the distance between the vehicle <b>10</b> and the curve position K<b>3</b>, l<sub>1</sub>(l<sub>1</sub>=(R+A/2)sin(&#x3b8;/2)) denotes the distance between the curve position K<b>3</b> and the starting point of the curve, l<sub>2 </sub>denotes a threshold value (fixed value) for determining the starting point to display the magnified image, and l<sub>0 </sub>denotes the distance between the curve position K<b>3</b> and the starting point to display the magnified image. The road information such as the road width A, the rotating angle &#x3b8; of the curve, the curve position K<b>3</b> and the radius of curvature R of the curve may be obtained from the navigation system <b>5</b> through the vehicle information input unit <b>8</b>.</p>
<p id="p-0166" num="0167">Similarly to the calculation of the non-blind spot area described with reference to <figref idref="DRAWINGS">FIGS. 11 and 12</figref>, the composition control unit <b>14</b> may calculate the non-blind spot area in the captured image by determining the positional relationship between the imaging area and the visual field area. Further, with the use of the following equations (23) to (28), the composition control unit <b>14</b> may calculate the size (W<b>2</b>, H<b>2</b>) of the magnified image appropriately in accordance with the distance L.</p>
<p id="p-0167" num="0168">When l<sub>0</sub>&#x3c;L:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i>2=0&#x2003;&#x2003;(23)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i>2=0&#x2003;&#x2003;(24)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0168" num="0169">When l<sub>1</sub>&#x3c;L&#x2266;l<sub>0</sub>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i>2=<i>W</i>1&#x2212;(<i>a&#xd7;L</i>)+(<i>e&#xd7;L</i>)&#x2003;&#x2003;(25)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i>2=<i>H</i>1&#x2212;(<i>b&#xd7;L</i>)+(<i>f&#xd7;L</i>)&#x2003;&#x2003;(26)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0169" num="0170">When 0&#x3c;L&#x2266;l<sub>1</sub>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i>2=<i>W</i>1&#x2212;(<i>a&#xd7;l</i><sub>1</sub>)+(<i>e&#xd7;l</i><sub>1</sub>)&#x2003;&#x2003;(27)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i>2=<i>H</i>1&#x2212;(<i>b&#xd7;l</i><sub>1</sub>)+(<i>f&#xd7;l</i><sub>1</sub>)&#x2003;&#x2003;(28)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0170" num="0171">Similarly to the coefficients a, b in the equations (12) to (15), the coefficients a, b in the equations (25) to (28) may be determined based on the camera properties and the road width A. The coefficients e, f are determined based on R of the curve. For example, for the coefficients e, f, values that become larger as R becomes smaller are predefined and stored in the recording unit <b>16</b>. Specifically, the values of the coefficients e, f may be determined with the use of a function that defines the values of the coefficients e, f that change in accordance with R or a table in which the values of the coefficients e, f respectively corresponding to the values of R are stored.</p>
<p id="p-0171" num="0172">In this way, even when the vehicle <b>10</b> approaches and passes a curve, the superimposition position and the magnification size of the magnified image of road mirrors may be determined by calculating the non-blind spot area. Further, since the size of the magnified image is adjusted in accordance with the curvature of curves, it is possible to display a road mirror in large size at curves with a large curvature, that is, at steep curves.</p>
<p id="p-0172" num="0173">&#x3c;Calculation for determining Magnification Level of Magnified Image According to Frequency of Accidents&#x3e;</p>
<p id="p-0173" num="0174">In addition to the road information and the vehicle position information, the composition control unit <b>14</b> may take a variety of factors into consideration when determining the magnification size of the magnified image. For example, the composition control unit <b>14</b> may determine the magnification size in accordance with the frequency of occurrence of accidents at the intersection into which the vehicle <b>10</b> is about to enter. Here, as illustrated in <figref idref="DRAWINGS">FIG. 9</figref>, an exemplary calculation when the vehicle <b>10</b> is about to enter an intersection will be described. For example, the composition control unit <b>14</b> may obtain a value representing the frequency of occurrence of accidents at the intersection into which the vehicle <b>10</b> is about to enter from the navigation system <b>5</b> through the vehicle information input unit <b>8</b>.</p>
<p id="p-0174" num="0175">For example, the value representing the frequency of occurrence of accidents is the number of accidents occurred in the vicinity of the intersection in the past 10 years. Alternatively, the value may represent the frequency of season-by-season or period-by-period occurrence of accidents. Upon these values, the fact that the number of accidents is small during the summer but the incidence of accidents increases during the winter due to the road being covered with snow or the number of accidents is small during the daytime but the number increases during the night time and early morning hours is reflected.</p>
<p id="p-0175" num="0176">For example, with the use of the following equations (29) to (34), the composition control unit <b>14</b> may calculate the size (W<b>2</b>, H<b>2</b>) of the magnified image appropriately in accordance with the distance L.</p>
<p id="p-0176" num="0177">When l<sub>0</sub>&#x3c;L:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i>2=0&#x2003;&#x2003;(29)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i>2=0&#x2003;&#x2003;(30)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0177" num="0178">When l<sub>1</sub>&#x3c;L&#x2266;l<sub>0</sub>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i>2=<i>W</i>1&#x2212;(<i>a&#xd7;L</i>)+(<i>c&#xd7;L</i>)&#x2003;&#x2003;(31)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i>2=<i>H</i>1&#x2212;(<i>b&#xd7;L</i>)+(<i>d&#xd7;L</i>)&#x2003;&#x2003;(32)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0178" num="0179">When 0&#x3c;L&#x2266;l<sub>1</sub>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i>2=<i>W</i>1&#x2212;(<i>a&#xd7;l</i><sub>1</sub>)+(<i>c&#xd7;l</i><sub>1</sub>)&#x2003;&#x2003;(33)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i>2=<i>H</i>1&#x2212;(<i>b&#xd7;l</i><sub>1</sub>)+(<i>d&#xd7;l</i><sub>1</sub>)&#x2003;&#x2003;(34)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0179" num="0180">Similarly to the coefficients a, b in the equations (12) to (15), the coefficients a, b in the equations (29) to (34) may be determined based on the camera properties and the road width A. Further, the coefficients c, d are determined based on the frequency of occurrence of accidents at the intersection into which the vehicle <b>10</b> is about to enter. For example, for the coefficients c, d, values that become larger as the frequency of occurrence of accidents increases are predefined and stored in the recording unit <b>16</b>. Specifically, the values of the coefficients c, d may be determined with the use of a function that defines the values of the coefficients c, d that change in accordance with the frequency of occurrence of accidents or a table in which the values of the coefficients c, d respectively corresponding to the values of the frequency of occurrence of accidents are stored.</p>
<p id="p-0180" num="0181">In this way, the magnification size of the magnified image of a road mirror may be determined in accordance with the frequency of occurrence of accidents at the intersection into which the vehicle <b>10</b> is about to enter. As a result, at an intersection with a high frequency of occurrence of accidents, the magnified image of a road mirror is increased in size, so that an image easily recognizable by the driver H<b>1</b> may be displayed on the monitor <b>6</b>.</p>
<p id="p-0181" num="0182">All examples and conditional language recited herein are intended for pedagogical purposes to aid the reader in understanding the invention and the concepts contributed by the inventor to furthering the art, and are to be construed as being without limitation to such specifically recited examples and conditions, nor does the organization of such examples in the specification relate to a showing of the superiority and inferiority of the invention. Although the embodiment(s) of the present invention has (have) been described in detail, it should be understood that the various changes, substitutions, and alterations could be made hereto without departing from the spirit and scope of the invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08624717-20140107-M00001.NB">
<img id="EMI-M00001" he="7.45mm" wi="76.20mm" file="US08624717-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08624717-20140107-M00002.NB">
<img id="EMI-M00002" he="7.45mm" wi="76.20mm" file="US08624717-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An image processor comprising:
<claim-text>an image input unit that obtains an image captured by an imaging device installed on a vehicle;</claim-text>
<claim-text>a vehicle information input unit that obtains a distance between the vehicle and a road junction or curve in a traveling direction of the vehicle based on information received from a vehicle-mounted device installed in the vehicle;</claim-text>
<claim-text>a recording unit in which imaging information indicating properties of the imaging device and driver information regarding a visual field of a driver of the vehicle are stored;</claim-text>
<claim-text>a magnifying object recognition unit that recognizes a certain magnifying object in the captured image; and</claim-text>
<claim-text>a composing unit that produces, when the magnifying object recognition unit recognizes the certain magnifying object, a composite image of a magnified image off of the magnifying object and the captured image,</claim-text>
<claim-text>wherein the composing unit uses the distance obtained by the vehicle information input unit as well as the imaging information and the driver information stored in the recording unit to determine a non-blind spot area in the captured image that does not include an image of a blind spot for the driver, and produces the composite image such that the magnified image is superimposed on the non-blind spot area.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The image processor according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the composing unit uses the distance between the vehicle and the junction or the curve and the imaging information to calculate an imaging area to be captured by the imaging device in a vicinity of a position of the junction or the curve,
<claim-text>uses the distance between the vehicle and the junction or the curve and the driver information to calculate a visual field area to be in the visual field of the driver in the vicinity of the position of the junction or the curve, and</claim-text>
<claim-text>uses a positional relationship between the imaging area and the visual field to determine a position of the non-blind spot area in the captured image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The image processor according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the composing unit uses the distance obtained by the vehicle information input unit to determine a magnification level of the magnified image.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The image processor according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the image input unit further obtains a horizontal rudder angle of the imaging device at a time of capturing the captured image, and
<claim-text>the composing unit uses the horizontal rudder angle to calculate the imaging area.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The image processor according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the vehicle information input unit further obtains a value representing a curvature of the curve in the traveling direction of the vehicle, and
<claim-text>the composing unit uses the curvature to determine a magnification level of the magnified image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The image processor according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the vehicle information input unit further obtains information indicating a frequency of occurrence of accidents at the junction or the curve, and
<claim-text>the composing unit uses the frequency of occurrence of accidents to determine a magnification level of the magnified image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The image processor according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image processor works in conjunction with a navigation system installed in the vehicle, and
<claim-text>the vehicle information input unit obtains the distance between the vehicle and the road junction or the curve in the traveling direction of the vehicle from the navigation system installed in the vehicle.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The image processor according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the vehicle information input unit further obtains information indicating presence or absence of the magnifying object at the junction or the curve from the navigation system, and
<claim-text>the magnifying object recognition unit recognizes the certain magnifying object in the captured image when the certain magnifying object is present at the junction or the curve.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The image processor according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the magnifying object recognition unit recognizes a certain magnifying object in the captured image when the distance becomes equal to or less than a certain distance.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A non-transitory recording medium storing an image processing program causing a computer to perform processing of:
<claim-text>obtaining an image captured by an imaging device installed on a vehicle;</claim-text>
<claim-text>obtaining a distance between the vehicle and a road junction or curve in a traveling direction of the vehicle based on information received from a vehicle-mounted device installed in the vehicle;</claim-text>
<claim-text>obtaining imaging information indicating properties of the imaging device and driver information regarding a visual field of a driver of the vehicle stored in a recording unit accessible to the computer;</claim-text>
<claim-text>recognizing a certain magnifying object in the captured image; and</claim-text>
<claim-text>producing, when the certain magnifying object is recognized in the magnifying object recognition, a composite image of a magnified image off of the magnifying object and the captured image,</claim-text>
<claim-text>wherein in the production of the composite image, the obtained distance as well as the obtained imaging information and driver information are used to determine a non-blind spot area in the captured image that does not include an image of a blind spot for the driver, and the composite image is produced such that the magnified image is superimposed on the non-blind spot area.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A vehicle-mounted terminal working in conjunction with an imaging device installed on a vehicle, the vehicle-mounted terminal comprising:
<claim-text>a navigation system having a function of identifying a position of the vehicle and including a map data recording unit in which road information including a position of each road junction or curve is stored;</claim-text>
<claim-text>an image input unit that obtains an image captured by the imaging device;</claim-text>
<claim-text>a vehicle information input unit that obtains a distance between the vehicle and a road junction or curve in a traveling direction of the vehicle with the use of the position of the vehicle identified by the navigation system and the road information;</claim-text>
<claim-text>a recording unit in which imaging information indicating properties of the imaging device and driver information regarding a visual field of a driver of the vehicle are stored;</claim-text>
<claim-text>a magnifying object recognition unit that recognizes a certain magnifying object in the captured image;</claim-text>
<claim-text>a composing unit that produces, when the magnifying object recognition unit recognizes the certain magnifying object, a composite image of a magnified image off of the magnifying object and the captured image; and</claim-text>
<claim-text>a display unit that displays the composite image produced by the composing unit,</claim-text>
<claim-text>wherein the composing unit uses the distance obtained by the vehicle information input unit as well as the imaging information and the driver information stored in the recording unit to determine a non-blind spot area in the captured image that does not include an image of a blind spot for the driver, and produces the composite image such that the magnified image is superimposed on the non-blind spot area. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
