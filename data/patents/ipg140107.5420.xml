<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626520-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626520</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13935230</doc-number>
<date>20130703</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20120101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>Q</subclass>
<main-group>10</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>705  11</main-classification>
<further-classification>705301</further-classification>
</classification-national>
<invention-title id="d2e43">Apparatus and method for processing service interactions</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5855003</doc-number>
<kind>A</kind>
<name>Ladden et al.</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6161087</doc-number>
<kind>A</kind>
<name>Wightman et al.</name>
<date>20001200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6535848</doc-number>
<kind>B1</kind>
<name>Ortega et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7133513</doc-number>
<kind>B1</kind>
<name>Zhang</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>8332212</doc-number>
<kind>B2</kind>
<name>Wittenstein et al.</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>8335689</doc-number>
<kind>B2</kind>
<name>Wittenstein et al.</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>8442197</doc-number>
<kind>B1</kind>
<name>Mazza et al.</name>
<date>20130500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>8498872</doc-number>
<kind>B2</kind>
<name>White et al.</name>
<date>20130700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2002/0152071</doc-number>
<kind>A1</kind>
<name>Chaiken et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2003/0110023</doc-number>
<kind>A1</kind>
<name>Bangalore et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  5</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2004/0093257</doc-number>
<kind>A1</kind>
<name>Rogers et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 10</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2005/0010407</doc-number>
<kind>A1</kind>
<name>Jaroker</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2006/0074895</doc-number>
<kind>A1</kind>
<name>Belknap</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2008/0319744</doc-number>
<kind>A1</kind>
<name>Goldberg</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2009/0124272</doc-number>
<kind>A1</kind>
<name>White et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2009/0276215</doc-number>
<kind>A1</kind>
<name>Hager</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2009/0306981</doc-number>
<kind>A1</kind>
<name>Cromack et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2009/0319265</doc-number>
<kind>A1</kind>
<name>Wittenstein et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2010/0268534</doc-number>
<kind>A1</kind>
<name>Kishan Thambiratnam et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2011/0099011</doc-number>
<kind>A1</kind>
<name>Jaiswal</name>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2011/0112833</doc-number>
<kind>A1</kind>
<name>Frankel et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Anonymous. &#x201c;Let your voice do the walking&#x201d;. Call Center Magazine. 15.11 (Nov. 2002): 28-40.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>United States Office Action, U.S. Appl. No. 12/618,742, Sep. 17, 2013, sixteen pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>10</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>705  11</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>705301</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>13708264</doc-number>
<date>20121207</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8484042</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13935230</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12551864</doc-number>
<date>20090901</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8332231</doc-number>
<date>20121211</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13708264</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10839536</doc-number>
<date>20040505</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7606718</doc-number>
<date>20091020</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12551864</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60467935</doc-number>
<date>20030505</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130297496</doc-number>
<kind>A1</kind>
<date>20131107</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>Interactions Corporation</orgname>
<address>
<city>Franklin</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Cloran</last-name>
<first-name>Michael Eric</first-name>
<address>
<city>Lawrenceville</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fenwick &#x26; West LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Interactions Corporation</orgname>
<role>02</role>
<address>
<city>Franklin</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>McCormick</last-name>
<first-name>Gabrielle</first-name>
<department>3629</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An interactive voice and data response system that directs input to a voice, text, and web-capable software-based router, which is able to intelligently respond to the input by drawing on a combination of human agents, advanced speech recognition and expert systems, connected to the router via a TCP/IP network. The digitized input is broken down into components so that the customer interaction is managed as a series of small tasks performed by a pool of human agents, rather than one ongoing conversation between the customer and a single agent. The router manages the interactions and keeps pace with a real-time conversation. The system utilizes both speech recognition and human intelligence for purposes of interpreting customer utterances or customer text, where the role of the human agent(s) is to input the intent of caller utterances, and where the computer system&#x2014;not the human agent&#x2014;determines which response to provide given the customer's stated intent (as interpreted/captured by the human agents). The system may use more than one human agent, or both human agents and speech recognition software, to interpret simultaneously the same component for error-checking and interpretation accuracy.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="178.48mm" wi="247.31mm" file="US08626520-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="251.63mm" wi="191.18mm" orientation="landscape" file="US08626520-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="272.37mm" wi="145.88mm" file="US08626520-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="267.97mm" wi="201.51mm" orientation="landscape" file="US08626520-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="269.07mm" wi="207.52mm" orientation="landscape" file="US08626520-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="264.67mm" wi="194.99mm" orientation="landscape" file="US08626520-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="260.69mm" wi="208.20mm" orientation="landscape" file="US08626520-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="275.42mm" wi="189.65mm" orientation="landscape" file="US08626520-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="271.27mm" wi="197.19mm" orientation="landscape" file="US08626520-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="235.80mm" wi="148.67mm" file="US08626520-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims priority to U.S. Provisional Application No. 60/467,935, filed May 3, 2003, and is a continuation of U.S. patent application Ser. No. 10/839,536, entitled &#x201c;APPARATUS AND METHOD FOR PROCESSING SERVICE INTERACTIONS&#x201d; and filed on May 5, 2004, pending.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD</heading>
<p id="p-0003" num="0002">This invention relates to the field of interactive response communication systems, and, more particularly to an interactive response communications system that use human interpretation of customer intent as input to a workflow on a computer.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Many companies interact with their customers via electronic means (most commonly via telephone, e-mail, and online text chat). Such electronic systems save the companies a large amount of money by limiting the number of customer service or support agents needed. These electronic systems, however, generally provide a less than satisfactory customer experience. The customer experience may be acceptable for simple transactions, but are frequently inconsistent or downright frustrating if the customer is not adept at talking to or interacting with a computer.</p>
<p id="p-0005" num="0004">Such interactive response systems are well known in the art. For example, providing customer service via telephone using an interactive voice response (IVR) system is one such system. An example of customer service systems utilizing IVR technology is described in U.S. Pat. No. 6,411,686. An IVR system typically communicates with customers using a set of prerecorded phrases, responds to some spoken input and touch-tone signals, and can route or transfer calls. A drawback to such IVR systems is that they are normally built around a &#x201c;menu&#x201d; structure, which presents callers with just a few valid options at a time and require a narrow range of responses from callers.</p>
<p id="p-0006" num="0005">Many of these IVR systems now incorporate speech recognition technology. An example of a system incorporating speech recognition technology is described in U.S. Pat. No. 6,499,013. The robustness of the speech recognition technology used by IVR systems vary, but at present all have a predetermined range of responses that they listen for and can understand, which limits the ability of the end user to interact with the system in everyday language. Therefore, the caller will often feel that they are being forced to speak to the system &#x201c;as though they are talking to a computer.&#x201d; Moreover, even when interacting with a system that utilizes speech recognition, customer input is often either not recognized or incorrectly determined, causing the customer to seek a connection to a human customer service agent as soon as possible.</p>
<p id="p-0007" num="0006">Human customer service agents continue to be used for more involved customer service requests. These agents may speak to the customer over the phone, respond to customer e-mails, and chat with customers online. Agents normally answer customer questions or respond to customer requests. Companies have customer service groups, which are sometimes outsourced to businesses that specialize in &#x201c;customer relations management.&#x201d; Such businesses run centers staffed by hundreds of agents who spend their entire working day on the phone or otherwise interacting with customers. An example of such system is described in U.S. Pat. No. 3,987,116.</p>
<p id="p-0008" num="0007">The typical model of customer service interaction is for one agent to assist a customer for the duration of the customer's interaction. At times, one agent (for example, a technical support representative) may transfer the customer to another agent (such as a sales representative) if the customer needs help with multiple requests. But in general, one agent spends his or her time assisting that one customer for the full duration of the customer's call or chat session, or is occupied resolving the customer's issues via e-mail. Most call centers also expect the agent to take the time to log (document) the call. Deficiencies in this heavy agent interface model is (1) there is a high agent turnover rate and (2) a great deal of initial and ongoing agent training is usually required, which all add up to making customer service a significant expense for these customer service providers.</p>
<p id="p-0009" num="0008">In order to alleviate some of the expenses associated with agents, some organizations outsource their customer service needs. One trend in the United States in recent years, as high-speed fiber optic voice and data networks have proliferated, is to locate customer service centers overseas to take advantage of lower labor costs. Such outsourcing requires that the overseas customer service agents be fluent in English. In cases where these agents are used for telephone-based support, the agent's ability to understand and speak clearly in English is often an issue. An unfortunate result of off-shore outsourcing is misunderstanding and a loss than satisfactory customer service experience for the person seeking service.</p>
<p id="p-0010" num="0009">Therefore, there is a need in the art for an interactive system that provides a consistently high-quality experience without the expense of a large staff of dedicated, highly trained agents.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0011" num="0010">It is therefore an object to provide an interactive response system with interactions portioning. That is, human agent would be able to interact intermittently through a customer call by hearing only those portions of the call requiring his or her interpretation so no one customer service agent is tied to the customer's conversation for its full duration.</p>
<p id="p-0012" num="0011">It is an additional object of some embodiments to provide an interactive response system with multiple-agent checking so that a customer's intent, input (data) or both, is accurately determined. Using double, triple or more checking, more than one human agent evaluates and chooses an interpretation for an instance of customer input, thus improving accuracy of the call and providing an objective measure of each human agent's speed and accuracy.</p>
<p id="p-0013" num="0012">It is also an object of some embodiments to provide an interactive response system with agent portability so that customer service agents can be located nearly anywhere in the world. If a human agent is needed to interpret intent, data, or both, one, or advantageously two or more, agents hear or see only the component of the interaction need to be interpreted or translated into a context that the interactive response system understands. The interactive response system handles outgoing communication to the end user, including speaking (using text-to-speech or professionally recorded clips) so the human agent's voice would never be heard by the customer, eliminating any concern of an agent's accent. The actual interaction with the customer is managed by a software-based router whose front end is email, interactive data or speech capable.</p>
<p id="p-0014" num="0013">It is a further object of some embodiments to provide an interactive response system in which the customer can speak in a conversational tone instead of responding as if &#x201c;speaking to a computer.&#x201d; A router in accordance with some embodiments of this disclosure seamlessly blends both human customer service agents (for interpretation of written or spoken input) and software speech recognition (for spoken word input) to interpret customer input in real-time for intelligent interpretation.</p>
<p id="p-0015" num="0014">It is an event further object of some embodiments to provide an interactive response system that allows for simplified human agent training and evaluation. This system provides the ability for multiple agents to evaluate the same component of customer input simultaneously. Further, this system provides a means to objectively rate the speed and accuracy of an agent's response to input, which greatly simplifies the hiring and training process. New agents can act in the customer service agent role without their response being weighted by the router, but the agent can still receive feedback on their performance. An objective performance measure then exists to decide when to promote a new hire from trainee status. In addition, all responses to customers used by the system are scripted and engineered, removing the huge requirements of training customer service agents in how to speak to customers.</p>
<p id="p-0016" num="0015">It is another object of some embodiments to provide an interactive response system that allows for workload balancing by dynamically adjusting the number of agents assigned to each component of customer interaction for purposes of multiple agent checking. For example, in times of heavier end-user traffic, the system advantageously evaluates and executes a tradeoff between agent accuracy and availability. To effect such balancing, some components of customer input are single-checked by the most accurate agents&#x2014;thereby maintaining 100% availability of the system. At times of lower traffic, accuracy is increased through triple or quadruple checking, which also creates a steady pace of work for human agents. Being able to ramp up availability without severely degrading accuracy is a significant enhancement over current call center models.</p>
<p id="p-0017" num="0016">It is yet another object of some embodiments to provide an interactive response system that provides speech acceleration to enable faster customer service and response time. Acceleration applied to audio being streamed across a TCP/IP network to help overcome delays introduced by application setup times.</p>
<p id="p-0018" num="0017">It is even yet another object of some embodiments to provide an interactive response system with interaction control such that interactive steps with customers are determined by choices in a workflow. Advantageously, workflows are updated any time by business analysts, process engineer, or company appointed personnel.</p>
<p id="p-0019" num="0018">It is a still further object of some embodiments to provide an interactive response system with end user security so that customer confidential data is kept secure. Workflows may be advantageously designed so that the automated portion of the system can handle tasks involving sensitive data such as social security numbers, credit cards, etc., whereby the human agents never have access to this data. Even if a workflow requires that customer service agents do handle sensitive data, the workflow may be engineered to distribute the components of the input in a manner such that no one agent handles the whole of the customer data. For example, one agent might see or hear a full customer name, while another has access to the customer's social security number, and neither learns the customer's home address.</p>
<p id="p-0020" num="0019">These and other objects are accomplished in various forms and embodiments in accordance with the various principles described herein by providing an interactive response system that uses human agents to interpret and input customer intent from customer utterances or written text. This embodiment provides a system and method of bleeding human interpretation, speech recognition technology, text parsing and lexical analysis, text-to-speech capabilities and other resources in a system for the automated processing of customer-company interactions.</p>
<p id="p-0021" num="0020">The exemplary system discussed in the present disclosure is a solution for customer relations management. The central element of the system is a software-based router that manages the conversation with the end user either in real-time (voice, online text chats) or correspondence (e-mail). The router follows rules (herein called &#x201c;workflows&#x201d;) developed and tweaked over time by business analysts. These rules form a script for the router to follow when interacting with end users. The router draws on both text-to-speech capabilities and prerecorded responses when replying to an end user. For interpretation of user utterances, the router employs both speech recognition technology and the interpretive abilities of human customer service agents, seamlessly blending the two. This blending can be performed in real-time or near real-time to allow the router to carry on a conversation-like interaction with an end user. The incorporation of human interpretation of user utterances or written text allows the router to use open-ended, conversational prompts, and to respond in context to user input that software might find ambiguous. Users are thus able to interact with the system using everyday language, and are not forced into a narrow range of responses.</p>
<p id="p-0022" num="0021">The exemplary system integrates human agents in an innovative way. Because user input is digitized, the router can direct only those portions of the input that require human interpretation to human agents. No one customer service agent is tied to a customer conversation for its entire duration; the interaction is managed by the router itself. Also, the router is able to send the digitized input to more than one human agent for simultaneous interpretation, which provides double and triple checking of each answer from the agents. Such doable and triple check also provides an objective measure and ranking of the speed and accuracy of the agents. The system is designed to work over a TCP/IP network, so that the customer service agents can be located virtually anywhere in the world. Advantageously, the system comprises off-the-shelf hardware and software, some customized software (as noted below), and can be integrated with existing company resources, such as databases and telephone networks.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0023" num="0022">Further features, the nature of various embodiments of the system, and various advantages will be more apparent from the following detailed description of the preferred embodiment, taken in conjunction with the accompanying drawings, in which like reference characters refer to like parts throughout, and in which:</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating one embodiment of an architecture of an interactive response system according to an exemplary embodiment;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 2</figref> is a flow chart illustrating an embodiment of a method for communication among a customer, the interactive response system and a human interface;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 3A</figref> is a chart illustrating one embodiment of a customer/interactive response system interaction in the context of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 3B</figref> is a computer screen illustrating one embodiment for capturing customer intent and data in the context of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 4A</figref> is a chart illustrating one embodiment of a customer/interactive response system interaction in the context of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 4B</figref> is a computer screen illustrating one embodiment for capturing customer intent and data in the context of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 5A</figref> is a chart illustrating one embodiment of a customer/interactive response system interaction in the context of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 5B</figref> is a computer screen illustrating one embodiment for capturing customer intent and data in the context of <figref idref="DRAWINGS">FIG. 2</figref>; and</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 6</figref> is a flow chart of processing an email in the context of an interactive response system in accordance with another aspect of this system.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DESCRIPTION</heading>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 1</figref> illustrates one embodiment of an architecture for connecting an interactions platform <b>102</b> to an interactive response system <b>100</b> through an interactive router <b>101</b> (herein referred to as an &#x201c;iRouter&#x201d;). As shown in <figref idref="DRAWINGS">FIG. 1</figref>, interactions platform <b>102</b> is connected to a customer <b>103</b> through communications link <b>104</b>. Interactions platform <b>102</b> is also connected to interactive response system <b>100</b> at iRouter <b>101</b> via a data link, which comprises a TCP/IP data link in this exemplary embodiment. Interactions platform <b>102</b> in this exemplary embodiment comprises a computer server. The exact configuration of the computer server varies with the implementation but typically consists of a Pentium-based server running an operating system such as Windows XP Professional or Linux, using a voice board from a vendor such as Dialogic. Interactions platform <b>102</b> can also be an e-mail gateway or web server. Thus, customer input enters interactive response system <b>100</b> via telephone or intercom and text is entered via email or an interactive chatting interface (e.g., a web page or a stand-alone application such as AOL Instant Messenger).</p>
<p id="p-0034" num="0033">In this architecture of <figref idref="DRAWINGS">FIG. 1</figref>, a number of different types of devices can be used to implement each of the interactions platform <b>102</b> and communications links <b>104</b>. Interactions platform <b>102</b> may be implemented by any device capable of communicating with the customer <b>103</b>. For example, interactions platform <b>102</b> may be a telephony server in interactive response system <b>100</b> where the customer is calling by telephone. The telephony server handles answering, transferring and disconnecting incoming calls. The telephony server is also a storehouse for prerecorded audio clips so that it can play any welcome prompt and as other audio clips as directed by iRouter <b>101</b>.</p>
<p id="p-0035" num="0034">A telephony server in accordance with this embodiment is assembled from off-the-shelf components, for example Windows XP Professional for an operating system, a central processor, such as a Pentium processor, and an Intel Dialogic voice board. Using this architecture, the communications link <b>104</b> may be implemented by any means of providing an interface between the customer's telephone and the telephony server. For example, communications link <b>104</b> may be a dial-up connection or a two-way wireless communication link.</p>
<p id="p-0036" num="0035">In another exemplary embodiment, interactions platform <b>102</b> may be a gateway server in interactive response systems <b>100</b>. In accordance with this exemplary embodiment, the customer interacts with the interactive response server by e-mail or by interactive text chats. The gateway server runs customized open source e-mail or www server software. Further, a gateway server in accordance with this exemplary embodiment is designed to conduct e-mail and interactive text chat transactions with customers, while also forwarding and receiving data to other elements of the system. Using this architecture, the communications link <b>104</b> may be implemented by any means of providing an interface between the customer's computer and the gateway server. For example, communications link <b>104</b> may be a dedicated interface, a single network, a combination of networks, a dial-up connection or a cable modem.</p>
<p id="p-0037" num="0036">While only one interactions platform <b>102</b> is illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, one skilled in the art will appreciate that multiple interactions platforms <b>102</b> may be used in this system after studying this specification. With multiple interactions platforms <b>102</b>, an interactive response system may communicate via voice and text data with a customer. Further, multiple customer bases may be accommodated by a dedicated interactions platform <b>102</b> for each of the customer bases. In this manner, a workflow (as will be described further, below) can be selected by determining which of the multiple interactions platforms <b>102</b> initiated the interaction.</p>
<p id="p-0038" num="0037">In the architecture of <figref idref="DRAWINGS">FIG. 1</figref>, the iRouter <b>101</b> comprises software to control interactive response system <b>100</b>. iRouter <b>101</b> &#x201c;owns&#x201d; the interaction with customer <b>103</b> from beginning to end by coordinating activity among other components and managing the transaction. iRouter <b>101</b> manages interactions with customer <b>103</b> according to one or more programmable scripts, called, according to this exemplary embodiment, &#x201c;workflows.&#x201d; In general, a workflow comprises an interaction flow wherein the path through the workflow depends upon intent input from the customer. Workflows are preprogrammed by system engineers and, advantageously, periodically &#x201c;tweaked&#x201d; in order to improve customer satisfaction, speed, accuracy, etc. In accordance with this exemplary embodiment, iRouter <b>101</b> is almost always &#x201c;in charge&#x201d; of selecting the next step or path in the workflow.</p>
<p id="p-0039" num="0038">iRouter <b>101</b> receives interaction input from interactions platform <b>102</b> in the form of audio clips, email, text data or other interaction type&#x2014;depending on the form of customer communication&#x2014;and forwards the input to one or more human agents <b>105</b>, speech recognition engines or expert systems (collectively <b>108</b>) and uses the responses to advance its current workflow. When human interpretation (or translation) of the input is necessary, iRouter <b>101</b> directs human agent desktop software to display an appropriate visual context of the current workflow. Once iRouter <b>101</b> understands the input, iRouter <b>101</b> advances through the workflow and directs interactions platform <b>102</b> to respond appropriately to customer <b>103</b>.</p>
<p id="p-0040" num="0039">In an exemplary embodiment wherein interactions platform <b>102</b> comprises a telephony server, iRouter <b>101</b> may deliver sound clips to play back to a customer, send text-to-speech clips or both. Alternatively, interactions platform <b>102</b> may store sound clips, have text-to-speech capability or both. In this embodiment, iRouter directs interactions platform <b>102</b> as to what to play to a customer and when.</p>
<p id="p-0041" num="0040">iRouter <b>101</b> comprises, in this exemplary embodiment, a networked, off-the-shelf commercially available processor running an operating system such as Windows XP or Linux. Further, iRouter <b>101</b> software includes a modified open VXML browser and voice XML script incorporating objects appropriate to the specific application. One skilled in the art will understand how to construct these objects after studying this specification.</p>
<p id="p-0042" num="0041">In accordance with the exemplary architecture of <figref idref="DRAWINGS">FIG. 1</figref>, interactive response system <b>100</b> includes at least one pool of human agents <b>105</b>. A pool of human agents <b>105</b> is often located at a contact center site. Human agents <b>105</b>, in accordance with the present embodiment, use specialized desktop software specific to system <b>100</b> (as will be described further, below, in connection with <figref idref="DRAWINGS">FIGS. 3B</figref>, <b>4</b>B and <b>5</b>B) that presents a collection of possible intents on their screen&#x2014;along with a history or context of the customer interaction to that point. The human agent or agents <b>105</b> interpret the input and select an appropriate customer intent, data or both.</p>
<p id="p-0043" num="0042">For telephone interactions, human agents <b>105</b> wear headphones and hear sound clips streamed from the telephony server <b>102</b> at the direction of iRouter <b>101</b>. In accordance with one aspect of this system, a single human agent <b>105</b> will not handle the entire transaction for customer <b>103</b>. Rather, human agent <b>105</b> handles some piece of the transaction that has been designated by the workflow designer as requiring human interpretation of customer's <b>103</b> utterance. iRouter <b>101</b> can send the same customer <b>103</b> interaction to any number of human agents <b>105</b>, and may distribute pieces of a given interaction to many different human agents <b>105</b>.</p>
<p id="p-0044" num="0043">In accordance with the exemplary embodiment of this system, human agents <b>105</b> are preferably off-site. Further, human agents <b>105</b> may be in diverse geographic areas of the world, such as India, the Philippines and Mexico. Human agents <b>105</b> may be in groups in a building or may be working from home. In applications that require 24/7 human agent support, human agents may be disposed around the world so that each human agent may work during suitable business hours.</p>
<p id="p-0045" num="0044">Interactive response system <b>100</b> employs custom human agent application software. Human agents <b>105</b> use a custom application developed in Java and running on a standard call center computer network workstation. Generally speaking, interactive response system <b>100</b> applies human intelligence towards interpretation of customer <b>103</b> input into &#x201c;intent&#x201d; (what the customer wants) and data (any input required to determine what the customer wants). The interpretation normally comprises selecting the most-correct interpretation of what was said from a list of choices, in this exemplary embodiment.</p>
<p id="p-0046" num="0045">Workflow server <b>106</b>, an off-the-shelf component, is an archive of the workflows used by the Interactions router. Workflow server <b>106</b> can be built with off-the-shelf hardware using a commercially available processor running a standard server operating system, with the workflow documents written in XML in this exemplary embodiment. Workflow server <b>106</b> maintains a compilation of business rules that govern the behavior of iRouter <b>101</b>.</p>
<p id="p-0047" num="0046">Interactive response system <b>100</b> employs a workflow designer used by a business analyst or process engineer to may out workflows. A workflow serves as the map that iRouter <b>100</b> follows in a given interaction, with speech recognition or human agents. The workflow &#x201c;steers&#x201d; iRouter <b>100</b> along a path in the workflow in response to customer input. A place in the workflow, along with data collected to that point is called a &#x201c;context.&#x201d;</p>
<p id="p-0048" num="0047">The workflow designer builds instructions for human agent <b>105</b> into the workflow in order to guide human agent <b>105</b> in interpreting intent. The workflow designer may include a version of Eclipse software development environment customized to focus on building XML documents. However, one skilled in the art will be able to develop a workflow designer after studying this specification.</p>
<p id="p-0049" num="0048">Performance and interactions archive <b>107</b> comprises a database that can be maintained on any common computer server hardware. Performance and interactions archive <b>107</b> contains both archival data of system transactions with customers <b>103</b> (i.e., a repository of sound clips, e-mails, chats, etc. from interactions with customer <b>103</b>) as well as performance data for human agents <b>105</b>.</p>
<p id="p-0050" num="0049">This exemplary embodiment employs &#x201c;reporter&#x201d; software to generate statistics about a group of interactions or to display performance ranking for human agent <b>105</b>. Reporter software can also reconstruct an interaction with customer <b>103</b> from sound clips, e-mails, or chat text that constituted customer's <b>103</b> contact stored in interactions archive <b>107</b>. Reporter software is a series of simple scripts, and can run on any common server hardware.</p>
<p id="p-0051" num="0050">This exemplary embodiment also comprises manager/administrator software, usually run from the same station as reporter software. Manager/administrator software sets operating parameters for interactive response system <b>100</b>. Such operating parameters include, but are not limited to, business rules for load balancing, uploading changes in workflow, and other administrative changes. Manager/administrator software is often a small custom Java application running on a standard call center computer workstation.</p>
<p id="p-0052" num="0051">Support system <b>108</b> consists of numerous databases and customer proprietary systems (also including off-the-shelf speech recognition software such as Speechworks) that may be employed in responding to customer <b>103</b> requests. For example, support system <b>108</b> may include a database for customer information or a knowledge base. Speech recognition software is, in this exemplary embodiment, an off-the-shelf component used to interpret customer <b>103</b> utterances. Support system <b>108</b> may also include a text-to-speech capability, often off-the-shelf software that reads text to customer <b>103</b>.</p>
<p id="p-0053" num="0052">Company agents <b>109</b> consist of human agents that handle customer <b>103</b> requests that the workflow refers to them. For example, should customer <b>103</b> intend to obtain assistance with a company matter, and an outsourced human agent <b>105</b> identifies that intent, the workflow may direct interactive response system <b>100</b> to transfer the call to company agent <b>109</b>.</p>
<p id="p-0054" num="0053">The elements of interactive response system <b>100</b> communicate over a TCP/IP network in this exemplary embodiment. Communication is driven by the workflow that iRouter <b>101</b> follows. &#x201c;Database&#x201d; in the present embodiment can be a flat file database, a relational database, an object database, or some combination thereof.</p>
<p id="p-0055" num="0054">&#x201c;Server&#x201d; and &#x201c;workstation&#x201d; refer to any general purpose computer system which is programmable using a computer programming language, such as C++, Java, or other language, such as a scripting language or assembly language. These computer systems may also include specially programmed, special purpose hardware, for example Intel Dialogic voice boards.</p>
<p id="p-0056" num="0055">The system is not limited to a particular computer platform, particular processor, particular operating system, or particular high-level programming language. Additionally, the computer system may be a multiprocessor computer system or may include multiple computers connected over a computer network. The system is also not limited to any particular implementation using software or hardware or firmware, or any combination thereof.</p>
<p id="p-0057" num="0056">Turning now to <figref idref="DRAWINGS">FIGS. 2 through 5</figref>, these figures illustrate an example of how information is retrieved and handled by interactive response system <b>100</b> when a customer interacts with the interactive response system <b>100</b> via telephone. The example shown in <figref idref="DRAWINGS">FIG. 2</figref> presupposes that all required hardware, software, networking and system integration is complete, and that a business analyst has mapped out the possible steps in a customer interaction using the graphic workflow designer. The business analyst also has scripted the text for anything that the interactive response system may say to a customer, including, but not limited to, the initial prompt (e.g., &#x201c;Thank you for calling, how can I help you today?&#x201d;), response(s) to a customer, requests for additional information, &#x201c;stutter speech&#x201d; (sounds sent to the customer while the iRouter is determining a response), and a closing statement. Either text-to-speech software or voice talent records the server-side speech pieces as written by the business analyst. This workflow is then loaded into the interactive response system where it is available to the iRouter.</p>
<p id="p-0058" num="0057">As shown in block <b>201</b>, the interaction begins with the customer calling the customer service telephone number of a company. The interactions platform, in this case a telephony server, answers the telephone call and retrieves the appropriate workflow stored in the workflow database, based on either (1) ANI/DNIS information of the caller or (2) other business rules (e.g., line or trunk the call came in on), as illustrated at <b>202</b>. The telephony server then plays the appropriate welcome prompt as illustrated at <b>203</b> and the customer then responds to that prompt (block <b>204</b>).</p>
<p id="p-0059" num="0058">For purpose of example, an imaginary airline, Interair, provides customer service via an interactive response system in accordance with a call center embodiment of this system. The interaction platform is therefore a telephony interface, and iRouter selects a workflow appropriate to Interair.</p>
<p id="p-0060" num="0059">A first point or context in the workflow is shown in the illustrative workflow of <figref idref="DRAWINGS">FIG. 3A</figref>. There is no customer utterance, thus no intent or data to capture (and respond to). The only response is the greeting and the prompt for customer input.</p>
<p id="p-0061" num="0060">Processing proceeds to box <b>204</b> in the flowchart of <figref idref="DRAWINGS">FIG. 2</figref>. The telephony server begins digitizing the customer's spoken input and connects to the iRouter. At this point, workflow or business rules determine if the interactive response to the customer needs to be handled by a human agent or speech recognition software. That is, the iRouter selects the appropriate workflow for the call from the workflow repository and follows the workflow rides to conduct a conversation with the customer.</p>
<p id="p-0062" num="0061">To interpret customer speech, iRouter uses software-based speech recognition from the support systems or has the customer's audio streamed to human agents in contact centers as appropriate, as illustrated in block <b>205</b>. If human agents are required by the workflow, iRouter identifies available human agents by applying a load balancing algorithm, triggers a pop-up on their screens (as illustrated in the initially blank pop-up screen, <figref idref="DRAWINGS">FIG. 3B</figref>), presents several selectable intent options, and begins streaming customer audio to the identified human agents, as shown at block <b>207</b>. This load balancing, at various times, includes identifying more or fewer human agents for interpreting the utterance based on any of a variety of factors, as will occur to those skilled in the art given the present disclosure. The human agent(s) hear the customer utterance in headphones, and computer software prompts for an interpretation of the utterance as shown in blocks <b>210</b> and <b>211</b>.</p>
<p id="p-0063" num="0062">In accordance with the exemplary workflow of <figref idref="DRAWINGS">FIG. 4A</figref>, the customer utterance that the human agent or agents hear is &#x201c;I need to check my flight from Chicago to London this afternoon.&#x201d; The agents' screen indicates the current context (or point in the workflow) as illustrated in <figref idref="DRAWINGS">FIG. 4B</figref>. In this illustrative screen shot, there are 12 possible requests (including unanswerable and terminate) that the human agent can select. In operation, there are several hundred possible interpretations available to the agents. Such multiplicity of selection allows the agents interpretive flexibility, which enables the iRouter to jump around in its workflow according to the interpreted intent. Thus, in accordance with one aspect of this system, the iRouter can respond appropriately even if the customer changes subjects in midstream.</p>
<p id="p-0064" num="0063">In each case, each agent selects what he or she feels is the best fit interpretation of the customer utterance in the current context of the workflow. In example of <figref idref="DRAWINGS">FIG. 4B</figref>, the human agent(s) selects &#x201c;CFT&#x201d; (Check Flight Time) and enters or selects from drop-down menus the departure and arrival cities (or other, preprogrammed information that the customer could possibly utter).</p>
<p id="p-0065" num="0064">Note that, in blocks <b>208</b> and <b>209</b>, human agents can elect to apply acceleration to the customer audio clip(s) received at the station in order to compensate for any response delay (usually due to lag time in application set-up&#x2014;the time it will take for human agent desktop software to accept the streaming audio and display the appropriate workflow). Network latency might be around 0.2 seconds, where application delay could be more in the 1+ second range. To compensate for the application delay, the interactive response system accelerates the voice clip (although not to the point of discernible distortion). The purpose is to strive for a more &#x201c;real-time&#x201d; conversational interaction, so that the customer does not experience a notable delay while awaiting a response. The acceleration is applied to the speech as it is streaming from the telephony server. The acceleration can never overcome the inherent latency of the link but allows human agents to &#x201c;recover&#x201d; the application set-up time and reduce the amount of lag time in the interaction, ideally up to the limits imposed by latency in the network. However, acceleration is optional in this embodiment, wherein a novice agent may need a slower playback, while a more experienced agent may apply acceleration.</p>
<p id="p-0066" num="0065">In test <b>213</b>, the iRouter evaluates the accuracy, in real time, of the customer audio interpretation and updates each agent's speed/accuracy profile. Next, in block <b>214</b>, the iRouter processes the interpretation and performs the next step(s) in the workflow (e.g., database lookup based on input data) and then forwards an appropriate response <b>218</b> to the customer through the telephony server if the interpretation is deemed accurate. If the iRouter determines the interpretation is accurate, it directs the playback of responses to the customer front the telephony server based on the interpretation of either the speech recognition software or by applying key algorithms to the responses of one or more human agents. In this example, the response is given to the customer in the last block of screen <b>2</b>, <figref idref="DRAWINGS">FIG. 4A</figref>.</p>
<p id="p-0067" num="0066">To determine accuracy, the iRouter compares the interpretation of two human agents and, if no consensus is reached, plays the customer audio clip for a third human agent for a further interpretation (i.e., &#x201c;majority rule&#x201d; determines which is the accurate response). Other business rules may also be used to determine the accurate interpretation. For example, an interpretation from the agent with the best accuracy score from past interpretations may be selected. Alternatively, one of the interpretations may be selected and played back to the customer (&#x201c;I understood you to say . . . &#x201d;) and the customer response determines whether the interpretation was correct. Further, the interpretations may be selected from known data (e.g., two interpretations of an email address could be compared against a database of customer email addresses, only one of two interpretations of a credit card number will pass a checksum algorithm, etc.).</p>
<p id="p-0068" num="0067">The interactive response system allows for virtually any number of human agents to handle to same customer interaction at once. That is, an interactive response system could have two agents listening during a busy time or have seven human agents listening during a more idle time. Moreover, during times of high call volume, accuracy can be decreasing by removing the &#x201c;double-checking&#x201d; rule to maintain high response time. An agent assigned a high trust ranking based on the agent's speed/accuracy profile may be asked to work without the double-checking. In addition to trading off accuracy for quicker system availability, a steady flow of audio clips is kept flowing by each agent, thereby decreasing human agent &#x201c;slack&#x201d; time.</p>
<p id="p-0069" num="0068">Returning to the flowchart of <figref idref="DRAWINGS">FIG. 2</figref>, either the customer will respond again as seen in black <b>204</b>, the call will be transferred (if so directed by a step in the workflow or by business rules), or the customer terminates the call, as shown in block <b>215</b>. If the interpretation is deemed inaccurate in block <b>213</b>, the iRouter plays a stutter speech recording to the customer (block <b>216</b>) and send the audio clip to additional human agents for another interpretation (block <b>217</b>) and then reevaluate its accuracy.</p>
<p id="p-0070" num="0069">The iRouter manages interaction with the customer to call completion, using the workflow as its guide. The iRouter may stream customer utterances to human agents for interpretation at numerous points in the call. Once the call has concluded, a snapshot of the customer interaction is preserved in the archive database. Human agents' speed/accuracy profiles are constantly updated and maintained.</p>
<p id="p-0071" num="0070">If human intervention is not needed to interpret a customer's request, speech recognition software interprets the audio clip and the iRouter determines the appropriate response as shown in blocks <b>206</b> and <b>214</b>.</p>
<p id="p-0072" num="0071">Continuing with the Interair example, the captured customer utterance, as seen in <figref idref="DRAWINGS">FIG. 5A</figref>, has two requests: food and entertainment queries. In accordance with another aspect of this system, the human agent captures two intents: meal and movie. There is no relevant data to enter because the interactive response system already knows the flight information from the previous data entered in <figref idref="DRAWINGS">FIG. 4B</figref> (this data is visible in <figref idref="DRAWINGS">FIG. 5B</figref>). As seen in <figref idref="DRAWINGS">FIG. 5B</figref>, the human agent enters &#x201c;General&#x201d; and &#x201c;Meal&#x201d; from an on-screen display of possible intents. The human agent also enters &#x201c;Movie.&#x201d; As seen in <figref idref="DRAWINGS">FIG. 5A</figref>, the interactive response system then provides the appropriate response. As seen in <figref idref="DRAWINGS">FIG. 5B</figref>, if the customer requests further information regarding the meal or movie such as: &#x201c;What meal is offered?&#x201d;, &#x201c;Are their special meals?&#x201d;, &#x201c;What is the movie rated?&#x201d;, the appropriate human agent interpretation options are located on the computer screen.</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an example of how information is retrieved and handled by the interactive response system when a customer interacts via electronic mail (email, as it is commonly known in the art). As shown in block <b>601</b>, the interaction begins with the customer emailing to the customer service email address of a company. The interactions platform, a gateway server, in this exemplary embodiment opens the email and retrieves the appropriate workflow stored in the workflow database based on either (1) the to/from information of the customer or (2) other business rules, as illustrated at <b>602</b>. The gateway server then sends the appropriate response acknowledgement as illustrated at <b>602</b>. Then the iRouter identifies available human agent(s) to handle the email by applying a load balancing algorithm, triggers a pop-up on their screens to show possible intents for interpretation, and sends the email content to the or those human agents, as shown at block <b>603</b>. The human agent(s) interpret the email as shown in blocks <b>604</b> and <b>605</b>. After test <b>606</b>, where the iRouter evaluates the accuracy, in real time, of the customer email interpretation and updates each agent's speed/accuracy profile, the iRouter processes the interpretation and performs the next steps in the workflow accordingly. Eventually, the iRouter forwards an appropriate email response to the customer through the gateway server (if the interpretation is deemed accurate) as seen in block <b>607</b>. The emails are then archived in the appropriate database as illustrated in block <b>608</b>. If the interpretation is deemed inaccurate, the iRouter sends the email to another human agent for another interpretation (block <b>609</b>) and then reevaluates its accuracy. The iRouter manages interaction with the customer through email responses, using the workflow as its guide.</p>
<p id="p-0074" num="0073">Other features of the present system include a seamless blend of speech recognition software and human agent interaction to provide added customer privacy and security such that human access to confidential customer data is minimized. In a customer contact center environment, customer personal data, such as credit card information, social security numbers and address, is routinely made available to human agents interacting with customers. The present system uses a software-based front-end that incorporates speech recognition technology to allow a computer to capture, verify, or update a customer's sensitive data. The software manages the customer interaction so that confidential data is stored in a database and not passed to any human agent. The software can stream audio clips of the customer's utterances over a TCP/IP network to client software being used by agents anytime human intervention is required. In cases where the workflow does require that a human agent handle sensitive customer information, the transaction is portioned into discrete, logical units allows business analysts to engineer the process so that the same human agent never sees more than one element of a given set of customer data. For example, if two agents see a particular customer's credit card number, two different agents see the customer's name. No one agent sees a full record or profile for a given customer. This helps call center operations, which often experience high agent turnover, minimize the problem of identify theft.</p>
<p id="p-0075" num="0074">Other features of some embodiments of the present system include interactions platform <b>102</b> optionally accommodating still pictures in any format (e.g., jpeg, tiff), motion pictures, scanned data, facsimiles, web pages, etc., which can be forwarded to a human agent's station. Such facility is useful, for example, for monitoring alarms, parsing faxes, etc. The human agent's interpretation is then delivered to the iRouter in die context of die workflow, as above.</p>
<p id="p-0076" num="0075">It will be understood that the foregoing is only illustrative of the principles of the disclosed systems and that various modifications can be made by those skilled in the art without departing form the scope of the invention, which is limited only by the claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method for operating an interactive response system comprising:
<claim-text>receiving data representing a multi-utterance transaction with a person, the data having a first portion including credit card name data and a second portion including credit card number data;</claim-text>
<claim-text>portioning the multi-utterance transaction into discrete, logical utterance units;</claim-text>
<claim-text>automatically presenting a first portion of the utterance units corresponding to the credit card name data through a routing device to a first subset of personnel;</claim-text>
<claim-text>automatically presenting a second subset of the utterance units corresponding to the credit card number data through the routing device to a second subset of personnel, the first and second subsets of personnel being selected such that no person in either the first subset of personnel or the second subset of personnel is presented with both the credit card name data and the credit card number data;</claim-text>
<claim-text>accepting input from the first subset of personnel and the second subset of personnel; and</claim-text>
<claim-text>using a processor, automatically processing the credit card name data and credit card number data based on the input accepted from the first subset of personnel and the second subset of personnel.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein first subset of personnel includes a plural number of intent analysts.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second subset of personnel includes a plural number of intent analysts.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising automatically communicating a message to the person based on the input accepted from the first subset of personnel.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising automatically communicating a message to the person based on the input accepted from the second subset of personnel.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A system for processing an interaction with a person, comprising a processor, two or more analyst user interface devices in communication with the processor, and a memory in communication with the processor, the memory storing programming instructions executable by the processor to:
<claim-text>receive data representing a multi-utterance transaction with a person, the data having a first portion including credit card name data and a second portion including credit card number data;</claim-text>
<claim-text>portion the multi-utterance transaction into discrete, logical utterance units;</claim-text>
<claim-text>automatically present a first portion of the utterance units corresponding to the credit card name data through a routing device to a first subset of personnel;</claim-text>
<claim-text>automatically present a second subset of the utterance units corresponding to the credit card number data through the routing device to a second subset of personnel, the first and second subsets of personnel being selected such that no person in either the first subset of personnel or the second subset of personnel is presented with both the credit card name data and the credit card number data;</claim-text>
<claim-text>accept input from the first subset of personnel and the second subset of personnel; and</claim-text>
<claim-text>automatically process the credit card name data and credit card number data based on the input accepted from the first subset of personnel and the second subset of personnel.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein first subset of personnel includes a plural number of intent analysts.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein second subset of personnel includes a plural number of intent analysts.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising automatically communicating a message to the person based on the input accepted from the first subset of personnel.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising automatically communicating a message to the person based on the input accepted from the first subset of personnel.</claim-text>
</claim>
</claims>
</us-patent-grant>
