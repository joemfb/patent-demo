<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625426-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625426</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13262474</doc-number>
<date>20100325</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="regional">
<country>EP</country>
<doc-number>09251007</doc-number>
<date>20090331</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>291</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>L</subclass>
<main-group>12</main-group>
<subgroup>26</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>15</main-group>
<subgroup>16</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>370232</main-classification>
<further-classification>370248</further-classification>
<further-classification>370252</further-classification>
<further-classification>709229</further-classification>
<further-classification>709234</further-classification>
</classification-national>
<invention-title id="d2e71">Network flow termination</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6414939</doc-number>
<kind>B1</kind>
<name>Yamato</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3702361</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7286552</doc-number>
<kind>B1</kind>
<name>Gupta et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370413</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7830801</doc-number>
<kind>B2</kind>
<name>Kwan et al.</name>
<date>20101100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7907519</doc-number>
<kind>B2</kind>
<name>Songhurst et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370229</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>8274974</doc-number>
<kind>B1</kind>
<name>Gupta et al.</name>
<date>20120900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370389</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>8315168</doc-number>
<kind>B2</kind>
<name>Davari</name>
<date>20121100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>8509085</doc-number>
<kind>B2</kind>
<name>Bader et al.</name>
<date>20130800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370236</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2003/0112756</doc-number>
<kind>A1</kind>
<name>Le Gouriellec et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2004/0037223</doc-number>
<kind>A1</kind>
<name>Harrison et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2005/0025158</doc-number>
<kind>A1</kind>
<name>Ishikawa et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2007/0237074</doc-number>
<kind>A1</kind>
<name>Curry</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370229</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2007/0268827</doc-number>
<kind>A1</kind>
<name>Csaszar et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2007/0291643</doc-number>
<kind>A1</kind>
<name>Charny et al.</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2008/0130502</doc-number>
<kind>A1</kind>
<name>Charny et al.</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2009/0003212</doc-number>
<kind>A1</kind>
<name>Kwan et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2009/0003354</doc-number>
<kind>A1</kind>
<name>Sreejith et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370396</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2010/0054126</doc-number>
<kind>A1</kind>
<name>Kwan et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2010/0208591</doc-number>
<kind>A1</kind>
<name>Corliano et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370237</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>EP</country>
<doc-number>1 936 880</doc-number>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>WO</country>
<doc-number>2008/145178</doc-number>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Feng et al, &#x201c;Impact of ATM ABR control on the performance of TCP-Tahoe and TCP-Reno&#x201d;; Global Telecommunications Conference, 1997. GLOBECOM '97, IEEE. Publication Year: 1997 , pp. 1832-1837, vol. 3.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Li et al; &#x201c;SMAQ: a measurement-based tool for traffic modeling and queuing analysis. I. Design methodologies and software architecture&#x201d;; vol. 36, Issue: 8; Publication Year: 1998 , pp. 56-65.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>International Search Report for PCT/GB2010/000571, mailed May 12, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Menth, M. et al., &#x201c;Threshold configuration and routing optimization for PCN-based resilient admission control&#x201d;, Comuter Networks, (Feb. 13, 2009), pp. 1-13.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Menth University of Wuerzburg et al., &#x201c;PCN Encoding for Packet-Specific Dual Marking (PSDM); draft-menth-pcn-psdm-encoding-00.txt&#x201d;, Internet Engineering Task Force; Internet Society, (Jul. 7, 2008).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Menth University of Wuerzburg et al., &#x201c;Deployment Models for PCN-Based Admission Control and Flow Termination Using Packet-Specific Dual Marking (PSDM); draft-menth-pcn-psdm-deployment-00.txt&#x201d;, Internet Engineering Task Force; Internet Society, (Oct. 24, 2008).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Eardly, P., &#x201c;Pre-Congestion Notification (PCN) Architecture; draft-ietf-pcn-architecture-10.txt&#x201d;, vol. pcn, No. 10; Internet Engineering Taskforce; Internet Society, (Mar. 16, 2009).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Charny Cisco Systems A et al., &#x201c;Pre-Congestion Notification Using Single Marking for Admission and Termination; draft-charny-pcn-single-marking-03.txt&#x201d;, Internet Engineering Task Force, No. 3, (Nov. 18, 2007).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Menth, Michael and Hartmann, Matthias, &#x201c;Threshold configuration and routing optimization for PCN-based resilient admission control&#x201d;, Univerity of Wuerzburg, Institute of Computer Science, Germany, Computer Networks 53 (2009) 1771-1783, available online Feb. 13, 2009 (14 pgs.).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Menth, M. et al., PCN Encoding for Packet-Specific Dual Marking (PSDM), Jun. 26, 2009, (10 pgs.).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Menth, M. et al., &#x201c;Deployment Models for PCN-Based Admission Control and Flow Termination Using Packet-Specific Dual Marking (PSDM)&#x201d;, Oct. 24, 2008, (20 pgs.).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Eardley, Philip, &#x201c;Pre-Congestion Notification (PCN) Architecture&#x201d;, Mar. 16, 2009, (62 pgs.).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Charny, A., &#x201c;Pre-Congestion Notification Using Single Marking for Admission and Termination&#x201d;, Nov. 18, 2007, (60 pgs.).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>Moncaster, T., &#x201c;Baseline Encoding and Transport of Pre-Congestion Information&#x201d;, Oct. 14, 2008, (12 pgs.).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>10</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>370216-218</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>370221-222</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>370230-236</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>370237-2381</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>370248-253</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>370254-258</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709220-221</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709225-226</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709229-235</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>709238-241</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>7</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120033553</doc-number>
<kind>A1</kind>
<date>20120209</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Strulo</last-name>
<first-name>Ben</first-name>
<address>
<city>Felixstowe</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Wennink</last-name>
<first-name>Marc</first-name>
<address>
<city>Ipswich</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Strulo</last-name>
<first-name>Ben</first-name>
<address>
<city>Felixstowe</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Wennink</last-name>
<first-name>Marc</first-name>
<address>
<city>Ipswich</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Nixon &#x26; Vanderhye P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>BRITISH TELECOMMUNICATIONS public limited company</orgname>
<role>03</role>
<address>
<city>London</city>
<country>GB</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Hoang</last-name>
<first-name>Thai</first-name>
<department>2463</department>
</primary-examiner>
</examiners>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/GB2010/000571</doc-number>
<kind>00</kind>
<date>20100325</date>
</document-id>
<us-371c124-date>
<date>20110930</date>
</us-371c124-date>
</pct-or-regional-filing-data>
<pct-or-regional-publishing-data>
<document-id>
<country>WO</country>
<doc-number>WO2010/112818</doc-number>
<kind>A </kind>
<date>20101007</date>
</document-id>
</pct-or-regional-publishing-data>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A network has a plurality of edge nodes (<b>7</b><i>a, </i><b>7</b><i>b, </i><b>7</b><i>c, </i><b>7</b><i>d</i>) and core nodes (<b>3</b><i>a, </i><b>3</b><i>b, </i><b>3</b><i>c</i>) for carrying flows of data from an ingress gateway (<b>7</b><i>a, </i><b>7</b><i>b, </i><b>7</b><i>c, </i><b>7</b><i>d</i>) to an egress gateway (<b>7</b><i>a, </i><b>7</b><i>b, </i><b>7</b><i>c, </i><b>7</b><i>d</i>) along a path of core nodes. For any given path in the network work from an ingress node to an egress node, the number of flows is controlled. When network congestion occurs, the egress gateway (<b>7</b><i>a, </i><b>7</b><i>b, </i><b>7</b><i>c, </i><b>7</b><i>d</i>) provides an indication of the level of network congestion. Terminating flows is disruptive and therefore the ingress gateway (<b>7</b><i>a, </i><b>7</b><i>b, </i><b>7</b><i>c, </i><b>7</b><i>d</i>) can mark other flows with a congestion marker and send them to the egress gateway (<b>7</b><i>a, </i><b>7</b><i>b, </i><b>7</b><i>c, </i><b>7</b><i>d</i>). These marked flows are routed by the core and egress gateway (<b>7</b><i>a, </i><b>7</b><i>b, </i><b>7</b><i>c, </i><b>7</b><i>d</i>) but ignored for calculating network congestion. When the network congestion is alleviated, the marked flows can be un-marked and treated as normal flows, hence the network avoids unnecessary flow termination.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="264.50mm" wi="118.79mm" file="US08625426-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="211.41mm" wi="156.29mm" orientation="landscape" file="US08625426-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="217.34mm" wi="144.10mm" orientation="landscape" file="US08625426-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="226.91mm" wi="144.19mm" file="US08625426-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="241.38mm" wi="157.48mm" file="US08625426-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="277.79mm" wi="141.05mm" file="US08625426-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<p id="p-0002" num="0001">This application is the U.S. national phase of International Application No. PCT/GB2010/000571, filed 25 Mar. 2010, which designated the U.S. and claims priority to EP Application No. 09251007.2, filed 31 Mar. 2009, the entire contents of each of which are hereby incorporated by reference.</p>
<p id="p-0003" num="0002">The present invention relates to data packet networks and in particular to a method of terminating flows of data packets during periods of network congestion.</p>
<heading id="h-0001" level="1">INTRODUCTION</heading>
<p id="p-0004" num="0003">Internet protocol networks are handling increasing volumes of data, with the data being of different types. For example, potential high value Internet services, such as voice and video, generate mostly constant bit-rate, inelastic traffic which is difficult to adapt to a change in network environment. Such traffic competes for bandwidth with data transfers, such as email and the like, which are much more reactive to changing network conditions. When congestion occurs, all traffic to a congested node is affected, meaning that packets can then be lost at the node. The consequences of packet loss for a particular data stream can vary, depending on the type of the stream. For voice-over-IP (VoIP), telephony, and video applications, packet losses manifest themselves as artefacts in the received audio or video, such as the audio breaking up, or videos having the image freeze.</p>
<p id="p-0005" num="0004">It is known to control admission of new data flows (a sequence of related data packets from a data sender to a data receiver via the network, e.g. a VoIP telephone conversation or a video stream) into a network, so as to only admit new data flows if the network has sufficient capacity in addition to the data packets of previously admitted data flows. This is known as admission control. In this field, recently measurement-based admission control (MBAC) has been of interest. One particular MBAC solution is known as pre-congestion notification (PCN) which has been developed by the Congestion and Pre-congestion Notification working group of the Internet Engineering Taskforce. A description of the present status of PCN can be found at http://tools.ietf.org/html/draft-ietf-pcn-architecture-03.</p>
<p id="p-0006" num="0005">In a case where serious network disruption occurs, such as link failures within the hardware of the network or extreme data loads, simply blocking new flow requests is not sufficient to maintain the Quality of Service (QoS) of existing flows on the network. In such a case, in addition to performing admission control existing admitted flows must be terminated to restore the level of congestion in the network to an acceptable state. This process is known as a flow termination mechanism.</p>
<p id="p-0007" num="0006">To implement flow termination, the PCN specification defines a packet marking scheme whereby a field in the header of a PCN data packet forming part of an admitted flow can be altered by PCN network nodes in response to detected congestion. In this marking scheme, packets are either &#x201c;Not Marked&#x201d; (NM) or carry the congestion marker &#x201c;To Terminate&#x201d; (TT).</p>
<p id="p-0008" num="0007">Each link in the PCN network (outgoing paths from a node) monitors the number of NM packets that flow along it and compares the rate of NM packets against a predetermined sustainable rate value for that link. The sustainable rate is the rate of PCN traffic that the link can support without needing to terminate any flows in order to maintain a level of QoS. The difference is used to identify how many flows may need to be terminated when congestion is detected. Each link calculates the difference between the rate of NM packets and the sustainable rate, and uses this difference value to mark a sufficient number of packets from NM to TT so that the rate at which NM packets leave the link is equal to or less than the sustainable rate on the link.</p>
<p id="p-0009" num="0008">Terminating flows to maintain the sustainable rate in response changing network conditions leads to a loss of service for some of the users or processes owning the flows until the network conditions become more favourable. Therefore it is important that a minimal number of flows that are terminated to minimize disruption.</p>
<p id="p-0010" num="0009">The present invention addresses the above problem. In particular the present invention changes both the marking and the termination behaviour to improve the accuracy and speed of flow termination.</p>
<p id="p-0011" num="0010">In one aspect the present invention provides a method as set out in claim <b>1</b>.</p>
<p id="p-0012" num="0011">In another aspect the present invention provides a network edge node configured as an ingress gateway as set out in claim <b>5</b>.</p>
<p id="p-0013" num="0012">In a further aspect, the present invention provides a network as set out in claim <b>9</b>.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0002" level="1">FIGURES</heading>
<p id="p-0014" num="0013">Features of the present embodiment will now be described with reference to the following figures in which:</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> shows an overview of a PCN network;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2</figref> shows a simplified view of the PCN network;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3</figref> shows the components of a core node illustrated in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> is a flow chart showing the operation of a packet marker of a core node illustrated in <figref idref="DRAWINGS">FIG. 3</figref>;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5</figref> shows the components of an edge node illustrated in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 6</figref> is a flow chart showing the operation of a packet inspector of an edge node when functioning as an egress gateway; and</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 7</figref> is a flow chart showing the operation of a packet marker of an edge node when functioning as an ingress gateway.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0003" level="1">FIRST EMBODIMENT</heading>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 1</figref> shows an overview of the network topology to which the flow control system can be applied. A network <b>1</b> is formed of nodes <b>3</b>, <b>7</b> connected by lines <b>5</b>. In the network, there are core nodes <b>3</b> and edge nodes <b>7</b>. Core nodes are only connected to other core nodes <b>3</b> and edge nodes <b>7</b> whilst edge nodes <b>7</b> define the edge of the network <b>1</b> and therefore are connected to core nodes <b>7</b> and also external entities such as customers <b>9</b>.</p>
<p id="p-0023" num="0022">Customers <b>9</b> communicate with each other via the network <b>1</b>. They are connected to the edge nodes <b>7</b> and therefore the edge nodes <b>7</b> act as Ingress Gateways when receiving data from a customer and as Egress Gateways when sending data from the core to the customer. The flow of data from customer to customer is therefore:</p>
<p id="p-0024" num="0023">Customer&#x2192;Ingress Gateway&#x2192;One of more Core Nodes&#x2192;Egress Gateway&#x2192;Customer.</p>
<p id="p-0025" num="0024">A continuous series of packets issued from a first customer to another customer and transported across the network <b>1</b> will be referred to as a flow for, the rest of the description. An example of a flow would be a telephone conversation.</p>
<p id="p-0026" num="0025">For ease of explanation, the operation of the network will be described in relation to a subset of the nodes shown in <figref idref="DRAWINGS">FIG. 1</figref>. <figref idref="DRAWINGS">FIG. 2</figref> shows a simplified view of the network shown in <figref idref="DRAWINGS">FIG. 1</figref> to explain how packets flow across the network.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 2</figref> shows only four of the edge nodes <b>7</b>A-<b>7</b>D, three core nodes <b>3</b>A-<b>3</b>C and customers <b>9</b>A-<b>9</b>G.</p>
<p id="p-0028" num="0027">When packets flow from edge node <b>7</b>A to edge node <b>7</b>B then node <b>7</b>A is the ingress node and node <b>7</b>B is the egress node. Conversely, when packets flow from node <b>7</b>B to node <b>7</b>A then node <b>7</b>B is the ingress node and node <b>7</b>A is the egress node. It will be clear to the skilled person that an edge node can function simultaneously as an ingress gateway and an egress gateway depending on whether data is entering or leaving the network.</p>
<p id="p-0029" num="0028">Although the flow of packets could traverse the network via any combination of core nodes from an ingress node to an egress node, in this network, set paths are defined for packet flow along predetermined routes. For example, flows entering the network from ingress gateway <b>7</b>A and leaving the network at egress gateway <b>7</b>B will only traverse the network via core nodes <b>3</b>A&#x2192;<b>3</b>B&#x2192;<b>3</b>C even though other paths are possible. The Open Shortest Path First (OSPF) dynamic routing protocol may be used to converge the network and define these paths. A summary of the paths is shown below.</p>
<p id="p-0030" num="0029">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="1" colwidth="63pt" align="center"/>
<colspec colname="2" colwidth="28pt" align="center"/>
<colspec colname="3" colwidth="56pt" align="center"/>
<colspec colname="4" colwidth="70pt" align="left"/>
<thead>
<row>
<entry namest="1" nameend="4" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry/>
<entry/>
<entry>Intermediate core</entry>
</row>
<row>
<entry>Path ID</entry>
<entry>Ingress</entry>
<entry>Egress</entry>
<entry>nodes</entry>
</row>
<row>
<entry namest="1" nameend="4" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="1" colwidth="63pt" align="char" char="."/>
<colspec colname="2" colwidth="28pt" align="center"/>
<colspec colname="3" colwidth="56pt" align="center"/>
<colspec colname="4" colwidth="70pt" align="left"/>
<tbody valign="top">
<row>
<entry>1</entry>
<entry>7A</entry>
<entry>7B</entry>
<entry>3A &#x2192; 3B &#x2192; 3C</entry>
</row>
<row>
<entry>2</entry>
<entry>7A</entry>
<entry>7C</entry>
<entry>3A &#x2192; 3B &#x2192; 3C</entry>
</row>
<row>
<entry>3</entry>
<entry>7A</entry>
<entry>7D</entry>
<entry>3A</entry>
</row>
<row>
<entry>4</entry>
<entry>7B</entry>
<entry>7A</entry>
<entry>3C &#x2192; 3B &#x2192; 3A</entry>
</row>
<row>
<entry>5</entry>
<entry>7B</entry>
<entry>7C</entry>
<entry>3C</entry>
</row>
<row>
<entry>6</entry>
<entry>7B</entry>
<entry>7D</entry>
<entry>3C &#x2192; 3B &#x2192; 3A</entry>
</row>
<row>
<entry>7</entry>
<entry>7C</entry>
<entry>7A</entry>
<entry>3C &#x2192; 3B &#x2192; 3A</entry>
</row>
<row>
<entry>8</entry>
<entry>7C</entry>
<entry>7B</entry>
<entry>3C</entry>
</row>
<row>
<entry>9</entry>
<entry>7C</entry>
<entry>7D</entry>
<entry>3C &#x2192; 3B &#x2192; 3A</entry>
</row>
<row>
<entry>10</entry>
<entry>7D</entry>
<entry>7A</entry>
<entry>3A</entry>
</row>
<row>
<entry>11</entry>
<entry>7D</entry>
<entry>7B</entry>
<entry>3A &#x2192; 3B &#x2192; 3C</entry>
</row>
<row>
<entry>12</entry>
<entry>7D</entry>
<entry>7C</entry>
<entry>3A &#x2192; 3B &#x2192; 3C</entry>
</row>
<row>
<entry namest="1" nameend="4" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0031" num="0030">Of course, these paths do not exist as discrete separate lines in the network but are logical paths defined by the routing tables of the core nodes <b>3</b>.</p>
<p id="p-0032" num="0031">As an example of the operation of the network, consider the situation of a customer <b>9</b>A sending data to customer <b>9</b>D and customer <b>9</b>G sending data to customer <b>9</b>F across the network.</p>
<p id="p-0033" num="0032">From a network wide perspective as defined in the above table, the data packets forming the flow from customer <b>9</b>A to <b>9</b>D travel along path <b>1</b> and the data packets forming the flow from customer <b>9</b>G to <b>9</b>F travel along path <b>12</b>.</p>
<p id="p-0034" num="0033">Each node does not store the complete routing table shown above, but contains enough local knowledge of the surrounding network to determine which output port the packets should be sent to on their onward journey.</p>
<p id="p-0035" num="0034">When node <b>7</b>A acting as an ingress gateway, receives data packets from customer <b>9</b>A addressed to customer <b>9</b>D, it consults its routing table and determines that the data should be forwarded to core node <b>3</b>A. All subsequent data packets in that flow are sent on the same path.</p>
<p id="p-0036" num="0035">Similarly when ingress node <b>7</b>D receives data packets from customer <b>9</b>G addressed to customer <b>9</b>F then, based on its routing table, the data packet is sent to core node <b>3</b>A and all subsequent data packets in the flow are sent along that path.</p>
<p id="p-0037" num="0036">Core node <b>3</b>A has an input port for traffic from edge node <b>7</b>A and a different input port for traffic received from edge node <b>7</b>D. In this case, the routing table tells the packets to be sent out on the same output port to core node <b>3</b>B.</p>
<p id="p-0038" num="0037">Core node <b>3</b>B receives the path <b>1</b> and path <b>12</b> data packets on the same inbound port and after consulting its routing table sends them out on the same outbound port to core node <b>3</b>C.</p>
<p id="p-0039" num="0038">At core node <b>3</b>C, the path <b>1</b> and path <b>12</b> traffic arrives on the same input port. The core node consults its routing table and determines that path <b>1</b> data is to be forwarded on an outbound port towards egress gateway <b>7</b>B and that path <b>12</b> traffic is to be forwarded on an outbound port towards egress gateway <b>7</b>C.</p>
<p id="p-0040" num="0039">As shown above, traffic within the core of the network flows in paths from ingress gateways to egress gateways. The paths may overlap and share the same physical links within the core but the nodes can examine the contents or headers of the packets to ensure correct delivery.</p>
<p id="p-0041" num="0040">Under normal circumstances the flows last as long as the customer desires. As mentioned earlier, when light congestion is experienced, then the ingress gateways are arranged to respond by preventing new flows from being admitted into the network. However, in the event of a serious failure in the network it is possible that some flows will need to be terminated. Such action is clearly undesirable but sacrificing some flows may be necessary in order to maintain the overall smooth operation of the network.</p>
<p id="p-0042" num="0041">The operation of each different type of node during network congestion will now be described. The edge nodes (ingress and egress) and core nodes act asynchronously according to a local schedule.</p>
<p id="p-0043" num="0042">Core Nodes</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 3</figref> shows the internal structure of a core node. Incoming packets are received via input ports <b>19</b>. Each input port being connected to a different node in the network. Since packets may arrive simultaneously, the packets are temporarily stored in a FIFO receiver buffer <b>21</b>. A packet inspector <b>23</b> takes packets from the receiver buffer <b>21</b> and examines the headers to determine the destination address of the packet. A controller <b>31</b> receives the extracted destination address and performs a lookup into a routing table <b>29</b> to determine the next node on the path to the destination. The controller then controls switch <b>25</b> to place the packets into an appropriate queue <b>26</b> inside a send buffer <b>27</b>. The packets are then processed by packet marker <b>33</b> before being sent towards their destination via output ports <b>35</b> connected to different nodes in the network.</p>
<p id="p-0045" num="0044">The core nodes <b>3</b> are designed to move packets across the network <b>1</b> as quickly as possible. Therefore the amount of processing carried out by each core node <b>3</b> on the incoming packets between the input ports <b>19</b> and the output ports <b>33</b> must be kept to a minimum. In this embodiment, the core nodes <b>3</b> do not have any awareness of flows, only incoming data packets. They merely process traffic (the plurality of packets) using simple algorithms.</p>
<p id="p-0046" num="0045">The routing behaviour of the node is conventional and will not be described in more detail. However, the process of the packet marker <b>33</b> in deciding whether or not to mark packets will now be described with reference to <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0047" num="0046">The packet marker <b>23</b> is arranged to recognise three possible packet markings located in the header of each packet:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0047">Not Marked&#x2014;&#x2018;NM&#x2019;;</li>
        <li id="ul0002-0002" num="0048">To Terminate&#x2014;&#x2018;TT&#x2019;; and</li>
        <li id="ul0002-0003" num="0049">Ready to Terminate&#x2014;&#x2018;RT&#x2019;.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0048" num="0050">The packet marker <b>33</b> monitors the queue <b>26</b> relating to each output port <b>35</b> of the core node <b>3</b>, and for each queue processes the packets according to each detected packet marking. In particular, the packet marker <b>33</b> continuously monitors the rate at which NM packets leave the output ports of the node. This monitoring is implemented using a token bucket. The token bucket has a predetermined token fill rate which is pre-set to be just below the line rate of the output port. This is known as the supportable rate and provides the packet marker <b>33</b> with a warning that the node may be close to dropping packets. As NM packets arrive, tokens are removed from the token bucket in proportion to their packet size.</p>
<p id="p-0049" num="0051">If there are sufficient tokens then the NM packets are sent without change. If the token bucket has insufficient tokens, the packet marker <b>33</b> starts marking subsequently received NM packets with a TT marking in the header field until the bucket has refilled with sufficient tokens. This indicates to later nodes that the output link of the core node is close to saturation and flows may need to be terminated.</p>
<p id="p-0050" num="0052">The packet marker <b>33</b> only monitors the flow of NM packets and ignores packets marked with RT headers or packets marked with TT headers, hereinafter referred to as RT packets and TT packets respectively.</p>
<p id="p-0051" num="0053"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart showing the operation of the packet marker <b>33</b> on each packet. At step s<b>1</b> the packet is examined to determine if it is a TT packet, i.e. it contains the marking TT in its header. If so, then processing ends and the packet is sent. If the packet is not a TT packet, in step s<b>3</b> the packet marker <b>33</b> determines if the packet is an RT packet. If it is then processing ends and the packet is sent.</p>
<p id="p-0052" num="0054">If the packet is an NM packet, then in step s<b>5</b> the packet marker <b>33</b> checks whether the supportable rate has been exceeded. If it has not then processing ends, however, if the supportable rate has been exceeded then in step s<b>7</b> the NM packet is marked with a TT marker to become a TT packet and processing ends.</p>
<p id="p-0053" num="0055">Referring to the example configuration shown in <figref idref="DRAWINGS">FIG. 2</figref>, core node <b>3</b>A receives NM packets from ingress gateways <b>7</b>A and <b>7</b>D addressed to different customers on different input lines, but due to the arrangement of the predetermined paths, those NM packets are directed to the same queue for the output line towards node <b>3</b>B. The packet marker of node <b>3</b>A meters the NM packet traffic and if the combined volume of NM traffic is less than the supportable rate then the packets are sent without change. However, if the number of packets from either or both ingress gateways <b>7</b>A, <b>7</b>B increases such that the supportable rate is exceeded (i.e. the token bucket runs out of tokens), then the packet marker <b>33</b> responds by marking subsequently received NM packets with the TT mark until the bucket refills. The TT packets can still be forwarded to node <b>3</b>B because the line rate of output port is higher than the supportable rate.</p>
<p id="p-0054" num="0056">When the TT marked packets reach node <b>3</b>B, the packet marker of node <b>3</b>B does not include the TT packets in its metering calculation, the TT packets are sent towards the destination node without requiring a tokens. In this example, since the supportable rate of the core nodes are the same, node <b>3</b>B will not need to mark any further NM packets as TT packets because node <b>3</b>A has already ensured that the rate of NM packets arriving at node <b>3</b>B from node <b>3</b>A is below the supportable rate.</p>
<p id="p-0055" num="0057">However, if packets from another node (not shown) arrived at node <b>3</b>B and were routed to node <b>3</b>C then further packets may be marked TT.</p>
<p id="p-0056" num="0058">The packets continue travelling between the core nodes until they reach their destination egress node. Note that packets can only be changed from NM to TT within the core. Packets cannot be &#x201c;unmarked&#x201d; from TT to NM within the core of the network.</p>
<p id="p-0057" num="0059">Egress Gateways</p>
<p id="p-0058" num="0060">As mentioned above, the core nodes identify when the packet flow on each of their output lines is higher than the predetermined supportable rate and if this threshold is exceeded, NM packets are marked as TT packets.</p>
<p id="p-0059" num="0061">Packets eventually reach their intended egress gateway. <figref idref="DRAWINGS">FIG. 5</figref> shows the internal structure of an edge node which functions as both an ingress and an egress gateway depending on whether packets are arriving from the core network via inputs <b>41</b><i>a </i>or from external customers outside of the network via inputs <b>41</b><i>b</i>. The function of the components of the edge node differs accordingly and the functions will now be described when the edge node is functioning as an egress gateway.</p>
<p id="p-0060" num="0062">Incoming packets are received from a core node <b>3</b> via input ports <b>41</b><i>a</i>. Each input port <b>41</b><i>a </i>is connected to a different core node <b>3</b> in the network. Since packets may arrive simultaneously, the packets are temporarily stored in a FIFO receiver buffer <b>42</b>. A packet inspector <b>43</b> takes packets from the receiver buffer <b>42</b> and examines the headers to determine the external destination address of the packet. A controller <b>53</b> receives the extracted destination address and performs a lookup into a routing table <b>57</b> to determine the output port <b>51</b><i>a </i>to forward the packet to the destination. The controller <b>53</b> then controls switch <b>45</b> to place the packets into an appropriate queue <b>47</b> inside a send buffer <b>49</b>. The packets are then processed by packet marker <b>55</b> to remove any network specific headers before being sent towards their destination via output ports <b>51</b><i>a </i>connected to external customers.</p>
<p id="p-0061" num="0063">Input ports <b>41</b><i>b </i>are linked to external customers and output ports <b>51</b><i>b </i>are linked to core nodes <b>3</b> since the edge node <b>7</b> can function both as an ingress gateway and an egress gateway.</p>
<p id="p-0062" num="0064">In addition to forwarding and delivering packets, the egress gateways are arranged to monitor network traffic from each of its input ports for reporting pre-congestion to the ingress gateway. Unlike the core nodes <b>3</b>, the edge nodes <b>7</b> are aware of the paths defined though the network. The egress gateways can examine incoming flows of packets to determine the path they took through the network and additionally whether they experienced congestion. For each path, the packet inspector <b>43</b> continually monitors the rate at which it is receiving NM packets and also RT/TT marked packets. It also periodically performs a process to determine congestion, in this embodiment, every 50 milliseconds.</p>
<p id="p-0063" num="0065"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart showing the periodic processing of the packet inspector on each path. In step s<b>11</b>, the packet inspector <b>43</b> determines the current rate at which it is receiving TT or RT marked packets and in step s<b>13</b> the determined rate is compared against a threshold. In this embodiment, the threshold is two TT or RT marked packets a second. If the current rate is below the threshold, then processing ends and no determination of congestion is made.</p>
<p id="p-0064" num="0066">If the current rate is higher than the threshold, then the packet inspector <b>43</b> can deduce that at least one link on the path between the ingress gateway and egress gateway is overloaded beyond its supportable rate. It is therefore necessary to terminate some of the flows and this must be performed at the ingress gateway.</p>
<p id="p-0065" num="0067">To provide the ingress gateway with sufficient information to determine how many flows of packets need to be terminated, in step s<b>15</b> the packet inspector determines the rate of NM packets being received and uses this figure as an estimate of the maximum supportable rate along the path. In step s<b>17</b> the egress gateway sends the result to the ingress gateway for that path in a control message on one of the core facing output lines <b>51</b><i>b. </i></p>
<p id="p-0066" num="0068">Ingress Gateways</p>
<p id="p-0067" num="0069">As mentioned above with reference to <figref idref="DRAWINGS">FIG. 5</figref>, the edge nodes can function simultaneously as egress gateways and ingress gateways depending on whether data is received via input lines <b>41</b><i>a </i>connected to the core of the network or <b>41</b><i>b </i>connected to external customers.</p>
<p id="p-0068" num="0070">Ingress gateways receive data on input ports <b>41</b><i>b </i>from external customers and are responsible for admission control of new flows of data packets and for introducing the data packets of previously admitted flows into the network for transmission across the network to an egress gateway along a predefined path. Each input port <b>41</b><i>b </i>is connected to a different external customer <b>9</b> which may itself be another network. Since packets may arrive simultaneously, the packets are temporarily stored in a FIFO receiver buffer <b>42</b>. A packet inspector <b>43</b> takes packets from the receiver buffer <b>42</b> and examines the headers to determine the external destination address of the packet. A controller <b>53</b> receives the extracted destination address and performs a lookup into a routing table <b>57</b> to determine which path the packet should be sent on to reach the destination. The controller <b>53</b> then controls a switch <b>45</b> to place the packets into an appropriate queue <b>47</b> inside a send buffer <b>49</b>. The packets are then processed by packet marker <b>55</b> to add any network specific headers and to mark packets (described in more detail below) before being sent towards their destination via output ports <b>51</b><i>a </i>into the core of the network.</p>
<p id="p-0069" num="0071">The packet marker <b>55</b> maintains a list of all the different incoming connections and assigns the respective flow identity to each of them. Normally packet marker <b>55</b> does not add any markings to packets within flows, i.e. the packets in all flows are sent as NM packets and the flows are logged as NM flows. However, when the network is congested, the ingress gateway is responsible for flow termination in addition to admission control by denying new flows from being accepted into the PCN network for transportation.</p>
<p id="p-0070" num="0072">In an ideal situation, the ingress node would terminate exactly the number of flows necessary to resolve the congestion situation. However, in complex network topologies, if all ingress nodes were to terminate flows to match the supportable rate estimate received from the egress gateway of each path, too many flows would be terminated leading to unnecessary loss of service on the network. Examples of loss of service include dropped telephone conversations and aborted video streaming.</p>
<p id="p-0071" num="0073">Instead of terminating the exact number of flows to meet the fed-back supportable rate estimates received from the egress gateway, the ingress gateway calculates a difference between the received supportable rate estimate and the rate of NM marked traffic being introduced into the network by the ingress gateway. The ingress gateway then only terminates a proportion of the flows. This proportion is predetermined prior to operation and is set at 30% of the calculated difference. Terminating just a proportion of the calculated difference will not resolve the congestion situation so in addition to terminating some flows, another proportion of the flows are marked RT. In this embodiment, 50% of the calculated flow differences are marked as RT.</p>
<p id="p-0072" num="0074">The processing of the ingress gateway for handling pre-congestion on a path is set out in the flowchart of <figref idref="DRAWINGS">FIG. 7</figref> and will be described for a path from edge node <b>7</b>A (ingress gateway) to edge node <b>7</b>B (egress gateway) carrying 100 flows each having a bandwidth of 1 Mbps. The process is performed every time a control message is received.</p>
<p id="p-0073" num="0075">Having received a control message from the egress gateway <b>7</b>B, in step s<b>21</b>, the packet inspector <b>43</b> of the ingress gateway <b>7</b>A extracts supportable NM packet rate estimate from the control message and forwards it via controller <b>53</b> to packet marker <b>55</b>. At packet marker <b>55</b> this estimate value is assigned to a variable M. In this example, M is 60 Mbps.</p>
<p id="p-0074" num="0076">In step s<b>23</b>, the packer marker <b>55</b> measures the rate of NM marked traffic, N, sent on the path identified in the control message, in this example N is 100 Mbps. The remaining steps are performed by the packet marker <b>55</b>.</p>
<p id="p-0075" num="0077">In step s<b>25</b>, the rate of traffic marked with the RT marking, R, currently being sent on the path is measured. In this example, R is 0 Mbps because until this point in time, no congestion has occurred.</p>
<p id="p-0076" num="0078">In step s<b>27</b>, a difference value, X, between the rate of NM traffic leaving the node and the rate estimate from the egress gateway is calculated. In this example, X is 40 Mbps.</p>
<p id="p-0077" num="0079">In step s<b>29</b>, the packet marker calculates whether the difference value is greater than 0. If it is not, then processing proceeds to step s<b>51</b> which will be described later. In this example, X is 40 and therefore processing proceeds to step s<b>31</b>.</p>
<p id="p-0078" num="0080">In step s<b>31</b>, the packet marker <b>55</b> calculates a correction bandwidth value W according to the formula W=0.3X+0.5R. As mentioned earlier, instead of simply terminating flows to meet the received supportable rate measurement M, the ingress gateway is arranged to terminate only a predetermined proportion of the difference (30% in this embodiment) and to mark a second predetermined portion of the difference (50% in this embodiment) as RT so that they do not get included in the calculations performed by the core nodes. In this example, W=(30% of 40)+(50% of 0)=12 Mbps.</p>
<p id="p-0079" num="0081">Next in step s<b>33</b> flows are terminated so as to meet the calculated bandwidth W. Since there are no RT marked flows and each flow has a bandwidth of 1 Mbps, in this embodiment, twelve flows are terminated. The ingress gateway <b>7</b>A uses a policy to choose flows for termination and marking as RT. In this embodiment, each flow has an associated priority and the ingress gateway ranks the priorities. The lower priority flows are chosen for termination and marking under RT than those which have a higher priority such as emergency calls.</p>
<p id="p-0080" num="0082">In step s<b>35</b>, the packet marker <b>55</b> calculates how many flows need to be marked as RT. This value Z is 50% of (X+R). In this example, Z is 20 Mbps.</p>
<p id="p-0081" num="0083">In step s<b>37</b>, flows are identified to be marked as RT. Since each flow is 1 Mbps, then 20 flows are chosen and in step s<b>39</b>, where packets belonging to an RT flow are received, they are marked as RT before transmission into the network. Processing of the control message then ends.</p>
<p id="p-0082" num="0084">The processing of the packet marker of the ingress gateway <b>7</b>A in response to the control message from egress gateway <b>7</b>B has resulted in twelve flows being terminated and twenty flows being marked as RT. The NM rate from the ingress gateway is therefore 68 Mbps. The changes to the path propagate through the network then propagate to the egress gateway and a new supportable rate estimate will be calculated if congestion still exists.</p>
<p id="p-0083" num="0085">The processing of the ingress gateway <b>7</b>A in a case where the egress gateway <b>7</b>B reports that the new supportable rate is 65 Mbps will now be described.</p>
<p id="p-0084" num="0086">At step s<b>21</b>, the variable M is set at 65 Mbps.</p>
<p id="p-0085" num="0087">At step s<b>23</b>, the rate of NM marked packets is determined to be 68 Mbps</p>
<p id="p-0086" num="0088">At step s<b>25</b>, the rate of RT marked packets is 20 Mbps.</p>
<p id="p-0087" num="0089">At step s<b>27</b>, the difference value X is 3 Mbps.</p>
<p id="p-0088" num="0090">At step s<b>29</b>, X is greater than 0 so processing proceeds to step s<b>51</b>.</p>
<p id="p-0089" num="0091">At step s<b>31</b>, W is (0.3*3+0.5*20)=11 flows to be terminated.</p>
<p id="p-0090" num="0092">At step s<b>33</b>, <b>11</b> of the RT marked flows are terminated since they take preference to the NM marked flows.</p>
<p id="p-0091" num="0093">At step s<b>35</b>, Z is 0.5*(3+20)=11 flows.</p>
<p id="p-0092" num="0094">At step s<b>37</b>, nine flows are already marked as RT so two additional flows are selected to be marked as RT flows and at step s<b>39</b>, the flows are marked RT.</p>
<p id="p-0093" num="0095">The processing of the ingress node in response to the second control message has resulted in 11 further flows being terminated and 11 flows being marked as RT. The NM rate from the ingress gateway is therefore 66 Mbps. The changes to the path propagate through the network then propagate to the egress gateway and a new supportable rate estimate will be calculated if congestion still exists.</p>
<p id="p-0094" num="0096">The processing of the ingress gateway <b>7</b>A in a case where the egress gateway <b>7</b>B reports that the new supportable rate is 66 Mbps will now be described.</p>
<p id="p-0095" num="0097">At step s<b>21</b>, the variable M is set at 66 Mbps.</p>
<p id="p-0096" num="0098">At step s<b>23</b>, the rate of NM marked packets is determined to be 66 Mbps</p>
<p id="p-0097" num="0099">At step s<b>25</b>, the rate of RT marked packets is 11 Mbps.</p>
<p id="p-0098" num="0100">At step s<b>27</b>, the difference value X is 0 Mbps.</p>
<p id="p-0099" num="0101">At step s<b>29</b> since X is 0, processing proceeds to step s<b>41</b> instead of step s<b>31</b>. At step s<b>41</b>, a check is made for any RT marked flows. If there are no RT flows, processing ends because a supportable rate estimate has been reached without any congestion occurring on the path. If there are RT flows, then processing proceeds to step s<b>43</b>.</p>
<p id="p-0100" num="0102">In step s<b>43</b>, instead of 30% of the difference between the supportable rate and the measured rate, a value equal to 30% of the RT marked flows is calculated. In this example, it is 0.3*11=3 flows that are terminated.</p>
<p id="p-0101" num="0103">In step s<b>45</b>, half of the currently RT marked flows are selected to be marked as normal NM flows. In this example 0.5*11=6 flows and in step s<b>47</b>, packets belonging to those selected six flows are transmitted from the ingress gateway with the NM marking. The transmission rate of the ingress node is now 72 NM marked flows with 5 RT flows.</p>
<p id="p-0102" num="0104">As can be shown from the above worked example, marking flows as RT instead of terminating them has a big advantage in recovering throughput once congestion has eased. RT marked flows can be &#x201c;un-terminated&#x201d; easily whereas actually terminating a flow cannot be reversed. Using the flexibility to reduce the amount of RT flows allows the ingress node to more quickly reach a safe level but without terminating too many flows.</p>
<heading id="h-0004" level="1">SECOND EMBODIMENT</heading>
<p id="p-0103" num="0105">In the first embodiment, the packet marking operation of the core nodes was implemented using a token bucket scheme. In the second embodiment, virtual queue marking is used.</p>
<p id="p-0104" num="0106">Core Nodes</p>
<p id="p-0105" num="0107">In this second embodiment, the core nodes meter the amount of traffic passing through them having the NM marking using a virtual queue having a token rate set at 90% of the line rate of the output port. As with the first embodiment, this will be referred to as the supportable rate.</p>
<p id="p-0106" num="0108">NM packets take tokens from the virtual queue in proportion to their packet size. If the queue has enough tokens then the marking of the packet is not changed. If the virtual queue has insufficient tokens then the marking of the packet is changed to TT.</p>
<p id="p-0107" num="0109">Packets marked TT (by another core node) or RT (by an ingress node) are not monitored by the core nodes. This is similar to the admission control marking defined by the PCN WG but differing in which packets are used for monitoring and which markings are used for marking.</p>
<p id="p-0108" num="0110">Egress Gateways</p>
<p id="p-0109" num="0111">As in the first embodiment the egress gateways measure the rate at which TT or RT marked packets are being received on each path and if the rate exceeds a threshold, then a control message is sent to the ingress gateway to inform it of the congestion. In this embodiment, the egress gateways measure the proportion of received packets which are TT and RT packets. This proportion gives a signal which indicates whether the supportable rate is being exceeded by the arriving NM marked traffic and if so, a measure of how far it is being exceeded. The egress gateway sends this signal in a control message to the ingress gateway of the path as in the first embodiment.</p>
<p id="p-0110" num="0112">Ingress Gateway</p>
<p id="p-0111" num="0113">As in the first embodiment, the ingress gateway normally sends data packets into the network without markings. However, in response to a control message from the egress gateway, some flows are terminated and others are marked RT. The operation of the ingress gateway in the second embodiment is more aggressive at marking packets as RT before any flows are terminated.</p>
<p id="p-0112" num="0114">In particular, the ingress gateway responds to the congestion signal by marking flows as RT flows and only terminating a small portion of flows. As the number of RT marked flows increases, then flows are terminated.</p>
<p id="p-0113" num="0115">The operation of the ingress gateway in the second embodiment will be described with reference to <figref idref="DRAWINGS">FIG. 2</figref>. The ingress gateway <b>7</b>D is carrying 100 flows each having a bandwidth of 1 Mbps along a path to egress gateway <b>7</b>C.</p>
<p id="p-0114" num="0116">If a congestion signal is received indicating 100% congestion, then the packet marker of ingress gateway <b>7</b>D may start to terminate NM flows at a rate of 1% per second. Simultaneously it will mark NM flows as RT flows at 20% per second and similarly terminate RT flows at a rate of 30% per second.</p>
<p id="p-0115" num="0117">After 1 second from reception of the control signal, the ingress gateway <b>7</b>D terminates 1 flow and marks 20 flows as RT. NM packets will now be entering the network at a rate of 79 Mbps.</p>
<p id="p-0116" num="0118">Now that less traffic is entering the network, core nodes which are shared by paths from ingress gateways to egress gateways will become less congested and similarly mark less packets as TT packets. This will cause some paths to reduce their congestion signal. In this example, the egress gateway <b>70</b> reports now reports a very low signal of 5% and sends this in a control message to the ingress gateway <b>7</b>D.</p>
<p id="p-0117" num="0119">Upon receiving the low congestion signal, the ingress gateway <b>7</b>D can deduce that 79% of the NM flows it is sending is below the supportable rate. However it has no indication of how much of the RT marked traffic can be carried. In this example the ingress gateway selects 20% of the RT marked flows to be sent as NM flows while continuing to terminate 30% of the RT marked flows. So in the next second, the ingress gateway terminates 4 flows and un-marks 6 flows. Now, there are 85 NM flows while 10 flows are marked RT.</p>
<p id="p-0118" num="0120">The ingress gateway continues to react to the congestion signals received from the egress gateway throughout the operation of the network.</p>
<p id="p-0119" num="0121">As with the previous embodiment, the advantage over a conventional system is that flows are marked RT so that they are not included in congestion calculations but are not actually terminated and hence can be un-terminated. This allows the network to quickly react to network conditions without unnecessarily terminating flows.</p>
<p id="p-0120" num="0122">In the embodiments, the nodes act asynchronously according to a local schedule. It will be clear to the skilled person that alternative coordination mechanisms can be used ranging from complete synchronicity to partially controlled interleaving.</p>
<p id="p-0121" num="0123">In the embodiments, the flows are terminated according to a policy specified at each ingress gateway which terminates flows according to their priority. In an alternative, the policy to terminate flows also takes into account other commercial considerations such as contractual arrangements with different customers.</p>
<p id="p-0122" num="0124">In the embodiment, only ingress gateways are allowed to terminate flows. However, in cases of high congestion it is possible the traffic flow through a core node will exceed the line rate on one or more output ports. Therefore in a modification of the embodiment systems, the core nodes are also enabled to drop packets. In such a case, the packet inspector of the core nodes recognises RT and TT packets and the controller causes the packet marker to drop RT and TT marked packets in preference to NM packets since RT and TT marked flows are likely to be terminated later anyway. This not only reduces congestion in the core of the network but also improves the accuracy of the supportable rate estimate.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of flow control on a network formed of a plurality of edge nodes and a plurality of core nodes, in which a plurality of flows, each formed of data packets, are introduced into the network from one of the edge nodes functioning as an ingress gateway, and travel on a predefined path via at least one of the core nodes to a different edge node functioning as an egress gateway, the method comprising:
<claim-text>each core node:
<claim-text>comparing a first reception rate of data packets against a first threshold value; and</claim-text>
<claim-text>if the first reception rate is greater than the first threshold value, adding a first congestion marker (TT) to the header of received packets;</claim-text>
</claim-text>
<claim-text>the egress gateway:
<claim-text>monitoring the number of received unmarked data packets, packets marked with the first congestion marker and packets marked with a second congestion marker (RT);</claim-text>
<claim-text>sending a network congestion message containing the monitored data as an indicator of network congestion, to the ingress gateway;</claim-text>
</claim-text>
<claim-text>the ingress gateway:
<claim-text>receiving the network congestion message from the egress gateway;</claim-text>
<claim-text>terminating a first subset of the flows such that data packets belonging to those flows are not sent to the at least one core node;</claim-text>
<claim-text>selecting a second subset of flows for possible termination;</claim-text>
<claim-text>marking received data packets belonging to the second subset of flows with the second congestion marker (RT); and</claim-text>
<claim-text>sending data packets marked with the second congestion marker (RT) to the egress gateway via the network,</claim-text>
</claim-text>
<claim-text>wherein the method further comprises:
<claim-text>the core nodes ignoring any data packets containing the first congestion marker (TT) or the second congestion marker (RT) in the comparison of the first reception rate; and</claim-text>
<claim-text>the ingress gateway unselecting flows from the second subset of flows, if the ingress gateway receives a network congestion message indicating that network congestion has decreased.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>the egress gateway:
<claim-text>comparing a second reception rate of packets containing either the first congestion marker (TT) or the second congestion marker (RT) against a second threshold value;</claim-text>
<claim-text>if the second reception rate is greater than the second threshold value, determining a third reception rate of unmarked data packets for sending to the ingress gateway;</claim-text>
</claim-text>
<claim-text>the ingress gateway:
<claim-text>calculating a difference value between the received third reception rate and a sending rate of unmarked flows being introduced onto the network;</claim-text>
</claim-text>
<claim-text>wherein:
<claim-text>the number of terminated flows in the first subset of flows is a first predetermined proportion of the calculated difference; and</claim-text>
<claim-text>the number of selected flows in the second subset of flows is a second predetermined proportion of the calculated difference, and</claim-text>
<claim-text>the sum of terminated flows and marked flows in accordance with the first and second proportion is less than the calculated difference.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>the egress gateway:
<claim-text>calculating a ratio of the number of received unmarked packets against the number of received data packets carrying the first or second congestion marker (TT, RT) for sending to the ingress gateway; and:</claim-text>
</claim-text>
<claim-text>the ingress gateway:
<claim-text>marking flows with the second congestion marker (RT) at a predetermined marking rate; and</claim-text>
<claim-text>terminating flows from said first subset of flows at a predetermined termination rate which is a proportion of the predetermined marking rate.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ingress gateway selects the set of flows for possible termination by:
<claim-text>accessing a flow processing policy and choosing flows in order of flow priority.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A network edge node for performing flow control within a network of edge nodes and core nodes for transporting a plurality of flows, each formed of data packets, along a predefined path across the network, the network edge node comprising:
<claim-text>a network interface for communication with at least one core network node and at least one data external network entity;</claim-text>
<claim-text>a data packet receiver for receiving data packets from the at least one core node and from the at least one external network entity;</claim-text>
<claim-text>a data packet processor determining where to send received data packets for onward transmission; and</claim-text>
<claim-text>a data packet transmitter for sending data packets to the at least one core node and to the at least one external network entity,</claim-text>
<claim-text>wherein the edge node has a first mode of operation as a network ingress gateway and a second mode of operation as a network egress gateway;</claim-text>
</claim-text>
<claim-text>wherein in the second mode as an egress gateway:
<claim-text>the data packet processor is operable to monitor the number of received data packets, and distinguish between an unmarked data packet, a data packet having a first congestion marker (TT) and a data packet having a second congestion marker (RT); and</claim-text>
<claim-text>generate a network congestion message containing the monitored data as an indicator of network congestion to another edge node in the network operating as an ingress gateway; and</claim-text>
</claim-text>
<claim-text>wherein in the first mode as an ingress gateway:
<claim-text>the data packet processor is operable to:
<claim-text>determine whether a network congestion message has been received;</claim-text>
<claim-text>terminate a first subset of flows;</claim-text>
<claim-text>select a second subset of flows for possible termination; and</claim-text>
<claim-text>mark received data packets belonging to the second subset of flows with the second congestion marker (RT);</claim-text>
</claim-text>
</claim-text>
<claim-text>wherein in the first mode the ingress gateway is operable to unselect flows from the second subset of flows, if a network congestion message indicating that network congestion has decreased is received.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. An edge node according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein
<claim-text>in the second mode as an egress node the data packet processor is further operable to:
<claim-text>compare a second reception rate of packets containing either the first congestion marker (TT) or the second congestion marker (RT) against a second threshold value; and</claim-text>
<claim-text>determine a third reception rate of unmarked data packets for sending to the ingress gateway if the second reception rate is greater than the second threshold value; and</claim-text>
</claim-text>
<claim-text>wherein in the first mode as an ingress gateway the data packet processor is operable to:
<claim-text>calculate a difference value between the received third reception rate and a sending rate of unmarked flows being introduced onto the network; and</claim-text>
</claim-text>
<claim-text>wherein:
<claim-text>the number of terminated flows in the first subset of flows is a first predetermined proportion of the calculated difference; and</claim-text>
<claim-text>the number of selected flows in the second subset of flows is a second predetermined proportion of the calculated difference, and</claim-text>
<claim-text>the sum of terminated flows and marked flows in accordance with the first and second proportion is less than the calculated difference.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A network edge node according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein
<claim-text>in the second mode as an egress gateway the data processor is operable to:
<claim-text>calculate a ratio of the number of received unmarked packets against the number of received data packets carrying the first or second congestion marker (TT, RT) for sending to the ingress gateway; and:</claim-text>
</claim-text>
<claim-text>in the first mode as an ingress gateway the data processor is operable to:
<claim-text>mark flows with the second congestion marker (RT) at a predetermined marking rate; and</claim-text>
<claim-text>terminate flows from said first subset of flows at a predetermined termination rate which is a proportion of the predetermined marking rate.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A network edge node according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein in the first mode the data packet processor is operable to choose flows for termination and marking with the second congestion marker in order of flow priority.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A data network for transporting flows of data packets, comprising:
<claim-text>a plurality of edge network nodes configured as an ingress gateway;</claim-text>
<claim-text>a plurality of edge network nodes configured as egress gateways; and</claim-text>
<claim-text>a plurality of core nodes for routing flows across the network from the ingress gateways to the egress gateways;</claim-text>
<claim-text>wherein at least one of the edge network nodes configured as an ingress gateway is a network edge node for performing flow control within a network of edge nodes and core nodes for transporting a plurality of flows, each formed of data packets, along a predefined path across the network, the network edge node comprising:</claim-text>
<claim-text>a network interface for communication with at least one core network node and at least one data external network entity;</claim-text>
<claim-text>a data packet receiver for receiving data packets from the at least one core node and from the at least one external network entity;</claim-text>
<claim-text>a data packet processor determining where to send received data packets for onward transmission; and</claim-text>
<claim-text>a data packet transmitter for sending data packets to the at least one core node and to the at least one external network entity,</claim-text>
<claim-text>wherein the network edge node has a mode of operation as a network ingress gateway;
<claim-text>wherein at least one of the egress gateways is configured to monitor the number of received data packets, and distinguish between an unmarked data packet, a data packet having a first congestion marker (TT) and a data packet having a second congestion marker (RT), and generate a network congestion message containing the monitored data as an indicator of network congestion to another edge node in the network operating as an ingress gateway; and</claim-text>
</claim-text>
</claim-text>
<claim-text>wherein in the mode as an ingress gateway:
<claim-text>the data packet processor is operable to:
<claim-text>determine whether a network congestion message has been received;</claim-text>
<claim-text>terminate a first subset of flows;</claim-text>
<claim-text>select a second subset of flows for possible termination; and</claim-text>
<claim-text>mark received data packets belonging to the second subset of flows with the second congestion marker (RT);</claim-text>
</claim-text>
</claim-text>
<claim-text>wherein in the mode the ingress gateway is operable to unselect flows from the second subset of flows, if a network congestion message indicating that network congestion has decreased is received.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A data network for transporting flows of data packets, comprising:
<claim-text>a plurality of edge network nodes configured as an ingress gateway;</claim-text>
<claim-text>a plurality of edge network nodes configured as egress gateways; and</claim-text>
<claim-text>a plurality of core nodes for routing flows across the network from the ingress gateways to the egress gateways;</claim-text>
<claim-text>wherein at least one of the edge network nodes configured as an egress gateway is a network edge node for performing flow control within a network of edge nodes and core nodes for transporting a plurality of flows, each formed of data packets, along a predefined path across the network, the network edge node comprising:</claim-text>
<claim-text>a network interface for communication with at least one core network node and at least one data external network entity;</claim-text>
<claim-text>a data packet receiver for receiving data packets from the at least one core node and from the at least one external network entity;</claim-text>
<claim-text>a data packet processor determining where to send received data packets for onward transmission; and</claim-text>
<claim-text>a data packet transmitter for sending data packets to the at least one core node and to the at least one external network entity,</claim-text>
<claim-text>wherein the network edge node has a mode of operation as a network egress gateway:
<claim-text>the data packet processor is operable to monitor the number of received data packets, and distinguish between an unmarked data packet, a data packet having a first congestion marker (T) and a data packet having a second congestion marker (RT); and</claim-text>
<claim-text>generate a network congestion message containing the monitored data as an indicator of network congestion to another edge node in the network operating as an ingress gateway; and</claim-text>
</claim-text>
</claim-text>
<claim-text>wherein in the ingress gateway is operable to:
<claim-text>determine whether a network congestion message has been received;</claim-text>
<claim-text>terminate a first subset of flows;</claim-text>
<claim-text>select a second subset of flows for possible termination;</claim-text>
<claim-text>mark received data packets belonging to the second subset of flows with the second congestion marker (RT); and</claim-text>
<claim-text>unselect flows from the second subset of flows, if a network congestion message indicating that network congestion has decreased is received. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
