<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625905-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625905</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13015675</doc-number>
<date>20110128</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>412</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>62</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382224</main-classification>
<further-classification>348142</further-classification>
<further-classification>348143</further-classification>
<further-classification>382103</further-classification>
<further-classification>382154</further-classification>
</classification-national>
<invention-title id="d2e53">Classification of target objects in motion</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5430445</doc-number>
<kind>A</kind>
<name>Peregrim</name>
<date>19950700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6720907</doc-number>
<kind>B1</kind>
<name>Miron</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7015855</doc-number>
<kind>B1</kind>
<name>Medl et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7411543</doc-number>
<kind>B1</kind>
<name>Boka et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7684020</doc-number>
<kind>B1</kind>
<name>Marti et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>8249301</doc-number>
<kind>B2</kind>
<name>Brown et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382104</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>8301344</doc-number>
<kind>B2</kind>
<name>Simon et al.</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701 45</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2002/0140924</doc-number>
<kind>A1</kind>
<name>Wangler et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>356 28</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2006/0047704</doc-number>
<kind>A1</kind>
<name>Gopalakrishnan</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>7071041</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2006/0233436</doc-number>
<kind>A1</kind>
<name>Ma et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2007/0070069</doc-number>
<kind>A1</kind>
<name>Samarasekera et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345427</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2010/0253597</doc-number>
<kind>A1</kind>
<name>Seder et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345  7</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2011/0231016</doc-number>
<kind>A1</kind>
<name>Goulding</name>
<date>20110900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>700246</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2012/0154579</doc-number>
<kind>A1</kind>
<name>Hampapur et al.</name>
<date>20120600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2012/0195459</doc-number>
<kind>A1</kind>
<name>Schmidt et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Duda, et al., Pattern Classification, 2nd Edition, 2001, John Wiley &#x26; Sons, Inc., Chapter 1. Introduction, pp. 1-19.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>15</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>348143</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348148</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382154</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120195459</doc-number>
<kind>A1</kind>
<date>20120802</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Schmidt</last-name>
<first-name>Michael S.</first-name>
<address>
<city>Carlisle</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Prativadi</last-name>
<first-name>Prakruti S.</first-name>
<address>
<city>Littleton</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Griffin</last-name>
<first-name>Shane A.</first-name>
<address>
<city>Danville</city>
<state>NH</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Phelps</last-name>
<first-name>Ethan J.</first-name>
<address>
<city>Arlington</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Schmidt</last-name>
<first-name>Michael S.</first-name>
<address>
<city>Carlisle</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Prativadi</last-name>
<first-name>Prakruti S.</first-name>
<address>
<city>Littleton</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Griffin</last-name>
<first-name>Shane A.</first-name>
<address>
<city>Danville</city>
<state>NH</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Phelps</last-name>
<first-name>Ethan J.</first-name>
<address>
<city>Arlington</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Pierce Atwood LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Maraia</last-name>
<first-name>Joseph M.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Raytheon Company</orgname>
<role>02</role>
<address>
<city>Waltham</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Desire</last-name>
<first-name>Gregory M</first-name>
<department>2668</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for classifying objects in motion that includes providing, to a processor, feature data for one or more classes of objects to be classified, wherein the feature data is indexed by object class, orientation, and sensor. The method also includes providing, to the processor, one or more representative models for characterizing one or more orientation motion profiles for the one or more classes of objects in motion. The method also include acquiring, via a processor, feature data for a target object in motion from multiple sensors and/or for multiple times and trajectory of the target object in motion to classify the target object based on the feature data, the one or more orientation motion profiles and the trajectory of the target object in motion.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="149.27mm" wi="208.20mm" file="US08625905-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="215.90mm" wi="184.66mm" orientation="landscape" file="US08625905-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="202.61mm" wi="162.14mm" orientation="landscape" file="US08625905-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="208.20mm" wi="161.80mm" orientation="landscape" file="US08625905-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="166.37mm" wi="137.75mm" orientation="landscape" file="US08625905-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="204.30mm" wi="126.83mm" orientation="landscape" file="US08625905-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="158.50mm" wi="117.26mm" orientation="landscape" file="US08625905-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="155.19mm" wi="110.07mm" orientation="landscape" file="US08625905-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="138.68mm" wi="121.58mm" orientation="landscape" file="US08625905-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0002" num="0001">The currently described invention relates to systems and methods for classifying objects in motion using more than a single sensor observation either by using multiple sensors, or multiple observations from a single sensor, or both. The currently described invention also relates to data fusion.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">Prior art methods for classifying target objects in motion (e.g., flight) involve generating training data for the classification systems in an offline mode. The training data, acquired for exemplary objects used for classifying target objects is generated prior to observing the target object to be classified. When fusing data from multiple sensors and multiple observations, the training data consists of joint feature distributions; joint over features to be collected from each sensor and each observation time. One approach attempts to capture the statistical dependencies between multiple observations of exemplary objects with predictable flight dynamics by training over a subset of all possible trajectories and sensor observation times for a fixed set of sensor locations. This method produces a sparse data set since the number of possible trajectories and observation times is infinitely large and the dimensionality of the joint feature space is likewise large and unknown since numbers and times of observation are unknown prior to observation of target object in flight. Therefore, the method does not adequately represent the statistics to successfully classify the target objects. An alternative approach does not attempt to capture the dependencies between multiple observations, but assumes sub-optimally that the observations are independent.</p>
<p id="p-0004" num="0003">These methods do not adequately classify the target objects. A need therefore exists for improved systems and methods for classifying objects in motion using multiple observations.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">Embodiments described herein relate generally to systems and methods for classifying objects in motion.</p>
<p id="p-0006" num="0005">One embodiment features a method for classifying objects in motion using multiple observations (e.g., from multiple sensors and/or multiple times). The method includes providing, to a processor, feature data for one or more classes of objects to be classified, wherein the feature data is indexed by object class, orientation, and sensor. The method also includes providing, to the processor, one or more representative models for characterizing one or more orientation motion profiles for the one or more classes of objects in motion. The method also include acquiring, via a processor, feature data for a target object in motion and trajectory of the target object in motion to classify the target object based on the feature data, the one or more orientation motion profiles and the trajectory of the target object in motion.</p>
<p id="p-0007" num="0006">In some embodiments, providing the one or more representative models for characterizing one or more orientation motion profiles includes acquiring orientation motion data for an exemplary object in motion. In some embodiments, providing the one or more representative models for characterizing one or more orientation motion profiles includes generating orientation motion data based on an analytical model for an exemplary object in motion.</p>
<p id="p-0008" num="0007">In some embodiments, the method includes acquiring the feature data for the target object in motion and the trajectory of the target object in motion at one or more instances of time, periods of time, or a combination of both. In some embodiments, the method includes acquiring the feature data for the target object in motion and the trajectory of the target object in motion using a plurality of sensors.</p>
<p id="p-0009" num="0008">In some embodiments, the feature data includes at least one of radar data, optical data or infrared data for each of the one or more classes of objects. The feature data can include radar cross section signals and time derivatives of the radar cross section signals. The sensor can be a radar system, lidar system, optical imaging system, or infrared monitoring system.</p>
<p id="p-0010" num="0009">In some embodiments, the method includes classifying the target object using Bayes' Rule the target object as belonging to a particular class of the one or more classes of objects based on the posterior probability the target object corresponds to the particular class. In some embodiments, the feature data for the one or more classes of objects and the one or more representative models for characterizing one or more orientation motion profiles for the feature data are indexed in a database stored on the processor.</p>
<p id="p-0011" num="0010">Another embodiment features a system for classifying objects in motion. The system including data collected prior to classifying a target object in motion, wherein the data includes a) feature data on one or more classes of objects to be classified, wherein the feature data is indexed by orientation, sensor, and object class; and b) one or more representative models for characterizing one or more orientation motion profiles for the feature data on the one or more classes of objects. The system also includes at least one sensor to acquire feature data for a target object in motion and trajectory of the target object in motion. The system also includes a first processor to generate reference feature data while the target object is in motion based on the object class and the trajectory of the target object in motion, wherein the at least one sensor provides feature data and time of feature data. The system also includes a second processor to classify the target object during motion of the target object based on the reference feature data generated by the first processor, and feature data for the target object in motion.</p>
<p id="p-0012" num="0011">In some embodiments, the system includes one or more sensors to acquire orientation motion data for an exemplary object in motion to generate the one or more orientation motion profiles for the one or more classes of objects.</p>
<p id="p-0013" num="0012">In some embodiments, the first processor generates reference feature vectors by: 1. selecting an object class, 2. selecting a orientation motion profile for the selected object class, 3. for each point in time that feature data is collected by the at least one sensor for the target object in motion, the selected orientation motion profile and the trajectory of the target object in motion are used to determine the orientation of the target object in motion, a feature is selected from the feature database based on the sensor, object class, and orientation of the target object in motion. In some embodiments, steps 1-3 are repeated to generate a collection of reference feature vectors.</p>
<p id="p-0014" num="0013">In some embodiments, the second processor is configured to perform Bayesian classification using the reference feature data generated by the first processor as a priori data and the feature data for the target object to be classified to generate posterior object class type probabilities. In some embodiments, the feature data for the target object in motion includes feature data collected from each sensor at single points in time.</p>
<p id="p-0015" num="0014">Other aspects and advantages of the current invention will become apparent from the following detailed description, taken in conjunction with the accompanying drawings, illustrating the principles of the invention by way of example only.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0016" num="0015">The foregoing features of various embodiments of the invention will be more readily understood by reference to the following detailed descriptions in the accompanying drawings, in which:</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic illustration of a system for classifying objects, according to an illustrative embodiment.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 2A</figref> is a flowchart of a prior art method for classifying objects in motion.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 2B</figref> is a flowchart of a method for classifying objects in motion, according to an illustrative embodiment.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 3A</figref> is a graphical representation of radar cross section measured by a first sensor versus radar cross section measured by a second sensor.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 3B</figref> is a graphical representation of radar cross section versus aspect angle for two classes of objects, using a method in one embodiment for a target object to be classified.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 3C</figref> is a graphical representation of rotation rate versus probability density for the two classes of objects of <figref idref="DRAWINGS">FIG. 3B</figref>, using a method in one embodiment for a target object to be classified.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 3D</figref> is a graphical representation of simulated measurements for a target object in motion.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 3E</figref> is a graphical representation identifying the classification of a target object based on the feature data and motion profile data of <figref idref="DRAWINGS">FIGS. 3B and 3C</figref> and measurements for a target object in motion, using a method to classify the target object.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS</heading>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic illustration of a system <b>200</b> for classifying objects, according to an illustrative embodiment. The system <b>200</b> includes two sensors <b>204</b><i>a </i>and <b>204</b><i>b </i>(generally <b>204</b>) to acquire feature data and motion profile data for an object <b>206</b> (e.g., target object) in motion. Each sensor <b>204</b> is coupled to a corresponding sensor subsystem <b>220</b>. Sensor <b>204</b><i>a </i>is coupled to sensor subsystem <b>220</b><i>a </i>(generally <b>220</b>) and sensor <b>204</b><i>b </i>is coupled to sensor subsystem <b>220</b><i>b</i>. Each sensor (<b>204</b><i>a </i>and <b>204</b><i>b</i>) is coupled to a sensor control module (<b>208</b><i>a </i>and <b>208</b><i>b</i>, respectively), processor (<b>216</b><i>a </i>and <b>216</b><i>b</i>, respectively), input device (<b>240</b><i>a </i>and <b>240</b><i>b</i>, respectively), output device (<b>244</b><i>a </i>and <b>244</b><i>b</i>, respectively), display device (<b>248</b><i>a </i>and <b>248</b><i>b</i>, respectively) and a storage device (<b>252</b><i>a </i>and <b>252</b><i>b</i>, respectively).</p>
<p id="p-0026" num="0025">Processors <b>216</b><i>a </i>and <b>216</b><i>b </i>provide emission signals to sensor control modules <b>208</b><i>a </i>and <b>208</b><i>b</i>, respectively, for emission (e.g., radar electromagnetic emissions) by the sensors <b>204</b><i>a </i>and <b>204</b><i>b</i>. The emitted signals are directed toward the object <b>206</b> in motion. Response signals (e.g., radar response signals) reflected back towards the sensors in response to the emitted signals impinging upon the object <b>206</b> are received by the sensors <b>204</b><i>a </i>and <b>204</b><i>b</i>. The sensor control modules <b>208</b><i>a </i>and <b>208</b><i>b </i>receive the response signals from the sensors <b>204</b><i>a </i>and <b>204</b><i>b</i>, respectively, and direct the signals to the processor <b>216</b><i>a </i>and <b>216</b><i>b</i>, respectively. The processors <b>216</b><i>a </i>and <b>216</b><i>b </i>process the response signals received from each of the sensors to, for example, determine the velocity and radar cross section (RCS) of the object <b>206</b>. The processors <b>216</b><i>a </i>and <b>216</b><i>b </i>store various information regarding the system <b>200</b> and its operation in the storage devices <b>252</b><i>a </i>and <b>252</b><i>b</i>, respectively.</p>
<p id="p-0027" num="0026">The storage devices <b>252</b><i>a </i>and <b>252</b><i>b </i>can store information and/or any other data associated with the system <b>200</b>. The storage devices can include a plurality of storage devices. The storage devices can include, for example, long-term storage (e.g., a hard drive, a tape storage device, flash memory, etc.), short-term storage (e.g., a random access memory, a graphics memory, etc.), and/or any other type of computer readable storage.</p>
<p id="p-0028" num="0027">The modules and devices described herein can, for example, utilize the processors <b>216</b><i>a </i>and <b>216</b><i>b </i>to execute computer executable instructions and/or include a processor to execute computer executable instructions (e.g., an encryption processing unit, a field programmable gate array processing unit, etc.). It should be understood that the system <b>200</b> can include, for example, other modules, devices, and/or processors known in the art and/or varieties of the illustrated modules, devices, and/or processors.</p>
<p id="p-0029" num="0028">The input devices <b>240</b><i>a </i>and <b>240</b><i>b </i>receive information associated with the system <b>200</b> (e.g., instructions from a user, instructions from another computing device) from a user (not shown) and/or another computing system (not shown). The input devices <b>240</b><i>a </i>and <b>240</b><i>b </i>can include, for example, a keyboard or a scanner. The output devices <b>244</b><i>a </i>and <b>244</b><i>b </i>output information associated with the system <b>200</b> (e.g., information to a printer (not shown), information to an audio speaker (not shown)).</p>
<p id="p-0030" num="0029">The display devices <b>248</b><i>a </i>and <b>248</b><i>b </i>display information associated with the system <b>200</b> (e.g., status information, configuration information). The processors <b>216</b><i>a </i>and <b>216</b><i>b </i>execute the operating system and/or any other computer executable instructions for the system <b>200</b> (e.g., processor <b>216</b><i>a </i>sends transmission signals to the sensor control module <b>208</b><i>a </i>for transmission by the sensor <b>204</b><i>a </i>and/or receives response signals from the sensor <b>204</b><i>a</i>).</p>
<p id="p-0031" num="0030">The system <b>200</b> also includes a separate processing system <b>224</b> that is coupled to sensor subsystem <b>220</b><i>a </i>and sensor subsystem <b>220</b><i>b</i>. The storage device <b>276</b> of the processing system <b>224</b> contains feature data and motion profile data on one or more classes of objects (e.g., the feature data and motion profile data for one or more classes of objects previously stored during offline processing steps). In some embodiments, the feature data and motion profile data are previously acquired and stored for one or more exemplary objects. The processing system <b>224</b> includes a processor <b>260</b> that classifies the target object <b>206</b> based, in part, on the previously stored feature data, motion profile data, and data acquired by the sensors <b>204</b><i>a </i>and <b>204</b><i>b </i>while the target object <b>206</b> is in motion.</p>
<p id="p-0032" num="0031">Alternative types of data may be acquired and used by the system <b>200</b> in alternative embodiments. In some embodiments, at least one of radar data (acquired using a radar system as the sensor), optical data (acquired using an optical imaging system as the sensor) and/or infrared data (acquired using an infrared monitoring system as the sensor) is used for one or more classes of objects.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 2A</figref> is s flowchart <b>300</b>A of a prior art method for classifying target objects in motion. The method includes providing training data (step <b>302</b>), namely an a priori joint feature distribution, to a processor. Joint feature distributions specify the likelihood of obtaining, via sensing, a set of given object features for an object class. The training data, acquired for exemplary objects used for classifying target objects, is generated prior to observing the target object to be classified. When fusing data from multiple sensors and multiple observations, the training data consists of joint feature distributions; joint over the features to be collected from each sensor and each observation time. In one instance, the classification system attempts to capture the statistical dependencies (the joint feature distribution) between multiple observations of exemplary objects with predictable flight dynamics by collecting feature data via digital simulation over a subset of all possible trajectories and sensor observation times for a fixed set of sensor locations. This method produces a sparse data set since the number of possible trajectories and observation times is infinitely large and the dimensionality of the joint feature space is likewise large and unknown since numbers and times of observation is unknown prior to observation of a target object in flight. This method does not adequately represent the statistics (joint feature distribution) to classify the target objects. An alternative prior art approach does not attempt to capture the dependencies between multiple observations, but assumes sub-optimally that the observations are independent.</p>
<p id="p-0034" num="0033">The prior art also includes acquiring (step <b>312</b>), via sensors and processors, feature data for a target object in motion and trajectory of the target object in motion. The feature data and trajectory for the target object in motion can be acquired at, for example, one or more instances of time, one or more sensors, or a combination of both.</p>
<p id="p-0035" num="0034">The prior art also includes classifying (step <b>324</b>) the target object based on the feature data. In one instance, classifying (step <b>324</b>) the target object is performed using a Bayes' classifier in which the target object is specified as belonging to a particular class of one or more classes of objects based on the posterior probability that the target object belongs to the particular class given the a priori joint feature distribution (step <b>302</b>) and the target object feature data (step <b>312</b>). Steps <b>312</b> and <b>324</b> can be performed, for example, using system <b>200</b> of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 2B</figref> is s flowchart <b>300</b>B of a method for classifying target objects in motion, according to an illustrative embodiment. The method includes providing (step <b>304</b>), to a processor, feature data for one or more exemplary classes of objects to be classified. In one embodiment, the feature data is indexed by object class, orientation and sensor. The feature data can be indexed by, for example, object class, object orientation, sensor, and/or sensor signal-to-noise ratio. Object classes can include, for example, missile, satellite, ground vehicle, aircraft and/or specific model or type of each of these (e.g., Boeing 747 aircraft, F-16 aircraft). The feature data consists of sensor signal data and can include, for example, radar cross section (RCS) signals, and time derivatives of the RCS signals. The data is generated either by using one or more digital simulations of sensors and target models or by collecting the data using one sensor systems and targets. The target objects are not required to be in flight to collect features indexed by aspect angle.</p>
<p id="p-0037" num="0036">The method also includes providing (step <b>308</b>), to a processor, one or more representative models for characterizing one or more orientation motion profiles (motion profile data) for the one or more classes of objects in motion. In some embodiments, providing (step <b>308</b>) the one or more representative models for characterizing one or more orientation motion profiles includes the step of acquiring (step <b>316</b>) orientation motion data for an exemplary object in motion by digital simulation (e.g., the models are statistical models). In an alternative embodiment, providing (step <b>308</b>) the one or more representative models for characterizing one or more orientation motion profiles includes the optional step of generating (step <b>320</b>) orientation motion data based on an analytical model for an exemplary object in motion (e.g., the models are physical models). In some embodiments, a combination of statistical models and physical models are used.</p>
<p id="p-0038" num="0037">In some embodiments, the representative models characterizing a motion profile for a class of objects is generated based on an analytical model of an exemplary object in motion. In one embodiment, an analytical model of an exemplary object in motion is a kinematic model describing the motion of the exemplary object along a curvilinear path in which each point along the path defines the location and velocity vector of the exemplary object. For example, a motion model might consist of a ballistic model for propagation of position and velocity and a statistical distribution for the angular momentum vector relative to the velocity vector, and a statistical distribution for the precession phase. Examples of ballistic motion models include 4<sup>th </sup>order Runge-Kutta integration of differential equations that represent gravity, Coriolis and centrifugal forces and the Simplified General Perturbation 4 (SGP4) algorithm.</p>
<p id="p-0039" num="0038">The feature and motion profile data (steps <b>304</b> and <b>308</b>) are stored in, for example, a database for subsequent use by a real time training method (e.g., stored by processor <b>260</b> in storage device <b>276</b> of <figref idref="DRAWINGS">FIG. 1</figref>).</p>
<p id="p-0040" num="0039">The method also includes generating a representative feature database specific to the observed trajectory of the target object to be classified and the sensor observation times of the target object to be classified (step <b>310</b>). The representative feature database is generated by sampling from the motion profiles of step <b>308</b>, using them to determine the orientation of the object to be classified at each sensor collection and selecting the feature with that orientation, class and SNR from feature data from step <b>304</b>. In one embodiment, a first processor generates the reference data (e.g., reference feature vectors) by: 1. selecting an object class, 2. selecting a orientation motion profile for the selected object class, 3. for each point in time that feature data is collected by the at least one sensor for the target object in motion, the selected orientation motion profile and the trajectory of the target object in motion are used to determine the orientation of the target object in motion, a feature is selected from the feature database based on the sensor, object class, and orientation of the target object in motion. In this way a classification database (namely the joint feature distribution which captures statistical dependencies) is constructed in near-real time specific to the trajectory of the object to be classified and the times and locations of the sensor observations.</p>
<p id="p-0041" num="0040">Similar as with the prior art method, the method of <figref idref="DRAWINGS">FIG. 2B</figref> also includes acquiring (step <b>312</b>), via a processor, feature data for a target object in motion and trajectory of the target object in motion. The feature data and trajectory for the target object in motion can be acquired at, for example, one or more instances of time, periods of time, or a combination of both. The feature data and trajectory for the target object in motion can be acquired using one or a plurality of sensors.</p>
<p id="p-0042" num="0041">Referring to <figref idref="DRAWINGS">FIG. 2B</figref>, the method depicted by flowchart <b>300</b>B includes classifying (step <b>380</b>) the target object based on the joint feature distribution (generated in step <b>310</b>) and the a posterior feature data (acquired in step <b>312</b>). The joint feature distribution (of step <b>310</b>) is based on the feature data, the one or more orientation motion profiles and the feature data and trajectory of the target object in motion. In one embodiment, classifying (step <b>380</b>) the target object is performed using Bayes' Rule in which the target object is specified as belonging to a particular class of one or more classes of objects based on the posterior probability the target object belongs to the particular class. Steps <b>310</b>, <b>312</b> and <b>324</b> can be performed, for example, using system <b>200</b> of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0043" num="0042">By way of illustration, a simulation was conducted to classify a target object using the system <b>200</b> of <figref idref="DRAWINGS">FIG. 1</figref> and the method of <figref idref="DRAWINGS">FIG. 2B</figref> in which there are two object classes (triangle and rectangle). Referring to <figref idref="DRAWINGS">FIG. 3D</figref>, there are two sensors, sensor <b>420</b><i>a </i>and sensor <b>420</b><i>b </i>(e.g., sensors <b>204</b><i>a </i>and <b>204</b><i>b </i>of <figref idref="DRAWINGS">FIG. 1</figref>) for measuring data associated with the objects in motion.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 3A</figref> illustrates the results using the prior art method of <figref idref="DRAWINGS">FIG. 2A</figref> and is a graphical representation of the joint feature distribution of radar cross section (RCS) of the two object classes (triangle and rectangle) measured by a first sensor (sensor <b>1</b> is sensor <b>420</b><i>a </i>of <figref idref="DRAWINGS">FIG. 3D</figref>) versus radar cross section (RCS) measured by a second sensor (sensor <b>2</b> is sensor <b>420</b><i>b </i>of <figref idref="DRAWINGS">FIG. 3D</figref>). The method of <figref idref="DRAWINGS">FIG. 2A</figref> assumes sub-optimally that the observations are independent (step <b>302</b>). The system implementing the method of <figref idref="DRAWINGS">FIG. 2A</figref> measures a value of 9 dBsm (decibel measure of the radar cross section of a target relative to one square meter) using the first sensor and a value of 15 dBsm using the second sensor for a target object (step <b>312</b>), and is unable to effectively classify the target object (step <b>324</b>). The system is unable to determine whether the target object is a rectangle or a triangle because the possible feature values (RCS values) for the two object classes (rectangle and triangle) overlap (as shown in the plot in <figref idref="DRAWINGS">FIG. 3A</figref>).</p>
<p id="p-0045" num="0044">Feature data and motion profile data was generated for the two classes of objects. <figref idref="DRAWINGS">FIG. 3B</figref> is a graphical representation of radar cross section versus aspect angle for the two classes of objects. The data in <figref idref="DRAWINGS">FIG. 3B</figref> is used as the feature data for the simulation (step <b>304</b> of <figref idref="DRAWINGS">FIG. 2B</figref>). <figref idref="DRAWINGS">FIG. 3C</figref> is a graphical representation of rotation rate versus probability density for the two classes of objects of <figref idref="DRAWINGS">FIG. 3B</figref>. The data in <figref idref="DRAWINGS">FIG. 3C</figref> is used as the motion profile data for the two classes of objects (e.g., step <b>308</b> of <figref idref="DRAWINGS">FIG. 2B</figref>). In this embodiment, the triangle class of objects is capable of spinning at a faster rate than the rectangular class of objects.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 3D</figref> is a graphical representation of simulated measurements for a target object <b>404</b> in motion. The target object <b>404</b> has an initial position on the Y-axis of the plot <b>408</b> at (0,53). The trajectory <b>412</b> of the target object <b>404</b> is at an angle <b>416</b> of seventeen (17) degrees. The speed of the target object <b>404</b> along the trajectory <b>412</b> is four (4) km/s. Sensor <b>420</b><i>a </i>observes (i.e., takes a measurement) at a time of two (2) seconds and sensor <b>420</b><i>b </i>observes at a time of five (5) seconds. Sensor <b>420</b><i>a </i>has a signal-to-noise ratio of eight (9) dB and sensor <b>420</b><i>b </i>has a signal-to-noise ratio of twelve (15) dB.</p>
<p id="p-0047" num="0046">The feature data and motion profile data of <figref idref="DRAWINGS">FIGS. 3B and 3C</figref> were then used as the basis for classifying the target object <b>404</b>. <figref idref="DRAWINGS">FIG. 3E</figref> illustrates the results of step <b>310</b> of <figref idref="DRAWINGS">FIG. 2B</figref>. <figref idref="DRAWINGS">FIG. 3E</figref> illustrates a new feature database (different from the prior art illustrated by the result in <figref idref="DRAWINGS">FIG. 3A</figref>) which is constructed after the target object in flight to be classified is observed. The method of the embodiment repeatedly randomly selects an initial assumption for the target object (triangle or rectangle), initial assumption for the trajectory angle (&#x2212;90 to 90 degrees) for the target object, and initial assumption of the object motion profile of the target object rotating at a rate in accordance with the exemplary rotation rates (degrees/second) of <figref idref="DRAWINGS">FIG. 3C</figref>. In this manner, the lightly shaded and darkly shaded points (joint feature distribution) in <figref idref="DRAWINGS">FIG. 3E</figref> are generated based on orientation of the object at the time of sensor measurements and <figref idref="DRAWINGS">FIG. 3B</figref>.</p>
<p id="p-0048" num="0047">Now, a system employing the inventive methods described herein that measures a value of 9 dBsm (decibel measure of the radar cross section of a target relative to one square meter) using the first sensor and a value of 15 dBsm using the second sensor for a target object is able to effectively classify the target object as a rectangle (step <b>380</b> <figref idref="DRAWINGS">FIG. 3B</figref>).</p>
<p id="p-0049" num="0048">The above-described systems and methods can be implemented in digital electronic circuitry, in computer hardware, firmware, and/or software. The implementation can be as a computer program product (i.e., a computer program tangibly embodied in an information carrier). The implementation can, for example, be in a machine-readable storage device and/or in a propagated signal, for execution by, or to control the operation of, data processing apparatus. The implementation can, for example, be a programmable processor, a computer, and/or multiple computers.</p>
<p id="p-0050" num="0049">A computer program can be written in any form of programming language, including compiled and/or interpreted languages, and the computer program can be deployed in any form, including as a stand-alone program or as a subroutine, element, and/or other unit suitable for use in a computing environment. A computer program can be deployed to be executed on one computer or on multiple computers at one site.</p>
<p id="p-0051" num="0050">Method steps can be performed by one or more programmable processors executing a computer program to perform functions of the invention by operating on input data and generating output. Method steps can also be performed by and an apparatus can be implemented as special purpose logic circuitry. The circuitry can, for example, be a FPGA (field programmable gate array) and/or an ASIC (application-specific integrated circuit). Modules, subroutines, and software agents can refer to portions of the computer program, the processor, the special circuitry, software, and/or hardware that implement that functionality.</p>
<p id="p-0052" num="0051">Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor receives instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Generally, a computer can include, can be operatively coupled to receive data from and/or transfer data to one or more mass storage devices for storing data (e.g., magnetic, magneto-optical disks, or optical disks).</p>
<p id="p-0053" num="0052">Data transmission and instructions can also occur over a communications network. Information carriers suitable for embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices. The information carriers can, for example, be EPROM, EEPROM, flash memory devices, magnetic disks, internal hard disks, removable disks, magneto-optical disks, CD-ROM, and/or DVD-ROM disks. The processor and the memory can be supplemented by, and/or incorporated in special purpose logic circuitry.</p>
<p id="p-0054" num="0053">To provide for interaction with a user, the above described techniques can be implemented on a computer having a display device. The display device can, for example, be a cathode ray tube (CRT) and/or a liquid crystal display (LCD) monitor. The interaction with a user can, for example, be a display of information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer (e.g., interact with a user interface element). Other kinds of devices can be used to provide for interaction with a user. Other devices can, for example, be feedback provided to the user in any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback). Input from the user can, for example, be received in any form, including acoustic, speech, and/or tactile input.</p>
<p id="p-0055" num="0054">The above described techniques can be implemented in a distributed computing system that includes a back-end component. The back-end component can, for example, be a data server, a middleware component, and/or an application server. The above described techniques can be implemented in a distributing computing system that includes a front-end component. The front-end component can, for example, be a client computer having a graphical user interface, a Web browser through which a user can interact with an example implementation, and/or other graphical user interfaces for a transmitting device. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (LAN), a wide area network (WAN), the Internet, wired networks, and/or wireless networks.</p>
<p id="p-0056" num="0055">The system can include clients and servers. A client and a server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p>
<p id="p-0057" num="0056">Packet-based networks can include, for example, the Internet, a carrier internet protocol (IP) network (e.g., local area network (LAN), wide area network (WAN), campus area network (CAN), metropolitan area network (MAN), home area network (HAN)), a private IP network, an IP private branch exchange (IPBX), a wireless network (e.g., radio access network (RAN), 802.11 network, 802.16 network, general packet radio service (GPRS) network, HiperLAN), and/or other packet-based networks. Circuit-based networks can include, for example, the public switched telephone network (PSTN), a private branch exchange (PBX), a wireless network (e.g., RAN, bluetooth, code-division multiple access (CDMA) network, time division multiple access (TDMA) network, global system for mobile communications (GSM) network), and/or other circuit-based networks.</p>
<p id="p-0058" num="0057">The computing device can include, for example, a computer, a computer with a browser device, a telephone, an IP phone, a mobile device (e.g., cellular phone, personal digital assistant (PDA) device, laptop computer, electronic mail device), and/or other communication devices. The browser device includes, for example, a computer (e.g., desktop computer, laptop computer) with a world wide web browser (e.g., Microsoft&#xae; Internet Explorer&#xae; available from Microsoft Corporation, Mozilla&#xae; Firefox available from Mozilla Corporation). The mobile computing device includes, for example, a Blackberry&#xae;.</p>
<p id="p-0059" num="0058">Comprise, include, and/or plural forms of each are open ended and include the listed parts and can include additional parts that are not listed. And/or is open ended and includes one or more of the listed parts and combinations of the listed parts.</p>
<p id="p-0060" num="0059">One skilled in the art will realize the invention may be embodied in other specific forms without departing from the spirit or essential characteristics thereof. The foregoing embodiments are therefore to be considered in all respects illustrative rather than limiting of the invention described herein. Scope of the invention is thus indicated by the appended claims, rather than by the foregoing description, and all changes that come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for classifying objects in motion, comprising:
<claim-text>providing, to one or more processors, feature data for one or more classes of objects to be classified, wherein the feature data is indexed by object class, orientation, and sensor;</claim-text>
<claim-text>providing, to the one or more processors, one or more representative models for characterizing one or more orientation motion profiles for the one or more classes of objects in motion;</claim-text>
<claim-text>acquiring, via the one or more processors, feature data for a target object in motion, time of feature data, and trajectory of the target object in motion;</claim-text>
<claim-text>generating, via the one or more processors, reference feature data while the target object is in motion based on the acquired object class and trajectory of the target object in motion by:
<claim-text>selecting an object class;</claim-text>
<claim-text>selecting an orientation motion profile for the selected object class;</claim-text>
<claim-text>for each point in time that feature data is collected by the at least one sensor for the target object in motion, the selected orientation motion profile and the trajectory of the target object in motion are used to determine the orientation of the target object in motion, a feature is selected from the feature database based on the sensor, object class, and orientation of the target object in motion;</claim-text>
</claim-text>
<claim-text>classifying, via the one or more processors, the target object during motion of the target object based on the generated reference feature data and the acquired feature data for the target object in motion.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein providing the one or more representative models for characterizing one or more orientation motion profiles comprises acquiring orientation motion data for an exemplary object in motion.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein providing the one or more representative models for characterizing one or more orientation motion profiles comprises generating orientation motion data based on an analytical model for an exemplary object in motion.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising acquiring the feature data for the target object in motion and the trajectory of the target object in motion at one or more instances of time, periods of time, or a combination of both.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising acquiring the feature data for the target object in motion and the trajectory of the target object in motion using a plurality of sensors.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the feature data comprises at least one of radar data, optical data or infrared data for each of the one or more classes of objects.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the feature data comprises radar cross section signals and time derivatives of the radar cross section signals.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the sensor is selected from the group consisting of a radar system, lidar system, optical imaging system, or infrared monitoring system.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising classifying the target object using Bayes' Rule the target object as belonging to a particular class of the one or more classes of objects based on the posterior probability the target object corresponds to the particular class.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the feature data for the one or more classes of objects and the one or more representative models for characterizing one or more orientation motion profiles for the feature data are indexed in a database stored on the processor.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A system for classifying objects in motion, comprising:
<claim-text>data collected prior to classifying a target object in motion, the data comprising:
<claim-text>a) feature data on one or more classes of objects to be classified, wherein the feature data is indexed by orientation, sensor, and object class; and</claim-text>
<claim-text>b) one or more representative models for characterizing one or more orientation motion profiles for the feature data on the one or more classes of objects;</claim-text>
</claim-text>
<claim-text>at least one sensor to acquire feature data for a target object in motion and trajectory of the target object in motion;</claim-text>
<claim-text>a first processor to generate reference feature data while the target object is in motion based on the object class and the trajectory of the target object in motion, wherein the at least one sensor provides feature data and time of feature data;</claim-text>
<claim-text>a second processor to classify the target object during motion of the target object based on the reference feature data generated by the first processor, and feature data for the target object in motion;</claim-text>
<claim-text>wherein the first processor generates reference feature data by:
<claim-text>1. selecting an object class,</claim-text>
<claim-text>2. selecting an orientation motion profile for the selected object class,</claim-text>
<claim-text>3. for each point in time that feature data is collected by the at least one sensor for the target object in motion, the selected orientation motion profile and the trajectory of the target object in motion are used to determine the orientation of the target object in motion, a feature is selected from the feature database based on the sensor, object class, and orientation of the target object in motion.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, comprising one or more sensors to acquire orientation motion data for an exemplary object in motion to generate the one or more orientation motion profiles for the one or more classes of objects.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein steps 1-3 are repeated to generate a collection of reference feature vectors.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the second processor performs Bayesian classification using the reference feature data generated by the first processor as a priori data and the feature data for the target object to be classified to generate posterior object class type probabilities.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the feature data for the target object in motion comprises feature data collected from each sensor at single points in time. </claim-text>
</claim>
</claims>
</us-patent-grant>
