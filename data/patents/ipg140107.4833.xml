<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625926-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625926</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13792858</doc-number>
<date>20130311</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>228</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382275</main-classification>
<further-classification>382107</further-classification>
<further-classification>3482081</further-classification>
<further-classification>3482084</further-classification>
</classification-national>
<invention-title id="d2e43">Method and apparatus for processing image data from a primary sensor and a secondary sensor</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5777612</doc-number>
<kind>A</kind>
<name>Kataoka</name>
<date>19980700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715203</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6173317</doc-number>
<kind>B1</kind>
<name>Chaddha et al.</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709219</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6272293</doc-number>
<kind>B1</kind>
<name>Matama</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396208</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6920181</doc-number>
<kind>B1</kind>
<name>Porter</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524028</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7190825</doc-number>
<kind>B2</kind>
<name>Yoon</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7626612</doc-number>
<kind>B2</kind>
<name>John et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482084</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2005/0128297</doc-number>
<kind>A1</kind>
<name>Katsuyama</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348155</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2005/0190274</doc-number>
<kind>A1</kind>
<name>Yoshikawa et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34823199</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2005/0212749</doc-number>
<kind>A1</kind>
<name>Marvit et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2005/0270368</doc-number>
<kind>A1</kind>
<name>Hashimoto</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 61</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2007/0013788</doc-number>
<kind>A1</kind>
<name>Kaibara</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482312</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2008/0025640</doc-number>
<kind>A1</kind>
<name>Trudeau et al.</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382294</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>WO</country>
<doc-number>WO 02104009</doc-number>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>WO</country>
<doc-number>WO02104009</doc-number>
<kind>A1</kind>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Ben-Ezra et al., &#x201c;Motion-Based Motion Deblurring,&#x201d; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, No. 6, pp. 689-698 (Jun. 2004).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382107</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382254</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382275</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348497</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>10</number-of-drawing-sheets>
<number-of-figures>10</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12109094</doc-number>
<date>20080424</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8396321</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13792858</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60913914</doc-number>
<date>20070425</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61018752</doc-number>
<date>20080103</date>
</document-id>
</us-provisional-application>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>Marvell International Ltd.</orgname>
<address>
<city>Hamilton</city>
<country>BM</country>
</address>
</addressbook>
<residence>
<country>BM</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Aldrich</last-name>
<first-name>Bradley C.</first-name>
<address>
<city>Austin</city>
<state>TX</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Marvell International Ltd.</orgname>
<role>03</role>
<address>
<city>Hamilton</city>
<country>BM</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Hung</last-name>
<first-name>Yubin</first-name>
<department>2666</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Systems and methods are provided for processing image data captured by a device. In one implementation, a system is provided to process primary image data based on motion of the device during generation of the image data, as detected by a secondary image sensor. In another implementation, a method is provided for processing image data by generating primary image data, generating secondary image data, calculating, using the secondary image data, at least one motion vector based on motion of the device during generation of the primary image data, processing the primary image data based on the at least one motion vector, and outputting the processed primary image data.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="243.16mm" wi="196.00mm" file="US08625926-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="237.24mm" wi="185.50mm" file="US08625926-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="210.90mm" wi="173.65mm" orientation="landscape" file="US08625926-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="174.92mm" wi="144.95mm" orientation="landscape" file="US08625926-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="254.00mm" wi="193.38mm" orientation="landscape" file="US08625926-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="178.22mm" wi="163.41mm" orientation="landscape" file="US08625926-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="220.13mm" wi="176.95mm" orientation="landscape" file="US08625926-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="255.35mm" wi="133.43mm" orientation="landscape" file="US08625926-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="226.99mm" wi="156.80mm" orientation="landscape" file="US08625926-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="168.66mm" wi="155.87mm" orientation="landscape" file="US08625926-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="246.46mm" wi="195.07mm" file="US08625926-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">The present disclosure is a continuation of and claims priority to U.S. patent application Ser. No. 12/109,094, filed Apr. 24, 2008, now U.S. Pat. No. 8,396,321, issued Mar. 12, 2013, which claims priority to U.S. Provisional Patent Application No. 60/913,914, filed Apr. 25, 2007, and U.S. Provisional Patent Application No. 61/018,752, filed Jan. 3, 2008, which are incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">Systems and methods disclosed herein relate to the field of processing image data and, more specifically, to systems and methods of processing image data captured by handheld devices.</p>
<heading id="h-0003" level="1">DESCRIPTION OF THE RELATED ART</heading>
<p id="p-0004" num="0003">For the past few years, handheld devices including digital cameras, cell phones, and personal digital assistants (PDAs) have been manufactured with the ability to generate and process image data. Users generally prefer smaller and lighter handheld devices, causing manufacturers to downsize components of the handheld devices, including digital circuits, image modules, image sensors, imaging screens, image stabilization modules, or the like. The downsizing of components may affect the ability of handheld devices to generate an adequate amount of image data required to produce a high quality image.</p>
<p id="p-0005" num="0004">Users may use handheld devices to capture images in difficult environments including dimly-lit indoor venues such as concerts and bars, high-contrast outdoor venues with bright sunshine, and other environments which may not be ideal for producing a high quality image. Moreover, handheld image capture devices are often used without tripods or even using only one hand. Further, users may shake or move their hands during image capturing, causing handheld devices to shake. The handshake or camera motion during image capture may be referred to as jitter. Shaking of the handheld devices during image capture may lead to producing of unstable video sequences and blurry still images. The unresolved effects of jitter on image producing abilities of the handheld devices may be increased by zoom operations, which may magnify motion associated with users' hands, by use of higher resolution image sensors, which may shrink the pixel size of captured image data, causing the blur effect to span more pixels; and use of slow shutter speeds.</p>
<p id="p-0006" num="0005">To compensate for the above-mentioned deficiencies image stabilization modules have been used in handheld devices. Image stabilization modules may generate motion vectors associated with jitter induced by hand movement of the user to process the image data and compensate for the jitter. One way of generating these motion vectors is to measure displacement in the x and y directions by using a mechanical gyroscope. To keep costs low and sizes of handheld devices manageable, mechanical gyroscopes may be implemented by using Micro-ElectroMechanical Systems (MEMS). However, both mechanical gyroscopes and gyroscopes being implemented using MEMS are difficult to implement in a limited space and may require additional circuitry, thereby increasing the size of the handheld devices. Further, many image stabilization modules compensate for jitter by adjusting image data after the image data has been processed, thus requiring additional processing capabilities leading, in turn, to increased power consumption.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0007" num="0006">In one embodiment consistent with the invention, a system for processing image data captured by a device is provided. The system comprises a primary image sensor to generate primary image data, a secondary image sensor to generate secondary image data, a correlation circuit to calculate, using the secondary image data, at least one motion vector based on motion of the device during generation of the primary image data, and a data processor to process the primary image data based on the at least one motion vector.</p>
<p id="p-0008" num="0007">Consistent with another embodiment of the present invention, there is provided a system for processing image data captured by a device, the system comprising a liquid crystal display (LCD) panel for previewing a scene, a keypad panel to generate a trigger event to terminate the previewing of the scene, the trigger event to initiate generation of primary image data, an image sensor generating reference image data, during the previewing and generating the primary image data after the trigger event initiates primary image data generation, a correlation circuit to calculate at least one motion vector based on motion of the device during the previewing, the at least one motion vector being calculated using the reference image data, and a data processor to process the primary image data based on the at least one motion vector</p>
<p id="p-0009" num="0008">In certain embodiments, the system may also include a memory device to store the processed primary image data. The system may also include a transmitter/receiver to transmit the processed primary image data, a buffer to provide the plurality of data frames of the secondary image data to the correlation circuit, and a kernel estimation circuit to convert the at least one motion to a spatially varying function.</p>
<p id="p-0010" num="0009">In another embodiment consistent with the present invention, a method for processing image data captured by a device, is provided. The method comprises generating primary image data, generating secondary image data, calculating, using the secondary image data, at least one motion vector based on motion of the device during generation of the primary image data, processing the primary image data based on the at least one motion vector, and outputting the processed primary image data.</p>
<p id="p-0011" num="0010">It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory only and are not restrictive of the invention, as claimed.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0012" num="0011">The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate various embodiments. In the drawings:</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a system for processing image data, consistent with one embodiment of the invention;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic diagram illustrating operation of a system for processing image data in an environment, consistent with one embodiment of the invention;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 3</figref> is a schematic diagram illustrating the structure of pre-processed image data, consistent with one embodiment of the invention;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 4</figref><i>a </i>is a schematic diagram illustrating a system for processing and adjusting image data, consistent with one embodiment of the invention;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 4</figref><i>b </i>is a schematic diagram illustrating a two-dimensional source block of image data, consistent with one embodiment of the invention;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 5</figref> is a timing diagram illustrating streams of video image data, consistent with one embodiment of the invention;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 6</figref> is a schematic diagram illustrating the use of cropping parameters to adjust video image data, consistent with one embodiment of the invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 7</figref><i>a </i>is a schematic diagram illustrating adjustment of image data for a still image, consistent with one embodiment of the invention;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 7</figref><i>b </i>is a schematic diagram illustrating motion vectors in the spatial domain, consistent with one embodiment of the invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 8</figref> is a flow diagram illustrating operation of a system for processing and adjusting image data, consistent with one embodiment of the invention;</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DESCRIPTION OF THE EMBODIMENTS</heading>
<p id="p-0023" num="0022">In the following description, for purposes of explanation and not limitation, specific techniques and embodiments are set forth, such as particular sequences of steps, interfaces and configurations, in order to provide a thorough understanding of the techniques presented herein. While the techniques and embodiments will primarily be described in context with the accompanying drawings, the techniques and embodiments can also be practiced in other circuit types.</p>
<p id="p-0024" num="0023">Reference will now be made in detail to the exemplary embodiments of the invention, examples of which are illustrated in the accompanying drawings. Wherever possible, the same reference numbers will be used throughout the drawings to refer to the same or like parts.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a system <b>100</b> for processing image data, consistent with one embodiment of the invention. System <b>100</b> may be integrated in a handheld device such as a digital camera, a cell phone, or a PDA. System <b>100</b> may comprise a system-on-chip (SoC) <b>102</b>. SoC <b>102</b> may include a plurality of modules, which may interface with each other using a system bus <b>104</b>. The plurality of modules in SoC <b>102</b> may also interface with external circuitry and modules. For example, SoC <b>102</b> may include a keypad interface <b>104</b> which may be connected to a panel keypad <b>106</b>, a liquid crystal display (LCD) interface <b>108</b> which may be connected to a LCD panel <b>110</b>, a primary interface <b>112</b> which may be connected to a primary image sensor <b>114</b>, a secondary interface <b>116</b> which may be connected to a secondary image sensor <b>118</b>, and a memory controller <b>120</b>.</p>
<p id="p-0026" num="0025">SoC <b>102</b> may also include a data processor <b>122</b>, a data compressor <b>124</b>, an internal memory <b>126</b>, and input/output (I/O) modules <b>128</b> which may be connected to various I/O peripherals <b>130</b>. I/O modules <b>128</b> may include one or more of a secure digital input output (SDIO) port, a universal asynchronous receiver/transmitter (UART), audio codec interfaces, and any other modules required for the functioning of SoC <b>120</b>. I/O peripherals <b>130</b> may include one or more of a secure digital card, a software-defined radio (SDR), a camera flash, Bluetooth or Wi-Fi transceivers, a speaker, a microphone, an external memory, and other peripherals required for the functioning of system <b>100</b> within a handheld device. Internal memory <b>126</b> and external memory may comprise various types of volatile or non-volatile memory, including RAM, DRAM, SRAM, ROM, EPROM, EEPROM, flash memory, or any other medium which can be used to store image data.</p>
<p id="p-0027" num="0026">A user of a handheld device that includes system <b>100</b> may initiate the handheld device by using panel keypad <b>106</b> to take a picture. Panel keypad <b>106</b> may transmit a signal to liquid crystal display (LCD) interface <b>108</b>, via system bus <b>104</b>. LCD interface <b>108</b> may initialize LCD panel <b>110</b>, which may enable the user to preview a scene of the environment that the user may be trying to capture as a video or still image. The user may further use panel keypad <b>106</b> to activate primary image sensor <b>114</b> to generate image data corresponding to the image displayed in LCD panel <b>110</b>. The generated image data may include a single data frame for a still image or multiple frames of data for a video corresponding to the environment.</p>
<p id="p-0028" num="0027">Secondary image sensor <b>118</b> may also be activated in relation to primary image sensor <b>114</b> and may begin generating reference and/or secondary image data. Secondary image sensor <b>118</b> may begin generating reference image data at the same time as initialization of LCD panel <b>110</b>, or at the same time as the activation of primary image sensor <b>114</b>, or a short period before or after the activation of primary image sensor <b>114</b>. Secondary image sensor <b>118</b> may operate at a higher frame rate than primary image sensor <b>114</b> to generate reference images including a plurality of data frames. The plurality of data frames may represent a video corresponding to an extended scene of the environment in which the handheld device may be operated in. Secondary image sensor <b>118</b> may be pointed in an opposite direction from primary image sensor <b>114</b>. For example, the reference image data may be a video of the user's face or the reference image data may be a video of an extended scene at a different angle, such as 180 degrees, from the scene that may be displayed on LCD panel <b>110</b>.</p>
<p id="p-0029" num="0028">Primary image sensor <b>114</b> and secondary image sensor <b>118</b> may be implemented on one or more chips and may include integrated image signal processors (ISPs) together with at least one of a complementary metal-oxide-semiconductor (CMOS) sensor, charge coupled device (CCD) sensor, or any other sensor that generates pixel data.</p>
<p id="p-0030" num="0029">As is described in further detail below, data processor <b>122</b> may analyze the reference image data to calculate parameters corresponding to the scene displayed on LCD panel <b>110</b>. The parameters may be provided to primary interface <b>116</b>. Primary interface <b>116</b> and data processor <b>122</b> may use the parameters during processing of the raw image data and may perform correction algorithms such as de-convolution, image stabilization, filtration, blur reduction, and kernel estimation, on raw image data.</p>
<p id="p-0031" num="0030">For example, the reference image data may be analyzed to determine scene spatial correction parameters, temporal correction parameters that may be relative to the time during which image data may have been generated, and any other parameter that may be used to calculate motion vectors corresponding to jitter of the handheld device induced by, for example, hand movement of the user during image capturing. In general, system <b>100</b> is an exemplary system and may include features and/or variations in addition to those set forth herein. For example, a plurality of image sensors may be used to generate reference image data and motion detection devices aside from secondary image sensor <b>118</b> may be incorporated into system <b>100</b>, which may calculate motion vectors corresponding to the hand movement of the user. Reference image data generated by the plurality of image sensors may provide additional information with respect to the motion of the handheld device and may be used to improve the accuracy of the correction algorithms. Miniature gyroscopes may be implemented using MEMS to provide a stream of displacements relating to motion of a user's hand. Further, the plurality of image sensors may be used in combination with miniature gyroscopes to improve precision and granularity of the calculations.</p>
<p id="p-0032" num="0031">Primary image sensor <b>114</b> may be set to begin generating image data at the initialization of LCD panel <b>110</b>, and image data generated by primary image sensor <b>114</b>, during preview, may be used as reference image data analyzed by data processor <b>122</b>. Panel keypad <b>106</b> may generate a trigger event, to activate primary image sensor <b>114</b> to generate image data corresponding to the image displayed in LCD panel <b>110</b>. The trigger event may terminate the preview and primary image sensor <b>114</b> may send the image data generated during preview to primary interface <b>112</b> for analysis.</p>
<p id="p-0033" num="0032">Image data generated by primary image sensor <b>114</b> and reference image data generated by secondary image sensor <b>118</b> may be provided to primary interface <b>112</b> and secondary interface <b>116</b> respectively. Necessary control signals for synchronization of the image data and the reference image data may also be provided by image sensors <b>114</b> and <b>118</b> to interfaces <b>112</b> and <b>116</b>, respectively. The control signals may include vertical synchronization signals, horizontal synchronization, and timing signals such as frame valid, line valid and time valid. Interfaces <b>112</b> and <b>116</b> may synchronize the image data with the reference image data. The synchronization of the image data may allow a frame of image data generated by primary image sensor <b>114</b> to be correlated in time with a frame of image data generated by secondary image sensor <b>118</b>. The synchronization may be performed by for example, embedding a count value for each data frame provided to interfaces <b>112</b> and <b>116</b>, where each image data frame may be embedded with a count value representing a time stamp. A time stamp of each image data frame generated by primary image sensor <b>114</b> may be matched with a time stamp of a corresponding image data frame generated by secondary image sensor <b>118</b>. Further, control signals may be used to directly synchronize image data frames generated by primary image sensor <b>114</b> with image data frames generated by secondary image sensor <b>118</b>.</p>
<p id="p-0034" num="0033">Memory controller <b>120</b> may transmit the synchronized image data and store the synchronized image data in internal memory <b>126</b> via system bus <b>104</b>. Data processor <b>122</b> may access the stored image data and may analyze the reference image data to calculate various values including environmental parameters, and/or blur reduction parameter, and/or motion vectors. Primary interface <b>116</b> and data processor <b>122</b> may then process the image data generated by primary image sensor <b>114</b> and stored in internal memory <b>126</b>, using the calculated values. As is described in further detail below, processing the image data may include applying different algorithms to the image data in order to produce a high quality image. The processed image data may be transmitted to data compressor <b>124</b> via system bus <b>104</b>. Data compressor <b>124</b> may compress the processed image data and may transmit the compressed image data to I/O modules <b>128</b>. I/O modules <b>128</b> may display the compressed image data as a high-quality image on LCD panel <b>110</b> or I/O modules may transmit the high-quality image to I/O peripherals <b>130</b> for further transmission or permanent storage.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an example of an environment <b>200</b> in which a handheld device <b>202</b> may be used to capture a (first) scene <b>204</b> and an extended (second) scene <b>206</b>. Handheld device <b>202</b> may include primary image sensor <b>114</b> and secondary image sensor <b>118</b>. In one embodiment, secondary image sensor <b>118</b> may be pointed in a direction 180 degrees away from primary image sensor <b>114</b>. For example, when a user captures a video or still image of scene <b>204</b>, primary image sensor <b>114</b> may generate one or more data frames corresponding to scene <b>204</b> and secondary image sensor <b>118</b> may generate data frames corresponding to extended scene <b>206</b>. During video capture of scene <b>204</b>, the user may rotate handheld device <b>202</b> in direction <b>208</b>, which may cause primary image sensor <b>114</b> to generate data frames corresponding to scene <b>212</b>. Further, due to the rotation, secondary image sensor may move in direction <b>210</b> and may generate data frames corresponding to extended scene <b>214</b>. Generating multiple data frames of image data may provide handheld device <b>202</b> with more image data regarding environment <b>200</b> and analyzing the image data may enable handheld device <b>202</b> to adjust image data and/or settings of handheld device <b>202</b> to produce a high-quality image or video of the scene that the user is trying to capture.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 3</figref> shows examples of raw Bayer pixel data in RGGB format (<b>305</b>) and YCbCr format (<b>310</b>). RGGB format <b>305</b> and YCbCr format <b>310</b> may represent pixels of image data generated by primary image sensor <b>114</b> and secondary image sensor <b>118</b>. RGGB format <b>305</b> may be raw Bayer data which may not have been processed by various camera processing modules or RGGB format <b>305</b> may be raw Bayer data which may have been partially processed by various camera processing algorithms. YCbCr format <b>310</b> may represent processed raw Bayer data. The image data may be provided to SoC <b>102</b> in raw format because external resources may be required to process the image data and elimination of the external resources may reduce implementation cost of systems such as system <b>100</b>.</p>
<p id="p-0037" num="0036">Referring now to <figref idref="DRAWINGS">FIG. 4</figref><i>a</i>, a system <b>400</b> for processing and adjusting image data, is provided. System <b>400</b> may also be integrated in a handheld device such as a digital camera, a cell phone, or a PDA. Similar to system <b>100</b>, system <b>400</b> may also include LCD panel <b>110</b>, primary interface <b>112</b>, primary image sensor <b>114</b>, secondary interface <b>116</b>, and secondary image sensor <b>118</b>. Further, system <b>400</b> may include a primary buffer <b>402</b>, a primary shutter <b>404</b>, a secondary buffer <b>406</b>, a correlation circuit <b>408</b>, a cropping pattern generator <b>410</b>, a kernel estimation circuit <b>412</b>, a deblurring circuit <b>414</b>, a still image compression circuit <b>416</b>, and a video compression circuit <b>418</b>.</p>
<p id="p-0038" num="0037">LCD panel <b>110</b> may be used by a user to preview a scene to be captured as a video or as a still image. The user may initialize primary image sensor <b>114</b> to generate image data corresponding to the scene. Alternatively, primary image sensor <b>114</b> may be set to automatically initialize and generate image data corresponding to the scene, while the user is previewing the scene. The image data generated by primary image sensor <b>114</b> is provided to primary interface <b>112</b>. The image data may be provided to primary interface <b>112</b> prior to processing and may be raw Bayer pixel data.</p>
<p id="p-0039" num="0038">Secondary image sensor <b>118</b> may also begin generating reference image data including data frames representing a video of an extended scene of the environment that the handheld device may be operated in. Secondary image sensor <b>118</b> may operate at a higher frame rate than primary image sensor <b>114</b> and may provide the reference image data to secondary interface <b>116</b>. Image sensors <b>114</b> and <b>118</b> may also provide synchronization control signals to interfaces <b>112</b> and <b>116</b>. Interfaces <b>112</b> and <b>116</b> may synchronize the image data and the reference image data and secondary interface <b>116</b> may provide the synchronized reference image data to secondary buffer <b>406</b>.</p>
<p id="p-0040" num="0039">Secondary buffer <b>406</b> may provide data frames of reference image data to correlation circuit <b>408</b> and correlation circuit <b>408</b> may compute motion vectors MV<sub>x </sub>and MV<sub>y</sub>. Motion vector MV<sub>x </sub>may represent the hand movement of the user in the horizontal direction, at a particular time during generation of the reference image data and may be calculated by measuring the displacement in the horizontal direction between two data frames of reference image data. Motion vector MV<sub>y </sub>may represent the hand movement of the user in the vertical direction at a particular time during generation of the reference image data, and may be calculated by measuring displacement in the vertical direction between two data frames of reference image data. Each motion vector may be calculated using various known block matching algorithms.</p>
<p id="p-0041" num="0040">Referring now to <figref idref="DRAWINGS">FIG. 4</figref><i>b</i>, block matching algorithms may be used to measure displacement of source block <b>452</b>, with respect to search window <b>454</b>. Search window <b>454</b> may represent a two-dimensional N+2n<sub>!</sub>&#xd7;M+2 m<sub>1 </sub>block of image data corresponding to data frames of reference image data generated by, for example, secondary image sensor <b>118</b> of system <b>400</b>. Source block <b>452</b> may represent a two-dimensional M&#xd7;N block of image data corresponding to a data frame of image data generated by, for example, primary image sensor <b>114</b> of system <b>400</b>. Further, source block <b>452</b> may represent image data generated between the generation of two data frames of reference image data. Horizontal and vertical displacements of source block <b>452</b> may be calculated with reference to search window <b>454</b> and the displacements may be used to calculate motion vectors for source block <b>452</b>.</p>
<p id="p-0042" num="0041">As is illustrated in <figref idref="DRAWINGS">FIG. 4</figref><i>b</i>, N, M, n<sub>1</sub>, and m<sub>1 </sub>may represent various lengths that may be used as constraints while applying block matching algorithms. For example, n<sub>1 </sub>may represent a number of columns of regions <b>456</b> and <b>458</b> and m<sub>1 </sub>may represent a number of rows of regions <b>460</b> and <b>462</b>. The horizontal and vertical displacement of source block <b>452</b> may be calculated with respect to displacement over a number of rows and columns. Block matching algorithms may be used to implement a motion estimation component of source block <b>452</b>. The size of search window <b>454</b> may constrain the range of the motion vector and a cost function may be defined by using a number of elements within a block, a metric used for block comparisons, number of comparisons required for each block, and a displacement search pattern. Motion vectors may be calculated by using the following equations:</p>
<p id="p-0043" num="0042">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>SAD</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>dx</mi>
              <mo>,</mo>
              <mi>dy</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>n</mi>
              <mo>=</mo>
              <mi>dx</mi>
            </mrow>
            <mrow>
              <mi>dx</mi>
              <mo>+</mo>
              <mi>N</mi>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </munderover>
          <mo>&#x2062;</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>m</mi>
                <mo>=</mo>
                <mi>dy</mi>
              </mrow>
              <mrow>
                <mi>dy</mi>
                <mo>+</mo>
                <mi>M</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <mo>&#xf603;</mo>
              <mrow>
                <mrow>
                  <mi>x</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>m</mi>
                      <mo>,</mo>
                      <mi>n</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <mi>y</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mrow>
                        <mi>m</mi>
                        <mo>+</mo>
                        <mi>dx</mi>
                      </mrow>
                      <mo>,</mo>
                      <mrow>
                        <mi>n</mi>
                        <mo>+</mo>
                        <mi>dy</mi>
                      </mrow>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>&#xf604;</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(MV<i>x</i>,MV<i>y</i>)=min<sub>((dx,dy)&#x3b5;</sub><img id="CUSTOM-CHARACTER-00001" he="3.13mm" wi="1.02mm" file="US08625926-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sub><sup2>2</sup2></sub><i>SAD</i>(<i>dx,dy</i>)&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>C</i><sub>FullSearch</sub>(2<i>n</i><sub>1</sub>+1)&#xd7;(2<i>m</i><sub>1</sub>+1)&#xd7;(<i>M&#xd7;N</i>)&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where equation 1 represents a Sum-of-Absolute (SAD) functions and may be used to calculate displacement between individual rows and columns of source block <b>452</b> and equation 2 may be used to calculate a motion vector using the results from equation 1. Similarly, a sum of displacements of all rows and all columns, in search window <b>454</b>, may be calculated using equation 3. Alternatively, fast search algorithms may be used to calculate displacements within subsamples of search window <b>454</b>. For example, Three Step Search (TSS) algorithm, Logarithmic search algorithm, and interpolation operations may be used in combination with SAD to correlate source block <b>452</b> with adjacent frames of image data and correlate pixels of image data of source block <b>452</b> with adjacent frames of image data.
</p>
<p id="p-0044" num="0043">Returning now to <figref idref="DRAWINGS">FIG. 4</figref><i>a</i>, correlation circuit <b>408</b> may calculate sets of motion vectors corresponding to the hand movement of the user at particular time instances during generation of the reference image data. Each motion vector may be referred to as a global motion vector and may represent a particular position in the generated image data. Multiple global motion vectors may be needed to deblur and/or remove jitter from image data. If the image data generated by primary image sensor <b>114</b> comprises video data, correlation circuit <b>408</b> may provide the computed sets of motion vectors to cropping pattern generator <b>410</b>.</p>
<p id="p-0045" num="0044">Cropping pattern generator <b>410</b> may generate cropping parameters based on the sets of motion vectors and may provide the cropping parameters to primary interface <b>112</b>. Based on the cropping parameters, primary interface <b>112</b> may adjust image data corresponding to the video captured by the user. The image data may be adjusted to remove the effects of jitter caused by hand movement of the user during capturing of the video. For example, cropping pattern generator <b>410</b> may generate cropping parameters corresponding to each data frame of raw or unprocessed image data provided to primary interface <b>112</b> by primary image sensor <b>114</b>. Primary interface <b>112</b> may skip a particular row or column of pixels while processing each data frame of the raw or unprocessed image data, based on the cropping parameters.</p>
<p id="p-0046" num="0045">After processing of the raw or unprocessed image data is complete, primary interface <b>112</b> may provide synthesized data frames of image data to primary buffer <b>402</b>. The synthesized data frames may correspond to a video of a scene captured by the user. The video may be of a high quality due to the removal of rows or columns pixels during synthesis and processing of the image data. The video may be sent to LCD panel <b>110</b> for display or may be sent to video compressor <b>418</b> and may be compressed for storage in internal or external memory (not shown) of system <b>400</b>.</p>
<p id="p-0047" num="0046">When the image data generated by primary image sensor <b>114</b> corresponds to a still image, primary interface <b>112</b> may send the raw image data corresponding to the still image to primary buffer <b>402</b>, before processing the image data. Primary buffer <b>402</b> may send the received raw image data to deblurring circuit <b>414</b>. Further, depending on the type of shutter or shutter mode being used for primary shutter <b>404</b>, correlation circuit <b>408</b> may provide the computed sets of motion vectors to kernel estimation circuit <b>412</b> or may provide the global motion vectors to kernel estimation circuit <b>412</b>. Kernel estimation circuit <b>412</b> may use the global motion vectors to calculate a global correction kernel to deblur the still image.</p>
<p id="p-0048" num="0047">For example, primary shutter <b>404</b> may be an electronic or global shutter (referred to hereinafter as &#x201c;global shutter&#x201d;), and correlation circuit <b>408</b> may provide the global motion vector to kernel estimation circuit <b>412</b>. A global shutter may operate like a mechanical shutter and may expose the entire scene, corresponding to the still image, to primary image sensor <b>114</b>. Primary image sensor <b>114</b> may generate image data corresponding to the still image and primary interface <b>112</b> may collectively readout all of the image data generated by primary image sensor <b>114</b>. The user may move his hand while capturing the scene and the hand movement may produce a global blur. The global blur may be dependent on the speed of the global shutter and may be represented by the global motion vector provided to kernel estimation circuit <b>412</b>. Kernel estimation circuit <b>412</b> may use the global motion vector to form a point spread function (PSF) and may provide the PSF to deblurring circuit <b>414</b>, as is known to those of skill in the art.</p>
<p id="p-0049" num="0048">Deblurring circuit <b>414</b> may adjust the image data received from primary buffer <b>402</b> and remove the global blur from the received image data based on the PSF. The global blur from the image data may be removed by using various algorithms such as de-convolution of the PSF and raw image data, inverse filtering, and interpolation operations in a manner known to those of skill in the art.</p>
<p id="p-0050" num="0049">Alternatively, primary image sensor may include a rolling shutter mode, and correlation circuit <b>408</b> may provide sets of motion vectors that represent the hand movement of the user at particular time instances during generation of the image data and the reference image data. During capture of the still image, the rolling shutter mode may expose different portions of the scene to primary image sensor <b>114</b> at different times and primary image sensor <b>114</b> may generate different rows or columns of pixels of a single data frames of image data at different times. The generated rows or columns of pixels may be read out from the primary image sensor <b>114</b> as the rows or columns of pixels are generated and may cause separate blurs in the still image.</p>
<p id="p-0051" num="0050">Kernel estimation circuit <b>412</b> may use the sets of motion vectors provided by correlation circuit <b>408</b> to adjust the raw image data provided by primary buffer <b>402</b>. Kernel estimation circuit <b>412</b> may spatially vary kernel coefficients to adjust for the different read-out times of the different rows or columns and may vary kernel coefficients to adjust for the separate blurs caused by different motion vectors. For example, when a motion vector for a particular area in an image indicates a displacement of ten in x direction and twelve in the y direction and a motion vector of an area adjacent to the particular area indicates displacement of five in the x direction and two in the y direction, the kernel coefficients may be adjusted to account for the different displacements. Further, the kernel coefficient may be provided to deblurring circuit <b>414</b>. The deblurring circuit may remove the blur by using the sets of motion vectors to model the hand movement as uniform linear motion in x (horizontal) and y (vertical) directions for each generated row or column and may adjust each row or column accordingly.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a timing diagram displaying the generation of image data stream <b>502</b> and reference data stream <b>504</b>, with respect to time. Image data stream <b>502</b> may be generated by primary image sensor <b>114</b>, in system <b>100</b> or <b>400</b>, or handheld device <b>202</b>, and may include data frames <b>502</b><i>a</i>-<i>b</i>. Image data stream <b>502</b> may represent a video of a scene captured by the user and the quality of the video may be effected by motion artifacts. Motion artifacts may be caused by movement of the hand of the user during video image capturing of the scene and any motion of an object present in the scene. The resultant motion artifact that may effect the quality of image data stream <b>502</b> may be represented by the equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>MV<sub>Resultant</sub>(<i>x,y,t</i>)=MV<sub>User</sub>(<i>x,y,t</i>)+MV<sub>Scene</sub>(<i>x,y,t</i>),&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where MV<sub>User</sub>(x, y, t) may represent a motion vector corresponding to the hand movement of the user, MV<sub>Scene</sub>(x, y, t) may represent a motion vector corresponding to any motion of an object present in the scene, x may represent movement in the horizontal direction, y may represent movement in the vertical direction, and t may represent the instance of time corresponding to the calculated motion vector.
</p>
<p id="p-0053" num="0052">Reference image data stream <b>504</b> may be generated by secondary image sensor <b>118</b>, in system <b>100</b> or <b>400</b>, or handheld device <b>202</b>, and may include reference data frames <b>504</b><i>a</i>-<i>n</i>. Reference data frames <b>504</b><i>a</i>-<i>n </i>may represent a video of an extended scene of the environment that the handheld device is being operated in and it may be a video of a scene opposite to or in a different direction relative to the scene that may be represented by image data stream <b>502</b>. Reference data stream <b>504</b> may consist of more frames than image data stream <b>502</b>. For example, image data stream <b>502</b> may include two data frames <b>502</b><i>a </i>and <b>502</b><i>b </i>between time t<sub>1 </sub>and t<sub>7 </sub>and reference data stream <b>504</b> may include six data frames <b>504</b><i>a</i>-<i>f </i>between time t<sub>1 </sub>and t<sub>7</sub>. Data frames <b>504</b><i>a</i>-<i>n </i>may be used to calculate a plurality of motion vectors (MV<sub>User</sub>(x, y, t)) that may represent the motion artifacts caused by the hand movement of the user during video image capturing. The motion vectors may be used to adjust image data stream <b>502</b> such that MV<sub>Resultant</sub>(x, y, t) of the image data stream may approximately be equal to MV<sub>Scene</sub>(x, y, t), after the adjustments have been made.</p>
<p id="p-0054" num="0053">For example, to adjust data frames <b>502</b><i>a </i>and <b>502</b><i>b</i>, a motion vector may be calculated to represent movement in the x and y direction between each data frame <b>504</b><i>a</i>-<i>n</i>. A motion vector that may represent the movement between data frames <b>504</b><i>a </i>and <b>504</b><i>b</i>, may be calculated between time t<sub>2 </sub>and t<sub>3</sub>. Similarly, motion vectors may be calculated between time t<sub>3 </sub>and t<sub>4</sub>, t<sub>4 </sub>and t<sub>5</sub>, t<sub>5 </sub>and t<sub>6</sub>, and t<sub>6 </sub>and t<sub>7 </sub>that may represent the movement between data frames <b>504</b><i>b </i>and <b>504</b><i>c</i>, <b>504</b><i>c </i>and <b>504</b><i>d</i>, <b>504</b><i>d </i>and <b>504</b><i>e</i>, and <b>504</b><i>f</i>, respectively. These motion vectors may be used to adjust data frames <b>502</b><i>a </i>and <b>502</b><i>b </i>and remove any effects that hand movement of the user or jitter of the handheld device may have on the quality of data frames <b>502</b><i>a </i>and <b>502</b><i>b</i>. Further, the motion vectors may also be used to calculate a global motion vector that may be provided to kernel estimation circuit <b>410</b>, of system <b>400</b>.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 6</figref> illustrates adjusting a frame of image data using cropping parameters that may be generated by, for example, cropping pattern generator <b>410</b> of system <b>400</b>. Data frame <b>602</b> may represent a data frame of video image data that may be generated when a user may capture a video of a scene. Data frame <b>602</b> may be generated by, for example, primary image sensor <b>114</b> of system <b>100</b> or <b>400</b>, or handheld device <b>202</b>, and may be generated during video capturing in which there may be no jitter in data frame <b>602</b>.</p>
<p id="p-0056" num="0055">Data frame <b>602</b> may represent a data frame that may have been synthesized and processed when no hand movement of the user is present by, for example, data processor <b>122</b> in system <b>100</b>, or primary interface <b>112</b> in system <b>100</b> or <b>400</b>, and may include cropped region <b>604</b> and active region <b>606</b>. Regions <b>604</b> and <b>606</b> may include rows and columns of pixels. Cropped region <b>604</b> may include rows and columns of pixels that may be cropped out of data frame <b>602</b> during processing and synthesizing of the image data and active region <b>606</b> may include rows and columns of pixels that may constitute a processed and synthesized data frame representing a portion of the scene captured by the user.</p>
<p id="p-0057" num="0056">Data frame <b>608</b> may represent cropped region <b>604</b> and active region <b>606</b> that have been affected by jitter that may be caused by hand movement of the user, during capturing of video image data. As is shown in <figref idref="DRAWINGS">FIG. 6</figref>, active region <b>606</b> may have shifted in direction <b>610</b> and may include rows and columns of pixels that may have been cropped when there was no hand movement. To adjust the shift of active region <b>606</b> in data frame <b>608</b>, data frames (not shown) of reference image data may be used to calculate motion vectors corresponding to the hand movement of the user. These motion vectors may be used by, for example, cropping generator <b>410</b> to generate cropping parameters. Cropping parameters may be used to shift active region <b>606</b> in direction <b>612</b> and produce an adjusted data frame <b>614</b>. Adjusted data frame <b>614</b> may constitute a processed and synthesized data frame that may not be affected by jitter and may represent a portion of the scene captured by the user. Each data frame representing the scene captured by the user may be adjusted by using the method described above.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 7</figref><i>a </i>is a timing diagram illustrating the adjustment of image data frame <b>702</b> representing a still image of a scene that a user may capture. Data frame <b>702</b> may be generated by, for example, primary image sensor <b>114</b> of system <b>100</b> or <b>400</b>, or handheld device <b>202</b>. Reference image stream <b>704</b> may include data frames <b>704</b><i>a</i>-<i>n </i>and may be generated by, for example, secondary image sensor <b>118</b> of system <b>100</b> or <b>400</b>, or handheld device <b>202</b>. Secondary image sensor may operate at a faster frame rate and may produce data frames <b>704</b><i>a</i>-<i>n </i>in relation to data frame <b>702</b>. Data frame <b>704</b><i>a</i>-<i>n </i>may be used to calculate sets of motion vectors MV<b>1</b>-MVn, to represent hand movement of a user in the x and y direction, during capturing of the still image. A motion vector may be calculated to represent the movement between two data frames of reference image stream <b>704</b>. MV<b>1</b>(<i>x, y, t</i>) may represent the movement between data frames <b>704</b><i>a </i>and <b>704</b><i>b</i>, similarly MV<b>2</b>(<i>x, y, t</i>) may represent the movement between data frames <b>704</b><i>b </i>and <b>704</b><i>c. </i></p>
<p id="p-0059" num="0058">Depending on the type of shutter or shutter mode used during capture of the still image, sets of motion vectors or a global motion vector based on the sets of motion vectors may be used to adjust data frame <b>702</b> and remove blur from data frame <b>702</b>. For example, data frame <b>702</b> may be image data that is generated using, for example, primary image sensor <b>114</b> of system <b>100</b> or <b>400</b>, or handheld device <b>202</b>, with a global shutter. The global shutter may expose the entire scene to primary image sensor <b>114</b> at the same time and primary image sensor <b>114</b> may generate rows and columns of pixel data corresponding to data frame <b>702</b> at the same time. Further, the pixel data corresponding to zones <b>702</b><i>a</i>-<i>n </i>may be read out from primary image sensor <b>114</b> at the same time and may cause a global blur across data frame <b>702</b>.</p>
<p id="p-0060" num="0059">The global blur across data frame <b>702</b> may be adjusted by calculating a global vector using the sets of motion vectors. The global blur may be dependent on the speed of the global shutter. Referring to <figref idref="DRAWINGS">FIG. 7</figref><i>b</i>, time varying motion vectors MV<b>1</b>-MVn, may be converted into a spatially-varying correction kernel <b>740</b>, as illustrated in <figref idref="DRAWINGS">FIG. 7</figref><i>b</i>. Spatially-varying correction kernel <b>740</b> represents a function displaying displacement of data frames <b>704</b><i>a</i>-<i>n </i>in the x and y direction. The displacement between consecutive data frames may correspond to movement of the handheld device during generation of image data corresponding to the consecutive data frames. Each motion vector MV<b>1</b>-MVn corresponds to a particular zone <b>702</b><i>a</i>-<i>n </i>of the still image.</p>
<p id="h-0007" num="0000">Spatially-varying correction kernel may be used to form a PSF, and algorithms such as inverse filtering or de-convolution may be performed by, for example, deblurring circuit <b>412</b> of system <b>400</b>, to remove the global blur from data frame <b>702</b>.</p>
<p id="p-0061" num="0060">Returning now to <figref idref="DRAWINGS">FIG. 7</figref><i>a</i>, data frame <b>702</b> may be image data that is generated by using, for example, primary image sensor <b>114</b> of system <b>100</b> or <b>400</b> or handheld device <b>202</b> in a &#x201c;rolling shutter mode.&#x201d; The rolling shutter mode may expose different portion of the scene to primary image sensor <b>114</b> at different times, and may cause primary image sensor <b>114</b> to generate rows and columns of pixel data corresponding to zones <b>702</b><i>a</i>-<i>n </i>at different times. For example, rows and columns of pixels data corresponding to zone <b>702</b><i>a </i>may be generated between time t<sub>1 </sub>and t<sub>2</sub>. Further, the rows and columns of pixel data corresponding to zone <b>702</b><i>a </i>may be read out from primary image sensor <b>114</b>, at the same time as rows and columns of pixel data corresponding to zone <b>702</b><i>b </i>may be generated. Reading out pixel data for each zone at a different time may cause a separate blur for each zone and a separate motion vector associated with each zone.</p>
<p id="p-0062" num="0061">Motion vector MV<b>1</b>(<i>x, y, t</i>) may be used to remove the blur from zone <b>702</b><i>a </i>and motion vector MV<b>2</b>(<i>x, y, t</i>) may be used to remove the blur from zone <b>702</b><i>c</i>. Zones <b>702</b><i>b</i>, <b>702</b><i>d</i>, and <b>702</b><i>f </i>may not have a separate motion vector and the blur from these zones may be removed by calculating a motion vector based on the average of the motion vectors corresponding to adjacent zones in data frame <b>702</b>.</p>
<p id="p-0063" num="0062">The blur may be removed using, for example, deblurring circuit <b>414</b> of system <b>400</b>. Deblurring circuit <b>414</b> may use motion vectors MV<b>1</b>-MVn to model the hand movement of the user as uniform linear motion in each zone and may perform de-convolution or simple interpolation to remove the blur. For example, deblurring circuit <b>414</b> may use the equation:</p>
<p id="p-0064" num="0063">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mi>g</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>x</mi>
                <mo>,</mo>
                <mi>y</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <msubsup>
              <mo>&#x222b;</mo>
              <mn>0</mn>
              <mi>T</mi>
            </msubsup>
            <mo>&#x2062;</mo>
            <mrow>
              <mrow>
                <mi>f</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mrow>
                      <mi>x</mi>
                      <mo>-</mo>
                      <mrow>
                        <msub>
                          <mi>x</mi>
                          <mn>0</mn>
                        </msub>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mi>t</mi>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                    <mo>,</mo>
                    <mrow>
                      <mi>y</mi>
                      <mo>-</mo>
                      <mrow>
                        <msub>
                          <mi>y</mi>
                          <mn>0</mn>
                        </msub>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mi>t</mi>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>&#x2062;</mo>
              <mrow>
                <mo>&#x2146;</mo>
                <mi>t</mi>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where g(x, y) may represent blurred image data corresponding to zone <b>702</b><i>a</i>, x<sub>0</sub>(t) may represent the motion represented by MV<b>1</b> in the x direction, y<sub>0</sub>(t) may represent the motion represented by MV<b>1</b> in the y direction, T may represent the time required to readout rows and pixels of data corresponding to zone <b>702</b><i>a</i>, and f (x, y) may represent image data corresponding to zone <b>702</b><i>a</i>, without the blur. Deblurring circuit <b>414</b> may use equation 5 to solve for f (x, y) and produce image data without the blur.
</p>
<p id="p-0065" num="0064">Deblurring circuit <b>414</b> may calculate averages of motion vectors MV<b>1</b>-MVn to remove blur from zones <b>702</b><i>b</i>, <b>702</b><i>d</i>, and <b>702</b><i>f</i>. For example, the average of MV<b>1</b>(<i>x, y, t</i>) and MV<b>2</b>(<i>x, y, t</i>) may be used to calculate an average motion vector to remove the blur from zone <b>702</b><i>b</i>. Techniques known to those of skill in the art may be used to measure the rate of change of displacement in each direction to prevent abrupt transitions in the blur correction. For example, deblurring circuit <b>414</b> may use the equations:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i><sub>0</sub>(<i>t</i>)=<i>c</i><sub>0</sub><i>&#xb7;t/T, y</i><sub>0</sub>(<i>t</i>)=<i>b</i><sub>0</sub><i>&#xb7;t/T; </i><?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i><sub>1</sub>(<i>t</i>)=<i>c</i><sub>1</sub><i>&#xb7;t/T, y</i><sub>1</sub>(<i>t</i>)=<i>b</i><sub>1</sub><i>&#xb7;t/T;</i>&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i><sub>trans(0 to 1)</sub>(<i>t</i>)=(<i>c</i><sub>0</sub><i>+c</i><sub>1</sub>)&#xb7;<i>t/T, y</i><sub>0</sub>(<i>t</i>)=(<i>b</i><sub>0</sub><i>+b</i><sub>1</sub>)&#xb7;<i>t/T;</i>&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where x<sub>0 </sub>and y<sub>0 </sub>of equation 6 may represent the rate of change of displacements in the x and y directions with respect to zone <b>702</b><i>a</i>, c<sub>0 </sub>and b<sub>0 </sub>may represent the displacement in the x and y direction with respect to zone <b>702</b><i>a </i>and T may represent the integration time. Similarly, x<sub>1 </sub>and y<sub>1 </sub>may represent the rate of change of displacements in x and y directions with respect to zone <b>702</b><i>c</i>, and c<sub>1 </sub>and b<sub>1 </sub>may represent the displacement in the x and y direction with respect to zone <b>702</b><i>c</i>. Equation 6 may be used to measure the rate of change of displacement between zones <b>702</b><i>a </i>and <b>702</b><i>c </i>and equation 7 may then be used to approximate the displacements with respect to zone <b>702</b><i>b</i>. Deblurring circuit <b>414</b> may use the approximate displacements to remove blur from zone <b>702</b><i>b</i>. Further, blur from zones <b>702</b><i>d </i>and <b>702</b><i>f </i>may be removed by using equations 6 and 7 to measure average displacements.
</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 8</figref> shows a flow diagram of a method <b>800</b> for processing and adjusting image data, consistent with one embodiment the invention. The method <b>800</b> begins in step <b>804</b> where a handheld device including a primary image sensor may be used by a user to capture a scene. Further, the primary image sensor may generate image data corresponding to the scene. In step <b>806</b>, a secondary image sensor may be activated simultaneously, with, before, or after the primary image sensor and may generate reference image data corresponding to an extended scene of the environment that the handheld device is being operated in. In step <b>808</b>, the image data may be synchronized with reference image data. The method moves to step <b>810</b>, where motion vectors may be calculated using reference image data. The motion vectors may represent the motion of the handheld device during capturing of the scene and may be caused by hand movement of the user</p>
<p id="p-0067" num="0066">Next, in step <b>812</b>, it may be determined if the image data corresponds to a still image or a video of the scene. If the image data corresponds to a video of the scene, the method may move to step <b>814</b>, where cropping parameters are generated using the motion vectors calculated in step <b>810</b>. In step <b>816</b>, the cropping parameters are used to remove jitter that may have been caused by hand movement of the user, from a data frame of the image data. The jitter may be removed in step <b>816</b>, by using the cropping parameters to skip rows or columns of pixels of a data frame. In step <b>818</b>, it may be determined if there are any remaining data frames of the image data generated, that may be effected by the hand movement of the user. If there are no remaining data frames then jitter has been removed from the image data and the method may end (step <b>836</b>). If there are remaining data frames, the method may move back to step <b>814</b>, where cropping parameters for a new data frame may be generated.</p>
<p id="p-0068" num="0067">Returning now to step <b>812</b>, when the image data is determined to correspond to a still image of the scene, the method may move to step <b>820</b>. In step <b>820</b>, it may be determined if a global shutter was used with the primary image sensor in step <b>804</b> or was primary image sensor used with a rolling shutter mode. When it is determined that global shutter was used, the method may move to step <b>822</b>, where a global motion vector is calculated using the motion vectors calculated in step <b>810</b>. The method moves to step <b>824</b>, where a PSF may be calculated from the global motion vector. In step <b>826</b>, a global blur, that may be present in the image data due to hand movement of the user during still image capturing, may be removed by performing de-convolution or inverse filtering using the PSF and the image data. The method may end when the global blur is removed (step <b>836</b>).</p>
<p id="p-0069" num="0068">Returning now to step <b>820</b>, when it is determined that a primary image sensor was used with the rolling shutter mode, in step <b>804</b>, the method may move to step <b>828</b>, where the motion vectors calculated in step <b>810</b> may be matched with zones of image data. The method moves to step <b>830</b>, where hand movement and/or motion of handheld device may be modeled as linear motions using a motion vector that may have been matched with a zone in step <b>828</b>. In step <b>832</b>, the blur in the matched zone of step <b>830</b> may be removed by using the linear motion models and the matched zone. The method may move to step <b>834</b>, where it may be determined if there are any more zones of the image data generated in step <b>804</b>, that may include blur. If there are remaining zones, the method may move back to step <b>828</b>, where a motion vector may be matched with a new zone. When there are no remaining zones then blur has been removed from the image data and the method may end (step <b>836</b>).</p>
<p id="p-0070" num="0069">The foregoing description has been presented for purposes of illustration. It is not exhaustive and does not limit the invention to the precise forms or embodiments disclosed. Modifications and adaptations of the invention can be made from consideration of the specification and practice of the disclosed embodiments of the invention. For example, one or more steps of methods described above may be performed in a different order or concurrently and still achieve desirable results.</p>
<p id="p-0071" num="0070">Other embodiments of the invention will be apparent to those skilled in the art from consideration of the specification and practice of the invention disclosed herein. It is intended that the specification and examples be considered as exemplary only, with a true scope of the invention being indicated by the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625926-20140107-M00001.NB">
<img id="EMI-M00001" he="9.57mm" wi="76.20mm" file="US08625926-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625926-20140107-M00002.NB">
<img id="EMI-M00002" he="7.03mm" wi="76.20mm" file="US08625926-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A system for stabilizing an image displayed on a device by processing image data captured by the device, the system comprising:
<claim-text>a primary image sensor configured to generate primary image data, wherein the primary image data comprises one or more first data frames;</claim-text>
<claim-text>a secondary image sensor configured to generate secondary image data, wherein the secondary image data comprises a plurality of second data frames (i) corresponding to the one or more first data frames of the primary image data and (ii) synchronized with the one or more first data frames of the primary image data;</claim-text>
<claim-text>a correlation circuit configured to calculate, using (i) second data frames of the plurality of second data frames of the secondary image data and (ii) the second data frames' corresponding first data frames of the one or more first data frames of the primary image data, at least one motion vector, wherein the at least one motion vector is based on a motion of the device during generation of the primary image data; and</claim-text>
<claim-text>a data processor configured to process the primary image data based on the at least one motion vector in order to stabilize the image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>a primary interface circuit to receive the primary image data from the primary image sensor; and</claim-text>
<claim-text>a secondary interface circuit to receive the secondary image data from the secondary image sensor;</claim-text>
<claim-text>wherein the primary interface circuit and the secondary interface circuit are configured to synchronize the plurality of second data frames of the secondary image data with the one or more first data frames based on (i) a first count value embedded in each of the one or more first data frames of the primary image data and (ii) a second count value embedded in each of the plurality of second data frames of the secondary image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a memory device to store the processed primary image data.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a liquid crystal display panel configured to at least one of (i) preview a scene and (ii) display the processed primary image data.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising a keypad panel to generate a trigger event to terminate the previewing of the scene, wherein the trigger event initiates generation of primary image data.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a transmitter to transmit the processed primary image data.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the primary image data represents one of (i) a still image or (ii) a video.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a kernel estimation circuit to convert the at least one motion vector to a spatially varying function.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the data processor comprises at least one of (i) a deblurring circuit or (ii) a third interface circuit.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a data compressor to compress the processed primary image data.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the primary image sensor operates in a rolling shutter mode.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is one of (i) a cell phone or (ii) a personal digital assistant.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<claim-text>the primary image sensor is pointed in a first direction; and</claim-text>
<claim-text>the secondary image sensor is pointed in a second direction.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A method for stabilizing an image displayed on a device by processing image data captured by the device, the method comprising:
<claim-text>generating primary image data, wherein the primary image data comprises one or more first data frames;</claim-text>
<claim-text>generating secondary image data, wherein the secondary image data comprises a plurality of second data frames (i) corresponding to the one or more first data frames of the primary image data and (ii) synchronized with the one or more first data frames of the primary image data;</claim-text>
<claim-text>calculating, using (i) second data frames of the plurality of second data frames of the secondary image data and (ii) the second data frames' corresponding first data frames of the primary image data, at least one motion vector, wherein the at least one motion vector is based on a motion of the device during generation of the primary image data;</claim-text>
<claim-text>based on the at least one motion vector, processing the primary image data in order to stabilize the image; and</claim-text>
<claim-text>outputting the processed primary image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising synchronizing the plurality of second data frames of the secondary image data with the one or more first data frames of the primary image data, wherein the synchronizing is based on (i) a first count value embedded in each of the one or more first data frames of the primary image data and (ii) a second count value embedded in each of the plurality of second data frames of the secondary image data.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising calculating motion of the device based on the plurality of second data frames.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising converting the at least one motion vector to a spatially varying function.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:
<claim-text>compressing the processed primary image data; and</claim-text>
<claim-text>displaying the compressed primary image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the primary image data represents one of (i) a still image or (ii) a video.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the device is one of (i) a cell phone or (ii) a personal digital assistant.</claim-text>
</claim>
</claims>
</us-patent-grant>
