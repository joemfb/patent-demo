<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625882-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625882</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13116583</doc-number>
<date>20110526</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="regional">
<country>EP</country>
<doc-number>10164471</doc-number>
<date>20100531</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>266</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382154</main-classification>
<further-classification>345419</further-classification>
<further-classification>345422</further-classification>
</classification-national>
<invention-title id="d2e71">User interface with three dimensional user input</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5844574</doc-number>
<kind>A</kind>
<name>Yang</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345534</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6073036</doc-number>
<kind>A</kind>
<name>Heikkinen et al.</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>4555501</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7331245</doc-number>
<kind>B2</kind>
<name>Nishimura et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 73862046</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7499040</doc-number>
<kind>B2</kind>
<name>Zadesky et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345204</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7519223</doc-number>
<kind>B2</kind>
<name>Dehlin et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382203</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>8209628</doc-number>
<kind>B1</kind>
<name>Davidson</name>
<date>20120600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715790</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>8363020</doc-number>
<kind>B2</kind>
<name>Li et al.</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2002/0180763</doc-number>
<kind>A1</kind>
<name>Kung</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345660</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2004/0021663</doc-number>
<kind>A1</kind>
<name>Suzuki et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2006/0001650</doc-number>
<kind>A1</kind>
<name>Robbins et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2006/0082573</doc-number>
<kind>A1</kind>
<name>Konno et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2008/0024454</doc-number>
<kind>A1</kind>
<name>Everest</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2008/0131019</doc-number>
<kind>A1</kind>
<name>Ng</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382255</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2008/0165141</doc-number>
<kind>A1</kind>
<name>Christie</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2009/0022396</doc-number>
<kind>A1</kind>
<name>Watanabe et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2009/0160793</doc-number>
<kind>A1</kind>
<name>Rekimoto</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2009/0251432</doc-number>
<kind>A1</kind>
<name>Wang et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2011/0050588</doc-number>
<kind>A1</kind>
<name>Li et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2011/0291943</doc-number>
<kind>A1</kind>
<name>Th&#xf3;rn et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2012/0105358</doc-number>
<kind>A1</kind>
<name>Momeyer et al.</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345174</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>WO</country>
<doc-number>WO 02069124</doc-number>
<kind>A1</kind>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-cpc-text>G06F 3/03</classification-cpc-text>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>European Search Report and Written Opinion dated Oct. 14, 2010 issued in corresponding EP application No. 10164471.4-1245, 9 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>10</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382154</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345419</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345422</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>4</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61349917</doc-number>
<date>20100531</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120057806</doc-number>
<kind>A1</kind>
<date>20120308</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Backlund</last-name>
<first-name>Erik Johan Vendel</first-name>
<address>
<city>Gantofta</city>
<country>SE</country>
</address>
</addressbook>
<residence>
<country>SE</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Bengtsson</last-name>
<first-name>Henrik</first-name>
<address>
<city>Lund</city>
<country>SE</country>
</address>
</addressbook>
<residence>
<country>SE</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Heringslack</last-name>
<first-name>Henrik</first-name>
<address>
<city>Malm&#xf6;</city>
<country>SE</country>
</address>
</addressbook>
<residence>
<country>SE</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sassi</last-name>
<first-name>Jari</first-name>
<address>
<city>Lund</city>
<country>SE</country>
</address>
</addressbook>
<residence>
<country>SE</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Th&#xf6;rn</last-name>
<first-name>Ola Karl</first-name>
<address>
<city>Malm&#xf6;</city>
<country>SE</country>
</address>
</addressbook>
<residence>
<country>SE</country>
</residence>
</us-applicant>
<us-applicant sequence="006" app-type="applicant" designation="us-only">
<addressbook>
<last-name>&#x212b;berg</last-name>
<first-name>Peter</first-name>
<address>
<city>Vinsl&#xf6;v</city>
<country>SE</country>
</address>
</addressbook>
<residence>
<country>SE</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Backlund</last-name>
<first-name>Erik Johan Vendel</first-name>
<address>
<city>Gantofta</city>
<country>SE</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Bengtsson</last-name>
<first-name>Henrik</first-name>
<address>
<city>Lund</city>
<country>SE</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Heringslack</last-name>
<first-name>Henrik</first-name>
<address>
<city>Malm&#xf6;</city>
<country>SE</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Sassi</last-name>
<first-name>Jari</first-name>
<address>
<city>Lund</city>
<country>SE</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Th&#xf6;rn</last-name>
<first-name>Ola Karl</first-name>
<address>
<city>Malm&#xf6;</city>
<country>SE</country>
</address>
</addressbook>
</inventor>
<inventor sequence="006" designation="us-only">
<addressbook>
<last-name>&#x212b;berg</last-name>
<first-name>Peter</first-name>
<address>
<city>Vinsl&#xf6;v</city>
<country>SE</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Snyder, Clark, Lesch &#x26; Chung, LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sony Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
<assignee>
<addressbook>
<orgname>Sony Mobile Communications AB</orgname>
<role>03</role>
<address>
<city>Lund</city>
<country>SE</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Patel</last-name>
<first-name>Jayesh A</first-name>
<department>2669</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A device and method for image manipulation is provided. The image manipulation may be performed as a function of a three dimensional user input and image or application specific data regarding the displayed image to be manipulated. The three dimensional input may be in the form of a two dimensional position on a touch screen and a measured force in the third dimension. The image or application specific data may be in the form of a maximum depth value indicating the maximum depth of the displayed image. The system may be configured to adjust the image based on a percentage of the measured user input force and a threshold force. The image may be adjusted based on the percentage and the maximum depth.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="127.25mm" wi="161.04mm" file="US08625882-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="94.91mm" wi="167.30mm" file="US08625882-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="159.51mm" wi="149.86mm" file="US08625882-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="97.20mm" wi="169.08mm" file="US08625882-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="142.66mm" wi="163.15mm" file="US08625882-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application claims priority under 35 U.S.C. &#xa7;119, based on U.S. Provisional Patent Application No. 61/349,917, filed May 31, 2010, the disclosure of which is hereby incorporated by reference herein.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">Example embodiments presented herein are directed towards a multimedia device, and methods for using the same, comprising a user interface configured to receive a three dimensional input/output and/or alter a displayed image based on the input.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Many multimedia devices employ touch screen technology. For example, a user may be able to enlarge or shrink an image on a display screen by placing two fingers on a screen and moving the two fingers away from each other, or closer to each other, respectively, to achieve the desired effect. The use of touch screen technology allows a user to easily navigate through menus and manipulate displayed data. Touch screen technology is also employed in various gaming applications.</p>
<p id="p-0005" num="0004">While current systems do exist for manipulating an image via input received from a touch screen, none of these systems utilize a measured force with respect to image and/or application specific data. Often user navigation through menus or gaming systems involves predefined steps or movements. Image manipulation or user navigation may be greatly improved with the use of measured force data and image or application specific data as it provides greater accuracy.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0006" num="0005">Thus, an objective of the present invention remedies the above mentioned problems with respect to current systems. According to example embodiments, an image adjusting system is provided. The image adjusting system may include a receiving port that may be configured to receive a user input through a touch screen interface. The image adjusting system may also include an analyzer that may be configured to provide a depth configuration value based on the user input and image depth information of a displayed image. The image adjusting system may further include a processor that may be configured to alter the displayed image based on the depth reconfiguration value.</p>
<p id="p-0007" num="0006">The image depth information may define a maximum depth of the image. The image depth information may be embedded in metadata or may be determined through image analysis. The user input may be in the form of a three dimensional input. The user input may be further defined as a location in a two dimensional location and an applied force measured in a third dimension.</p>
<p id="p-0008" num="0007">In example embodiments, the analyzer may further include a comparator that may be configured to compare the applied force to a threshold force value. The comparator may further be configured to determine a difference percentage of the applied force and the threshold value. The analyzer may be further configured to provide the depth reconfiguration value based on the difference percentage. The threshold value may be programmably adjustable.</p>
<p id="p-0009" num="0008">Example embodiments further include a multimedia device for displaying images, characterized that the multimedia device may comprise a touch screen configured to receive a three dimensional user input. At least one dimension of the user input may be a measure of force. The device may further include a processor that may be configured to alter a displayed image as a function of a percentage of the applied force.</p>
<p id="p-0010" num="0009">Example embodiments also include a method for providing a user interface. The method may include receiving a user input through a touch screen interface, evaluating a depth reconfiguration value based on the user input and image depth information of a displayed image, and altering the displayed image based on the depth reconfiguration value. The evaluating may further include comparing the applied force to a threshold force value, determining a difference percentage of the applied force and threshold force value, and providing the depth reconfiguration value based on the difference percentage.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010">The foregoing will be apparent from the following more particular description of example embodiments of the invention, as illustrated in the accompanying drawings in which like reference characters refer to the same parts throughout the different views. The drawings are not necessarily to scale, emphasis instead being placed upon illustrating embodiments of the present invention.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1</figref> is an illustrative example employing example embodiments;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2</figref> is an illustration of a multimedia device according to an example embodiment;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> is a block schematic of an image adjust system according to example embodiments; and</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> is a flow diagram depicting operational steps which may be taken by the system of <figref idref="DRAWINGS">FIG. 3</figref> according to example embodiments.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0016" num="0015">In the following description, for purposes of explanation and not limitation, specific details are set forth, such as particular components, elements, techniques, etc. in order to provide a thorough understanding of the example embodiments presented herein. However, it will be apparent to one skilled in the art that the example embodiments may be practiced in other manners that depart from these specific details. In other instances, detailed descriptions of well-known methods and elements are omitted so as not to obscure the description of the example embodiments.</p>
<p id="p-0017" num="0016">Example embodiments presented herein are directed towards image manipulation or navigation through an interface utilizing a three dimensional user input and image or application specific data. For example, embodiments presented herein may be utilized in gaming applications where a user may walk through a virtual world. Based on a measurement of a force component of the user input, a determination may be made as to how far (with respect to depth perception) a user may navigate through the virtual world.</p>
<p id="p-0018" num="0017">Similarly, a user may zoom in or enlarge a displayed image, where the amount of enlargement is determined in part by the measured force component of the user input. In contrast to current systems which require the use of two fingers to perform such tasks, example embodiments are presented herein which require the use of only one finger and provide great accuracy through the use of image or application specific data.</p>
<p id="p-0019" num="0018">Embodiments may also be employed for user navigation through a menu. For example, a user may cycle through a number of cascaded menus and a determination as to how for within the menu selection a user may go may be based on a force measurement of the user input.</p>
<p id="p-0020" num="0019">It should be appreciated that example embodiments presented herein may be employed in a variety of applications, for example, applications involving image, movie, gaming, menu systems, or any other type of applications known in the art which involve a form of display.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 1</figref> provides an illustrative example use of the embodiments presented. In <figref idref="DRAWINGS">FIG. 1</figref>, a user is navigating through a virtual world in a gaming application. The virtual user is represented as the stick <figref idref="DRAWINGS">FIG. 401</figref>. A user may navigate through the virtual world by pressing the touch screen, for example, in location <b>403</b>. An imaging system, according to example embodiments, may recognize the X and Y coordinates, as shown in axis <b>103</b>, as the location which the user desires to navigate to. The imaging system may further measure a force exerted by the user interacting the touch screen <b>101</b> in the Z direction, as shown in axis <b>103</b>.</p>
<p id="p-0022" num="0021">In the current example, the measured force exerted by the user is 2N. A threshold force (or maximum force) associated with the gaming application is 5N. The imaging system may compare the two force values and the system may determine a difference percentage of 40% (e.g., 2N is 40% of 5N).</p>
<p id="p-0023" num="0022">The imaging system may further provide a depth reconfiguration value based on image depth information associated with the gaming application. In <figref idref="DRAWINGS">FIG. 1</figref> the image depth information is the maximum depth, which is shown as Z max. In the current example Z max is set as 20 km. Given that the difference percentage has been calculated as 40%, the system may calculate the depth configuration value as 8 km (40% of the Z max value 20 km). This information may then be utilized to alter the displayed image in a manner that the user will navigate through the virtual world a total distance of 8 km in the direction of user defined location <b>403</b>.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an example of a multimedia device in the form of a mobile phone <b>100</b> that may employ the example embodiments presented herein. Example embodiments presented herein will be explained with a mobile phone being the multimedia device. It should be appreciated that example embodiments may be applied to other multimedia devices having an electronic visual display known in the art, for example, PDAs or video game devices.</p>
<p id="p-0025" num="0024">The multimedia device <b>100</b> may include a touch screen <b>101</b>. The multimedia touch screen <b>101</b> may employ, for example, capacitive sensing technology to detect when a user is providing input via the screen <b>101</b>. It should be appreciated that any other sensing means known in the art may be employed, for example, resistive technology, surface acoustic wave technology, surface capacitance technology, projected capacitance technology, mutual capacitive sensors, self capacitance sensors, strain gauge or force panel technology, optical imaging, dispersive signal technology, acoustic pulse recognition, and coded LCD technology.</p>
<p id="p-0026" num="0025">The multimedia device <b>100</b> may further include an image adjusting system <b>200</b> that may be configured to adjust an image displayed on the screen <b>101</b> of the multimedia device <b>100</b>, as shown in <figref idref="DRAWINGS">FIG. 3</figref>. <figref idref="DRAWINGS">FIG. 4</figref> is a flow chart illustrating example operational steps that may be taken by the image adjusting system <b>200</b> of <figref idref="DRAWINGS">FIG. 3</figref>. The image adjusting system <b>200</b> may include an input port <b>201</b> that may be configured to receive user data <b>203</b> (<b>301</b>). The user data may be three dimensional data providing position data and force data. The position data may include an X and Y coordinate providing information as to where a user touched the screen <b>101</b>, as shown by coordinate axis <b>103</b>. The force data may be a pressure reading as to how much force a user touched the screen <b>101</b> within a Z direction, as indicated by the coordinate axis <b>103</b>.</p>
<p id="p-0027" num="0026">Once received by the input port <b>201</b>, the user data <b>203</b> may then be transferred to an analyzer <b>205</b>. The analyzer may also be configured to receive image depth information regarding an image currently being displayed on screen <b>101</b>. The image depth information may be included within the image itself as embedded metadata. Alternatively, the image depth information may be obtained through image analysis. It should be appreciated that image depth information may also be obtained through any other means known in the art.</p>
<p id="p-0028" num="0027">Upon receiving the image depth information <b>207</b>, the analyzer <b>205</b> may utilize a comparator <b>209</b> to compare the applied force in the Z coordinate and a threshold force value (<b>302</b>). The threshold force value may define a maximum force to be applied on the multimedia device <b>100</b> or a maximum force to be applied to a specific image. The threshold force may be programmably adjusted by a user. The adjustment to the threshold force may be with respect to the maximum force numerical value as well as to what the threshold force will be associated with (e.g., the multimedia device, a specific image, or a specific application). It should be appreciated that the threshold force value may be included in the image depth information <b>207</b> or may be registered within the analyzer <b>205</b>, or within else where in the image adjusting system <b>200</b>.</p>
<p id="p-0029" num="0028">Once the analyzer <b>205</b> has received the threshold force value and the user applied force value, the comparator <b>209</b> may compare the two force values and provide a percentage difference of the two values (<b>303</b>, <b>305</b>). Upon computation of the percentage difference, the analyzer <b>205</b> may be configured to determine a depth reconfiguration value based on the determined percentage difference (<b>307</b>). For example, if the determined percentage difference is 50%, the analyzer may calculate the depth reconfiguration to be 50% of a Z coordinate maximum. The Z coordinate maximum may be defined as the maximum depth associated with a particular image or application. For example, if a user is utilizing an application in which an associated Z coordinate maximum is 10 km, a percentage difference of 50% would result in a depth reconfiguration value of 5 km (50% of the Z coordinate maximum). It should also be appreciated that the Z coordinate maximum depth may be associated with the multimedia device. It should further be appreciated that the Z coordinate maximum depth value may also be programmably adjustable.</p>
<p id="p-0030" num="0029">Once the depth reconfiguration has been calculated, the analyzer <b>205</b> may be configured to send the depth reconfiguration <b>213</b> to a processor <b>215</b>. The processor <b>215</b> may be configured to provide alteration instructions <b>217</b> to adjust the image displayed on the screen <b>101</b> (<b>309</b>).</p>
<p id="p-0031" num="0030">It should be appreciated that, using example embodiments, a user may be able to steer a screen pointer directly to a three dimensional point by, for example, enabling 100% of the total depth.</p>
<p id="p-0032" num="0031">The various embodiments of the present invention described herein is described in the general context of method steps or processes, which may be implemented in one embodiment by a computer program product, embodied in a computer-readable medium, including computer-executable instructions, such as program code, executed by computers in networked environments. A computer-readable medium may include removable and non-removable storage devices including, but not limited to, Read Only Memory (ROM), Random Access Memory (RAM), compact discs (CDs), digital versatile discs (DVD), etc. Generally, program modules may include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. Computer-executable instructions, associated data structures, and program modules represent examples of program code for executing steps of the methods disclosed herein. The particular sequence of such executable instructions or associated data structures represents examples of corresponding acts for implementing the functions described in such steps or processes.</p>
<p id="p-0033" num="0032">The above mentioned and described embodiments are only given as examples and should not be limiting to the example embodiments. Other solutions, uses, objectives, and functions within the scope of the invention as claimed in the below described patent claims should be apparent for the person skilled in the art.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of providing a user interface comprising:
<claim-text>receiving a three dimensional user input through a touch screen interface, wherein the three dimensional user input defines a two dimensional location and an applied force in a third dimension;</claim-text>
<claim-text>evaluating a depth reconfiguration value based on a difference percentage of the user input and image depth information of a displayed image, wherein evaluating the depth reconfiguration value further comprises:</claim-text>
<claim-text>comparing the applied force to a threshold force value;</claim-text>
<claim-text>determining the difference percentage of the user input based on the applied force and the threshold force value; and</claim-text>
<claim-text>providing the depth reconfiguration value based on the difference percentage;</claim-text>
<claim-text>wherein the depth reconfiguration value is a distance value in a z-axis; and altering the displayed image based on the depth reconfiguration value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the image depth information defines a maximum depth of the image.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the image depth information is embedded in metadata.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising determining the image depth information through image analysis.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the threshold force value is programmably adjustable.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. An image adjusting system comprising a receiving port configured to receive a three dimensional user input through a touch screen interface, comprising:
<claim-text>an analyzer configured to provide a depth reconfiguration value based on a difference percentage of the user input and image depth information of a displayed image, wherein the user input defines a two-dimensional location and an applied force in a third dimension and wherein the depth reconfiguration value comprises a distance value in a z-axis; wherein the analyzer further comprises a comparator configured to compare the applied force to a threshold force value, wherein the analyzer is further configured to determine the difference percentage of the user input based on the applied force and the threshold force value, and provide the depth reconfiguration value based on the difference percentage; and</claim-text>
<claim-text>a processor configured to alter the displayed image based on the depth reconfiguration value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the image depth information defines a maximum depth of the image.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the image depth information is embedded in metadata.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the analyzer is further configured to determine the image depth information through image analysis.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the threshold force value is programmably adjustable. </claim-text>
</claim>
</claims>
</us-patent-grant>
