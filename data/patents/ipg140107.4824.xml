<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625917-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625917</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13418063</doc-number>
<date>20120312</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>CN</country>
<doc-number>2006 1 0064858</doc-number>
<date>20060316</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>36</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>46</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382239</main-classification>
</classification-national>
<invention-title id="d2e69">Method and apparatus for realizing adaptive quantization in process of image coding</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4774574</doc-number>
<kind>A</kind>
<name>Daly et al.</name>
<date>19880900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3484061</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5157488</doc-number>
<kind>A</kind>
<name>Pennebaker</name>
<date>19921000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524004</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5231484</doc-number>
<kind>A</kind>
<name>Gonzales et al.</name>
<date>19930700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>37524004</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5398078</doc-number>
<kind>A</kind>
<name>Masuda et al.</name>
<date>19950300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5881177</doc-number>
<kind>A</kind>
<name>Kim</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5892548</doc-number>
<kind>A</kind>
<name>Kim</name>
<date>19990400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6148109</doc-number>
<kind>A</kind>
<name>Boon et al.</name>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6256423</doc-number>
<kind>B1</kind>
<name>Krishnamurthy et al.</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6292588</doc-number>
<kind>B1</kind>
<name>Shen et al.</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6360016</doc-number>
<kind>B1</kind>
<name>Shen et al.</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6366703</doc-number>
<kind>B1</kind>
<name>Boon et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6377708</doc-number>
<kind>B1</kind>
<name>Shen et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6532306</doc-number>
<kind>B1</kind>
<name>Boon et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6782135</doc-number>
<kind>B1</kind>
<name>Viscito et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382239</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>7970223</doc-number>
<kind>B2</kind>
<name>Sugimoto et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>7991237</doc-number>
<kind>B2</kind>
<name>Sekiguchi et al.</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>8160374</doc-number>
<kind>B2</kind>
<name>Zheng et al.</name>
<date>20120400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382239</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2003/0036382</doc-number>
<kind>A1</kind>
<name>Chen</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2003/0036901</doc-number>
<kind>A1</kind>
<name>Chen</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2003/0059120</doc-number>
<kind>A1</kind>
<name>Boon et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2004/0125204</doc-number>
<kind>A1</kind>
<name>Yamada et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2004/0179595</doc-number>
<kind>A1</kind>
<name>Abramov</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2005/0008232</doc-number>
<kind>A1</kind>
<name>Shen et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2005/0058356</doc-number>
<kind>A1</kind>
<name>Shen et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2005/0187764</doc-number>
<kind>A1</kind>
<name>Chen</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2005/0232354</doc-number>
<kind>A1</kind>
<name>Chen</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2007/0065027</doc-number>
<kind>A1</kind>
<name>Boon et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2007/0065028</doc-number>
<kind>A1</kind>
<name>Boon et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2007/0065029</doc-number>
<kind>A1</kind>
<name>Shen et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2007/0065030</doc-number>
<kind>A1</kind>
<name>Shen et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2007/0258519</doc-number>
<kind>A1</kind>
<name>Srinivasan</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2009/0010557</doc-number>
<kind>A1</kind>
<name>Zheng et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2009/0034612</doc-number>
<kind>A1</kind>
<name>Zheng et al.</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2010/0080287</doc-number>
<kind>A1</kind>
<name>Ali</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2012/0170859</doc-number>
<kind>A1</kind>
<name>Zheng et al.</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382238</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2013/0002965</doc-number>
<kind>A1</kind>
<name>Chan et al.</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348731</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>CN</country>
<doc-number>1164167</doc-number>
<kind>A</kind>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>CN</country>
<doc-number>1350401</doc-number>
<kind>A</kind>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>CN</country>
<doc-number>1450497</doc-number>
<kind>A</kind>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>WO</country>
<doc-number>WO 0018131</doc-number>
<kind>A1</kind>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>WO</country>
<doc-number>WO 02096007</doc-number>
<kind>A2</kind>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>WO</country>
<doc-number>WO 2006/099229</doc-number>
<kind>A1</kind>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>In corresponding U.S. Appl. No. 12/210,939 (Mar. 12, 2012).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>Written Opinion of the International Searching Authority in corresponding PCT Application No. PCT/CN2007/000863 (Jun. 14, 2007).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>International Search Report in corresponding PCT Application No. PCT/CN2007/000863 (Jun. 14, 2007).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00046">
<othercit>1<sup>st </sup>Office Action in corresponding Chinese Application No. 200780000409.1 (Sep. 25, 2009).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>Extended European Search Report Action in corresponding European Application No. 07720440.2 (Sep. 14, 2009).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00048">
<othercit>1<sup>st </sup>Office Action in corresponding European Application No. 07720440.2 (Mar. 25, 2010).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00049">
<othercit>Chen et al., &#x201c;Macroblock-Level Adaptive Frequency Weighting for Perceptual Video Coding,&#x201d; <i>IEEE Transactions on Consumer Electronics</i>, 53(2): 775-781 (May 2007).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00050">
<othercit>Lee et al., &#x201c;On the Error Distribution and Scene Change for the BIT Rate Control of MPEG,&#x201d; <i>IEEE International Conference on Consumer Electronics</i>, 286-287 (Jun. 1993).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00051">
<othercit>Rijkse, &#x201c;Draft Recommendation H.263 (Video Coding for Narrow Telecommunication Channels),&#x201d; <i>International Telecommunication Union </i>(<i>ITU</i>)&#x2014;<i>Telecommunication Standardization Sector</i>, Study Group 15(Document LBC-95-163):1-50 (Jun. 1995).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00052">
<othercit>Weigand et al., &#x201c;Rate-Constrained Coder Control and Comparison of Video Coding Standards,&#x201d; <i>IEEE Transactions on Circuits and Systems for Video Technology</i>, 13(7): 688-703 (Jul. 2003).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00053">
<othercit>Wenger, &#x201c;Test model 11,&#x201d; Study Group 16, Video Coding Experts Group (Question 15), Feb. 16-19, 1999, International Telecommunications Union, Monterey, California.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00054">
<othercit>Summon to Attend Oral Proceedings in corresponding European Patent Application No. 07720440.2 (Feb. 27, 2013).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00055">
<othercit>Chinese Search Report in corresponding Chinese Patent Application No. 2012100434849 (Aug. 20, 2013).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>26</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382173</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382232</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382233</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382238</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382239</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382248</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382250</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382251</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524003</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524004</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524014</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>37524017</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>375 E714</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>375 E7027</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>375 E7133</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>375 E7134</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3484061</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348699</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348731</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348E05097</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>1</number-of-drawing-sheets>
<number-of-figures>2</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12210939</doc-number>
<date>20080915</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8160374</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13418063</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>PCT/CN2007/000863</doc-number>
<date>20070316</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12210939</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120170859</doc-number>
<kind>A1</kind>
<date>20120705</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Zheng</last-name>
<first-name>Jianhua</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
<residence>
<country>CN</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>He</last-name>
<first-name>Yun</first-name>
<address>
<city>Shenzhen</city>
<country>CN</country>
</address>
</addressbook>
<residence>
<country>CN</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Jianwen</first-name>
<address>
<city>Shenzhen</city>
<country>CN</country>
</address>
</addressbook>
<residence>
<country>CN</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Zheng</last-name>
<first-name>Jianhua</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>He</last-name>
<first-name>Yun</first-name>
<address>
<city>Shenzhen</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Chen</last-name>
<first-name>Jianwen</first-name>
<address>
<city>Shenzhen</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Leydig, Voit &#x26; Mayer, Ltd.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Huawei Technologies Co., Ltd.</orgname>
<role>03</role>
<address>
<city>Shenzhen</city>
<country>CN</country>
</address>
</addressbook>
</assignee>
<assignee>
<addressbook>
<orgname>Tsinghua University</orgname>
<role>03</role>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Alavi</last-name>
<first-name>Amir</first-name>
<department>2668</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method and apparatus for realizing adaptive quantization in the process of image/video coding, which includes the following steps: acquiring the parameter information of the neighbor blocks of the current block; determining the quantization mode of the current block according to the parameter information of the neighbor blocks; and performing quantization processing on transform coefficients of the current block by using the determined quantization mode. Wherein, block coding type and motion vector of the neighbor blocks are used as the parameter information for determining the quantization mode of the current block. An apparatus for adaptive quantization in process of image/video coding is provided. The invention can use different quantization mode for adaptive quantization in different image content of an image sequence, thereby improving the subjective quality of the compressed images at the same bit rate.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="111.25mm" wi="165.78mm" file="US08625917-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="218.69mm" wi="175.77mm" file="US08625917-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 12/210,939, filed Sep. 15, 2008, now U.S. Pat. No. 8,160,374 which is a continuation of International Patent Application No. PCT/CN2007/000863, filed Mar. 16, 2007. The International Application claims priority to Chinese Patent Application No. 200610064858.X, filed Mar. 16, 2006, each of which is hereby incorporated by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">The present invention relates to the field of image/video coding technology, and more particularly, to a technology for deciding quantization modes in the process of image/video coding.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">In image encoding and decoding technology, with respect to the process of image encoding, an image is generally partitioned into coding blocks and then subject to the encoding process. The partitioned image blocks is then transformed by an orthogonal transform to obtain corresponding block transform coefficients, and the block transform coefficients are then quantized and clipped to integers and are then subject to entropy coding to finally obtain a encoded bit stream corresponding to the image, thereby realizing the encoding process of the image.</p>
<p id="p-0005" num="0004">In the architecture of video compression/coding, data feed to quantization computation during intra coding is the values of image block transform coefficients, while data feed to quantization computation during inter-coding is values of residual coefficients. Since content information of an image is completely stored in the transform coefficients or residual coefficients, quality control of compressed image may be realized by controlling quantization process in image coding process.</p>
<p id="p-0006" num="0005">Images described in the present invention include static images, moving images, residual image of two adjacent images of moving images, target image obtained by performing an operation on any number of moving images, etc.</p>
<p id="p-0007" num="0006">In encoding, quantization of transform coefficients is generally realized by a quantization matrix, e.g. by the following equation:</p>
<p id="p-0008" num="0007">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>Q</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>i</mi>
              <mo>,</mo>
              <mi>j</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mo>[</mo>
          <mfrac>
            <mrow>
              <mi>Coe</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>,</mo>
                  <mi>j</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mrow>
              <mi>QM</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>,</mo>
                  <mi>j</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mfrac>
          <mo>]</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where, Coe(i, j) is a value of a pixel at location (i, j) after transform computation on image blocks, which is referred to as transform coefficient, QM(i, j) is a quantization matrix, Q(i, j) is a value of transform coefficient after quantization and truncation, which is referred as quantized coefficient value, and [&#x25cf;] is the truncation calculation.
</p>
<p id="p-0009" num="0008">Since the details of images of different contents represent different image frequencies and human eyes have different subjective feeling for different parts of an image, different quantization methods that match features of human eyes should be used for images of different contents.</p>
<p id="p-0010" num="0009">At present, in image/video coding standards such as Joint Photographic Experts Group (JPEG), MPEG-1 (MPEG, Motion Picture Experts Group), MPEG-2 and MPEG-4, fixed quantization matrixes are used for quantization computation in image coding, wherein in JPEG image coding standard, the quantization matrix is stored in the image header, while in MPEG-1, MPEG-2 and MPEG-4, the quantization matrixes are stored in the sequence header. Therefore, for a sequence of images, the MPEG standard allows only one quantization matrix in sequence bit stream, that is, the whole sequence uses a same fixed quantization matrix in image quantization.</p>
<p id="p-0011" num="0010">While an image is observed from human eyes, the quality evaluation of this image is made according to the subjective quality of the image perceived by human eyes, a better subjective image quality is obtained while a suitable quantization method is used for image quantization to match vision features of human eyes, i.e. for an image sequence, better subjective image quality is obtained only if appropriate quantization matrixes are selected in quantization.</p>
<p id="p-0012" num="0011">However, the contents of images in a sequence are rarely identical, instead, they may vary greatly, that is, details of images in a same sequence are different from each other, and if a same quantization matrix is used in quantization for the entire sequence, the compressed images cannot achieve optimal subjective quality.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0013" num="0012">Embodiments of the present invention provide a method and apparatus for implementing adaptive quantization in the process of image coding, which can improve the subjective quality of compressed images at the same bit rate.</p>
<p id="p-0014" num="0013">Embodiments of the present invention provide a method for implementing adaptive quantization in the process of image coding, including:</p>
<p id="p-0015" num="0014">acquiring parameter information on neighbor blocks of a current block;</p>
<p id="p-0016" num="0015">determining a quantization mode for the current block according to the parameter information on the neighbor blocks; and</p>
<p id="p-0017" num="0016">performing quantization processing on transform coefficients of the current block with the determined quantization mode.</p>
<p id="p-0018" num="0017">Embodiments of the present invention provide an apparatus for implementing adaptive quantization in the process of image coding, including:</p>
<p id="p-0019" num="0018">a neighbor block parameter information acquiring unit, adapted to acquire parameter information on neighbor blocks of a current block;</p>
<p id="p-0020" num="0019">a quantization mode decision unit, adapted to determine the quantization mode for the current block according to parameter information on neighbor blocks acquired by the neighbor block parameter information extractor; and</p>
<p id="p-0021" num="0020">a quantization processing unit, adapted to perform the quantization processing on transform coefficients of the current block with the quantization mode determined by the quantization mode decision unit.</p>
<p id="p-0022" num="0021">As can be seen from the technical solution provided in embodiments of the present invention, embodiments of the present invention are implemented according to vision features of human eyes, that is, quantization processing is carried out by selecting appropriate quantization matrixes in the process of image coding. Therefore, in image coding, quantization processing on image sequence may be well adapted to content features of the image, thereby greatly improving the subjective quality of compressed images at the same bit rate.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic diagram showing the layout of the current block and neighbor blocks; and</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic diagram showing the structure of an apparatus according to an embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0025" num="0024">Quantization modes used in embodiments of the present invention include, but are not limited to, quantization matrix, quantization parameter and quantization step. The encoding and decoding of images may be performed with a determined quantization mode.</p>
<p id="p-0026" num="0025">In particular, in embodiments of the present invention, corresponding quantization mode is determined according to image information, as a result, a certain extent optimal quantization mode can be obtained for each area of the image, wherein the image information includes, but is not limited to, information of image content and information generated from the image coding procedure. Therefore, the subjective quality at the same bit rate of the compressed image can be greatly improved by performing quantization calculation for image regions with the quantization mode determined in embodiments of the present invention.</p>
<p id="p-0027" num="0026">The method of adaptive quantization provided in the present invention, as called as the quantization mode decision method can be embodied in the following specific implementations.</p>
<p id="h-0007" num="0000">Implementation Mode 1</p>
<p id="p-0028" num="0027">It may be decided whether the quantization mode used for the current block is different from that of neighbor blocks according to the block mode information on neighbor blocks of a current block, and the quality of the specifically used quantization mode is determined when it is determined to use a different quantization mode, so as to determine the quantization mode to be selected for the current block. The size of the block herein may be M&#xd7;N, M, N=2, 4, 8, 16 or other sizes. The block mode information includes, but is not limited to, at least one of block size, block type, direction features of the block and block mode distribution of neighbor blocks.</p>
<p id="p-0029" num="0028">Specifically, in this implementation, it may be decided, in an intra-frame or inter-frame manner, whether a different quantization mode is used for the current block according to different frame types and the block mode information of neighbor blocks of the current block. The frame types include, but are not limited to, any frame type that contains block mode information, such as I frame, P frame, and B frame.</p>
<p id="h-0008" num="0000">Implementation Mode 2</p>
<p id="p-0030" num="0029">It is decided whether a different quantization mode is used for current block according to the motion vector information on neighbor blocks of the current block, and the quality of the specifically used quantization mode is determined when it is determined to use a different quantization mode, so as to determine the quantization mode to be selected for the current block. The motion vector information includes, but is not limited to, motion vector magnitude, motion vector direction, and motion vector distribution of neighbor blocks.</p>
<p id="p-0031" num="0030">Specifically, in the implementation 2, it may also be decided whether a different quantization mode is used for the current block according to different frame types and motion vector information on neighbor blocks of the current block. The frame types include, but are not limited to, any frame types that contain motion vector information, such as P frame and B frame.</p>
<p id="h-0009" num="0000">Implementation Mode 3</p>
<p id="p-0032" num="0031">The quantization mode quality used for the current block is decided according to information combining not only on neighbor blocks but also on the current block itself, and the corresponding quantization mode is further determined.</p>
<p id="p-0033" num="0032">Embodiments of the present invention provide a solution for realizing quantization adapted to image content, in which different quantization modes are used for different content parts of an image sequence to greatly improve the subjective quality of compressed image at the same bit rate.</p>
<p id="p-0034" num="0033">In particular, in embodiments of the present invention, the quantization mode for the current block is decided with neighbor blocks parameter information in the image coding procedure. The parameter information of neighbor blocks includes, but is not limited to, at least one of the block mode information of neighbor blocks, prediction information of neighbor blocks and motion vector information of neighbor blocks. The above information of neighbor blocks may all predict the image content features of the current block, and thus at least one of them may be used for determining the quantization mode of the current block.</p>
<p id="p-0035" num="0034">With consideration of vision feature of human eyes, corresponding rules that should be followed during the decision of quantization modes for the current block include, but are not limited to, the following.
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0035">(1) For regions belonging to details of an image should be maintained as possible as enough, and the corresponding quantization mode to be used should maintain more high frequency information, that is, high quality quantization modes should be used.</li>
        <li id="ul0002-0002" num="0036">(2) For regions belonging to edges of a moving part of an image, more high frequency information should be maintained, and high quality quantization modes also need to be used.</li>
        <li id="ul0002-0003" num="0037">(3) For regions with rapid motion of an image, a certain decrease of objective quality would not impose great influence on subjective quality, and thus slightly rough quantization modes, i.e., low quality quantization modes may be used.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0036" num="0038">In the above three cases, different quality quantization modes may be used for characteristics of different parts of an image, wherein the high quality quantization modes and low quality quantization modes may be predefined, and meanwhile, more levels of different quality quantization modes may be set, for example, high, middle and low quality quantization matrixes may be set as quantization modes available for various current blocks. Specifically, quantization modes of multiple level quantization qualities may be first set in the system, and corresponding selection conditions for selecting a quantization mode of each level of quantization quality are set based on the parameter information of neighbor blocks or parameter information of current block and neighbor blocks, then the quantization mode to be used for the current block may be determined according to the parameter information of neighbor blocks or the parameter information of current block and neighbor blocks.</p>
<p id="p-0037" num="0039">For example, for quantization matrixes of different qualities, quantization modes may be classified into the following type.</p>
<p id="h-0010" num="0000">(1) Quantization Mode 0:</p>
<p id="p-0038" num="0040">Default quantization mode, that is, the quantization matrix used for the current coding macroblock is same as the current default quantization mode.</p>
<p id="h-0011" num="0000">(2) Quantization Mode 1:</p>
<p id="p-0039" num="0041">Details-preserving mode, that is, the quantization matrix used for the current coding macroblock should be that the quantized image preserves image details as more as enough, and this quantization mode is a high quality quantization mode.</p>
<p id="h-0012" num="0000">(3) Quantization Mode 2:</p>
<p id="p-0040" num="0042">Undetailed quantization mode, that is, the current coding macroblock is not the details of an image and image details of the quantized image needs not to be preserved. This quantization mode is a low quality quantization mode.</p>
<p id="p-0041" num="0043">Then, conditions for selecting different quantization modes are set based on block parameter information, and hence the quantization mode to be used for the current block may be selected at the encoder side and the decoder side, respectively, according to parameter information conditions.</p>
<p id="p-0042" num="0044">In other words, in practical procedure of image/video coding, based on preset rules, the quantization mode for current block is decided with parameter information on neighbor blocks, that is, quantization modes of different qualities corresponding to parameter information on neighbor blocks are preset as the rules of decision. The rules to be set include: the conditions that are determined to use predetermined high quality quantization mode for the current block when the parameter information on neighbor blocks meet these conditions, as well as the condition that are determined to use predetermined low quality quantization mode for the current block when the parameter information on neighbor blocks meet these conditions, wherein the high quality quantization modes refer to quantization modes that preserve more frequency information and have high image fidelity, and the low quality quantization modes refer to quantization modes that lose some frequency information and have low image fidelity.</p>
<p id="p-0043" num="0045">In addition, for different frame types, i.e. for intra-coding frames and inter-coding frames, different quantization modes decision rules need to be used. For example, in an intra-coding picture, mode information of the detailed part of image is generally more plenty than that of the undetailed part of an image, and thus, if block mode information of neighbor macroblocks (i.e. neighbor blocks) is different from each other, a higher quality quantization mode should be used for the current block. In inter-coding picture, the prediction mode of detailed parts of an image is generally much plenty than that of undetailed part of image, and block size information on neighbor blocks and mode information on neighbor blocks may be used in decision condition of quantization modes decision.</p>
<p id="p-0044" num="0046">To facilitate understanding of embodiments of the present invention, specific processing of a quantization mode decision according to embodiments of the present invention will be described below, which mainly includes the following cases:</p>
<p id="p-0045" num="0047">Different decision rules are used for different frame types.</p>
<p id="p-0046" num="0048">For different frame types, namely intra-coding frames and inter-coding frames, the decision rules may be different. The above-mentioned neighbor macroblock is defined in <figref idref="DRAWINGS">FIG. 1</figref>. In case of an intra-coding picture, intra-prediction mode and block type of neighbor blocks may be used in the decision conditions. In case of inter-coding picture, inter-prediction mode (including block size and block type) and motion vector of neighbor blocks may be used in the decision conditions.</p>
<p id="p-0047" num="0049">In <figref idref="DRAWINGS">FIG. 1</figref>, the current block is E, neighbor blocks of E are A, B, C and D. Further, block sizes of A, B, C, D, and E may be identical or may be different, wherein neighbor blocks in vertical direction of the current block E are A and D, and neighbor blocks in horizontal direction of the current block E are B and D or C and D. The neighbor blocks may be direct neighbor blocks of E or indirect neighbor blocks. The direct neighbor blocks of E are adjacent border blocks of E, and the indirect neighbor blocks are neighbor blocks of adjacent border blocks of E.</p>
<p id="p-0048" num="0050">Decision for different frame types will be described below.</p>
<p id="p-0049" num="0051">I. Quantization Modes Decision for Intra Coding Picture.</p>
<p id="p-0050" num="0052">In this case, the current frame is an intra-coding picture such as an I frame, and corresponding quantization modes of the current block are determined with an intra-quantization mode decision. The intra-coding block refers to a block with intra-prediction mode.</p>
<p id="p-0051" num="0053">Specifically, for an intra-coding picture, the prediction mode of image detail parts is more plenty than that of others and mode information on neighbor blocks is typically different. Thus, if prediction modes of neighbor blocks are different, it means that the current macroblock may be a detail region or region edges of the image, and then a high quality quantization mode that can preserve details should be used for the current macroblock. For example, in <figref idref="DRAWINGS">FIG. 1</figref>, a low-quality quantization mode that does not preserve details is selected for the current macroblock only when prediction mode information on neighbor blocks meets one of the following conditions.
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0054">(1) Neighbor blocks in vertical direction edge of E use identical quantization modes, e.g. blocks A and D use identical quantization modes;</li>
        <li id="ul0004-0002" num="0055">(2) Neighbor blocks in horizontal direction edge of E use identical quantization modes, e.g. blocks B (or C) and D use identical quantization modes.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0052" num="0056">The identical prediction mode means that prediction modes of two blocks are the same. For example, two blocks are both of vertical prediction mode or horizontal prediction mode.</p>
<p id="p-0053" num="0057">II. Quantization Modes Decision for Inter-Coding Picture.</p>
<p id="p-0054" num="0058">In this case, the current frame is an inter-coding picture such as P frame, B frame, and then corresponding quantization modes of the current block are determined with an inter-quantization mode decision.</p>
<p id="p-0055" num="0059">Specifically, for an inter-coding picture, the block mode information of image detailed parts is more plenty than that of others. Thus, if inter-coding modes of neighbor macroblocks are different, it means that the current macro block may be a detail region or region edge of the image, and then a high quality quantization mode that can preserve details should be used for the current macroblock. For example, in <figref idref="DRAWINGS">FIG. 1</figref>, a low quality quantization mode that does not preserve details may be selected for the current macro block only when prediction mode information on neighbor macroblocks meets one of the following conditions:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0060">(1) No one in neighbor blocks A, B, C and D is of an intra-coding block;</li>
        <li id="ul0006-0002" num="0061">(2) No one in neighbor blocks A, B, C and D is of a small size coding block;</li>
        <li id="ul0006-0003" num="0062">(3) Neighbor blocks in vertical directions, such as A and D, are both large size coding blocks; and</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0056" num="0063">(4) Neighbor blocks in horizontal directions, such as B (or C) and D, are both large size coding blocks.</p>
<p id="p-0057" num="0064">In addition, according to vision feature of human eyes, human eyes are not sensitive for image regions with fast motion, and thus low quality quantization modes may be used for these regions.</p>
<p id="p-0058" num="0065">Further, in the neighbor blocks, each macroblock contains motion vectors itself and each motion vector contains motion vector magnitude and motion vector direction. Therefore, motion vectors of each neighbor macroblock and motion vector distribution of neighbor macroblocks may be used to estimate motion characteristics of the current coding area, and hence may be used in decision conditions for inter-coding quantization modes.</p>
<p id="p-0059" num="0066">The mode information of coding blocks include, but are not limited to, information such as block size and prediction mode of the coding block. The so-called block size refers to the size of a block used for intra-frame prediction and compensation of intra-coding block or for motion compensation of inter-coding blocks, and in state-to-art video coding standards, inter-coding allows motion compensation with multiple size of blocks. The so-called small size mode coding block refers to a block used with small coding size, such as a 4&#xd7;4 block, and the so-called large size mode coding block refers to a block used with large size such as a 16&#xd7;16 block and SKIP block.</p>
<p id="p-0060" num="0067">Quantization modes for the current block may be obtained through the above process and quantization procedure on coefficient data may be further performed.</p>
<p id="p-0061" num="0068">The process of a quantization mode decision according to the invention will now be described by taking H.264/AVC standard as an example.</p>
<p id="p-0062" num="0069">Different decision rules are used for different frame types.</p>
<p id="p-0063" num="0070">The frame types include I frames, P frames and B frames in H.264/AVC. The above-mentioned definition of neighbor macroblocks is still as shown in <figref idref="DRAWINGS">FIG. 1</figref>. In an I frame, intra-prediction modes of neighbor blocks may be used in the decision conditions. In a P frame, information such as inter-prediction mode and motion vector may be used in the decision conditions for determine the quantization mode. In H.264/AVC, for a 4&#xd7;4 block, the prediction mode of neighbor intra-blocks has up to 9 prediction directions. In H.264/AVC, the prediction mode of inter-coding frame supports motion compensation technology with multiple block sizes in which inter-frame prediction mode indicate the block size information, such as 16&#xd7;16, 16&#xd7;18, 8&#xd7;8, etc.</p>
<p id="p-0064" num="0071">In H.264/AVC, inter picture may have blocks with intra prediction mode. The small block size mode may refer to modes of block size 8&#xd7;8 or below in H.264/AVC. The large block size mode may refer to modes of block size 8&#xd7;8 or above in H.264/AVC.</p>
<p id="p-0065" num="0072">Embodiments of the present invention will be described below, assuming that neighbor blocks of E are edge blocks directly adjacent to E.</p>
<p id="p-0066" num="0073">I. If the Current Frame is I Frame, Intra Quantization Mode Decision is Used.</p>
<p id="p-0067" num="0074">For an intra-coding image, prediction modes of image details are usually more complex than those of undetailed parts of image, and mode information on neighbor blocks is generally different. Therefore, if prediction modes of neighbor macroblocks are different, high quality quantization modes should be used. In <figref idref="DRAWINGS">FIG. 1</figref>, low quality quantization modes can be selected only if prediction mode information on neighbor macroblocks meets one of the following conditions:
<ul id="ul0007" list-style="none">
    <li id="ul0007-0001" num="0000">
    <ul id="ul0008" list-style="none">
        <li id="ul0008-0001" num="0075">(1) neighbor blocks in vertical edge of E have identical prediction mode, e.g. block A and D have identical modes, in which the prediction modes are identical means for example they are both of the horizontal direction prediction mode;</li>
        <li id="ul0008-0002" num="0076">(2) neighbor blocks in horizontal edge of E have identical prediction mode, e.g. blocks B (or C) and D have identical modes, in which the prediction modes are identical means for example they are both of the horizontal direction prediction mode.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0068" num="0077">For <figref idref="DRAWINGS">FIG. 1</figref>, with reference to the above-described quantization modes 0, 1 and 2, quantization modes used for the current block E may be determined in the following decision conditions:
<ul id="ul0009" list-style="none">
    <li id="ul0009-0001" num="0000">
    <ul id="ul0010" list-style="none">
        <li id="ul0010-0001" num="0078">(1) If one of A, B, C and D is of non-directional prediction mode or average (DC) prediction mode, quantization mode 0, namely the default quantization mode, is used for E;</li>
        <li id="ul0010-0002" num="0079">(2) If neighbor blocks in a vertical edge of E have the identical prediction mode, e.g. block A and D have the identical prediction mode, e.g. horizontal direction prediction mode, or neighbor blocks in horizontal edge of E have identical prediction mode, e.g. block B and D or block B and C have identical modes, quantization mode 0, namely the default quantization mode, is used for E; and</li>
        <li id="ul0010-0003" num="0080">(3) If A, B, C and D do not satisfy the above conditions (1) and (2), the quantization mode 1, namely the quantization mode that preserves details, is used for E.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0069" num="0081">II. If the Current Frame is a P Frame, Inter Quantization Mode Decision is Used.</p>
<p id="p-0070" num="0082">In <figref idref="DRAWINGS">FIG. 1</figref>, low quality quantization modes can be selected only if mode information on neighbor macro blocks satisfies one of the following conditions:
<ul id="ul0011" list-style="none">
    <li id="ul0011-0001" num="0000">
    <ul id="ul0012" list-style="none">
        <li id="ul0012-0001" num="0083">(1) No one of neighbor blocks A, B, C and D are of intra-coding blocks;</li>
        <li id="ul0012-0002" num="0084">(2) No one of neighbor blocks A, B, C and D are of blocks with small size prediction modes;</li>
        <li id="ul0012-0003" num="0085">(3) Neighbor blocks in vertical edge of E have identical modes, e.g. A and D have identical modes, both of which are of blocks with large size prediction modes; and</li>
        <li id="ul0012-0004" num="0086">(4) Neighbor blocks in horizontal edge of E have identical modes, e.g. B (or C) and D have identical modes, both of which are of blocks with large size prediction modes.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0071" num="0087">Still referring to the above-mentioned three quantization modes 0, 1 and 2, in P frames, quantization mode 0 is defaulted to be used for the current block E, and quantization decision conditions for the current block E include:
<ul id="ul0013" list-style="none">
    <li id="ul0013-0001" num="0000">
    <ul id="ul0014" list-style="none">
        <li id="ul0014-0001" num="0088">(1) if one of A, B, C and D is an intra-coding block, quantization mode 1, namely the details preserved mode, is used for E;</li>
        <li id="ul0014-0002" num="0089">(2) if one of A, B, C and D is a small size block, such as a 4&#xd7;4 block or a 8&#xd7;8 block, quantization mode 1, namely the details preserved mode, is used for E;</li>
        <li id="ul0014-0003" num="0090">(3) if neighbor blocks in vertical edge and/or neighbor blocks in horizontal edges are of skip mode, for example, blocks A and D in vertical neighbor edges are of skip mode and /or blocks B (or C) and D in horizontal neighbor edge are of skip mode, which indicates that the prediction mode of the current block E may also be skip mode and indicates that the current block E just corresponds to a coding block in the previous frame but it cannot be determined whether the current block E belongs to details of the image, and thus quantization mode 0, namely the default quantization mode, is used for E; and quantization mode 1, namely the details preserved mode, is also used for E;</li>
        <li id="ul0014-0004" num="0091">(4) if neighbor blocks in a vertical edge or neighbor blocks in a horizontal edge are of blocks with large block size prediction modes, for example, A and D are of block size 16&#xd7;16 prediction modes or B and D are of block size 16&#xd7;16 prediction modes or B and C are of block size 16&#xd7;16 prediction modes, which indicates that the current block E is likely to be a undetailed region, and thus quantization mode 2, namely the quantization mode that does not preserve details, is used for E; and</li>
        <li id="ul0014-0005" num="0092">(5) if A, B, C and D do not satisfy the above conditions (1)-(4), the quantization mode 1, namely the quantization mode that preserves details, is used for E.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0072" num="0093">Similarly, if the current frame is a B frame, inter-quantization mode decision is also used for the current block E, and specific decision conditions are as follows:
<ul id="ul0015" list-style="none">
    <li id="ul0015-0001" num="0000">
    <ul id="ul0016" list-style="none">
        <li id="ul0016-0001" num="0094">(1) if neighbor blocks in the vertical edge and/or neighbor blocks in the horizontal edge are of skip modes, for example, blocks A and D in the vertical neighbor edge are of skip modes and/or blocks B (or C) and D in the horizontal neighbor edge are of skip modes, quantization mode 0, namely the default quantization mode, is used for E;</li>
        <li id="ul0016-0002" num="0095">(2) if neighbor blocks in vertically edge and/or neighbor blocks in horizontal edge are of block size 16&#xd7;16 prediction modes, for example, blocks A and D in the vertical neighbor edge are of 16&#xd7;16 modes and/or blocks B (or C) and D in with the horizontally neighbor edge are of block size 16&#xd7;16 prediction modes, quantization mode 2, namely the quantization mode that does not preserve details, is used for E; and</li>
        <li id="ul0016-0003" num="0096">(3) if A, B, C and D do not satisfy the above conditions (1) and (2), the quantization mode 1, namely the quantization mode that preserves details, is used for E.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0073" num="0097">Embodiments of the present invention also provide an apparatus for realizing adaptive quantization in the procedure of image/video coding, which decides quantization modes for the current block according to coding parameter information on neighbor blocks. As shown in <figref idref="DRAWINGS">FIG. 2</figref>, the specific implementation structure of the apparatus includes:</p>
<p id="p-0074" num="0098">a neighbor block parameter information acquiring unit, adapted to acquire parameter information on neighbor blocks of the current block, wherein the neighbor block parameter information acquiring unit is embodied as a neighbor macroblock coding information extractor;</p>
<p id="p-0075" num="0099">a quantization mode decision unit, adapted to determine the quantization mode for the current block according to parameter information on neighbor blocks acquired by the neighbor block parameter information extractor; wherein the quantization mode decision unit specifically includes an intra-coding quantization mode decision subunit, an inter-coding quantization mode decision subunit, and a block quantization mode determining unit adapted to determine quantization mode of the current coding block; and</p>
<p id="p-0076" num="0100">a quantization processing unit, adapted to perform quantization computation on transform coefficients of the current block with the quantization mode determined by the quantization mode decision unit, wherein the quantization processing unit may specifically be a quantizer in encoder or a dequantizer in decoder.</p>
<p id="p-0077" num="0101">The apparatus may also comprise a frame type classifier for acquiring the type of the current coding frame and notifying the type to the quantization mode decision unit.</p>
<p id="p-0078" num="0102">Individual description of the parts specifically contained in the quantization mode decision apparatus will be given below.</p>
<p id="p-0079" num="0103">The frame type classifier is adapted to acquire the type of frame which the current coding block belongs, such as I frame, P frame or B frame, and notify frame type information to the intra-coding quantization mode decision subunit and the inter-coding quantization mode decision subunit. Different frame types would have different influence on the choice of the decision subunit and the result of quantization mode decision.</p>
<p id="p-0080" num="0104">The neighbor block coding information extractor is adapted to acquire parameter information on neighbor blocks of the current coding block, in which acquiring parameter information includes at least one of the block partition information of neighbor blocks, prediction information of neighbor blocks, motion vector information of neighbor blocks and information of the quantization mode used for neighbor blocks, and notify this parameter information to the intra-coding quantization mode decision subunit and the inter-coding quantization mode decision subunit in order to select different sub-decision unit in the quantization mode decision unit according to different coding parameter information of neighbor blocks.</p>
<p id="p-0081" num="0105">The intra-coding quantization mode decision subunit is adapted to decide the quantization mode for the current coding block according to the frame type and the acquired intra-coding information of neighbor blocks, wherein the acquired intra-coding information may include block size, block prediction mode, direction of intra-block prediction, and quantization mode information, etc. Typically, the intra-quantization mode decision subunit may be divided into a neighbor block size based sub-decision unit, a neighbor block type based sub-decision unit and/or a neighbor block intra-prediction mode (or direction) based sub-decision unit according to the input intra-coding information of neighbor blocks.</p>
<p id="p-0082" num="0106">The inter-coding quantization mode decision subunit is adapted to decide the quantization mode for the current coding block according to the frame type and the acquired inter-coding information of neighbor blocks, wherein the acquired inter-coding information may include block size, block mode, inter-prediction direction information of block, direction of block motion vector, magnitude of block motion vector, quantization mode information of neighbor blocks, etc. Typically, the inter-quantization mode decision subunit may be divided into a neighbor block size based sub-decision unit, a neighbor block type based sub-decision unit, a neighbor block inter-prediction mode (or direction) based sub-decision unit and/or a neighbor block motion vector based sub-decision unit according to the input inter-coding information of neighbor blocks.</p>
<p id="p-0083" num="0107">The block quantization mode determining unit is to finally select and determine a quantization mode for the current coding block as one of several quantization modes, according to the output from the intra-coding quantization mode decision subunit or the inter-coding quantization mode decision subunit, input the determined quantization mode to the quantization processor so as to update parameter values of the quantization processor, and thus control the quantization quality of quantization processor. The quantization modes may be of various types, and typical modes may include a quantization mode that preserves image details, a quantization mode that does not preserve image details, and a quantization mode which is the same as the default quantization mode.</p>
<p id="p-0084" num="0108">The operation principle of the quantization mode decision device is to select a proper quantization mode decision subunit according to frame type and coding parameter information of neighbor blocks, and</p>
<p id="p-0085" num="0109">(1) In case of an intra-coding image, the intra-quantization mode decision subunit is selected, and according to intra-coding information of neighbor blocks, one sub-decision unit is further selected among the neighbor block size based sub-decision unit, the neighbor block type based sub-decision unit and the neighbor block intra-prediction mode (or direction) based sub-decision unit, and then the mode decision result is output, wherein</p>
<p id="p-0086" num="0110">the neighbor block size based sub-decision unit is adapted to decide quantization mode according to neighbor block size information;</p>
<p id="p-0087" num="0111">the neighbor block type based sub-decision unit is adapted to decide quantization mode according to neighbor block type information; and</p>
<p id="p-0088" num="0112">the neighbor block intra prediction mode based sub-decision unit is adapted to decide quantization mode according to neighbor block intra-prediction mode information.</p>
<p id="p-0089" num="0113">(2) In case of an inter-coding image, either the intra-quantization mode decision subunit or the inter-quantization mode decision subunit is selected for current block according to the block type of the current coding block.</p>
<p id="p-0090" num="0114">(21) If intra coding is used for the current block, the intra quantization mode decision subunit is used and the structure thereof may be same as (1). However, the input coding information of neighbor blocks to be used may be different from (1). When the intra-quantization mode decision subunit is used for inter-coding images, the coding parameter information on neighbor blocks may include more types, for example, sizes of neighbor blocks may include 8&#xd7;8, 16&#xd7;8, 8&#xd7;16, and 16&#xd7;16.</p>
<p id="p-0091" num="0115">(22) If inter-coding is used for the current block, the inter-quantization mode decision subunit is used. Specifically, one sub-decision unit is selected among a neighbor block size based sub-decision unit, a neighbor block type based sub-decision unit, a neighbor block intra-prediction mode (or direction) based sub-decision unit, a neighbor block motion vector based sub-decision unit according to inter-coding information of neighbor blocks, and then the quantization mode decision result is output; wherein</p>
<p id="p-0092" num="0116">the neighbor block size based sub-decision unit is adapted to decide quantization mode according to neighbor block size information;</p>
<p id="p-0093" num="0117">the neighbor block type based sub-decision unit is adapted to decide quantization mode according to neighbor block type information;</p>
<p id="p-0094" num="0118">the neighbor block inter prediction mode based sub-decision unit is adapted to decide quantization mode according to neighbor block inter prediction mode information; and</p>
<p id="p-0095" num="0119">the neighbor block motion vector based sub-decision unit is adapted to decide quantization mode according to motion vector information on neighbor blocks.</p>
<p id="p-0096" num="0120">In summary, the quantization mode decision solution provided in embodiments of the present invention has the following advantages.</p>
<p id="p-0097" num="0121">With this quantization mode decision method, adaptive quantization based on macroblock size may be achieved in image coding, that is, each macroblock may have individual quantization mode.</p>
<p id="p-0098" num="0122">Further, for image coding at block level, while achieving block level adaptive quantization, no quantization matrix needs to be transferred in the image level and macroblock level bit stream, which results in that no additional bit stream overhead exists for the block level.</p>
<p id="p-0099" num="0123">In addition, embodiments of the present invention are implemented according to vision features of human eyes. Thus, during coding procedure, quantization computation of image sequence can be well adapted to image content, and thus subjective quality of the compressed image may be greatly improved at the same bit rate.</p>
<p id="p-0100" num="0124">The above description is only preferred embodiments of the present invention, and the scope of the present invention is not limited thereto. Any variations or replacements easily occur to those skilled in the art in the technical scope disclosed by the present invention, and these variations or replacements should be covered by the scope of the present invention. Therefore, the protection scope of the present invention should be defined by the claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625917-20140107-M00001.NB">
<img id="EMI-M00001" he="7.03mm" wi="76.20mm" file="US08625917-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for realizing adaptive quantization in an image encoding process, wherein an image to be encoded is divided into one or more blocks, and each block is transformed to obtain one or more transform coefficients, the method comprising:
<claim-text>acquiring information of neighboring blocks of a current block;</claim-text>
<claim-text>determining a quantization mode for the current block according to the information of the neighboring blocks, wherein the determined quantization mode for the current block corresponds to a level of quantization quality of the current block; and</claim-text>
<claim-text>quantizing the transform coefficients of the current block in the determined quantization mode.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the information of the neighboring blocks, based on which the quantizing mode of the current block is determined, comprises one or more of the following:
<claim-text>frame type information,</claim-text>
<claim-text>block mode information of the neighboring blocks,</claim-text>
<claim-text>prediction information of the neighboring blocks,</claim-text>
<claim-text>motion vector information of the neighboring blocks, and</claim-text>
<claim-text>information of quantization modes used by the neighboring blocks.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein an individual quantization mode with a respective quantization quality is determined for each block of the image.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a quantization mode is in a form of:
<claim-text>a quantization matrix, a quantization parameter or a quantization step.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A method for realizing adaptive quantization in an image decoding process, wherein an image is divided into one or more blocks, each block is transformed to obtain one or more transform coefficients, transform coefficients of a current block are quantized under a quantizing mode that corresponds to a level of quantizing quality of the current block, and a bit stream carries the quantized transform coefficients, the method comprising:
<claim-text>obtaining the quantized transform coefficients of the current block from the bit stream;</claim-text>
<claim-text>determining the quantizing mode used in quantizing the transform coefficients of the current block; and</claim-text>
<claim-text>dequantizing the quantized transform coefficients of the current block in the determined quantization mode,</claim-text>
<claim-text>wherein the quantizing mode is determined by:</claim-text>
<claim-text>acquiring parameter information of neighboring blocks of the current block; and</claim-text>
<claim-text>determining the quantization mode for the current block according to the information of the neighboring blocks, wherein the determined quantization mode for the current block corresponds to a level of quantization quality of the current block.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the information of the neighboring blocks, based on which the quantizing mode of the current block is determined, comprises one or more of the following:
<claim-text>frame type information,</claim-text>
<claim-text>block mode information of the neighboring blocks,</claim-text>
<claim-text>prediction information of the neighboring blocks,</claim-text>
<claim-text>motion vector information of the neighboring blocks, and</claim-text>
<claim-text>information of quantization modes used by the neighboring blocks.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein an individual quantization mode with respective quantization quality is determined for each block of the image.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein a quantization mode is in a form of:
<claim-text>a quantization matrix, a quantization parameter or a quantization step.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. An apparatus for realizing adaptive quantization in an image encoding process, wherein an image to be encoded is divided into one or more blocks, and each block is transformed to obtain one or more transform coefficients, comprising:
<claim-text>a neighboring block information extractor, configured to acquire information of neighboring blocks of a current block;</claim-text>
<claim-text>a quantization mode decision unit, configured to determine a quantization mode for the current block according to the information of the neighboring blocks, wherein the determined quantization mode for the current block corresponds to a level of quantization quality of the current block; and</claim-text>
<claim-text>a quantization processing unit, configured to quantizing the transform coefficients of the current block in the determined quantization mode.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the information of the neighboring blocks, based on which the quantizing mode of the current block is determined, comprises one or more of the following:
<claim-text>frame type information,</claim-text>
<claim-text>block mode information of the neighboring blocks,</claim-text>
<claim-text>prediction information of the neighboring blocks,</claim-text>
<claim-text>motion vector information of the neighboring blocks, and</claim-text>
<claim-text>quantization modes used by the neighboring blocks.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein an individual quantization mode with respective quantization quality is determined for each block of the image.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein a quantization mode is in a form of:
<claim-text>a quantization matrix, a quantization parameter or a quantization step.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. An apparatus for realizing adaptive quantization in an image decoding process, wherein an image is divided into one or more blocks, each block is transformed to obtain one or more transform coefficients, transform coefficients of a current block are quantized under a quantizing mode that corresponds to a level of quantizing quality of the current block, and a bit stream carries the quantized transform coefficients, the apparatus comprising:
<claim-text>a transform coefficients extractor, configured to acquire the quantized transform coefficients of the current block from the bit stream;</claim-text>
<claim-text>a quantization mode decision unit, configured to determine the quantization mode used in quantizing the transform coefficients of the current block; and</claim-text>
<claim-text>a dequantization processing unit, configured to dequantizing the quantized transform coefficients of the current block in the determined quantization mode;</claim-text>
<claim-text>wherein the quantizing mode is determined by:</claim-text>
<claim-text>acquiring parameter information of neighboring blocks of the current block; and</claim-text>
<claim-text>determining the quantization mode for the current block according to the information of the neighboring blocks, wherein the determined quantization mode for the current block corresponds to a level of quantization quality of the current block.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the information of the neighboring blocks, based on which the quantizing mode of the current block is determined, comprises one or more of the following:
<claim-text>frame type information,</claim-text>
<claim-text>block mode information of the neighboring blocks,</claim-text>
<claim-text>prediction information of the neighboring blocks,</claim-text>
<claim-text>motion vector information of the neighboring blocks, and</claim-text>
<claim-text>quantization modes used by the neighboring blocks.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein an individual quantization mode with respective quantization quality is determined for each block of the image.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein a quantization mode is in a form of:
<claim-text>a quantization matrix, a quantization parameter or a quantization step.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. An image encoder, comprising a non-transitory computer readable storage medium storing program codes for realizing adaptive quantization, wherein an image to be encoded is divided into one or more blocks, and each block is transformed to obtain one or more transform coefficients, and wherein the program codes comprise:
<claim-text>a neighboring block information module for obtaining information of neighboring blocks of a current block;</claim-text>
<claim-text>a quantization mode decision module for determining a quantization mode for the current block according to the information of the neighboring blocks, wherein the determined quantization mode for the current block corresponds to a level of quantization quality of the current block; and</claim-text>
<claim-text>a quantizing module for quantizing the transform coefficients of the current block in the determined quantization mode.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The image encoder of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the information of the neighboring blocks, based on which the quantizing mode is determined, comprises one or more of the following:
<claim-text>frame type information,</claim-text>
<claim-text>block mode information of the neighboring blocks,</claim-text>
<claim-text>prediction information of the neighboring blocks,</claim-text>
<claim-text>motion vector information of the neighboring blocks, and</claim-text>
<claim-text>quantization modes used by the neighboring blocks.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The image encoder of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein an individual quantization mode with respective quantization quality is determined for each block of the image.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The image encoder of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein a quantization mode is in a form of:
<claim-text>a quantization matrix, a quantization parameter or a quantization step.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The image encoder of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the quantization mode for the current block is determined in an intra-frame or inter-frame manner.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. An image decoder, comprising a non-transitory computer readable storage medium storing program codes for realizing adaptive dequantization, wherein an image is divided into one or more blocks, each block is transformed to obtain one or more transform coefficients, transform coefficients of a current block are quantized under a quantizing mode that corresponds to a level of quantizing quality of the current block, and a bit stream carries the quantized transform coefficients, and wherein the program codes comprise:
<claim-text>a transform coefficients module for acquiring the quantized transform coefficients of the current block from the bit stream;</claim-text>
<claim-text>a quantization mode decision module for determining the quantization mode used in quantizing the transform coefficients of the current block; and</claim-text>
<claim-text>a dequantizing module for dequantizing the quantized transform coefficients of the current block in the determined quantization mode,</claim-text>
<claim-text>wherein the quantizing mode is determined by:</claim-text>
<claim-text>acquiring parameter information of neighboring blocks of the current block; and</claim-text>
<claim-text>determining the quantization mode for the current block according to the information of the neighboring blocks, wherein the determined quantization mode for the current block corresponds to a level of quantization quality of the current block.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The image decoder of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the information of the neighboring blocks comprises one or more of the following:
<claim-text>frame type information,</claim-text>
<claim-text>block mode information of the neighboring blocks,</claim-text>
<claim-text>prediction information of the neighboring blocks,</claim-text>
<claim-text>motion vector information of the neighboring blocks, and</claim-text>
<claim-text>information of quantization modes used by the neighboring blocks.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The video decoder of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein an individual quantization mode with respective quantization quality is determined for each block of the image.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The image decoder of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein a quantization mode is in a form of:
<claim-text>a quantization matrix, a quantization parameter or a quantization step.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The image decoder of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the quantization mode for the current block is determined in an intra-frame or inter-frame manner.</claim-text>
</claim>
</claims>
</us-patent-grant>
