<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625019-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625019</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13312344</doc-number>
<date>20111206</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2008-148323</doc-number>
<date>20080605</date>
</priority-claim>
<priority-claim sequence="02" kind="national">
<country>JP</country>
<doc-number>2009-065222</doc-number>
<date>20090317</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>12</us-term-extension>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>222</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>03</class>
<subclass>B</subclass>
<main-group>13</main-group>
<subgroup>02</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>34833303</main-classification>
<further-classification>348346</further-classification>
<further-classification>396374</further-classification>
</classification-national>
<invention-title id="d2e92">Image processing and displaying apparatus, control method, and program for displaying a predetermined image</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5041911</doc-number>
<kind>A</kind>
<name>Moorman</name>
<date>19910800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348364</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5164836</doc-number>
<kind>A</kind>
<name>Jackson et al.</name>
<date>19921100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348364</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5877809</doc-number>
<kind>A</kind>
<name>Omata et al.</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348345</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6535245</doc-number>
<kind>B1</kind>
<name>Yamamoto</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7129980</doc-number>
<kind>B1</kind>
<name>Ashida</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833304</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7397968</doc-number>
<kind>B2</kind>
<name>Stavely et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382274</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7773132</doc-number>
<kind>B2</kind>
<name>Ozaki</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348241</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7982791</doc-number>
<kind>B2</kind>
<name>Nonaka et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833301</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2002/0154829</doc-number>
<kind>A1</kind>
<name>Tsukioka</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2006/0192878</doc-number>
<kind>A1</kind>
<name>Miyahara et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833301</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2007/0177036</doc-number>
<kind>A1</kind>
<name>Kawada</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348239</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2008/0309811</doc-number>
<kind>A1</kind>
<name>Fujinawa et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833301</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2009/0231454</doc-number>
<kind>A1</kind>
<name>Miura</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2009/0251568</doc-number>
<kind>A1</kind>
<name>Nakatani</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348234</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2010/0097513</doc-number>
<kind>A1</kind>
<name>Takada et al.</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833303</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2010/0157129</doc-number>
<kind>A1</kind>
<name>Lee</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833304</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2010/0259635</doc-number>
<kind>A1</kind>
<name>Oka</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>JP</country>
<doc-number>04328964</doc-number>
<kind>A</kind>
<date>19921100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>JP</country>
<doc-number>05260351</doc-number>
<kind>A</kind>
<date>19931000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>JP</country>
<doc-number>2003250067</doc-number>
<kind>A</kind>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>JP</country>
<doc-number>2005078002</doc-number>
<kind>A</kind>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>JP</country>
<doc-number>2007266680</doc-number>
<kind>A</kind>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>JP</country>
<doc-number>2008054115</doc-number>
<kind>A</kind>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>JP</country>
<doc-number>2008099159</doc-number>
<kind>A</kind>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
</us-references-cited>
<number-of-claims>14</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>34833301</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34833302</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34833303</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34833304</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34833312</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348346</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>396374</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>18</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12472056</doc-number>
<date>20090526</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8089550</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13312344</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120075517</doc-number>
<kind>A1</kind>
<date>20120329</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ohyama</last-name>
<first-name>Nana</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Ohyama</last-name>
<first-name>Nana</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Cowan, Liebowitz &#x26; Latman, P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Canon Kabushiki Kaisha</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Villecco</last-name>
<first-name>John</first-name>
<department>2661</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An image sensing apparatus includes a display control unit which controls an image display unit to display an image sensed by an image input unit. The image sensing apparatus also includes a signal strength detection unit which detects, from a sensed image, a luminance area formed from pixels satisfying a predetermined luminance condition, and a special area detection unit which detects, from a sensed image, an object area having a predetermined feature. The display control unit controls the image display unit to display an assist image representing the luminance area, so as to be superimposed on a sensed image. The display control unit changes the assist image display method between a case in which the luminance area and object area overlap each other and a case in which they do not overlap each other.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="140.38mm" wi="172.30mm" file="US08625019-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="201.17mm" wi="180.51mm" orientation="landscape" file="US08625019-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="223.27mm" wi="149.61mm" orientation="landscape" file="US08625019-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="220.05mm" wi="99.57mm" file="US08625019-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="229.87mm" wi="183.30mm" file="US08625019-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="176.95mm" wi="154.94mm" orientation="landscape" file="US08625019-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="235.12mm" wi="168.57mm" orientation="landscape" file="US08625019-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="222.17mm" wi="184.32mm" file="US08625019-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="179.41mm" wi="107.61mm" orientation="landscape" file="US08625019-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="195.58mm" wi="174.16mm" orientation="landscape" file="US08625019-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="220.47mm" wi="172.38mm" file="US08625019-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="173.14mm" wi="176.95mm" file="US08625019-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="176.61mm" wi="108.63mm" orientation="landscape" file="US08625019-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application is a continuation of application Ser. No. 12/472,056, filed May 26, 2009, the entire disclosure of which is hereby incorporated by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to an image sensing apparatus, control method thereof, and program.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">Conventional image sensing apparatuses such as a digital video camera include a display unit for displaying a sensed image, such as an electronic viewfinder (EVF) or liquid crystal display monitor. The user of the image sensing apparatus performs exposure control and focus adjustment while visually checking a sensed image on the display unit. However, most display units in image sensing apparatuses are small in size and are not suitable for strict exposure control and focus adjustment in terms of the display resolution.</p>
<p id="p-0007" num="0006">From this, some image sensing apparatuses assist user operation by displaying an image for assisting exposure control or one for assisting focus adjustment, so as to be superimposed on a sensed image on the display unit. An example of the exposure control assist image is a pattern image (to be referred to as a &#x201c;zebra pattern&#x201d; hereinafter) of hatched lines, which is called a zebra pattern signal generated based on a preset luminance value and the luminance range of a sensed image. An example of the focus adjustment assist image is an edge image which is generated by extracting edge signals from a sensed image and is displayed to be superimposed on a sensed image. The edge image is obtained by edge correction, peaking, or enhancer (to be referred to as &#x201c;peaking&#x201d; hereinafter).</p>
<p id="p-0008" num="0007">For example, the luminance range for displaying a zebra pattern is set to a predetermined level convenient for stop adjustment. In general, the threshold of the luminance level or a predetermined luminance level range is set according to IRE (Institute of Radio Engineers) used as a unit representing the amplitude of a video signal. The IRE has originally meant a standard defined by the Institute of Radio Engineers (IEEE now), and is generally used as a unit. The IRE defines pedestal level (black)=0 IRE (in some cases, black=7.5 IRE), and white=100 IRE. When manually controlling the exposure, the luminance level range is set to about 70 IRE to 100 IRE. A zebra pattern is displayed to be superimposed in the image area of a sensed image that corresponds to this range.</p>
<p id="p-0009" num="0008">Japanese Patent Laid-Open No. 4-328964 is known as a technique of displaying an assist image to be superimposed on a sensed image. Japanese Patent Laid-Open No. 4-328964 discloses a technique of displaying zebra patterns in the image area of a signal strength equal to or higher than an arbitrary luminance level and that of a signal strength equal to or lower than the luminance level.</p>
<p id="p-0010" num="0009">The conventional technique simply displays an assist image to be superimposed in an image area or in-focus area which satisfies a predetermined luminance condition. This technique displays an assist image even in an image area overlapping an object area having a predetermined feature, like a face area containing a person's face.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0011" num="0010">The present invention provides an image sensing apparatus which displays an assist image while discriminating an area overlapping an object area having a predetermined feature from the remaining area when displaying the assist image in an image area or in-focus area which satisfies a predetermined luminance condition, a control method thereof, and a program.</p>
<p id="p-0012" num="0011">According to the first aspect of the present invention, there is provided an image sensing apparatus comprising an image sensing unit, a display unit, a display control unit which controls the display unit to display an image sensed by the image sensing unit, a first detection unit which detects, from the sensed image, a luminance area formed from pixels satisfying a predetermined luminance condition, and a second detection unit which detects an object area having a predetermined feature from the sensed image, wherein the display control unit controls the display unit to display an assist image representing the luminance area detected by the first detection unit, so as to be superimposed on the sensed image, and changes a display method of the assist image between a case in which the luminance area and the object area detected by the second detection unit overlap each other, and a case in which the luminance area and the object area do not overlap each other.</p>
<p id="p-0013" num="0012">According to the second aspect of the present invention, there is provided an image sensing apparatus comprising an image sensing unit, a display unit, a display control unit which controls the display unit to display an image sensed by the image sensing unit, a first detection unit which detects, from the sensed image, a luminance area formed from pixels satisfying a predetermined luminance condition, and a second detection unit which detects an object area having a predetermined feature from the sensed image, wherein the display control unit controls the display unit to display a pattern display representing the luminance area detected by the first detection unit, so as to be superimposed on the sensed image, and changes a display method of the pattern display between a case in which the luminance area and the object area detected by the second detection unit overlap each other, and a case in which the luminance area and the object area do not overlap each other.</p>
<p id="p-0014" num="0013">According to the third aspect of the present invention, there is provided an image sensing apparatus comprising an image sensing unit, a display unit, a display control unit which controls the display unit to display an image sensed by the image sensing unit, a first detection unit which detects an edge of an object from the sensed image, and a second detection unit which detects an object area containing a person's face from the sensed image, wherein the display control unit controls the display unit to display a peaking display representing the edge of the object detected by the first detection unit, so as to be superimposed on the sensed image, and changes a display method of the peaking display between a case in which the edge of the object falls within the object area detected by the second detection unit, and a case in which the edge of the object falls outside the object area.</p>
<p id="p-0015" num="0014">According to the fourth aspect of the present invention, there is provided a method of controlling an image sensing apparatus having an image sensing unit and a display unit, the method comprising a display control step of controlling the display unit to display an image sensed by the image sensing unit, a first detection step of detecting, from the sensed image, a luminance area formed from pixels satisfying a predetermined luminance condition, and a second detection step of detecting an object area having a predetermined feature from the sensed image, wherein in the display control step, the display unit is controlled to display an assist image representing the luminance area, so as to be superimposed on the sensed image, and a display method of the assist image is changed between a case in which the luminance area and the object area overlap each other and a case in which the luminance area and the object area do not overlap each other.</p>
<p id="p-0016" num="0015">According to the fifth aspect of the present invention, there is provided a method of controlling an image sensing apparatus having an image sensing unit and a display unit, the method comprising a display control step of controlling the display unit to display an image sensed by the image sensing unit, a first detection step of detecting, from the sensed image, a luminance area formed from pixels satisfying a predetermined luminance condition, and a second detection step of detecting an object area having a predetermined feature from the sensed image, wherein in the display control step, the display unit is controlled to display a pattern display representing the luminance area, so as to be superimposed on the sensed image, and a display method of the pattern display is changed between a case in which the luminance area and the object area overlap each other and a case in which the luminance area and the object area do not overlap each other.</p>
<p id="p-0017" num="0016">According to the sixth aspect of the present invention, there is provided a method of controlling an image sensing apparatus having an image sensing unit and a display unit, the method comprising a display control step of controlling the display unit to display an image sensed by the image sensing unit, a first detection step of detecting an edge of an object from the sensed image, and a second detection step of detecting an object area containing a person's face from the sensed image, wherein in the display control step, the display unit is controlled to display a peaking display representing the edge of the object, so as to be superimposed on the sensed image, and a display method of the peaking display is changed between a case in which the edge of the object falls within the object area, and a case in which the edge of the object falls outside the object area.</p>
<p id="p-0018" num="0017">Further features of the present invention will become apparent from the following description of exemplary embodiments with reference to the attached drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram schematically showing the functional arrangement of an image sensing apparatus according to the present invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram showing the schematic arrangement of a display control unit;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 3A</figref> is a conceptual view showing a luminance condition associated with zebra pattern display;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 3B</figref> is a conceptual view showing a luminance condition associated with zebra pattern display;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 3C</figref> is a conceptual view showing luminance condition associated with zebra pattern display;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart showing the display control operation of an image sensing apparatus according to the first embodiment;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 5</figref> is a conceptual view exemplifying a sensed image displayed on an image display unit in the first embodiment;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 6A</figref> is a conceptual view exemplifying a sensed image displayed on an image display unit in the second embodiment;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 6B</figref> is a conceptual view exemplifying a sensed image displayed on the image display unit in the second embodiment;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 6C</figref> is a conceptual view exemplifying a sensed image displayed on the image display unit in the second embodiment;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 6D</figref> is a conceptual view exemplifying a sensed image displayed on the image display unit in the second embodiment;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 6E</figref> is a conceptual view exemplifying a sensed image displayed on the image display unit in the second embodiment;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart showing the display control operation of an image sensing apparatus according to the third embodiment;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 8</figref> is a conceptual view exemplifying a sensed image displayed on an image display unit in the third embodiment;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 9</figref> shows conceptual views exemplifying the correspondence between a special area and an assist image according to the fourth embodiment;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 10</figref> is a conceptual view exemplifying a sensed image displayed on an image display unit in the fourth embodiment;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 11</figref> is a flowchart showing the display control operation of an image sensing apparatus according to the fifth embodiment; and</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 12</figref> is a conceptual view exemplifying a sensed image displayed on an image display unit in the fifth embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DESCRIPTION OF THE EMBODIMENTS</heading>
<p id="p-0037" num="0036">Embodiments of the present invention will be described below with reference to the accompanying drawings, but the present invention is not limited to them. The following embodiments of the present invention are merely examples for embodying the invention, and may not be construed to limit the scope of the present invention.</p>
<p id="p-0038" num="0037">[First Embodiment]</p>
<p id="p-0039" num="0038">An image sensing apparatus according to the first embodiment of the present invention will be described in detail below with reference to <figref idref="DRAWINGS">FIGS. 1 to 5</figref>. <figref idref="DRAWINGS">FIG. 1</figref> is a block diagram schematically showing the functional arrangement of an image sensing apparatus <b>1</b> according to the present invention. <figref idref="DRAWINGS">FIG. 2</figref> is a block diagram showing the schematic arrangement of a display control unit <b>107</b>. <figref idref="DRAWINGS">FIGS. 3A to 3C</figref> are conceptual views showing luminance conditions associated with zebra pattern display. <figref idref="DRAWINGS">FIG. 4</figref> is a flowchart showing the display control operation of the image sensing apparatus <b>1</b>. <figref idref="DRAWINGS">FIG. 5</figref> is a conceptual view exemplifying a sensed image displayed on an image display unit <b>108</b>.</p>
<p id="p-0040" num="0039">As shown in <figref idref="DRAWINGS">FIG. 1</figref>, the image sensing apparatus <b>1</b> can record image data input from an image input unit <b>101</b> on a recording medium <b>109</b> such as an optical disk, hard disk, or semiconductor memory card. The image sensing apparatus <b>1</b> includes the image input unit <b>101</b>, an image processing unit <b>102</b>, a format control unit <b>103</b>, a recording/reading unit <b>104</b>, an operation unit <b>105</b>, a special area detection unit <b>106</b>, the display control unit <b>107</b>, the image display unit <b>108</b>, and a built-in memory <b>110</b>.</p>
<p id="p-0041" num="0040">The image input unit <b>101</b> is an image sensing unit formed from, e.g., a plurality of optical lenses and a CCD (Charge Coupled Device) or CMOS (Complementary Metal Oxide Semiconductor) image sensor. Data of an image (to be referred to as a &#x201c;sensed image&#x201d; hereinafter) input from the image input unit <b>101</b> is output to the image processing unit <b>102</b>. The image processing unit <b>102</b> performs predetermined image processing such as tone conversion for sensed image data output from the image input unit <b>101</b> or image data read out from the recording medium <b>109</b>.</p>
<p id="p-0042" num="0041">The format control unit <b>103</b> converts the data format of input data into a data format displayable on the image display unit <b>108</b> or a data format suited to recording on the recording medium <b>109</b>. For example, the format control unit <b>103</b> converts still image data into JPEG (Joint Photographic Experts Group) data and vice versa. Also, the format control unit <b>103</b> converts moving image data into an MPEG (Moving Picture Experts Group) moving image data and vice versa. The recording/reading unit <b>104</b> includes a module for read/write from/on a disk, and an interface for connecting a semiconductor memory card. The recording/reading unit <b>104</b> records data converted by the format control unit <b>103</b> on the recording medium <b>109</b>, and reads out data from the recording medium <b>109</b>.</p>
<p id="p-0043" num="0042">The operation unit <b>105</b> includes buttons and switches attached to the housing of the image sensing apparatus <b>1</b>. The operation unit <b>105</b> accepts an operation input from the user. More specifically, the operation unit <b>105</b> accepts a power operation with a power switch, an image sensing operation with a shutter switch, a function setting operation with a function switch for setting various functions, and a setting operation with a setting switch for adjusting image sensing conditions and the like.</p>
<p id="p-0044" num="0043">Based on preset special image information, the special area detection unit <b>106</b> serving as the second detection unit detects a special image area contained in image data processed by the image processing unit <b>102</b>. Examples of the special image area are a face area including a person's face, and an in-focus area. The special image information includes information necessary to specify a face in an image, and information necessary to extract an in-focus area. When detecting a face area, the special area detection unit <b>106</b> detects it from the image area of image data based on special image information containing a feature amount associated with a face image, such as the position of an eye, mouth, or nose, or the edge of a face. When detecting an in-focus area, the special area detection unit <b>106</b> detects it from the image area of image data based on special image information containing a contrast value serving as the criterion of the in-focus state. Upon receiving sensed image data from the image input unit <b>101</b>, the special area detection unit <b>106</b> detects an object area such as a face area or in-focus area contained in the sensed image. The special area detection unit <b>106</b> supplies information on the detection result to the display control unit <b>107</b>.</p>
<p id="p-0045" num="0044">The display control unit <b>107</b> serving as a display control unit controls the image display unit <b>108</b> to display an image output from the image processing unit <b>102</b> on the display screen of the image display unit <b>108</b>. More specifically, the display control unit <b>107</b> accepts image data of images successively sensed by the image input unit <b>101</b> from the image processing unit <b>102</b>, and controls the image display unit <b>108</b> to sequentially display them. The image display unit <b>108</b> is an electronic viewfinder (EVF) or liquid crystal display monitor. The image display unit <b>108</b> displays an image on the display screen under the control of the display control unit <b>107</b>. While checking images which are successively sensed by the image input unit <b>101</b> and sequentially displayed on the image display unit <b>108</b>, the user can use the operation unit <b>105</b> to perform adjustment of image sensing conditions (e.g., focus adjustment or exposure control) in the image input unit <b>101</b>.</p>
<p id="p-0046" num="0045">The built-in memory <b>110</b> is a RAM (Random Access Memory) or ROM (Read Only Memory). The built-in memory <b>110</b> provides a temporary storage for sensed image data, and a work area for performing a variety of data processes. The built-in memory <b>110</b> stores in advance program data to be executed by a CPU (Central Processing Unit), set conditions used for determination of the signal strength and the like, and various kinds of setting information such as special image information. The built-in memory <b>110</b> is organically connected to the CPU of the image sensing apparatus <b>1</b> and other functional blocks, details of which will be described with reference to <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0047" num="0046">In the image sensing apparatus <b>1</b>, the image processing unit <b>102</b> executes image processing for image data sensed by the image input unit <b>101</b>. The format control unit <b>103</b> converts the processed sensed image data into a format recordable on the recording medium <b>109</b>. The recording/reading unit <b>104</b> records, on the recording medium <b>109</b>, the sensed image data converted by the format control unit <b>103</b>. The special area detection unit <b>106</b> and display control unit <b>107</b> receive the sensed image data having undergone image processing by the image processing unit <b>102</b>. The special area detection unit <b>106</b> detects a special image area such as a face area or in-focus area from input sensed image data, and outputs the detection result to the display control unit <b>107</b>. The display control unit <b>107</b> controls the display of the image display unit <b>108</b> based on the sensed image data input from the image processing unit <b>102</b> and the detection result input from the special area detection unit <b>106</b>. The display control unit <b>107</b> controls the image display unit <b>108</b> to display an image on its display screen.</p>
<p id="p-0048" num="0047">The recording/reading unit <b>104</b> reads out image data recorded on the recording medium <b>109</b>, and the format control unit <b>103</b> converts the readout image data into a displayable/outputtable format. The image processing unit <b>102</b> performs image processing for the image data converted by the format control unit <b>103</b>. Then, the display control unit <b>107</b> receives the image data having undergone image processing, and controls the image display unit <b>108</b> to display it as an image on the display screen.</p>
<p id="p-0049" num="0048">The display control unit <b>107</b> will be explained in detail. As shown in <figref idref="DRAWINGS">FIG. 2</figref>, the display control unit <b>107</b> includes a signal strength detection unit <b>201</b> and special display control unit <b>202</b>. The display control unit <b>107</b> controls the screen display of the image display unit <b>108</b> under the control of a CPU <b>203</b>. The CPU <b>203</b> reads out program data stored in the built-in memory <b>110</b>, and comprehensively controls the respective units of the image sensing apparatus <b>1</b> in cooperation with the program data.</p>
<p id="p-0050" num="0049">The signal strength detection unit <b>201</b> serving as the first detection unit determines whether the signal strength of each pixel of image data input from the image processing unit <b>102</b> satisfies a condition set in advance in the built-in memory <b>110</b>. More specifically, based on image data input from the image processing unit <b>102</b>, the signal strength detection unit <b>201</b> detects at least one of a luminance area and in-focus area formed from pixels satisfying a predetermined luminance condition. In an image area containing pixels determined by the signal strength detection unit <b>201</b> to have a signal strength satisfying the set condition in an image based on image data input from the image processing unit <b>102</b>, the special display control unit <b>202</b> displays an assist image representing the determination result, so as to be superimposed in the image area.</p>
<p id="p-0051" num="0050">For example, when the signal strength of each pixel that is detected by the signal strength detection unit <b>201</b> is a signal strength associated with luminance, an assist image is displayed to be superimposed in an image area formed from pixels satisfying a set condition serving as a preset luminance condition. The assist image can be a zebra pattern image obtained by patterning a luminance area satisfying the set condition into a pattern image of hatched lines. That is, when displaying a sensed image on the image display unit <b>108</b>, the image sensing apparatus <b>1</b> displays a generated zebra pattern image to be superimposed in an area satisfying a preset luminance condition in accordance with the luminance level of each pixel in the sensed image. The user can control the exposure by referring to the zebra pattern image.</p>
<p id="p-0052" num="0051">The user may use the operation unit <b>105</b> to set the condition (e.g., luminance) in advance in the built-in memory <b>110</b> to display an assist image by superimposition. As for the set condition for the luminance, a lower limit luminance level <b>401</b> and upper limit luminance level <b>402</b> may also be set based on IRE or the like as a luminance range for displaying a zebra pattern, as shown in <figref idref="DRAWINGS">FIG. 3A</figref>. A luminance level <b>403</b> serving as a lower limit threshold for displaying a zebra pattern as shown in <figref idref="DRAWINGS">FIG. 3B</figref>, or a luminance level <b>404</b> serving as an upper limit threshold as shown in <figref idref="DRAWINGS">FIG. 3C</figref> may also be set based on IRE or the like.</p>
<p id="p-0053" num="0052">The signal strength detection unit <b>201</b> may also detect an in-focus area or the edge of an object by determining whether the signal strength difference between pixels of image data input from the image processing unit <b>102</b> satisfies a preset condition. For example, the in-focus area may also be detected according to a contrast method of determining, as an in-focus area, an image area where the contrast value is equal to or larger than a predetermined threshold. When detecting the edge of an object, the special display control unit <b>202</b> displays an assist image to be superimposed on an image displayed on the image display unit <b>108</b>, based on image data input from the image processing unit <b>102</b>. The assist image represents an edge based on the edge detection result of the signal strength detection unit <b>201</b>. The assist image representing the edge is an image used for performing peaking described above. This image represents an edge line based on the edge detection result by a predetermined line type (e.g., dotted line or bold line) and a predetermined display color. That is, when displaying a sensed image on the image display unit <b>108</b>, the image sensing apparatus <b>1</b> presents a peaking display extracted from a sensed image, so as to be superimposed on the sensed image. The user can adjust the focus by referring to the peaking display.</p>
<p id="p-0054" num="0053">Based on the detection result of the special area detection unit <b>106</b>, the special display control unit <b>202</b> controls the method of displaying an assist image such as a zebra pattern or peaking display to be superimposed on an image displayed on the image display unit <b>108</b>. More specifically, the special display control unit <b>202</b> changes the assist image display method between an area overlapping a special image area in the image area of image data, and an area overlapping no special image area. When displaying an assist image such as a zebra pattern or peaking display on the image display unit <b>108</b>, the image sensing apparatus <b>1</b> can display it while discriminating an area overlapping a special image area in a sensed image from an area overlapping no special image area.</p>
<p id="p-0055" num="0054">For example, when the special area detection unit <b>106</b> detects a face area as a special image area, the display method is changed between a zebra pattern or peaking display in the face area and a zebra pattern or peaking display in the remaining area. Similarly, when the special area detection unit <b>106</b> detects an in-focus area as a special image area, the display method is changed between a zebra pattern or peaking display in the in-focus area and a zebra pattern or peaking display in the remaining area. In this way, the image sensing apparatus <b>1</b> can display an assist image such as a pattern display (e.g., zebra pattern) or peaking display while changing the assist image display method between face and in-focus areas and the remaining image area.</p>
<p id="p-0056" num="0055">Processing by the display control unit <b>107</b> will be explained. This processing is executed as the display control operation of the image sensing apparatus <b>1</b> under the control of the CPU <b>203</b> according to the first embodiment. In the first embodiment, the image display unit <b>108</b> displays a zebra pattern to be superimposed on an image sensed by the image input unit <b>101</b>. A special image area detected by the special area detection unit <b>106</b> is a face area. When the sum of pixels to display an assist image in an object area detected by the special area detection unit <b>106</b> becomes equal to or larger than a predetermined ratio, the function of the present invention is enabled to change the assist image display method. For example, in the first embodiment, when the sum of pixels to display a zebra pattern in a face area detected by the special area detection unit <b>106</b> becomes equal to or larger than 50% of all pixels in the face area, the assist image display method is changed. For descriptive convenience, the first embodiment will be described on the premise that the sum of pixels to display an assist image in an object area detected by the special area detection unit <b>106</b> is always equal to or larger than a predetermined ratio (e.g., 50%).</p>
<p id="p-0057" num="0056">As shown in <figref idref="DRAWINGS">FIG. 4</figref>, when display control processing starts, the display control unit <b>107</b> determines whether the user has turned on the image sensing apparatus <b>1</b> with the power switch of the operation unit <b>105</b> or the like (S<b>101</b>). If the display control unit <b>107</b> determines that the user has turned on the image sensing apparatus <b>1</b> (YES in S<b>101</b>), it determines whether the user has set zebra pattern display ON with the setting switch of the operation unit <b>105</b> or the like (S<b>102</b>). If the display control unit <b>107</b> determines that the user has set zebra pattern display ON (YES in S<b>102</b>), it determines whether the user has set, ON with the function switch of the operation unit <b>105</b> or the like, a function of performing display control based on a face area detected by the special area detection unit <b>106</b> (S<b>103</b>).</p>
<p id="p-0058" num="0057">If the display control unit <b>107</b> determines that the function of performing display control based on a face area is OFF (NO in S<b>103</b>), it detects the luminance level of an image signal corresponding to each pixel of an input sensed image (S<b>104</b>). Then, the display control unit <b>107</b> determines whether the detected luminance level of the image signal corresponding to each pixel satisfies a preset condition and falls within a luminance level range for displaying a zebra pattern (S<b>105</b>). If the display control unit <b>107</b> determines that the detected luminance level of the image signal corresponding to each pixel falls within the luminance level range for displaying a zebra pattern (YES in S<b>105</b>), it superimposes a zebra pattern on the image signal (S<b>106</b>). If the display control unit <b>107</b> determines that the detected luminance level of the image signal corresponding to each pixel falls outside the luminance level range for displaying a zebra pattern (NO in S<b>105</b>), it does not superimpose a zebra pattern on the image signal. The display control unit <b>107</b> repeats the processes in S<b>101</b> to S<b>106</b> until the image sensing apparatus <b>1</b> is turned off.</p>
<p id="p-0059" num="0058">If the display control unit <b>107</b> determines that the function of performing display control based on a face area is ON (YES in S<b>103</b>), the special area detection unit <b>106</b> detects a face area from the sensed image (S<b>107</b>). Similar to S<b>104</b> and S<b>105</b>, the display control unit <b>107</b> detects the luminance level of an image signal corresponding to each pixel of the input sensed image (S<b>108</b>). The display control unit <b>107</b> determines whether the detected luminance level of the image signal falls within the luminance level range for displaying a zebra pattern (S<b>109</b>). If the display control unit <b>107</b> determines that the detected luminance level of the image signal corresponding to each pixel falls outside the luminance level range for displaying a zebra pattern (NO in S<b>109</b>), it does not superimpose a zebra pattern on the image signal.</p>
<p id="p-0060" num="0059">If the display control unit <b>107</b> determines that the detected luminance level of the image signal corresponding to each pixel falls within the luminance level range for displaying a zebra pattern (YES in S<b>109</b>), it determines whether the image signal is that of a pixel falling within the face area detected by the special area detection unit <b>106</b> (S<b>110</b>). If the display control unit <b>107</b> determines that the image signal is not that of a pixel falling within the face area (NO in S<b>110</b>), it superimposes a zebra pattern on the image signal (S<b>111</b>). If the display control unit <b>107</b> determines that the image signal is that of a pixel falling within the face area (YES in S<b>110</b>), it superimposes a zebra pattern in an image area other than the face area (S<b>112</b>). The display control unit <b>107</b> repeats the processes in S<b>101</b> to S<b>103</b> and S<b>107</b> to S<b>112</b> until the image sensing apparatus <b>1</b> is turned off.</p>
<p id="p-0061" num="0060">By this processing, the image display unit <b>108</b> of the image sensing apparatus <b>1</b> displays a sensed image exemplified in <figref idref="DRAWINGS">FIG. 5</figref>. Referring to <figref idref="DRAWINGS">FIG. 5</figref>, sensed images <b>501</b><i>a </i>and <b>501</b><i>b </i>are sensed images of persons H<b>1</b> to H<b>3</b>. Sensed images <b>501</b><i>c </i>and <b>501</b><i>d </i>are sensed images of a person H<b>4</b> with a window frame W<b>1</b> in the background. The image display unit <b>108</b> displays the sensed images <b>501</b><i>a </i>and <b>501</b><i>c </i>when the function of performing display control based on a face area is OFF. The image display unit <b>108</b> displays the sensed images <b>501</b><i>b </i>and <b>501</b><i>d </i>when the function is ON.</p>
<p id="p-0062" num="0061">When the function of performing display control based on a face area is OFF, zebra patterns <b>503</b><i>a </i>to <b>503</b><i>e </i>are displayed to be superimposed in face areas <b>502</b><i>a </i>to <b>502</b><i>d </i>of the persons H<b>1</b> to H<b>4</b> and other areas such as a clock without discriminating the face areas and other areas, like the sensed images <b>501</b><i>a </i>and <b>501</b><i>c</i>. When the function of performing display control based on a face area is ON, no zebra pattern is displayed in the face areas <b>502</b><i>a </i>to <b>502</b><i>d </i>of the persons H<b>1</b> to H<b>4</b> that are detected as special image areas, and zebra patterns <b>504</b><i>a </i>to <b>504</b><i>d </i>are displayed in outer image areas along the edges of the face areas. The zebra pattern <b>503</b><i>e </i>for the clock or the like other than the face areas is kept displayed. That is, the image sensing apparatus <b>1</b> displays a zebra pattern while changing the display method of zebra pattern display between a face area and the remaining image area.</p>
<p id="p-0063" num="0062">[Second Embodiment]</p>
<p id="p-0064" num="0063">An image sensing apparatus according to the second embodiment will be described in detail with reference to <figref idref="DRAWINGS">FIGS. 6A to 6E</figref>. The second embodiment is directed to a modification of the display form described with reference to <figref idref="DRAWINGS">FIG. 5</figref> in the first embodiment. The arrangement of the apparatus and the contents of processing are the same as those in the first embodiment, and a description thereof will not be repeated. <figref idref="DRAWINGS">FIGS. 6A to 6E</figref> are conceptual views exemplifying sensed images displayed on an image display unit in the second embodiment. <figref idref="DRAWINGS">FIG. 6A</figref> is a conceptual view exemplifying a sensed image <b>601</b><i>a</i>. <figref idref="DRAWINGS">FIG. 6B</figref> is a conceptual view exemplifying a sensed image <b>601</b><i>b</i>. <figref idref="DRAWINGS">FIG. 6C</figref> is a conceptual view exemplifying a sensed image <b>601</b><i>c</i>. <figref idref="DRAWINGS">FIG. 6D</figref> is a conceptual view exemplifying a sensed image <b>601</b><i>d</i>. <figref idref="DRAWINGS">FIG. 6E</figref> is a conceptual view exemplifying a sensed image <b>601</b><i>e. </i></p>
<p id="p-0065" num="0064">As shown in <figref idref="DRAWINGS">FIG. 6A</figref>, the sensed image <b>601</b><i>a </i>is a sensed image of persons H<b>5</b> to H<b>7</b>. An image display unit <b>108</b> displays the sensed image <b>601</b><i>a </i>when the function of performing display control based on a face area is ON. Zebra patterns for face areas <b>602</b><i>a </i>to <b>602</b><i>c </i>of the persons H<b>5</b> to H<b>7</b> may also be displayed as zebra patterns <b>603</b><i>a </i>to <b>603</b><i>c </i>in tag areas associated with the face areas <b>602</b><i>a </i>to <b>602</b><i>c</i>, like the sensed image <b>601</b><i>a. </i></p>
<p id="p-0066" num="0065">As shown in <figref idref="DRAWINGS">FIG. 6B</figref>, the sensed image <b>601</b><i>b </i>is a sensed image of a person H<b>8</b>. The image display unit <b>108</b> displays the sensed image <b>601</b><i>b </i>when the function of performing display control based on a face area is ON. A zebra pattern for a face area <b>602</b><i>d </i>of the person H<b>8</b> may also be displayed as a zebra pattern <b>604</b> in a display area set at the end of the screen, like the sensed image <b>601</b><i>b</i>. When displaying a zebra pattern, as represented by the sensed image <b>601</b><i>b</i>, it is not superimposed in an area overlapping an object area such as the face area <b>602</b><i>d </i>of the person H<b>8</b>, but is displayed in an area other than the overlapping object area. For example, the zebra pattern may also be displayed around the object area.</p>
<p id="p-0067" num="0066">As shown in <figref idref="DRAWINGS">FIG. 6C</figref>, the sensed image <b>601</b><i>c </i>is a sensed image of the persons H<b>5</b> to H<b>7</b>. The image display unit <b>108</b> displays the sensed image <b>601</b><i>c </i>when the function of performing display control based on a face area is ON. In the sensed image <b>601</b><i>c</i>, tag images <b>605</b><i>a </i>to <b>605</b><i>c </i>are set for the detected face areas <b>602</b><i>a </i>to <b>602</b><i>c </i>of the persons H<b>5</b> to H<b>7</b>. An area for displaying zebra patterns <b>604</b><i>a </i>to <b>604</b><i>c </i>corresponding to the tag images <b>605</b><i>a </i>to <b>605</b><i>c </i>is set at, e.g., the end of the screen at which the area does not overlap a face area. When there are a plurality of face areas, the zebra patterns <b>604</b><i>a </i>to <b>604</b><i>c </i>corresponding to the respective face areas are displayed side by side in the area at the end of the screen. In this fashion, zebra patterns for the face areas <b>602</b><i>a </i>to <b>602</b><i>c </i>of the persons H<b>5</b> to H<b>7</b> may also be displayed in correspondence with the tag images <b>605</b><i>a </i>to <b>605</b><i>c </i>of the face areas <b>602</b><i>a </i>to <b>602</b><i>c </i>in an area at the end of the screen or the like at which the area does not overlap a face area. At this time, each of the tag images <b>605</b><i>a </i>to <b>605</b><i>c </i>and a corresponding one of the zebra patterns <b>604</b><i>a </i>to <b>604</b><i>c </i>may also be made to correspond to each other by a display form using the same display color, same hatching, or the like.</p>
<p id="p-0068" num="0067">As shown in <figref idref="DRAWINGS">FIG. 6D</figref>, the sensed image <b>601</b><i>d </i>is a sensed image of the persons H<b>5</b> to H<b>7</b>. The image display unit <b>108</b> displays the sensed image <b>601</b><i>d </i>when the function of performing display control based on a face area is ON. In the sensed image <b>601</b><i>d</i>, face area frames <b>606</b><i>a </i>to <b>606</b><i>c </i>are set in the detected face areas of the persons H<b>5</b> to H<b>7</b>. An area for displaying zebra patterns <b>604</b><i>d </i>to <b>604</b><i>f </i>corresponding to the face area frames <b>606</b><i>a </i>to <b>606</b><i>c </i>is set at, e.g., the end of the screen at which the area does not overlap a face area. When there are a plurality of face areas, the zebra patterns <b>604</b><i>d </i>to <b>604</b><i>f </i>corresponding to the respective face areas are displayed in the area at the end of the screen. Zebra patterns for the face areas of the persons H<b>5</b> to H<b>7</b> may also be displayed as the zebra patterns <b>604</b><i>d </i>to <b>604</b><i>f </i>corresponding to the face area frames <b>606</b><i>a </i>to <b>606</b><i>c </i>in an area at the end of the screen or the like at which the area does not overlap a face area. At this time, each of the face area frames <b>606</b><i>a </i>to <b>606</b><i>c </i>and a corresponding one of the zebra patterns <b>604</b><i>d </i>to <b>604</b><i>f </i>may also be made to correspond to each other by a display form using the same display color, same hatching, or the like.</p>
<p id="p-0069" num="0068">As shown in <figref idref="DRAWINGS">FIG. 6E</figref>, the sensed image <b>601</b><i>e </i>is a sensed image of a person H<b>9</b> with a window frame W<b>2</b> in the background. The image display unit <b>108</b> displays the sensed image <b>601</b><i>e </i>when the function of performing display control based on a face area is ON. A zebra pattern for a detected face area <b>602</b> of the person H<b>9</b> may not be displayed, like the sensed image <b>601</b><i>e. </i></p>
<p id="p-0070" num="0069">[Third Embodiment]</p>
<p id="p-0071" num="0070">An image sensing apparatus according to the third embodiment will be described in detail with reference to <figref idref="DRAWINGS">FIGS. 7 and 8</figref>. The third embodiment is a modification to the first embodiment when a special image area detected by a special area detection unit <b>106</b> is an in-focus area. The arrangement of the apparatus is the same as that in the first embodiment, and a description thereof will not be repeated. <figref idref="DRAWINGS">FIG. 7</figref> is a flowchart showing the display control operation of an image sensing apparatus <b>1</b>. <figref idref="DRAWINGS">FIG. 8</figref> is a conceptual view exemplifying a sensed image displayed on an image display unit <b>108</b>.</p>
<p id="p-0072" num="0071">Processing by a display control unit <b>107</b> will be explained. This processing is executed as the display control operation of the image sensing apparatus <b>1</b> under the control of a CPU <b>203</b> according to the third embodiment. In the third embodiment, the image display unit <b>108</b> displays a zebra pattern to be superimposed on an image sensed by an image input unit <b>101</b>. A special image area detected by the special area detection unit <b>106</b> is an in-focus area.</p>
<p id="p-0073" num="0072">As shown in <figref idref="DRAWINGS">FIG. 7</figref>, when display control processing starts, the display control unit <b>107</b> determines whether the user has turned on the image sensing apparatus <b>1</b> with the power switch of an operation unit <b>105</b> or the like (S<b>201</b>). If the display control unit <b>107</b> determines that the user has turned on the image sensing apparatus <b>1</b> (YES in S<b>201</b>), it determines whether the user has set zebra pattern display ON with the setting switch of the operation unit <b>105</b> or the like (S<b>202</b>). If the display control unit <b>107</b> determines that the user has set zebra pattern display ON (YES in S<b>202</b>), it determines whether the user has set, ON with the function switch of the operation unit <b>105</b> or the like, a function of performing display control based on an in-focus area detected by the special area detection unit <b>106</b> (S<b>203</b>).</p>
<p id="p-0074" num="0073">If the display control unit <b>107</b> determines that the function of performing display control based on an in-focus area is OFF (NO in S<b>203</b>), it detects the luminance level of an image signal corresponding to each pixel of an input sensed image (S<b>204</b>). Then, the display control unit <b>107</b> determines whether the detected luminance level of the image signal corresponding to each pixel satisfies a preset condition and falls within a luminance level range for displaying a zebra pattern (S<b>205</b>). If the display control unit <b>107</b> determines that the detected luminance level of the image signal corresponding to each pixel falls within the luminance level range for displaying a zebra pattern (YES in S<b>205</b>), it superimposes a zebra pattern on the image signal (S<b>206</b>). If the display control unit <b>107</b> determines that the detected luminance level of the image signal corresponding to each pixel falls outside the luminance level range for displaying a zebra pattern (NO in S<b>205</b>), it does not superimpose a zebra pattern on the image signal. The display control unit <b>107</b> repeats the processes in S<b>201</b> to S<b>206</b> until the image sensing apparatus <b>1</b> is turned off.</p>
<p id="p-0075" num="0074">If the display control unit <b>107</b> determines that the function of performing display control based on an in-focus area is ON (YES in S<b>203</b>), the special area detection unit <b>106</b> detects an in-focus area from the sensed image (S<b>207</b>). Similar to S<b>204</b> and S<b>205</b>, the display control unit <b>107</b> detects the luminance level of an image signal corresponding to each pixel of the input sensed image (S<b>208</b>). The display control unit <b>107</b> determines whether the detected luminance level of the image signal falls within the luminance level range for displaying a zebra pattern (S<b>209</b>). If the display control unit <b>107</b> determines that the detected luminance level of the image signal corresponding to each pixel falls outside the luminance level range for displaying a zebra pattern (NO in S<b>209</b>), it does not superimpose a zebra pattern on the image signal.</p>
<p id="p-0076" num="0075">If the display control unit <b>107</b> determines that the detected luminance level of the image signal corresponding to each pixel falls within the luminance level range for displaying a zebra pattern (YES in S<b>209</b>), it determines whether the image signal is that of a pixel falling within the in-focus area detected by the special area detection unit <b>106</b> (S<b>210</b>). If the display control unit <b>107</b> determines that the image signal is not that of a pixel falling within the in-focus area (NO in S<b>210</b>), it superimposes a zebra pattern on the image signal (S<b>211</b>). If the display control unit <b>107</b> determines that the image signal is that of a pixel falling within the in-focus area (YES in S<b>210</b>), it superimposes a zebra pattern in an image area other than the in-focus area (S<b>212</b>). The display control unit <b>107</b> repeats the processes in S<b>201</b> to S<b>203</b> and S<b>207</b> to S<b>212</b> until the image sensing apparatus <b>1</b> is turned off.</p>
<p id="p-0077" num="0076">By this processing, the image display unit <b>108</b> of the image sensing apparatus <b>1</b> displays a sensed image exemplified in <figref idref="DRAWINGS">FIG. 8</figref>. Referring to <figref idref="DRAWINGS">FIG. 8</figref>, sensed images <b>801</b><i>a </i>and <b>801</b><i>b </i>are sensed images of a person H<b>10</b> with a window frame W<b>3</b> in the background. The image display unit <b>108</b> displays the sensed image <b>801</b><i>a </i>when the function of performing display control based on an in-focus area is OFF. The image display unit <b>108</b> displays the sensed image <b>801</b><i>b </i>when the function is ON.</p>
<p id="p-0078" num="0077">When the function of performing display control based on an in-focus area is OFF, zebra patterns <b>803</b><i>a </i>and <b>803</b><i>b </i>are displayed to be superimposed in an in-focus area <b>802</b> in which the face of the person H<b>10</b> is focused, and another area such as a clock without discriminating these areas, like the sensed image <b>801</b><i>a</i>. When the function of performing display control based on an in-focus area is ON, no zebra pattern is displayed in the in-focus area <b>802</b> detected as a special image area, and a zebra pattern <b>804</b> is displayed in an outer image area along the edge of the in-focus area. The zebra pattern <b>803</b><i>b </i>for the clock or the like other than an in-focus area is kept displayed. That is, the image sensing apparatus <b>1</b> displays a zebra pattern while changing the display method of zebra pattern display between an in-focus area and the remaining image area.</p>
<p id="p-0079" num="0078">[Fourth Embodiment]</p>
<p id="p-0080" num="0079">An image sensing apparatus according to the fourth embodiment will be described in detail with reference to <figref idref="DRAWINGS">FIGS. 9 and 10</figref>. The first, second, and third embodiments change the assist image display method when the sum of pixels to display an assist image in an object area detected by the special area detection unit <b>106</b> becomes equal to or larger than a predetermined ratio. In this case, however, the position of a pixel to display an assist image in the object area cannot be determined from the displayed assist image. To solve this, the fourth embodiment divides an object area into a plurality of small areas, also divides an assist image in correspondence with the positions of the divided small areas, and displays the divided assist images. More specifically, when displaying an assist image to be superimposed in an area other than an object area, the assist image is displayed finely so that a portion where pixels to display the assist image are distributed in the object area can be discriminated from an portion where no such pixel is distributed. The fourth embodiment is a modification to the first embodiment when a special image area detected by a special area detection unit <b>106</b> is a face area. The arrangement of the apparatus is the same as that in the first embodiment, and a description thereof will not be repeated.</p>
<p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. 9</figref> shows the display form of an assist image corresponding to a face area serving as a special image area. The fourth embodiment will explain a special image display form corresponding to a face area by using two patterns of a special image <b>1102</b> in b in <figref idref="DRAWINGS">FIG. 9</figref> and a special image <b>1103</b> in c in <figref idref="DRAWINGS">FIG. 9</figref>. In the special image <b>1102</b>, a zebra pattern is displayed in an outer image area along the edge of a face area. In the special image <b>1103</b>, a zebra pattern is displayed in a tag area corresponding to a face area.</p>
<p id="p-0082" num="0081">As shown in a in <figref idref="DRAWINGS">FIG. 9</figref>, a face area <b>1101</b> is divided into four areas A, B, C, and D. The special image <b>1102</b> or <b>1103</b> is also divided into four in correspondence with the four divided areas. Special image areas A&#x2032;, B&#x2032;, C&#x2032;, and D&#x2032; correspond to the face areas A, B, C, and D. Special image displays <b>1102</b><i>a </i>and <b>1103</b><i>a </i>correspond to the face area A. Special image displays <b>1102</b><i>b </i>and <b>1103</b><i>b </i>correspond to the face area B. Special image displays <b>1102</b><i>c </i>and <b>1103</b><i>c </i>correspond to the face area C. Special image displays <b>1102</b><i>d </i>and <b>1103</b><i>d </i>correspond to the face area D. The conceptual views of displays combined with actual face areas are shown in d and e in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. 10</figref> is a conceptual view exemplifying a sensed image displayed on an image display unit in the fourth embodiment.</p>
<p id="p-0084" num="0083">Sensed images <b>1201</b><i>a </i><b>1201</b><i>f </i>are sensed images of a person H<b>12</b>. Sensed images <b>1201</b><i>g </i>and <b>1201</b><i>h </i>are sensed images of persons H<b>13</b> to H<b>15</b>. An image display unit <b>108</b> displays the sensed images <b>1201</b><i>a</i>, <b>1201</b><i>c</i>, <b>1201</b><i>e</i>, and <b>1201</b><i>g </i>when the function of performing display control based on a face area is OFF. The image display unit <b>108</b> displays the sensed images <b>1201</b><i>b</i>, <b>1201</b><i>d</i>, <b>1201</b><i>f</i>, and <b>1201</b><i>h </i>when the function is ON. In the sensed images <b>1201</b><i>b</i>, <b>1201</b><i>d</i>, and <b>1201</b><i>f</i>, a zebra pattern is displayed in an outer image area along the edge of a face area, as represented by the special image <b>1102</b> of b in <figref idref="DRAWINGS">FIG. 9</figref>. In the sensed image <b>1201</b><i>h</i>, a zebra pattern is displayed in a tag area, as represented by the special image <b>1103</b> of c in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0085" num="0084">When the function of performing display control based on a face area is OFF, zebra patterns <b>1204</b><i>a </i>to <b>1204</b><i>m </i>are displayed in face areas <b>1202</b><i>a </i>to <b>1202</b><i>d </i>of the persons H<b>12</b> to H<b>15</b> and other areas without discriminating them, like the sensed images <b>1201</b><i>a</i>, <b>1201</b><i>c</i>, <b>1201</b><i>e</i>, and <b>1201</b><i>g</i>. When the function of performing display control based on a face area is ON, no zebra pattern is displayed in the face areas <b>1202</b><i>a </i>to <b>1202</b><i>d </i>of the persons H<b>12</b> to H<b>15</b> that are detected as special image areas, and zebra patterns <b>1203</b><i>a </i>to <b>1203</b><i>f </i>corresponding to the four divided face areas are displayed in image areas outside the face areas. The zebra patterns <b>1204</b><i>a </i>to <b>1204</b><i>c </i>for the clock and the like other than face areas are kept displayed. That is, an image sensing apparatus <b>1</b> displays a zebra pattern while changing the zebra pattern display method of between a face area and the remaining image area.</p>
<p id="p-0086" num="0085">[Fifth Embodiment]</p>
<p id="p-0087" num="0086">An image sensing apparatus according to the fifth embodiment will be described in detail with reference to <figref idref="DRAWINGS">FIGS. 11 and 12</figref>. The fifth embodiment is a modification to the first embodiment when an image display unit <b>108</b> presents a peaking display to be superimposed on an image sensed by an image input unit <b>101</b>. The arrangement of the apparatus is the same as that in the first embodiment, and a description thereof will not be repeated. <figref idref="DRAWINGS">FIG. 11</figref> is a flowchart showing the display control operation of an image sensing apparatus <b>1</b>. <figref idref="DRAWINGS">FIG. 12</figref> is a conceptual view exemplifying a sensed image displayed on the image display unit <b>108</b>.</p>
<p id="p-0088" num="0087">Processing by a display control unit <b>107</b> will be explained. This processing is executed as the display control operation of the image sensing apparatus <b>1</b> under the control of a CPU <b>203</b> according to the fifth embodiment. In the fifth embodiment, the image display unit <b>108</b> presents a peaking display to be superimposed on an image sensed by the image input unit <b>101</b>. A special image area detected by a special area detection unit <b>106</b> is a face area.</p>
<p id="p-0089" num="0088">As shown in <figref idref="DRAWINGS">FIG. 11</figref>, when display control processing starts, the display control unit <b>107</b> determines whether the user has turned on the image sensing apparatus <b>1</b> with the power switch of an operation unit <b>105</b> or the like (S<b>301</b>) If the display control unit <b>107</b> determines that the user has turned on the image sensing apparatus <b>1</b> (YES in S<b>301</b>), it determines whether the user has set peaking display ON with the setting switch of the operation unit <b>105</b> or the like (S<b>302</b>). If the display control unit <b>107</b> determines that the user has set peaking display ON (YES in S<b>302</b>), it determines whether the user has set, ON with the function switch of the operation unit <b>105</b> or the like, a function of performing display control based on a face area detected by the special area detection unit <b>106</b> (S<b>303</b>).</p>
<p id="p-0090" num="0089">If the display control unit <b>107</b> determines that the function of performing display control based on a face area is OFF (NO in S<b>303</b>), it detects the edge level of the image signal of an input sensed image, i.e., executes the foregoing edge detection (S<b>304</b>). Then, the display control unit <b>107</b> superimposes a peaking display based on the detected edge on the sensed image in a normal display color, and displays it on the display screen of the image display unit <b>108</b>. The display control unit <b>107</b> repeats the processes in S<b>301</b> to S<b>305</b> until the image sensing apparatus <b>1</b> is turned off.</p>
<p id="p-0091" num="0090">If the display control unit <b>107</b> determines that the function of performing display control based on a face area is ON (YES in S<b>303</b>), the special area detection unit <b>106</b> detects a face area from the sensed image (S<b>306</b>). Similar to S<b>304</b>, the display control unit <b>107</b> detects the edge level of the image signal of the input sensed image, i.e., executes the above-described edge detection (S<b>307</b>). The display control unit <b>107</b> determines whether the detected edge falls, within the face area detected by the special area detection unit <b>106</b> (S<b>308</b>). If the display control unit <b>107</b> determines that the detected edge falls outside the face area (NO in S<b>308</b>), it superimposes a peaking display based on the detected edge on the sensed image in a normal display color, and displays it on the display screen of the image display unit <b>108</b> (S<b>309</b>). If the display control unit <b>107</b> determines that the detected edge falls within the face area (YES in S<b>308</b>), it superimposes a peaking display based on the detected edge on the sensed image in a face area display color, and displays it on the display screen of the image display unit <b>108</b> (S<b>310</b>). The display control unit <b>107</b> repeats the processes in S<b>301</b> to S<b>303</b> and S<b>306</b> to S<b>310</b> until the image sensing apparatus <b>1</b> is turned off.</p>
<p id="p-0092" num="0091">By this processing, the image display unit <b>108</b> of the image sensing apparatus <b>1</b> displays a sensed image exemplified in <figref idref="DRAWINGS">FIG. 12</figref>. Referring to <figref idref="DRAWINGS">FIG. 12</figref>, sensed images <b>901</b><i>a </i>and <b>901</b><i>b </i>are sensed images of a person H<b>11</b> with a window frame W<b>4</b> in the background. The image display unit <b>108</b> displays the sensed image <b>901</b><i>a </i>when the function of performing display control based on a face area is OFF. The image display unit <b>108</b> displays the sensed image <b>901</b><i>b </i>when the function is ON.</p>
<p id="p-0093" num="0092">When the function of performing display control based on a face area is OFF, a peaking display <b>903</b> along the edge line of the person H<b>11</b> is presented without discriminating the inside and outside of a detected face area <b>902</b> of the person H<b>11</b>, like the sensed image <b>901</b><i>a</i>. When the function of performing display control based on a face area is ON, a peaking display <b>904</b> is presented while discriminating the display color between the inside and outside of the face area <b>902</b>. That is, the image sensing apparatus <b>1</b> presents a peaking display while changing the peaking display color between a face area and the remaining image area.</p>
<p id="p-0094" num="0093">The above-described embodiments are merely examples, and the present invention is not limited to them. The arrangements and operations in the embodiments can be appropriately changed. For example, a zebra pattern for a special image area is displayed outside the special image area such as a face area or in-focus area. As exemplified in the fifth embodiment, a zebra pattern may also be displayed by changing the display color and display method between the inside and outside of a special image area. Also, a peaking display for a special image area may also be presented outside the special image area.</p>
<p id="p-0095" num="0094">The above-described embodiments may also be combined. More specifically, the display method may also change a combination of the display color and the display form (e.g., flickering/non-flickering or line type). In other words, the assist image display method for a special image area in the image area of a sensed image and that for the remaining image area may also differ between each other in accordance with a combination of the display color and display form.</p>
<p id="p-0096" num="0095">As described above, the image sensing apparatus <b>1</b> includes the display control unit <b>107</b> which controls the image display unit <b>108</b> to display an image sensed by the image input unit <b>101</b>. The image sensing apparatus <b>1</b> also includes the signal strength detection unit <b>201</b> which detects, from a sensed image, a luminance area formed from pixels satisfying a predetermined luminance condition, and the special area detection unit <b>106</b> which detects, from a sensed image, an object area having a predetermined feature. The display control unit <b>107</b> controls the image display unit <b>108</b> to display an assist image representing the luminance area, so as to be superimposed on a sensed image. The display control unit <b>107</b> changes the assist image display method between a case in which the luminance area and object area overlap each other and a case in which they do not overlap each other. When displaying an assist image in an image area which meets a predetermined luminance condition, the image sensing apparatus <b>1</b> can display the assist image while discriminating an area overlapping an object area having a predetermined feature from the remaining area.</p>
<p id="p-0097" num="0096">The display control unit <b>107</b> controls the image display unit <b>108</b> to present a pattern display representing a luminance area, so as to be superimposed on a sensed image. The display control unit <b>107</b> changes the display method of the pattern display between a case in which the luminance area and object area overlap each other and a case in which they do not overlap each other. Hence, the image sensing apparatus <b>1</b> can present a pattern display representing a luminance area while discriminating an area overlapping an object area having a predetermined feature from the remaining area.</p>
<p id="p-0098" num="0097">The signal strength detection unit <b>201</b> detects the edge of an object from a sensed image. The special area detection unit <b>106</b> detects an object area containing a person's face. The display control unit <b>107</b> controls the image display unit <b>108</b> to present a peaking display representing the edge of the object, so as to be superimposed on a sensed image. Further, the display control unit <b>107</b> changes the display method of the peaking display between a case in which the edge of an object falls within the object area and a case in which it falls outside the object area. Accordingly, the image sensing apparatus <b>1</b> can present a peaking display representing the edge of an object while discriminating an area overlapping an object area having a predetermined feature from the remaining area.</p>
<p id="p-0099" num="0098">(Other Embodiments)</p>
<p id="p-0100" num="0099">The above-described embodiments can also be implemented as software by the computer (or CPU or MPU) of a system or apparatus. A computer program itself supplied to the computer in order to implement the embodiments by the computer also constitutes the present invention. That is, the present invention also includes the computer program for implementing the functions of the embodiments.</p>
<p id="p-0101" num="0100">The computer program for implementing the embodiments can take any form such as an object code, a program executed by an interpreter, or script data supplied to an OS as long as the computer can read the program. However, the computer program is not limited to them. The computer program for implementing the embodiments is supplied to the computer by a storage medium or wired/wireless communication. The storage medium for supplying the program includes a flexible disk, hard disk, magnetic storage medium (e.g., magnetic tape), optical/magnetooptical storage medium (e.g., MO, CD, or DVD), and nonvolatile semiconductor memory.</p>
<p id="p-0102" num="0101">The method of supplying the computer program using wired/wireless communication may use a server on a computer network. In this case, the server stores a data file (program file) serving as the computer program which constitutes the present invention. The program file may also be an executable file or source code. The program file is supplied by downloading it to a client computer which accesses the server. In this case, the program file can also be divided into a plurality of segment files to distributedly store the segment files in different servers. That is, the present invention also includes the server apparatus which provides a client computer with the program file for implementing the above-described embodiments.</p>
<p id="p-0103" num="0102">It is also possible to encrypt the computer program for implementing the above-described embodiments, store the encrypted program in a storage medium, and distribute the storage medium to users. In this case, decryption key information is supplied to a user who satisfies a predetermined condition, permitting him to install the computer program in his computer. For example, the key information can be supplied by downloading it from a homepage via the Internet. The computer program for implementing the above-described embodiments may also utilize the function of an OS running on the computer. Further, the computer program for implementing the above-described embodiments may also be partially formed from firmware such as an expansion board mounted in the computer, or executed by the CPU of the expansion board or the like.</p>
<p id="p-0104" num="0103">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.</p>
<p id="p-0105" num="0104">This application claims the benefit of Japanese Patent Application Nos. 2008-148323, filed Jun. 5, 2008 and 2009-065222, filed Mar. 17, 2009, which are hereby incorporated by reference herein in their entirety.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An image processing apparatus comprising:
<claim-text>an image obtaining unit that obtains an image;</claim-text>
<claim-text>a first detection unit that detects, from the image obtained by the image obtaining unit, a luminance area formed from pixels satisfying a predetermined luminance condition;</claim-text>
<claim-text>a second detection unit that detects an object area having a predetermined feature from the image obtained by the image obtaining unit; and</claim-text>
<claim-text>a display control unit that displays the image obtained by the image obtaining unit and a predetermined image indicating the luminance area, on a display apparatus;</claim-text>
<claim-text>wherein the display control unit displays the predetermined image in a first display form when the luminance area and the object area overlap with each other and displays the predetermined image in a second display form when the luminance area and the object area do not overlap.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the display control unit displays a frame image indicating the object area.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the display control unit displays the predetermined image on the frame image when the luminance area and the object area overlap with each other.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second detection unit detects a face area of a man as the object area.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second detection unit detects in-focus area as the object area.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising an operating unit which can be operated by a user to set the predetermined luminance condition.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A method of controlling an image processing apparatus having an image obtaining unit that obtains an image, comprising:
<claim-text>a first detection step of detecting, from the image obtained by the image obtaining unit, a luminance area formed from pixels satisfying a predetermined luminance condition;</claim-text>
<claim-text>a second detection step of detecting an object area having a predetermined feature from the image obtained by the image obtaining unit; and</claim-text>
<claim-text>a display control step of displaying the image obtained by the image obtaining unit and a predetermined image indicating the luminance area, on a display apparatus;</claim-text>
<claim-text>wherein the display control step displays the predetermined image in a first display form when the luminance area and the object area overlap with each other and displays the predetermined image in a second display form when the luminance area and the object area do not overlap.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A non-transitory computer readable storage medium storing a program causing a computer to execute the control method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. An image processing apparatus comprising:
<claim-text>an image obtaining unit that obtains an image;</claim-text>
<claim-text>a first detection unit that detects, from the image obtained by the image obtaining unit, a luminance area formed from pixels satisfying a predetermined luminance condition;</claim-text>
<claim-text>a second detection unit that detects an object area having a predetermined feature from the image obtained by the image obtaining unit; and</claim-text>
<claim-text>a display control unit that displays the image obtained by the image obtaining unit and a predetermined image indicating the object area, on a display apparatus;</claim-text>
<claim-text>wherein the display control unit displays the predetermined image in a first display form when the luminance area and the object area overlap with each other and displays the predetermined image in a second display form when the luminance area and the object area do not overlap.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the display control unit controls the display apparatus to change at least one of color and pattern of the predetermined image depending on a case that the object area and the luminance area overlap or a case that the object area and the luminance area do not overlap.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the second detection unit detects a face area of a man as the object area.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising an operating unit which can be operated by a user to set the predetermined luminance condition.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A method of controlling an image processing apparatus having an image obtaining unit that obtains an image, comprising:
<claim-text>a first detection step of detecting, from the image obtained by the image obtaining unit, a luminance area formed from pixels satisfying a predetermined luminance condition;</claim-text>
<claim-text>a second detection step of detecting an object area having a predetermined feature from the image obtained by the image obtaining unit; and</claim-text>
<claim-text>a display control step of displaying the image obtained by the image obtaining unit and a predetermined image indicating the object area, on a display apparatus;</claim-text>
<claim-text>wherein the display control step displays the predetermined image in a first display form when the luminance area and the object area overlap with each other and displays the predetermined image in a second display form when the luminance area and the object area do not overlap.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A non-transitory computer readable storage medium storing a program causing a computer to execute the control method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>. </claim-text>
</claim>
</claims>
</us-patent-grant>
