<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625018-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625018</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13768072</doc-number>
<date>20130215</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>222</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>34833301</main-classification>
</classification-national>
<invention-title id="d2e51">Synchronized, interactive augmented reality displays for multifunction devices</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5625765</doc-number>
<kind>A</kind>
<name>Ellenby et al.</name>
<date>19970400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5682332</doc-number>
<kind>A</kind>
<name>Ellenby et al.</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5742521</doc-number>
<kind>A</kind>
<name>Ellenby et al.</name>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5815411</doc-number>
<kind>A</kind>
<name>Ellenby et al.</name>
<date>19980900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6020931</doc-number>
<kind>A</kind>
<name>Bilbrey et al.</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6031545</doc-number>
<kind>A</kind>
<name>Ellenby et al.</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6037936</doc-number>
<kind>A</kind>
<name>Ellenby et al.</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6064398</doc-number>
<kind>A</kind>
<name>Ellenby et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6173239</doc-number>
<kind>B1</kind>
<name>Ellenby</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6278461</doc-number>
<kind>B1</kind>
<name>Ellenby et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6307556</doc-number>
<kind>B1</kind>
<name>Ellenby et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6396475</doc-number>
<kind>B1</kind>
<name>Ellenby et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6414696</doc-number>
<kind>B1</kind>
<name>Ellenby et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6522292</doc-number>
<kind>B1</kind>
<name>Ellenby et al.</name>
<date>20030200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6535210</doc-number>
<kind>B1</kind>
<name>Ellenby et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6690370</doc-number>
<kind>B2</kind>
<name>Ellenby et al.</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7007236</doc-number>
<kind>B2</kind>
<name>Dempski et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7031875</doc-number>
<kind>B2</kind>
<name>Ellenby et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7301536</doc-number>
<kind>B2</kind>
<name>Ellenby et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7720436</doc-number>
<kind>B2</kind>
<name>Hamynen et al.</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>455 131</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7817104</doc-number>
<kind>B2</kind>
<name>Ryu et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7916138</doc-number>
<kind>B2</kind>
<name>John et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>8301159</doc-number>
<kind>B2</kind>
<name>Hamynen et al.</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>4554561</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>8400548</doc-number>
<kind>B2</kind>
<name>Bilbrey et al.</name>
<date>20130300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833301</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2003/0128162</doc-number>
<kind>A1</kind>
<name>Ellenby et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2003/0184594</doc-number>
<kind>A1</kind>
<name>Ellenby et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2004/0095345</doc-number>
<kind>A1</kind>
<name>Ellenby et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2004/0189675</doc-number>
<kind>A1</kind>
<name>Pretlove et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2004/0219961</doc-number>
<kind>A1</kind>
<name>Ellenby et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2006/0161379</doc-number>
<kind>A1</kind>
<name>Ellenby et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2006/0190812</doc-number>
<kind>A1</kind>
<name>Ellenby et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2006/0241792</doc-number>
<kind>A1</kind>
<name>Pretlove et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2007/0162942</doc-number>
<kind>A1</kind>
<name>Hamynen et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2008/0123910</doc-number>
<kind>A1</kind>
<name>Zhu</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2009/0102859</doc-number>
<kind>A1</kind>
<name>Athsani et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345619</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2009/0189974</doc-number>
<kind>A1</kind>
<name>Deering</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2009/0232354</doc-number>
<kind>A1</kind>
<name>Camp et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2010/0030578</doc-number>
<kind>A1</kind>
<name>Siddique et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705  3</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2010/0160014</doc-number>
<kind>A1</kind>
<name>Galasso et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463  6</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2010/0257252</doc-number>
<kind>A1</kind>
<name>Dougherty et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709217</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2010/0302141</doc-number>
<kind>A1</kind>
<name>Shankar et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345156</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2011/0050909</doc-number>
<kind>A1</kind>
<name>Ellenby et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2011/0098029</doc-number>
<kind>A1</kind>
<name>Rhoads et al.</name>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>455418</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2011/0143811</doc-number>
<kind>A1</kind>
<name>Rodriguez</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>4555561</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2011/0161076</doc-number>
<kind>A1</kind>
<name>Davis et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2011/0164163</doc-number>
<kind>A1</kind>
<name>Bilbrey et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>&#x201c;U.S. Appl. No. 12/652,725 , Response filed Oct. 29, 2012 to Final Office Action mailed Jun. 28, 2012&#x201d;, 9 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00048">
<othercit>&#x201c;U.S. Appl. No. 12/652,725, Examiner Interview Summary mailed Apr. 30, 2012&#x201d;, 3 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00049">
<othercit>&#x201c;U.S. Appl. No. 12/652,725, Final Office Action mailed Jun. 28, 2012&#x201d;, 10 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00050">
<othercit>&#x201c;U.S. Appl. No. 12/652,725, Non Final Office Action mailed Feb. 9, 2012&#x201d;, 9 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00051">
<othercit>&#x201c;U.S. Appl. No. 12/652,725, Notice of Allowance mailed Nov. 14, 2012&#x201d;, 8 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00052">
<othercit>&#x201c;U.S. Appl. No. 12/652,725, Response filed May 15, 2012 to Non Final Office Action mailed Feb. 9, 2012&#x201d;, 8 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00053">
<othercit>&#x201c;U.S. Appl. No. 12/652,725, Supplemental Notice of Allowance mailed Dec. 12, 2012&#x201d;, 2 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00054">
<othercit>Bilbrey, B., &#x201c;Live Video Object Manipulation Now: A Revolution in the Making&#x201d;, Advanced Imaging, (Jul. 1994), 2 pgs.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>25</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>34833301</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12652725</doc-number>
<date>20100105</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8400548</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13768072</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130155307</doc-number>
<kind>A1</kind>
<date>20130620</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<orgname>Apple Inc.</orgname>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Bilbrey</last-name>
<first-name>Brett C.</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>King</last-name>
<first-name>Nicholas V.</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Pance</last-name>
<first-name>Aleksandar</first-name>
<address>
<city>Saratoga</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Schwegman Lundberg &#x26; Woessner, P.A.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Apple Inc.</orgname>
<role>02</role>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Hannett</last-name>
<first-name>James</first-name>
<department>2662</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A device can receive live video of a real-world, physical environment on a touch sensitive surface. One or more objects can be identified in the live video. An information layer can be generated related to the objects. In some implementations, the information layer can include annotations made by a user through the touch sensitive surface. The information layer and live video can be combined in a display of the device. Data can be received from one or more onboard sensors indicating that the device is in motion. The sensor data can be used to synchronize the live video and the information layer as the perspective of video camera view changes due to the motion. The live video and information layer can be shared with other devices over a communication link.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="224.28mm" wi="174.92mm" file="US08625018-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="233.60mm" wi="164.68mm" file="US08625018-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="233.09mm" wi="169.93mm" file="US08625018-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="231.48mm" wi="172.55mm" file="US08625018-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="243.08mm" wi="171.53mm" orientation="landscape" file="US08625018-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="236.81mm" wi="164.17mm" file="US08625018-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="244.18mm" wi="178.39mm" orientation="landscape" file="US08625018-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="244.69mm" wi="135.21mm" file="US08625018-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="232.58mm" wi="179.41mm" file="US08625018-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="237.83mm" wi="174.16mm" orientation="landscape" file="US08625018-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CLAIM OF PRIORITY</heading>
<p id="p-0002" num="0001">This application is a continuation of and claims the benefit of priority under 35 U.S.C. &#xa7;120 to U.S. patent application Ser. No. 12/652,725, filed on Jan. 5, 2010, the benefit of priority of which is claimed hereby and is incorporated herein by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">This is related generally to augmented reality applications on multifunction devices.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Augmented Reality (AR) technology combines a live view of a real-world, physical environment with computer-generated imagery. Information about the real world environment can be stored and retrieved as an information layer which can be overlaid on the live view and interacted with by a user. Despite strong academic and commercial interest in AR systems, many existing AR systems are complex and expensive making such systems unsuitable for general use by the average consumer.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">A device can receive images and/or live video of a real-world, physical environment on a touch sensitive surface. One or more objects can be identified in the live video. One or more information layers can be generated related to the objects. In some implementations, an information layer can include annotations made by a user through the touch sensitive surface. The information layer and live video can be combined in a display of the device. Data can be received from one or more onboard sensors indicating that the device is in motion. The sensor data can be used to synchronize the live video and the information layer as the perspective of video camera view changes due to the motion. The live video and information layer can be shared with other devices over a communication link.</p>
<p id="p-0006" num="0005">In one embodiment, a device can provide a split screen display that can include a first display area for displaying the live video combined with the information layer and a second display area for displaying computer-generated imagery representing objects in the live video. The computer-generated imagery can be combined with the information layer in the second display area. A navigation control for allowing the user to navigate the computer-generated imagery can be provided with the split screen display. Alternatively, the user can navigate the computer-generated imagery by physically moving the device.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 1A</figref> illustrates an exemplary device for receiving live video of a real-world, physical environment.</p>
<p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. 1B</figref> illustrates the exemplary device of <figref idref="DRAWINGS">FIG. 1A</figref> displaying the live video combined with an information layer.</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 1C</figref> illustrates the exemplary device of <figref idref="DRAWINGS">FIG. 1B</figref> displaying a three-dimensional (3D) perspective view of the live video combined with the information layer.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 1D</figref> illustrates an exemplary method of synchronizing live video displays on first and second devices and sharing changes to the information layer.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 2A</figref> illustrates an exemplary device having a split screen display with computer-generated imagery.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 2B</figref> illustrates synchronizing split screen displays of exemplary first and second devices.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram of an exemplary process for synchronizing interactive AR displays.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of exemplary device architecture for implementing synchronized, interactive AR displays.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram of an exemplary network operating environment for devices implementing synchronized, interactive AR displays.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<heading id="h-0007" level="1">AR Display Overview</heading>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 1A</figref> illustrates example device <b>100</b> for receiving live video of a real-world, physical environment. Device <b>100</b> can be any device capable of supporting AR displays, including but not limited to personal computers, mobile phones, electronic tablets, game consoles, media players, etc. In some implementations, device <b>100</b> can be an electronic tablet having a touch sensitive surface <b>102</b>. In one embodiment, device <b>100</b> can include a video camera on a back surface (not shown). Other device configurations are possible including devices having video cameras on one or more surfaces.</p>
<p id="p-0017" num="0016">In the example shown, the user is holding device <b>100</b> over a circuit board. A live video <b>104</b> of the circuit board is shown on surface <b>102</b>. Various objects are shown in live video <b>104</b>. For example, the circuit board shown includes processor chip <b>106</b>, capacitor <b>108</b>, memory cards <b>110</b> and other components. The circuit board also includes bar code <b>112</b> and markers <b>114</b><i>a</i>, <b>114</b><i>b</i>. Virtual button <b>115</b> can be used to capture one or more frames of live video.</p>
<heading id="h-0008" level="1">Example Information Layer</heading>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 1B</figref> illustrates example device <b>100</b> of <figref idref="DRAWINGS">FIG. 1A</figref> displaying live video <b>104</b> combined with an information layer. Components <b>106</b>, <b>110</b> and <b>108</b> can be been outlined (e.g., with dashed or colored lines), highlighted or otherwise annotated by the information layer (hereafter referred to collectively as &#x201c;annotations&#x201d;). For example, memory cards <b>110</b> are shown outlined with dashed line <b>130</b> and processor <b>106</b> and capacitor <b>108</b> are shown with thick outlines. Generally, any visual attribute that can set off an object from other objects in live video <b>104</b> can be an annotation.</p>
<p id="p-0019" num="0018">Annotations can include text, images or references to other information (e.g., links). The annotations can be displayed proximate to their corresponding objects in live video <b>104</b>. Annotations can describe or otherwise provide useful information about the objects to a user (e.g., a computer technician). In the example shown, balloon call out <b>120</b> identifies memory cards <b>110</b>, balloon callout <b>122</b> identifies capacitor <b>108</b>, balloon callout <b>126</b> identifies processor <b>106</b> and balloon call out <b>128</b> identifies the circuit board. Additional related information, such as the manufacturer and part number can be included in the balloon callouts. Information layer can display annotations automatically or in response to trigger events. For example, the balloon call outs may only appear in live video <b>104</b> when the user is touching the corresponding annotated component.</p>
<p id="p-0020" num="0019">Before an information layer can be generated, the objects to be annotated can be identified. The identification of objects in live video <b>104</b> can occur manually or automatically. If automatically, a frame of live video <b>104</b> can be &#x201c;snapped&#x201d; (e.g., by pressing button <b>115</b>) and processed using known object recognition techniques, including but not limited to: edge detection, Scale-invariant Feature Transform (SIFT), template matching, gradient histograms, intraclass transfer learning, explicit and implicit 3D object models, global scene representations, shading, reflectance, texture, grammars, topic models, window-based detection, 3D cues, context, leveraging Internet data, unsupervised learning and fast indexing. The object recognition can be performed on device <b>100</b> or by a network resource (e.g., AR service <b>570</b> of <figref idref="DRAWINGS">FIG. 5</figref>).</p>
<p id="p-0021" num="0020">To assist in identification, barcode <b>112</b> can be identified by an image processor and used to retrieve a predefined information layer. To assist in overlaying the information layer onto live video <b>104</b>, and to align the annotations to the correct components, the image processor can identify marker <b>114</b><i>a </i>as indicating the top left corner of the circuit board. One or more markers can be used for an object. A location of a given annotation (e.g., dashed line <b>130</b>) in live video <b>104</b> can be a fixed distance and orientation with respect to marker <b>114</b><i>a. </i></p>
<p id="p-0022" num="0021">The information layer can include a variety of information from a variety of local or network information sources. Some examples of information include without limitation specifications, directions, recipes, data sheets, images, video clips, audio files, schemas, user interface elements, thumbnails, text, references or links, telephone numbers, blog or journal entries, notes, part numbers, dictionary definitions, catalog data, serial numbers, order forms, marketing or advertising and any other information that may be useful to a user. Some examples of information resources include without limitation: local databases or cache memory, network databases, Websites, online technical libraries, other devices, or any other information resource that can be accessed by device <b>100</b> either locally or remotely through a communication link. In the example shown, balloon call out <b>124</b> includes a manufacturer (&#x201c;Acme&#x201d;), name of component <b>108</b> (&#x201c;Capacitor&#x201d;) and part number (&#x201c;#C10361&#x201d;).</p>
<p id="p-0023" num="0022">Magnifying glass tool <b>116</b> can be manipulated by a user to magnify or zoom an object in live video <b>104</b>. For example, if the user wanted to see a detail of processor <b>106</b>, the user could move the magnifying glass tool <b>116</b> over processor <b>106</b> and live video <b>104</b> would zoom on processor <b>106</b> resulting in more detail. The view of the magnifying glass tool <b>116</b> can be sized using, for example, pinch gestures.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 1C</figref> illustrates the example device of <figref idref="DRAWINGS">FIG. 1B</figref> displaying a three-dimensional (3D) perspective view of the live video combined with the information layer. In this example, the user is pointing the video camera of device <b>100</b> at a different location to obtain a 3D perspective view of the circuit board. The information layer can be overlaid on the perspective view and aligned without having to re-perform object recognition using data output from onboard motion sensors. For example, outputs from onboard gyros, magnetometers or other motion sensors can be used to determine current video camera view angles relative to a reference coordinate frame and then use the view angles to redraw the information layer over the perspective view such that annotations remain properly aligned with their respective objects. In the example shown, annotation <b>130</b> (the dashed line) has been relocated to surround memory cards <b>110</b> without re-performing manual or automatic object recognition. Using onboard sensors is advantageous in that a user can maneuver device around a collection of objects and have annotations appear without incurring delays associated with object recognition processing. Object recognition can be performed once on a collection of objects and the sensor data can be use to update annotations for the objects.</p>
<p id="p-0025" num="0024">In some implementations, current video camera view angles can be used to index a look-up table of information layer data (e.g., annotations) for generating overlays that align correctly with objects in the live video. The video camera view angles can be represented by yaw, pitch and roll angles in a reference coordinate frame. For example, if we assume the yaw, pitch and roll angles are all zero when the video camera is pointing directly over the circuit board as shown in <figref idref="DRAWINGS">FIG. 1A</figref>, then the angle set (0,0,0) can be associated with the particular annotations shown in <figref idref="DRAWINGS">FIG. 1A</figref>. If the user pitches the video camera up by +90 degrees, then the angle set (0, 90, 0) can be associated with the annotations shown in <figref idref="DRAWINGS">FIG. 1C</figref>. The look up table can be stored on the device or provided by a network resource.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 1D</figref> illustrates synchronizing live video displays on first and second devices and sharing changes to the information layer. In the example shown, first device <b>100</b><i>a </i>is displaying live video <b>104</b><i>a</i>, which is capturing a perspective view of the circuit board. Live video <b>104</b><i>a </i>can be fed to second device <b>100</b><i>b </i>through a communication link (e.g., unidirectional or bidirectional) so that second device <b>100</b><i>b </i>displays live video <b>104</b><i>b </i>of the circuit board. The information layer generated for live video <b>104</b><i>a </i>on device <b>100</b><i>a </i>can also shared with device <b>100</b><i>b </i>by sending the information layer data with the live video feed over the communication link. The communication link can be wired or wireless (e.g., Bluetooth, WiFi).</p>
<p id="p-0027" num="0026">In some implementations, the sensor output data (e.g., video camera view angles) can be communicated to device <b>100</b><i>b </i>over the communication link so that the current orientation of the video camera on device <b>100</b><i>a </i>relative to the object is known to device <b>100</b><i>b</i>. This sensor data can be used by device <b>100</b><i>b </i>to regenerate the information overlay on device <b>100</b><i>b </i>without sending device <b>100</b><i>b </i>the actual information layer data.</p>
<p id="p-0028" num="0027">In some implementations, the user of either device <b>100</b><i>a </i>or device <b>100</b><i>b </i>can use touch input or gestures to generate new annotations (e.g., a draw a circle around a component) and those annotations can be shared with the other device through the communication link. In some implementations, a gesture itself can indicate desired information. For example, drawing a circle around processor <b>106</b> in live video <b>104</b> can indicate that the user wants more information about processor <b>106</b>. As a user draws annotations on live video <b>104</b><i>a </i>those annotations can be reflected to live video <b>104</b><i>b</i>. This feature allows users of devices <b>100</b><i>a</i>, <b>100</b><i>b </i>to interact and collaborate through the information layer. In some implementations, if devices <b>100</b><i>a</i>, <b>100</b><i>b </i>have telephony capability the users can speak to each other while observing live video <b>104</b><i>a</i>, <b>104</b><i>b </i>and the information layer.</p>
<heading id="h-0009" level="1">Other Example Applications</heading>
<p id="p-0029" num="0028">In one example application, device <b>100</b> can capture images or live video of a document and the text of the document can be recognized in the images or the live video. An information layer (e.g., an answer sheet) can be generated and combined with the live video. For example, a teacher can hold device <b>100</b> over a student's exam paper and an outline showing incorrect answers to exam questions can be displayed in the live video to assist the teach in grading the exam paper.</p>
<p id="p-0030" num="0029">In another example, device <b>100</b> can capture a live video of an engine of a car or other vehicle and the parts of the engine can be recognized from the live video. An information layer (e.g., a manual excerpt) can be generated and combined with the live video. For example, a car mechanic can hold device <b>100</b> over a car engine and an outline identifying parts and providing excerpts from a repair manual or schematics can be displayed in the live video to assist the mechanic in repairing the engine.</p>
<p id="p-0031" num="0030">Device <b>100</b> can be used in a variety of medical applications. In some implementations, a doctor can use device <b>100</b> to capture a live video of the patient's face. Using pattern recognition and/or other information (e.g., a bar code or other patient identifier), information related to the patient (e.g., medical history, drug prescriptions) can be displayed on device <b>100</b>. In other implementations, a live video of a body part that needs medical attention can be captured and augmented with annotations that can help the doctor make a diagnosis. The video can be shared with other doctors who can generate annotations on their respective devices to assist the doctor in a diagnosis. Pattern matching or other image processing can be used to identify problems with the injured body part based on its visual appearance (e.g., color). In one example application, an x-ray or MRI video can be displayed with the live video.</p>
<heading id="h-0010" level="1">Example Split Screen Display with Computer-Generated Imagery</heading>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 2A</figref> illustrates example device <b>200</b> having a split screen display with computer-generated imagery. In some implementations, a split screen display can be used to display an object or other subject matter on one side of the split, and computer-generated imagery (e.g., in either two or three dimensions) on the other side of the split. In the example shown, a user is viewing a live video of the skyline of downtown San Francisco in first display area <b>202</b>. Object recognition has been performed on a captured frame of video and an information layer has been generated. Specifically, balloon call outs have been displayed proximate to their respective buildings or structures in the live video. The user can interact with the information layer as described in reference to <figref idref="DRAWINGS">FIGS. 1A-1D</figref>.</p>
<p id="p-0033" num="0032">In some implementations, the live video scene can be determined and object recognition assisted by using an onboard positioning system (e.g., GPS, WiFi, Cell ID). For example, a frame of captured video of downtown San Francisco can be transmitted to a network resource, together with the current geographic coordinates of device <b>200</b> received from the onboard positioning system. Additionally, motion sensor data (e.g., angle data) can be sent to the network service that defines the current view of the onboard video camera capturing the live video. The motion sensor can be used to select a subset of pre-computed computer-generated imagery of downtown San Francisco that is relevant to the current view of the onboard video camera.</p>
<p id="p-0034" num="0033">Second display area <b>204</b> of the split screen display can show computer-generated imagery of the objects (e.g., buildings) in the images (e.g., live video) of display area <b>202</b>. In some implementations, the computer-generated imagery can be created on the fly or can be retrieved from a repository. For example, once the live video has been identified as downtown San Francisco, computer-generated imagery of downtown San Francisco can be downloaded from a network resource. Alternatively, known real-time rendering techniques can be used to generate 3D computer-generated imagery that can be navigated by the user. For example, 3D models of recognized objects of downtown San Francisco can be constructed out of geometrical vertices, faces, and edges in a 3D coordinate system. The models can be rendered using known real-time rendering techniques (e.g., orthographic or perspective projection, clipping, screen mapping, rasterizing) and transformed into the current view space of the live video camera. Transforming models into the current view space can be accomplished using sensor output from onboard sensors. For example, gyroscopes, magnetometers and other motion sensors can provide angular displacements, angular rates and magnetic readings with respect to a reference coordinate frame, and that data can be used by a real-time onboard rendering engine to generate 3D imagery of downtown San Francisco. If the user physically moves device <b>200</b>, resulting in a change of the video camera view, the information layer and computer-generated imagery can be updated accordingly using the sensor data. In some implementations, the user can manipulate navigation control <b>212</b> to navigate the 3D imagery (e.g., tilting, zooming, panning, moving).</p>
<p id="p-0035" num="0034">In some implementations, the current location of device <b>200</b> can be used to compute a route for display in the 3D computer-generated imagery. In the example shown, marker <b>206</b> (e.g., a pushpin) can be used to identify the current location of device <b>200</b> (in this example indicated as &#x201c;You&#x201d;), and second marker <b>210</b> can be used to identify a destination or another device (in this example indicated by &#x201c;Joe&#x201d;). A route can then be computed and overlaid on the 3D computer-generated imagery as shown in <figref idref="DRAWINGS">FIG. 2A</figref>. Touching markers <b>206</b>, <b>210</b> can invoke various applications on device <b>200</b>, such as a communication application (e.g., text messaging, chat session, email, telephony) for allowing communication between device <b>200</b><i>a </i>and device <b>200</b><i>b. </i></p>
<heading id="h-0011" level="1">Example Synchronization of Split Screen Displays</heading>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 2B</figref> illustrates synchronizing split screen displays of first and second devices <b>200</b><i>a</i>, <b>200</b><i>b</i>. In the example shown, device <b>200</b><i>a </i>has established communication with device <b>200</b><i>b</i>. The image (e.g., live video) scene of downtown San Francisco captured by the video camera on device <b>200</b><i>a </i>can be displayed in display area <b>202</b><i>b </i>of device <b>200</b><i>b</i>. Also, computer-generated imagery shown in display area <b>204</b><i>a </i>can be shown in display area <b>204</b><i>b </i>of device <b>200</b><i>b</i>. Note that in display area <b>204</b><i>b</i>, the location of device <b>200</b><i>b </i>is indicated by &#x201c;You&#x201d; and the destination or device <b>200</b><i>a </i>is indicated by the marker &#x201c;Mark,&#x201d; i.e., the user of device <b>200</b><i>a</i>. The communication link can be a direct communication link or an indirect communication link using wireless network access points (e.g., WiFi access points). The communication link can also include a wide area network, such as the Internet.</p>
<p id="p-0037" num="0036">When a user moves device <b>200</b><i>a</i>, resulting in a change in the video camera view, motion sensor data can be used to update the computer-generated imagery in display areas <b>204</b><i>a</i>, <b>204</b><i>b</i>, thus maintaining synchronization between display areas <b>202</b><i>a</i>, <b>204</b><i>a </i>and display areas <b>202</b><i>b</i>, <b>204</b><i>b</i>. In some implementations, share button <b>214</b> can be used to initiate sharing of live video, the information layer and computer-generated imagery with another device.</p>
<heading id="h-0012" level="1">Example Process for Synchronizing Displays</heading>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram of an example process <b>300</b> for synchronizing, interactive AR displays. Process <b>300</b> can be described in reference to devices <b>100</b>, <b>200</b>.</p>
<p id="p-0039" num="0038">In some implementations, process <b>300</b> can begin on a device (e.g., device <b>100</b> or <b>200</b>) by capturing live video of a real-world, physical environment (<b>302</b>). One or more objects in the live video can be identified (<b>304</b>). The objects can be identified manually (e.g., by user selection using touch input) or automatically using known object recognition techniques. An information layer related to the one or more objects is generated and can include one or more annotations (<b>306</b>). The information layer and live video are combined in a display (<b>308</b>). Sensor data generated by one or more onboard sensors is received (<b>310</b>). The data can be angle data from a gyro, for example. The live video and information layer are synchronized using the sensor data (<b>312</b>). Optionally, computer imagery can be generated representing objects in the live video (<b>314</b>). The computer imagery can be pre-computed and retrieved from a repository or generated on the fly using known real-time rendering techniques. Optionally, the annotated live video, computer-generated imagery and information layer can be displayed in a split screen display (<b>316</b>), as described in reference to <figref idref="DRAWINGS">FIG. 2A</figref>. Optionally, the annotated live video, computer-generated imagery and information layer can be shared (<b>318</b>) with one or more other devices, and the AR displays of the devices can be synchronized to account for changes in video views.</p>
<heading id="h-0013" level="1">Example Device Architecture</heading>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram of an example architecture for a device <b>400</b> implementing synchronized, interactive AR displays. Device <b>400</b> can include memory interface <b>402</b>, one or more data processors, image processors and/or central processing units <b>404</b>, and peripherals interface <b>406</b>. Memory interface <b>402</b>, one or more processors <b>404</b> and/or peripherals interface <b>406</b> can be separate components or can be integrated in one or more integrated circuits. The various components in device <b>400</b> can be coupled by one or more communication buses or signal lines.</p>
<p id="p-0041" num="0040">Sensors, devices, and subsystems can be coupled to peripherals interface <b>406</b> to facilitate multiple functionalities. For example, motion sensor <b>410</b>, light sensor <b>412</b>, and proximity sensor <b>414</b> can be coupled to peripherals interface <b>406</b> to facilitate various orientation, lighting, and proximity functions. For example, in some implementations, light sensor <b>412</b> can be utilized to facilitate adjusting the brightness of touch screen <b>446</b>. In some implementations, motion sensor <b>411</b> can be utilized to detect movement of the device. Accordingly, display objects and/or media can be presented according to a detected orientation, e.g., portrait or landscape.</p>
<p id="p-0042" num="0041">Other sensors <b>416</b> can also be connected to peripherals interface <b>406</b>, such as a temperature sensor, a biometric sensor, a gyroscope, magnetometer or other sensing device, to facilitate related functionalities.</p>
<p id="p-0043" num="0042">For example, positioning information can be received by device <b>400</b> from positioning system <b>432</b>. Positioning system <b>432</b>, in various implementations, can be a component internal to device <b>400</b>, or can be an external component coupled to device <b>400</b> (e.g., using a wired connection or a wireless connection). In some implementations, positioning system <b>432</b> can include a GPS receiver and a positioning engine operable to derive positioning information from received GPS satellite signals. In other implementations, positioning system <b>432</b> can include a compass (e.g., a magnetic compass) and an accelerometer, as well as a positioning engine operable to derive positioning information based on dead reckoning techniques. In still further implementations, positioning system <b>432</b> can use wireless signals (e.g., cellular signals, IEEE 802.11 signals) to determine location information associated with the device Hybrid positioning systems using a combination of satellite and television signals.</p>
<p id="p-0044" num="0043">Broadcast reception functions can be facilitated through one or more radio frequency (RF) receiver(s) <b>418</b>. An RF receiver can receive, for example, AM/FM broadcasts or satellite broadcasts (e.g., XM&#xae; or Sirius&#xae; radio broadcast). An RF receiver can also be a TV tuner. In some implementations, RF receiver <b>418</b> is built into wireless communication subsystems <b>424</b>. In other implementations, RF receiver <b>418</b> is an independent subsystem coupled to device <b>400</b> (e.g., using a wired connection or a wireless connection). RF receiver <b>418</b> can receive simulcasts. In some implementations, RF receiver <b>418</b> can include a Radio Data System (RDS) processor, which can process broadcast content and simulcast data (e.g., RDS data). In some implementations, RF receiver <b>418</b> can be digitally tuned to receive broadcasts at various frequencies. In addition, RF receiver <b>418</b> can include a scanning function which tunes up or down and pauses at a next frequency where broadcast content is available.</p>
<p id="p-0045" num="0044">Camera subsystem <b>420</b> and optical sensor <b>422</b>, e.g., a charged coupled device (CCD) or a complementary metal-oxide semiconductor (CMOS) optical sensor, can be utilized to facilitate camera functions, such as recording photographs and video clips.</p>
<p id="p-0046" num="0045">Communication functions can be facilitated through one or more communication subsystems <b>424</b>. Communication subsystem(s) can include one or more wireless communication subsystems and one or more wired communication subsystems. Wireless communication subsystems can include radio frequency receivers and transmitters and/or optical (e.g., infrared) receivers and transmitters. Wired communication system can include a port device, e.g., a Universal Serial Bus (USB) port or some other wired port connection that can be used to establish a wired connection to other computing devices, such as other communication devices, network access devices, a personal computer, a printer, a display screen, or other processing devices capable of receiving and/or transmitting data. The specific design and implementation of communication subsystem <b>424</b> can depend on the communication network(s) or medium(s) over which device <b>400</b> is intended to operate. For example, device <b>400</b> may include wireless communication subsystems designed to operate over a global system for mobile communications (GSM) network, a GPRS network, an enhanced data GSM environment (EDGE) network, 802.x communication networks (e.g., WiFi, WiMax, or 3G networks), code division multiple access (CDMA) networks, and a Bluetooth&#x2122; network. Communication subsystems <b>424</b> may include hosting protocols such that Device <b>400</b> may be configured as a base station for other wireless devices. As another example, the communication subsystems can allow the device to synchronize with a host device using one or more protocols, such as, for example, the TCP/IP protocol, HTTP protocol, UDP protocol, and any other known protocol.</p>
<p id="p-0047" num="0046">Audio subsystem <b>426</b> can be coupled to speaker <b>428</b> and one or more microphones <b>430</b>. One or more microphones <b>430</b> can be used, for example, to facilitate voice-enabled functions, such as voice recognition, voice replication, digital recording, and telephony functions.</p>
<p id="p-0048" num="0047">I/O subsystem <b>440</b> can include touch screen controller <b>442</b> and/or other input controller(s) <b>444</b>. Touch-screen controller <b>442</b> can be coupled to touch screen <b>446</b>. Touch screen <b>446</b> and touch screen controller <b>442</b> can, for example, detect contact and movement or break thereof using any of a number of touch sensitivity technologies, including but not limited to capacitive, resistive, infrared, and surface acoustic wave technologies, as well as other proximity sensor arrays or other elements for determining one or more points of contact with touch screen <b>446</b> or proximity to touch screen <b>446</b>.</p>
<p id="p-0049" num="0048">Other input controller(s) <b>444</b> can be coupled to other input/control devices <b>448</b>, such as one or more buttons, rocker switches, thumb-wheel, infrared port, USB port, or a pointer device such as a stylus. The one or more buttons (not shown) can include an up/down button for volume control of speaker <b>428</b> and/or microphone <b>430</b>.</p>
<p id="p-0050" num="0049">In one implementation, a pressing of the button for a first duration may disengage a lock of touch screen <b>446</b>; and a pressing of the button for a second duration that is longer than the first duration may turn power to device <b>400</b> on or off. The user may be able to customize a functionality of one or more of the buttons. Touch screen <b>446</b> can, for example, also be used to implement virtual or soft buttons and/or a keyboard.</p>
<p id="p-0051" num="0050">In some implementations, device <b>400</b> can present recorded audio and/or video files, such as MP3, AAC, and MPEG files. In some implementations, device <b>400</b> can include the functionality of an MP3 player, such as an iPhone&#x2122;.</p>
<p id="p-0052" num="0051">Memory interface <b>402</b> can be coupled to memory <b>450</b>. Memory <b>450</b> can include high-speed random access memory and/or non-volatile memory, such as one or more magnetic disk storage devices, one or more optical storage devices, and/or flash memory (e.g., NAND, NOR). Memory <b>450</b> can store operating system <b>452</b>, such as Darwin, RTXC, LINUX, UNIX, OS X, WINDOWS, or an embedded operating system such as VxWorks. Operating system <b>452</b> may include instructions for handling basic system services and for performing hardware dependent tasks. In some implementations, operating system <b>452</b> can be a kernel (e.g., UNIX kernel).</p>
<p id="p-0053" num="0052">Memory <b>450</b> may also store communication instructions <b>454</b> to facilitate communicating with one or more additional devices, one or more computers and/or one or more servers. Communication instructions <b>454</b> can also be used to select an operational mode or communication medium for use by the device, based on a geographic location (obtained by GPS/Navigation instructions <b>468</b>) of the device. Memory <b>450</b> may include graphical user interface instructions <b>456</b> to facilitate graphic user interface processing; sensor processing instructions <b>458</b> to facilitate sensor-related processing and functions; phone instructions <b>460</b> to facilitate phone-related processes and functions; electronic messaging instructions <b>462</b> to facilitate electronic-messaging related processes and functions; web browsing instructions <b>464</b> to facilitate web browsing-related processes and functions; media processing instructions <b>466</b> to facilitate media processing-related processes and functions; GPS/Navigation instructions <b>468</b> to facilitate GPS and navigation-related processes and instructions, e.g., mapping a target location; camera instructions <b>470</b> to facilitate camera-related processes and functions (e.g., live video); and augmented reality instructions <b>472</b> to facilitate the processes and features described in reference to <figref idref="DRAWINGS">FIGS. 1-3</figref>. Memory <b>450</b> may also store other software instructions (not shown), such as web video instructions to facilitate web video-related processes and functions; and/or web shopping instructions to facilitate web shopping-related processes and functions. In some implementations, media processing instructions <b>466</b> are divided into audio processing instructions and video processing instructions to facilitate audio processing-related processes and functions and video processing-related processes and functions, respectively.</p>
<p id="p-0054" num="0053">Each of the above identified instructions and applications can correspond to a set of instructions for performing one or more functions described above. These instructions need not be implemented as separate software applications, procedures, or modules. Memory <b>450</b> can include additional instructions or fewer instructions. Furthermore, various functions of device <b>400</b> may be implemented in hardware and/or in software, including in one or more signal processing and/or application specific integrated circuits.</p>
<heading id="h-0014" level="1">Example Network Operating Environment</heading>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram of an example network operating environment for devices implementing synchronized, interactive augmented reality displays. Devices <b>502</b><i>a </i>and <b>502</b><i>b </i>can, for example, communicate over one or more wired and/or wireless networks <b>510</b> in data communication. For example, wireless network <b>512</b>, e.g., a cellular network, can communicate with a wide area network (WAN) <b>514</b>, such as the Internet, by use of gateway <b>516</b>. Likewise, access device <b>518</b>, such as an 802.11g wireless access device, can provide communication access to wide area network <b>514</b>. In some implementations, both voice and data communications can be established over wireless network <b>512</b> and access device <b>518</b>. For example, device <b>502</b><i>a </i>can place and receive phone calls (e.g., using VoIP protocols), send and receive e-mail messages (e.g., using POP3 protocol), and retrieve electronic documents or streams, such as Web pages, photographs, and videos, over wireless network <b>512</b>, gateway <b>516</b>, and wide area network <b>514</b> (e.g., using TCP/IP or UDP protocols). Likewise, in some implementations, device <b>502</b><i>b </i>can place and receive phone calls, send and receive e-mail messages, and retrieve electronic documents over access device <b>1218</b> and wide area network <b>514</b>. In some implementations, devices <b>502</b><i>a </i>or <b>502</b><i>b </i>can be physically connected to access device <b>518</b> using one or more cables and access device <b>518</b> can be a personal computer. In this configuration, device <b>502</b><i>a </i>or <b>502</b><i>b </i>can be referred to as a &#x201c;tethered&#x201d; device.</p>
<p id="p-0056" num="0055">Devices <b>502</b><i>a </i>and <b>502</b><i>b </i>can also establish communications by other means. For example, wireless device <b>502</b><i>a </i>can communicate with other wireless devices, e.g., other devices <b>502</b><i>a </i>or <b>502</b><i>b</i>, cell phones, etc., over wireless network <b>512</b>. Likewise, devices <b>502</b><i>a </i>and <b>502</b><i>b </i>can establish peer-to-peer communications <b>520</b>, e.g., a personal area network, by use of one or more communication subsystems, such as a Bluetooth&#x2122; communication device. Other communication protocols and topologies can also be implemented.</p>
<p id="p-0057" num="0056">Devices <b>502</b><i>a </i>or <b>502</b><i>b </i>can, for example, communicate with one or more services over one or more wired and/or wireless networks <b>510</b>. These services can include, for example, navigation services <b>530</b>, messaging services <b>540</b>, media services <b>550</b>, location based services <b>580</b>, syncing services <b>560</b> and AR services <b>570</b>. Syncing services <b>560</b> can support over network syncing of AR displays on two or more devices. AR services <b>570</b> can provide services to support the AR features and processes described in reference to <figref idref="DRAWINGS">FIGS. 1-3</figref>.</p>
<p id="p-0058" num="0057">Device <b>502</b><i>a </i>or <b>502</b><i>b </i>can also access other data and content over one or more wired and/or wireless networks <b>510</b>. For example, content publishers, such as news sites, RSS feeds, web sites, blogs, social networking sites, developer networks, etc., can be accessed by Device <b>502</b><i>a </i>or <b>502</b><i>b</i>. Such access can be provided by invocation of a web browsing function or application (e.g., a browser) in response to a user touching, for example, a Web object.</p>
<p id="p-0059" num="0058">The features described can be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them. The features can be implemented in a computer program product tangibly embodied in an information carrier, e.g., in a machine-readable storage device, for execution by a programmable processor; and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output. Alternatively or addition, the program instructions can be encoded on a propagated signal that is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information fro transmission to suitable receiver apparatus for execution by a programmable processor.</p>
<p id="p-0060" num="0059">The described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from, and to transmit data and instructions to, a data storage system, at least one input device, and at least one output device. A computer program is a set of instructions that can be used, directly or indirectly, in a computer to perform a certain activity or bring about a certain result. A computer program can be written in any form of programming language (e.g., Objective-C, Java), including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.</p>
<p id="p-0061" num="0060">Suitable processors for the execution of a program of instructions include, by way of example, both general and special purpose microprocessors, and the sole processor or one of multiple processors or cores, of any kind of computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally, a computer will also include, or be operatively coupled to communicate with, one or more mass storage devices for storing data files; such devices include magnetic disks, such as internal hard disks and removable disks; magneto-optical disks; and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, ASICs (application-specific integrated circuits).</p>
<p id="p-0062" num="0061">To provide for interaction with a user, the features can be implemented on a computer having a display device such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.</p>
<p id="p-0063" num="0062">The features can be implemented in a computer system that includes a back-end component, such as a data server, or that includes a middleware component, such as an application server or an Internet server, or that includes a front-end component, such as a client computer having a graphical user interface or an Internet browser, or any combination of them. The components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include, e.g., a LAN, a WAN, and the computers and networks forming the Internet.</p>
<p id="p-0064" num="0063">The computer system can include clients and servers. A client and server are generally remote from each other and typically interact through a network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p>
<p id="p-0065" num="0064">One or more features or steps of the disclosed embodiments can be implemented using an Application Programming Interface (API). An API can define on or more parameters that are passed between a calling application and other software code (e.g., an operating system, library routine, function) that provides a service, that provides data, or that performs an operation or a computation.</p>
<p id="p-0066" num="0065">The API can be implemented as one or more calls in program code that send or receive one or more parameters through a parameter list or other structure based on a call convention defined in an API specification document. A parameter can be a constant, a key, a data structure, an object, an object class, a variable, a data type, a pointer, an array, a list, or another call. API calls and parameters can be implemented in any programming language. The programming language can define the vocabulary and calling convention that a programmer will employ to access functions supporting the API.</p>
<p id="p-0067" num="0066">In some implementations, an API call can report to an application the capabilities of a device running the application, such as input capability, output capability, processing capability, power capability, communications capability, etc.</p>
<p id="p-0068" num="0067">A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made. For example, elements of one or more implementations may be combined, deleted, modified, or supplemented to form further implementations. As yet another example, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, or steps may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other implementations are within the scope of the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A non-transitory machine-readable storage medium comprising a plurality of instructions that, in response to being executed on a computing device, cause the computing device to:
<claim-text>capture live video of a real-world, physical environment, and display the live video on a touch sensitive surface of the computing device;</claim-text>
<claim-text>combine an information layer and the live video, the information layer related to one or more objects in the live video;</claim-text>
<claim-text>generate computer-generated imagery based on the live video;</claim-text>
<claim-text>display the computer-generated imagery representing one or more objects in the live video on the touch sensitive surface;</claim-text>
<claim-text>overlay the information layer on the computer-generated imagery;</claim-text>
<claim-text>receive sensor data from one or more onboard motion sensors indicating that the computing device is in motion; and</claim-text>
<claim-text>synchronize the display of the live video, the computer-generated imagery, and the information layer on the touch sensitive surface using the sensor data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising instructions that cause the computing device to:
<claim-text>share at least one of the synchronized live video, the computer-generated imagery, and information layer with a second device over a communication link.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising instructions that cause the computing device to:
<claim-text>generate a split screen display having first and second display areas, the first display area configured for displaying the live video combined with the information layer and the second display area for displaying the computer-generated imagery combined with the information layer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising instructions that cause the computing device to:
<claim-text>display a navigation control for navigating the computer-generated imagery on the touch sensitive surface.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising instructions that cause the computing device to:
<claim-text>perform object recognition on the live video;</claim-text>
<claim-text>obtain object information for one or more recognized objects; and</claim-text>
<claim-text>generate the information layer for the one or more recognized objects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00005">claim 5</claim-ref>, where the instructions that cause the computing device to generate the information layer, further comprise instructions that cause the computing device to:
<claim-text>include in the information layer one or more links to one or more network resources providing information related to the one or more recognized objects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising instructions that cause the computing device to:
<claim-text>label the recognized one or more objects in the live video display.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00007">claim 7</claim-ref>, where at least one object is labeled with a bar code for identifying the object during the object recognition.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00007">claim 7</claim-ref>, where at least one object is labeled with one or more markers for identifying an orientation of the object in the live video during object recognition.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00007">claim 7</claim-ref>, where at least one label includes content related to the recognized object.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising instructions that cause the computing device to:
<claim-text>receive input through the touch sensitive display, the input identifying an object in the live video;</claim-text>
<claim-text>perform object recognition on the live video to identify the identified object;</claim-text>
<claim-text>obtain object information for the recognized object; and</claim-text>
<claim-text>generate an information layer based on the object information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the computing device is a mobile device.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, where at least one of the computing device and the second device is an electronic tablet.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising instructions that cause the computing device to:
<claim-text>determine a route between the computing device and a destination; and</claim-text>
<claim-text>display the route with the computer-generated imagery.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where an object in the real-world, physical environment captured by the live video is a document, the machine-readable storage medium further comprising instructions that cause the computing device to:
<claim-text>generate annotations related to the document; and</claim-text>
<claim-text>overlay the annotations proximate the document on the live video.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where an object in the real-world, physical environment captured by the live video is a machine, circuit board or part thereof, the machine-readable storage medium further comprising instructions that cause the computing device to:
<claim-text>generate annotations related to the object; and</claim-text>
<claim-text>overlay the annotations on the live video or computer-generated imagery.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the instructions that cause the computing device to generate the computer-generated imagery based on the live video includes instructions that cause the computing device to generate a three-dimensional model of the one or more objects in the live video.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The non-transitory machine-readable storage medium of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the instructions that cause the computing device to combine the information layer includes instructions to generate one or more annotations for display in response to a touch input at a location on the touch sensitive surface corresponding to the object.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. An augmented reality system, comprising:
<claim-text>a touch sensitive surface configured for receiving touch input;</claim-text>
<claim-text>one or more onboard motion sensors configured for sensing motion;</claim-text>
<claim-text>a video camera for capturing live video of a real-world, physical environment, for display on the touch sensitive surface;</claim-text>
<claim-text>a processor coupled to the touch sensitive surface, the motion sensor and the video camera, the processor configured for,
<claim-text>combining an information layer and the live video for display on the touch sensitive surface,</claim-text>
<claim-text>modeling computer-generated imagery based on the live video,</claim-text>
<claim-text>displaying computer-generated imagery representing one or more objects in the live video on the touch sensitive surface,</claim-text>
<claim-text>overlaying the information layer on the computer-generated imagery for display on the touch sensitive surface,</claim-text>
<claim-text>receiving sensor data from the one or more onboard motion sensors indicating that the apparatus is in motion, and</claim-text>
<claim-text>synchronizing the live video, computer-generated imagery, and the information layer using the sensor data; and</claim-text>
</claim-text>
<claim-text>a communication interface configured for sharing the live video over a communication link.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, where modeling the computer-generated imagery based on the live video includes generating a three-dimensional model of the one or more objects in the live video.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, where the processor is further configured for:
<claim-text>generating a split screen display having first and second display areas, the first display area configured for displaying the live video combined with the information layer and the second display area for displaying the computer-generated imagery combined with the information layer; and</claim-text>
<claim-text>displaying the split screen on the touch sensitive surface.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, where the processor is further configured for:
<claim-text>accessing an information resource via the communication interface; and</claim-text>
<claim-text>retrieving information to be presented on the touch sensitive surface as the information layer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, where the information layer includes one or more annotations related to an object in the live video, the one or more annotations being displayed in response to the touch input at a location on the touch sensitive surface corresponding to the object.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, where the one or more annotations may include any one of: specifications, directions, recipes, data sheets, images, video clips, audio files, schemas, user interface elements, thumbnails, text, references, hyperlinks, telephone numbers, notes, part numbers, dictionary definitions, catalog data, serial numbers, order forms, marketing material or advertisements.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. A mobile device comprising:
<claim-text>a touch sensitive display configured for displaying a graphical user interface and receiving touch input;</claim-text>
<claim-text>a sensor configured to generate data related to position of the mobile device;</claim-text>
<claim-text>a video camera to capture live video for display on the touch sensitive display; and</claim-text>
<claim-text>a processor coupled to the touch sensitive display, the sensor and the video camera, the processor configured to,
<claim-text>generate an information layer including data related to an object within the live video captured by the video camera,</claim-text>
<claim-text>integrate the information layer and the live video for display on the touch sensitive display,</claim-text>
<claim-text>model computer-generated imagery based at least in part on the live video,</claim-text>
<claim-text>display the computer-generated imagery representing one or more objects in the live video on the touch sensitive display,</claim-text>
<claim-text>overlay the information layer on the computer-generated imagery for display on the touch sensitive display,</claim-text>
<claim-text>process sensor data from the sensors indicating that the mobile device is in motion, and</claim-text>
<claim-text>synchronize the live video, computer-generated imagery, and the information layer using the sensor data. </claim-text>
</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
