<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627016-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627016</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13936727</doc-number>
<date>20130708</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>08</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>711141</main-classification>
<further-classification>711148</further-classification>
</classification-national>
<invention-title id="d2e43">Maintaining data coherence by using data domains</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5909553</doc-number>
<kind>A</kind>
<name>Campbell et al.</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6085295</doc-number>
<kind>A</kind>
<name>Ekanadham et al.</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7146489</doc-number>
<kind>B2</kind>
<name>Dowling</name>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7194517</doc-number>
<kind>B2</kind>
<name>Conway et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2004/0083284</doc-number>
<kind>A1</kind>
<name>Ofek et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2005/0005074</doc-number>
<kind>A1</kind>
<name>Landin et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2007/0106850</doc-number>
<kind>A1</kind>
<name>Dai et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2008/0082756</doc-number>
<kind>A1</kind>
<name>Shen</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2008/0225687</doc-number>
<kind>A1</kind>
<name>Oksman</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2010/0142401</doc-number>
<kind>A1</kind>
<name>Morris</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>JP</country>
<doc-number>2007160922</doc-number>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>Leung, H. K. Y. et al., &#x201c;Efficient Matching for State-Persistent Publish/Subscribe Systems&#x201d; IBM Centre for Advanced Studies Conference (2003) pp. 182-196.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Mattos, N. M., &#x201c;Integrating Information for on Demand Computing&#x201d; Proceedings of the 29th International Conference on Very Large Data Bases (2003) pp. 8-14, vol. 29.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Phan, X. H. et al., &#x201c; Learning to Classify Short and Sparse Text and Web with Hidden Topics from Large-scale Data Collections&#x201d; ACM Transactions on Asian Language Information Processing (TALIP) (2009) pp. 91-100, vol. 8(3).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>5</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>4</number-of-drawing-sheets>
<number-of-figures>4</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12633428</doc-number>
<date>20091208</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8484422</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13936727</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130297914</doc-number>
<kind>A1</kind>
<date>20131107</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>International Business Machines Corporation</orgname>
<address>
<city>Armonk</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Ekanadham</last-name>
<first-name>Kattamuri</first-name>
<address>
<city>Yorktown Heights</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Il</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Pattnaik</last-name>
<first-name>Pratap</first-name>
<address>
<city>Yorktown Heights</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Scully, Scott, Murphy &#x26; Presser PC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Davis, Esq.</last-name>
<first-name>Jennifer</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>International Business Machines Corporation</orgname>
<role>02</role>
<address>
<city>Armonk</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Nguyen</last-name>
<first-name>Than</first-name>
<department>2188</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method, system and computer program product are disclosed for maintaining data coherence, for use in a multi-node processing system where each of the nodes includes one or more components. In one embodiment, the method comprises establishing a data domain, assigning a group of the components to the data domain, sending a coherence message from a first component of the processing system to a second component of the processing system, and determining if that second component is assigned to the data domain. In this embodiment, if that second component is assigned to the data domain, the coherence message is transferred to all of the components assigned to the data domain to maintain data coherency among those components. In an embodiment, if that second component is assigned to the data domain, the first component is assigned to the data domain.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="166.88mm" wi="220.98mm" file="US08627016-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="224.03mm" wi="178.56mm" orientation="landscape" file="US08627016-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="201.34mm" wi="174.58mm" file="US08627016-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="148.59mm" wi="158.83mm" file="US08627016-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="233.26mm" wi="188.13mm" orientation="landscape" file="US08627016-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application is a continuation of copending U.S. patent application Ser. No. 12/633,428, filed Dec. 8, 2009, the entire content and disclosure of which is hereby incorporated herein by reference.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">This invention generally relates to data processing systems, and more specifically, to maintaining data coherence in multi-node data processing systems.</p>
<p id="p-0005" num="0004">2. Background Art</p>
<p id="p-0006" num="0005">Large-scale shared memory multi-processor computer systems typically have a large number of processing nodes (e.g., with one or more microprocessors and local memory) that cooperate to perform a common task. For example, selected nodes on a multi-processor computer system may cooperate to multiply a complex matrix. To do this in a rapid and efficient manner, such computer systems typically divide the task into discrete parts that each are executed by one or more of the nodes.</p>
<p id="p-0007" num="0006">When dividing a task, the nodes often share data. To that end, the microprocessors within the nodes each may access the memory of many of the other nodes. Those other microprocessors could be in the same node, or in different nodes. For example, a microprocessor may retrieve data from the memory of another node. Also, rather than retrieving the data from another node each time the data is needed, a microprocessor may store and access its locally held copies (cached copies) of data to perform local functions.</p>
<p id="p-0008" num="0007">Problems may arise, however, when the data that held by one microprocessor changes, and another microprocessor that uses the data has not been notified of the change. When that happens, the locally held data may no longer be accurate, potentially corrupting operations that rely upon the retrieved data. To mitigate these problems, computer systems that share data in this manner typically execute cache coherency protocols to ensure that all copies of the data are consistent.</p>
<p id="p-0009" num="0008">As data processing systems grow larger, operations for maintaining system-wide coherence incur larger latencies as these operations involve round-trips to all units participating in the coherence mechanism. Modern systems employ optimization schemes to limit the propagation of coherence traffic to a subset of units, wherever it is possible to detect that it is sufficient to limit the coherence check to that Subset. A simple example is node pumping in a large system comprised of several nodes, where if a node can detect that a certain line is currently confined to caches within that node, then it is sufficient to send an invalidation message only to caches within that node, in order to gain exclusive access to that line. Often such tracking is complex and tends to be speculative.</p>
<heading id="h-0003" level="1">BRIEF SUMMARY</heading>
<p id="p-0010" num="0009">The central activity in a computing system is the movement of data among the various components. Consequently, system performance is intimately tied to the manner in which data movements are orchestrated. Embodiments of the invention utilize a mechanism, referred to as Data Domain, that tracks movement of data within a set of components associated with the domain. Embodiments of the invention provide domain controls which can be used by system software to (a) reduce unnecessary coherence traffic in the systems, (b) co-locate producers and consumers of shared data in close proximity, and (c) automate auxiliary operations on data, such as compression, encryption, mirroring.</p>
<p id="p-0011" num="0010">Embodiments of the invention provide a method, system and computer program product for maintaining data coherence, for use in a multi-node processing system. Each of the nodes includes one or more components. In one embodiment, the method comprises establishing a data domain, assigning a group of said components to the data domain, sending a coherence message from one of the components of the multi-node processing system to another of the components of the multi-node processing system, and determining if said another of the components is assigned to the data domain. In this embodiment, if said another of the components is assigned to the data domain, said coherence message is transferred to all of the components assigned to the data domain to maintain data coherency among said all of the components.</p>
<p id="p-0012" num="0011">In an embodiment, if said another of the components is assigned to the data domain, the method further comprises assigning said one of the components to the data domain. In an embodiment, the method further comprises, if said another of the components is not assigned to the data domain, establishing a new data domain and assigning said another of the components to said new data domain.</p>
<p id="p-0013" num="0012">In one embodiment, the sending includes obtaining an address for the data in said another of the components, and the determining includes determining if said address is assigned to the data domain. In an embodiment, the address is associated to a memory location referred by a processor with a domain that includes that processor as well as the memory bank supporting that address, and this association is made during an address translation time and can be remembered in tables or other apparatus used to implement address translation in the processing system.</p>
<p id="p-0014" num="0013">In an embodiment, the transferring said coherence message to all of the components assigned to the data domain includes transferring said coherence message only to said all of the components assigned to the data domain to reduce data coherence traffic in the processing system. In one embodiment, the method further comprises co-locating or rescheduling operations of the components to reduce the number of nodes having components assigned to the data domain. For example, in an embodiment, a plurality of components on a first node are assigned to the data domain, and at least one component on a second node is assigned to the data domain. In this example, said co-locating or rescheduling includes rescheduling operations of said at least one component to one of the components of the first node.</p>
<p id="p-0015" num="0014">In an embodiment, the method further comprises identifying one or more actions to be performed on data moving to any of the components assigned to the data domain. In one embodiment, the establishing includes establishing a domain table for the data domain, and the domain table identifies all of the components assigned to the data domain. In an embodiment, the domain table identifies said one or more actions to be performed on the data moving to any of the components assigned to the data domains.</p>
<p id="p-0016" num="0015">One or more embodiments of the invention provide a means of dynamically creating domains, a domain being a set of components, a component being either a processor port or a memory port; a means of dynamically changing existing domains, by adding/removing components to/from a domain, and allowing the possibility of a component to be present in multiple domains at the same time; and a means to uniquely identify a domain in the system.</p>
<p id="p-0017" num="0016">One or more embodiments of the invention provide a means for a processor port to submit a message, tagged with a domain identification; a means to maintain coherence within a domain, which means that whenever a message with a domain id is submitted, the message will be delivered to all components in that domain and need not be seen by components outside that domain; and a means to associate a domain with auxiliary functions performed on a message being communicated in a domain, examples of functions being encryption/decryption, compression/decompression or mirroring (which means a copy of the message is automatically stored at another specified memory component).</p>
<p id="p-0018" num="0017">One or more embodiments of the invention provide a method to associate the address to a memory location referred by a process with a domain that includes that processor as well as the memory bank supporting that address and this association is made during the address translation time and can be remembered in the tables or other apparatus used to implement address translation in a computer system; and a method for the communication subsystem to maintain the domain table, associating components and domains, so that each message tagged with a domain id can be delivered to all the components of that domain.</p>
<p id="p-0019" num="0018">One or more embodiments of the invention provide a method to keep the above domain table consistent, as changes are made to domains dynamically; a method for the communication subsystem to implement the auxiliary functions associated with a domain; a method for memory allocation manager to consult the domain map, so that the newly allocated memory is conveniently located to reduce the latency of communication within a domain; and a method for process scheduler manager to consult the domain map, so that processes are scheduled on processors to reduce the latency of their memory accesses.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS</heading>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 1</figref> shows a collection of data processing components, a plurality of data domains, and a domain table in accordance with an embodiment of the invention.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 2</figref> illustrates procedures for assigning and obtaining domain ids.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 3</figref> shows examples of the use of a data domain in accordance with embodiments of the invention.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 4</figref> depicts a data processing system in which the present invention may be embodied.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0024" num="0023">As will be appreciated by one skilled in the art, the present invention may be embodied as a system, method or computer program product. Accordingly, the present invention may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a &#x201c;circuit,&#x201d; &#x201c;module&#x201d; or &#x201c;system.&#x201d; Furthermore, the present invention may take the form of a computer program product embodied in any tangible medium of expression having computer usable program code embodied in the medium.</p>
<p id="p-0025" num="0024">Any combination of one or more computer usable or computer readable medium(s) may be utilized. The computer-usable or computer-readable medium may be, for example but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, device, or propagation medium. More specific examples (a non-exhaustive list) of the computer-readable medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CDROM), an optical storage device, a transmission media such as those supporting the Internet or an intranet, or a magnetic storage device. Note that the computer-usable or computer-readable medium could even be paper or another suitable medium, upon which the program is printed, as the program can be electronically captured, via, for instance, optical scanning of the paper or other medium, then compiled, interpreted, or otherwise processed in a suitable manner, if necessary, and then stored in a computer memory. In the context of this document, a computer-usable or computer-readable medium may be any medium that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, or device. The computer-usable medium may include a propagated data signal with the computer-usable program code embodied therewith, either in baseband or as part of a carrier wave. The computer usable program code may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, RF, etc.</p>
<p id="p-0026" num="0025">Computer program code for carrying out operations of the present invention may be written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++ or the like and conventional procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).</p>
<p id="p-0027" num="0026">The present invention is described below with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks. These computer program instructions may also be stored in a computer-readable medium that can direct a computer or other programmable data processing apparatus to function in a particular manner, such that the instructions stored in the computer-readable medium produce an article of manufacture including instruction means which implement the function/act specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0028" num="0027">The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0029" num="0028">Embodiments of the invention provide a method, system and computer program product for maintaining data coherence, for use in a multi-node processing system. Each of the nodes includes one or more components. Consider, as an example, shown in <figref idref="DRAWINGS">FIG. 1</figref>, a system <b>10</b>, which is a collection of components, each component being either a memory bank <b>12</b>, that stores data, or a processor <b>14</b>, that produces and consumes data. In this example, each independently running thread in the hardware is considered as a processor. It may internally have a layer of caches, and this invention is not specifically concerned with details of the internal movement of data between a processor and its caches. All the components are connected to some logical bus <b>16</b>, which implements transactions to enforce coherence of data. Each component is connected to the bus through a port <b>20</b>, which implements a coherence protocol. Any suitable coherence protocol may be used, and a number of common coherence protocols are well known in the literature.</p>
<p id="p-0030" num="0029">The term data movement is used to refer to movement of any data either (a) between a processor <b>14</b> and memory <b>12</b> or (b) between two processors <b>14</b>. A data movement is always initiated by some processor. Memory banks <b>12</b> are passive and either receive or supply the requested data. Hence, the ports connected to processors <b>14</b> are called active and ports connected to the memory <b>12</b> are called passive. A data movement is always between a pair of ports <b>20</b>, at least one of which is active. Every data movement is uniquely identified by its real address, and the real address space <b>22</b> is partitioned by the memory banks. All processors refer to the data using addresses from the virtual address space <b>24</b>, which is translated into real address space by system software, usually using tables known as TLBs.</p>
<p id="p-0031" num="0030">A data domain <b>30</b> is an entity uniquely identified by a data domain id. In embodiments of the invention, data domains <b>30</b> are allocated and managed by the system software, and each domain is associated with a list of ports <b>20</b> and a list of actions. The system software controls the data movements using these domains. Each data movement in the system is assigned a unique domain id by the system software, with the implication that only the ports <b>20</b> listed for that domain need to participate in the coherence protocol for that data movement. Furthermore, when the data movement takes place, auxiliary actions specified for that domain will be performed on the data. Actions are implementation-dependent and specify the necessary parameters and conditions under which an action must be performed on the data. Examples of actions are compression/decompression, encryption/decryption and mirroring. All the information pertaining to a domain is conveniently stored in a domain table <b>32</b>, which can be indexed by the domain id. A copy of the domain table is made available to the coherence bus controller <b>34</b>, as shown in <figref idref="DRAWINGS">FIG. 1</figref>. The system software, which dynamically updates the table <b>32</b>, is responsible to keep the copy of the table in the coherence controller up-to-date. The controller <b>34</b>, which implements the coherence protocol, uses the table <b>32</b> for routing coherence transactions to the relevant ports <b>20</b> and also to perform the necessary actions on the data during the data movement.</p>
<p id="p-0032" num="0031">The data Domain <b>30</b> are used to control the data movement. Two factors affect data movement: address mapping and process scheduling. In embodiments of the invention, both of these are controlled by system software and manifest when a TLB entry is created. Any program asking for the data movement must consult its TLB entry, and the other ports involved in the data movement will be included in the port list associated with the domain. Hence, with reference to <figref idref="DRAWINGS">FIG. 2</figref>, it is convenient for the system software <b>40</b> to do the domain assignment when the system software sets up a TLB entry <b>42</b>. Each time another processor <b>14</b> is set up with a translation for the same real address, that processor's port name will be added to the port list of the domain, thereby confining the coherence transactions related to that data to that subset of ports. Since any processor must first obtain a translation before accessing the data, the processor can obtain the corresponding domain id for that data and submit that domain id along with the processor's request to the bus controller <b>34</b>.</p>
<p id="p-0033" num="0032">As mentioned earlier, there are three principal uses for the concept of domain and each of these principals is discussed below. Consider, as shown in <figref idref="DRAWINGS">FIG. 4</figref>, a system of 4 nodes <b>50</b>, <b>52</b>, <b>54</b>, <b>56</b>, where each node has 4 processors and a memory bank. All the memory banks and processors are connected over a logical bus <b>60</b> and, in this example, access latency increases when a node boundary is crossed.</p>
<p id="h-0006" num="0000">(a) Reduction of Unnecessary Coherence Traffic in the System.</p>
<p id="p-0034" num="0033">Suppose a program runs over processors A, B and H and accesses locations in memory bank M<b>1</b>, then a single domain can be set up for all their accesses and the port list will contain {a,b,h,m<b>1</b>}. All coherence traffic for these accesses are confined into the four listed ports and so, all unnecessary traffic into the unlisted ports is avoided.</p>
<p id="h-0007" num="0000">(b) Co-Location of Program and Data.</p>
<p id="p-0035" num="0034">The performance of the above example can be improved by rescheduling the program on H to C. The port list will be changed to {a,b,c,m<b>1</b>}. Therefore, all coherence traffic will be confined into a single node. As a result, the system is able to not only reduce the coherence traffic but also reduce the latency. Alternatively, shifting programs from A and B to F and G and remapping data from M<b>1</b> to M<b>2</b> will achieve the same effect. This will involve copying data from M<b>1</b> to M<b>2</b>. All this information is available to the system software through the domain table, and suitable choices can be made depending on the situation.</p>
<p id="h-0008" num="0000">(c) Automation of Auxiliary Operations on Data.</p>
<p id="p-0036" num="0035">Embodiments of the invention also enable automation of auxiliary operations on data. these operations can be used to achieve a number of objectives. As one example, these auxiliary operations can be used to keep a mirror image of M<b>1</b> in M<b>3</b>. To do this in the above example, a domain action will specify that all writes into M<b>1</b> are to be mirrored into M<b>3</b>. The port list will remain the same, ensuring all coherence traffic will remain within the node. Data gets mirrored automatically, without having to have additional software to do extra loads and stores.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a multi-node processing system <b>100</b> in which embodiments of the invention may be implemented. <figref idref="DRAWINGS">FIG. 4</figref> shows three nodes <b>140</b>A-<b>140</b>C (collectively referred to as nodes <b>140</b>), and each node includes several client devices. For example, node <b>140</b>A includes processing subsystems <b>142</b>AA and <b>142</b>AB, memory subsystems <b>144</b>AA and <b>144</b>AB, I/O subsystem <b>146</b>A, and interface <b>148</b>A. The client devices in node <b>140</b>A share address network <b>150</b>A and data network <b>152</b>A. In the illustrated embodiment, nodes <b>140</b>B and <b>140</b>C contain similar client devices (identified by reference identifiers ending in &#x201c;B&#x201d; and &#x201c;C&#x201d; respectively). Note that different nodes may include different numbers of and/or types of client devices, and that some types of client devices may not be included in some nodes.</p>
<p id="p-0038" num="0037">As used herein, a node is a group of client devices (e.g., processing subsystems <b>142</b>, memory subsystems <b>144</b>, and/or I/O subsystems <b>146</b>) that share the same address and data networks. By linking multiple nodes, the number of client devices in the processing system <b>100</b> may be adjusted independently of the size limitations of any individual node <b>140</b>. Although three nodes are depicted in <figref idref="DRAWINGS">FIG. 4</figref>, it is understood that more nodes can be included in the system <b>100</b>.</p>
<p id="p-0039" num="0038">Each node <b>140</b> communicates with other nodes in processing system <b>100</b> via an interface <b>148</b> (interfaces <b>148</b>A-<b>148</b>C are collectively referred to as interfaces <b>148</b>). Some nodes may include more than one interface. Interfaces <b>148</b> may communicate by sending packets of address and/or data information on inter-node network <b>154</b>. Each of processing subsystems <b>142</b> and I/O subsystem <b>146</b> may access memory subsystems <b>144</b>. Each client in <figref idref="DRAWINGS">FIG. 4</figref> may be configured to participate in the coherency protocol by sending address messages on address network <b>150</b> and data messages on data network <b>152</b> using split-transaction packets.</p>
<p id="p-0040" num="0039">Memory subsystems <b>144</b> are configured to store data and instruction code for use by processing subsystems <b>142</b> and I/O subsystem <b>146</b>. Memory subsystems <b>144</b> may include dynamic random access memory (DRAM), although other types of memory may be used in some embodiments. I/O subsystem <b>146</b> is illustrative of a peripheral device such as an input-output bridge, a graphics device, a networking device, etc. In some embodiments, I/O subsystem <b>146</b> may include a cache memory subsystem similar to those of processing subsystems <b>142</b> for caching data associated with addresses mapped within one of memory subsystems <b>144</b>.</p>
<p id="p-0041" num="0040">In one embodiment, data network <b>152</b> may be a logical point-to-point network. Data network <b>152</b> may be implemented as an electrical bus, a circuit-switched network, or a packet-switched network. In embodiments where data network <b>152</b> is a packet-switched network, packets may be sent through the data network using techniques such as wormhole, store and forward, or virtual cut-through. In a circuit-switched network, a particular client device may communicate directly with a second client device via a dedicated point-to-point link that may be established through a switched interconnect mechanism. To communicate with a third client device, the particular client device utilizes a different link as established by the switched interconnect than the one used to communicate with the second client device. Messages upon data network <b>152</b> are referred to herein as data packets. Note that in some embodiments, address network <b>150</b> and data network <b>152</b> may be implemented using the same physical interconnect.</p>
<p id="p-0042" num="0041">Address network <b>150</b> accommodates communication between processing subsystems <b>142</b>, memory subsystems <b>144</b>, and I/O subsystem <b>146</b>. Messages upon address network <b>150</b> are generally referred to as address packets. In some embodiments, address packets may correspond to requests for an access right (e.g., a readable or writable copy of a cacheable coherency unit) or requests to perform a read or write to a non-cacheable memory location. Address packets may be sent by a device in order to initiate a coherency transaction. Subsequent address packets may be sent by other devices in order to implement the access right and/or ownership changes needed to satisfy the coherence request.</p>
<p id="p-0043" num="0042">In the processing system <b>100</b> shown in <figref idref="DRAWINGS">FIG. 4</figref>, a coherency transaction may include one or more packets upon address network <b>150</b> and data network <b>152</b>. Typical coherency transactions involve one or more address and/or data packets that implement data transfers, ownership transfers, and/or changes in access privileges. If activity within more than one node <b>140</b> is needed to complete a coherency transaction, that coherency transaction may also involve one or more packets on inter-node network <b>154</b>.</p>
<p id="p-0044" num="0043">Various devices such as I/O subsystems <b>146</b> and/or processing subsystems <b>142</b> may be configured to access data in any node <b>140</b> within processing system <b>100</b>. Several different address spaces may be used to describe the data stored in processing system <b>100</b>. Virtual addresses, which may be generated within each processing device while executing program instructions, may form one address space. A global address space may include addresses that identify each unique coherency unit stored within any of the nodes in processing system <b>100</b>, allowing a device in one node to identify data stored in another node. Local physical address space may be unique to each node and contains the physical addresses that are used to access coherency units within the local memory of each node. The local memory of each node includes the memory included in the memory subsystem(s) <b>144</b> in that node <b>140</b>.</p>
<p id="p-0045" num="0044">Active devices within each node <b>140</b> may be configured to use global addresses to specify data when sending address packets in coherency transactions. An active device in one node <b>140</b>A may access data in another node <b>140</b>B by sending an address packet specifying the data's global address. The memory subsystems <b>144</b> may translate a global address received in an address packet to a local physical address and use that local physical address to access the specified coherency unit.</p>
<p id="p-0046" num="0045">Thus, methods, systems, and computer program products for maintaining cache coherence using data domains have been described. In the foregoing specification, the invention has been described with reference to specific exemplary embodiments thereof. It will be evident that various modifications may be made thereto without departing from the broader spirit and scope of the invention as set forth in the following claims. The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An article of manufacture comprising:
<claim-text>at least one tangible computer readable device having computer readable program code logic tangibly embodied therein to execute machine instructions in one or more processing units for maintaining data coherence for use in a multi-node processing system, said computer readable program code logic, when executing, performing the following:</claim-text>
<claim-text>establishing a data domain in a multi-node processing system, each of the nodes including one or more components;</claim-text>
<claim-text>assigning a group of said components to the data domain;</claim-text>
<claim-text>sending a coherence message from one of the components of the multi-node processing system to another of the components of the multi-node processing system;</claim-text>
<claim-text>determining if said another of the components is assigned to the data domain; and</claim-text>
<claim-text>if said another of the components is assigned to the data domain, transferring said coherence message to all of the components assigned to the data domain to maintain data coherency among said all of the components.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The article of manufacture according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising, if said another of the components is assigned to the data domain, assigning said one of the components to the data domain.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The article of manufacture according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein if said another of the components is not assigned to the data domain, establishing a new data domain and assigning said another of the components to said new data domain.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The article of manufacture according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<claim-text>the sending includes obtaining an address for the coherence message in said another of the components; and</claim-text>
<claim-text>the determining includes determining if said address is assigned to the data domain.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The article of manufacture according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<claim-text>the sending includes sending a plurality of coherence messages to a plurality of the components, and obtaining addresses for the plurality of coherence messages; and</claim-text>
<claim-text>the assigning includes assigning the plurality of components to the data domain when obtaining said addresses for the plurality of coherence messages. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
