<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625897-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625897</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12790026</doc-number>
<date>20100528</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>557</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>34</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382173</main-classification>
</classification-national>
<invention-title id="d2e53">Foreground and background image segmentation</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5633948</doc-number>
<kind>A</kind>
<name>Kegelmeyer, Jr.</name>
<date>19970500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382132</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6937744</doc-number>
<kind>B1</kind>
<name>Toyama</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7512250</doc-number>
<kind>B2</kind>
<name>Lim et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7676081</doc-number>
<kind>B2</kind>
<name>Blake et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2003/0204384</doc-number>
<kind>A1</kind>
<name>Owechko et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>703  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2004/0001612</doc-number>
<kind>A1</kind>
<name>Gutta et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2005/0201591</doc-number>
<kind>A1</kind>
<name>Kiselewich</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382104</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2006/0167655</doc-number>
<kind>A1</kind>
<name>Barrow et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>702181</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2007/0055153</doc-number>
<kind>A1</kind>
<name>Simopoulos et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>600437</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2007/0133880</doc-number>
<kind>A1</kind>
<name>Sun et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2007/0269108</doc-number>
<kind>A1</kind>
<name>Steinberg et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2008/0103886</doc-number>
<kind>A1</kind>
<name>Li et al.</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 14</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2008/0219554</doc-number>
<kind>A1</kind>
<name>Dorai et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2009/0290795</doc-number>
<kind>A1</kind>
<name>Criminisi et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2010/0070339</doc-number>
<kind>A1</kind>
<name>Bae et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 10</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2010/0278384</doc-number>
<kind>A1</kind>
<name>Shotton et al.</name>
<date>20101100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>U.S. Appl. No. 12/454,628, filed May 20, 2009, Andrew Fitzgibbon and Jamie Shotten, &#x201c;Human Body Post Estimation&#x201d;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>U.S. Appl. No. 12/575,363, filed Oct. 7, 2009, Craig Peeper, Johnny Lee, Tommy Leyvand, and Szymon Stachniak, &#x201c;Systems and Methods for Removing a Background of an Image&#x201d;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>U.S. Appl. No. 61/174,878, filed on Mar. 1, 2009, Andrew Fitzgibbon and Jamie Shotten, &#x201c;Human Body Post Estimation&#x201d;.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Criminisi, et al., &#x201c;GeoS: Geodesic Image Segmentation&#x201d;, Springer-Verlag Berlin, Lecture Notes in Computer Science, vol. 5302, Proceedings of the European Conference on Computer Vision: Part I (ECCV), Marseille, FR, 2008, pp. 99-112.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Goldlucke, et al., &#x201c;Joint 3D-Reconstruction and Background Separation in Multiple Views using Graph Cuts&#x201d;, retrieved on Mar. 31, 2010 at &#x3c;&#x3c;http://graphics.tu-bs.de/people/magnor/publications/cvpr03.pdf&#x3e;&#x3e;, IEEE Computer Society, Conference on Computer Vision and Pattern Recognition (CVPR), Madison, WI, vol. 1, 2003, pp. 683-688.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Tsai, et al., &#x201c;Background Removal of Multiview Images by Learning Shape Priors&#x201d;, retrieved on Mar. 31, 2010 at &#x3c;&#x3c;http://www.znu.ac.ir/data/members/fazli<sub>&#x2014;</sub>saeid/DIP/Paper/ISSUE10/04303155.pdf&#x3e;&#x3e;, IEEE Transactions on Image Processing, vol. 16, No. 10, Oct. 2007, pp. 2607-2616.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Tzovaras, et al., &#x201c;3-D Camera Motion Estimation and Foreground / Background Separation for Stereoscopic Image Sequences&#x201d;, retrieved on Mar. 31, 2010 at &#x3c;&#x3c;http://www.iti.gr/files/3-d%20camera%20motion%20estimation%20and%20foreground.pdf&#x3e;&#x3e;, Optical Engineering, vol. 36, No. 2, 1997, pp. 574-579.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>8</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382173</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>17</number-of-drawing-sheets>
<number-of-figures>17</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110293180</doc-number>
<kind>A1</kind>
<date>20111201</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Criminisi</last-name>
<first-name>Antonio</first-name>
<address>
<city>Cambridge</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Shotton</last-name>
<first-name>Jamie Daniel Joseph</first-name>
<address>
<city>Cambridge</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Fitzgibbon</last-name>
<first-name>Andrew</first-name>
<address>
<city>Cambridge</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sharp</last-name>
<first-name>Toby</first-name>
<address>
<city>Cambridge</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Cook</last-name>
<first-name>Matthew Darius</first-name>
<address>
<city>Cambridge</city>
<country>GB</country>
</address>
</addressbook>
<residence>
<country>GB</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Criminisi</last-name>
<first-name>Antonio</first-name>
<address>
<city>Cambridge</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Shotton</last-name>
<first-name>Jamie Daniel Joseph</first-name>
<address>
<city>Cambridge</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Fitzgibbon</last-name>
<first-name>Andrew</first-name>
<address>
<city>Cambridge</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Sharp</last-name>
<first-name>Toby</first-name>
<address>
<city>Cambridge</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Cook</last-name>
<first-name>Matthew Darius</first-name>
<address>
<city>Cambridge</city>
<country>GB</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Zete Law, P.L.L.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Key</last-name>
<first-name>MacLane C.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Microsoft Corporation</orgname>
<role>02</role>
<address>
<city>Redmond</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Wu</last-name>
<first-name>Jingge</first-name>
<department>2665</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Foreground and background image segmentation is described. In an example, a seed region is selected in a foreground portion of an image, and a geodesic distance is calculated from each image element to the seed region. A subset of the image elements having a geodesic distance less than a threshold is determined, and this subset of image elements are labeled as foreground. In another example, an image element from an image showing at least a user, a foreground object in proximity to the user, and a background is applied to trained decision trees to obtain probabilities of the image element representing one of these items, and a corresponding classification assigned to the image element. This is repeated for each image element. Image elements classified as belonging to the user are labeled as foreground, and image elements classified as foreground objects or background are labeled as background.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="265.09mm" wi="154.77mm" file="US08625897-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="214.29mm" wi="177.63mm" file="US08625897-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="222.17mm" wi="188.72mm" file="US08625897-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="260.27mm" wi="135.55mm" file="US08625897-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="228.85mm" wi="169.59mm" file="US08625897-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="251.04mm" wi="156.72mm" file="US08625897-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="239.78mm" wi="188.72mm" file="US08625897-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="255.44mm" wi="176.45mm" file="US08625897-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="256.79mm" wi="181.86mm" file="US08625897-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="229.70mm" wi="177.29mm" file="US08625897-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="242.49mm" wi="170.26mm" file="US08625897-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="255.44mm" wi="192.11mm" file="US08625897-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="254.76mm" wi="187.37mm" file="US08625897-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="271.19mm" wi="153.75mm" file="US08625897-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="250.70mm" wi="178.48mm" file="US08625897-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="259.50mm" wi="151.21mm" file="US08625897-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="256.79mm" wi="183.22mm" file="US08625897-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="267.04mm" wi="194.14mm" file="US08625897-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">In many computing applications, a user manipulates or controls an application or game using specific user input hardware devices. Examples, of such hardware devices include game controllers, remote controls, keyboards and mice. Such controls can be difficult to learn and hence create a barrier to adoption of the application or game. An example of this is a computer game which is controlled by a game controller. To play the game successfully, the user first has to learn how the manipulation of the game controller relates to the control of the game (e.g. which button controls which aspect of an on-screen character). This initial learning period may be sufficient to dissuade a user from playing the game. Furthermore, the movements used to operate an input device generally do not correlate closely to the resulting action in the game or application. For example, the movement of a joystick or pressing of a button does not correspond closely to the movement of a bat or racket in a game environment.</p>
<p id="p-0003" num="0002">Motion-based controller devices can be used to more accurately reflect the movement of the user in the application or game. However, hardware input devices are still operated by the user in such systems (e.g. held, pointed or swung). Camera-based user input does not use input devices. Rather a camera captures images of the user and interprets these as input gestures or movements. However, camera-based user input produces a large amount of image data, which needs to be processed in real-time to accurately control a game or application. For example, the captured camera images should be segmented in real-time so that a user in the foreground of camera image is separated from any surrounding background, enabling the user's gestures and pose to be analyzed.</p>
<p id="p-0004" num="0003">The embodiments described below are not limited to implementations which solve any or all of the disadvantages of known camera-based user input techniques.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">The following presents a simplified summary of the disclosure in order to provide a basic understanding to the reader. This summary is not an extensive overview of the disclosure and it does not identify key/critical elements of the invention or delineate the scope of the invention. Its sole purpose is to present some concepts disclosed herein in a simplified form as a prelude to the more detailed description that is presented later.</p>
<p id="p-0006" num="0005">Foreground and background image segmentation is described. In an example, a seed region is selected in a foreground portion of an image, and a geodesic distance is calculated from each image element to the seed region. A subset of the image elements having a geodesic distance less than a threshold is determined, and this subset of image elements are labeled as foreground. In another example, an image element from an image showing at least a user, a foreground object in proximity to the user, and a background is applied to trained decision trees to obtain probabilities of the image element representing one of these items, and a corresponding classification assigned to the image element. This is repeated for each image element. Image elements classified as belonging to the user are labeled as foreground, and image elements classified as foreground objects or background are labeled as background.</p>
<p id="p-0007" num="0006">Many of the attendant features will be more readily appreciated as the same becomes better understood by reference to the following detailed description considered in connection with the accompanying drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0008" num="0007">The present description will be better understood from the following detailed description read in light of the accompanying drawings, wherein:</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an example camera-based control system for controlling a computer game;</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a schematic diagram of an image capture device;</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 3</figref> illustrates an example scene as captured by a depth camera;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 4</figref> illustrates a flowchart of a process for segmenting a depth camera image;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a flowchart of a process for geodesic image segmentation;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 6</figref> illustrates the operation of the flowchart of <figref idref="DRAWINGS">FIG. 5</figref> in the <figref idref="DRAWINGS">FIG. 3</figref> scene;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a kernel used for calculating geodesic distances;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a flowchart of a process for symmetric geodesic image segmentation;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 9</figref> illustrates calculation of a geodesic distance from a background seed in the <figref idref="DRAWINGS">FIG. 3</figref> scene;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 10</figref> illustrates the combination of background and foreground seed geodesic distances in the <figref idref="DRAWINGS">FIG. 3</figref> scene;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 11</figref> illustrates a process for training an image classifier for foreground objects;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 12</figref> illustrates example trained decision trees;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 13</figref> illustrates a process for classifying image elements using trained decision trees;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 14</figref> illustrates an example scene with foreground object removal;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 15</figref> illustrates a flowchart of a process for user separation;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 16</figref> illustrates an example scene showing the operation of the <figref idref="DRAWINGS">FIG. 15</figref> flowchart; and</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 17</figref> illustrates an exemplary computing device in which embodiments of the image segmentation technique may be implemented.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="h-0004" num="0000">Like reference numerals are used to designate like parts in the accompanying drawings.</p>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0026" num="0025">The detailed description provided below in connection with the appended drawings is intended as a description of the present examples and is not intended to represent the only forms in which the present example may be constructed or utilized. The description sets forth the functions of the example and the sequence of steps for constructing and operating the example. However, the same or equivalent functions and sequences may be accomplished by different examples.</p>
<p id="p-0027" num="0026">Although the present examples are described and illustrated herein as being implemented in a computer games system, the system described is provided as an example and not a limitation. As those skilled in the art will appreciate, the present examples are suitable for application in a variety of different types of computing and image processing systems.</p>
<p id="p-0028" num="0027">Note that the term &#x201c;image element&#x201d; is used hereinafter to refer to a pixel, group of pixels, voxel or other higher level component of an image.</p>
<p id="p-0029" num="0028">Reference is first made to <figref idref="DRAWINGS">FIG. 1</figref>, which illustrates an example camera-based control system <b>100</b> for controlling a computer game. <figref idref="DRAWINGS">FIG. 1</figref> shows a user <b>102</b> playing, in this illustrative example, a boxing game. In some examples, camera-based control system <b>100</b> can be used to, among other things, determine body pose, bind, recognize, analyze, track, associate to a human target, provide feedback, interpret gestures, and/or adapt to aspects of a human target such as the user <b>102</b>.</p>
<p id="p-0030" num="0029">The camera-based control system <b>100</b> comprises a computing device <b>104</b>. The computing device <b>104</b> can be a general purpose computer, gaming system or console, or dedicated image processing device. The computing device <b>104</b> can include hardware components and/or software components such that the computing device <b>104</b> can be used to execute applications such as gaming applications and/or non-gaming applications. The structure of the computing device <b>104</b> is discussed hereinafter with reference to <figref idref="DRAWINGS">FIG. 17</figref>.</p>
<p id="p-0031" num="0030">The camera-based control system <b>100</b> further comprises a capture device <b>106</b>. The capture device <b>106</b> can be, for example, an image sensor or detector that can be used to visually monitor one or more users (such user <b>102</b>) such that gestures performed by the one or more users can be captured, analyzed, processed, and tracked to perform one or more controls or actions within a game or application, as described in more detail below.</p>
<p id="p-0032" num="0031">The camera-based control system <b>100</b> can further comprise a display device <b>108</b> connected to the computing device <b>104</b>. The computing device can be a television, a monitor, a high-definition television (HDTV), or the like that can provide game or application visuals (and optionally audio) to the user <b>102</b>.</p>
<p id="p-0033" num="0032">In operation, the user <b>102</b> can be tracked using the capture device <b>106</b> such that the position, movements and size of user <b>102</b> can be interpreted by the computing device <b>104</b> (and/or the capture device <b>106</b>) as controls that can be used to affect the application being executed by computing device <b>104</b>. As a result, the user <b>102</b> can move his or her body to control an executed game or application.</p>
<p id="p-0034" num="0033">In the illustrative example of <figref idref="DRAWINGS">FIG. 1</figref>, the application executing on the computing device <b>104</b> is a boxing game that the user <b>102</b> is playing. In this example, the computing device <b>104</b> controls the display device <b>108</b> to provide a visual representation of a boxing opponent to the user <b>102</b>. The computing device <b>104</b> also controls the display device <b>108</b> to provide a visual representation of a user avatar that the user <b>102</b> can control with his or her movements. For example, the user <b>102</b> can throw a punch in physical space to cause the user avatar to throw a punch in game space. Thus, according to this example, the computing device <b>104</b> and the capture device <b>106</b> of the camera-based control system <b>100</b> can be used to recognize and analyze the punch of the user <b>102</b> in physical space such that the punch may be interpreted as a game control of the user avatar in game space.</p>
<p id="p-0035" num="0034">Furthermore, some movements can be interpreted as controls that correspond to actions other than controlling the avatar. For example, the user can use movements to enter, exit, turn system on or off, pause, save a game, select a level, profile or menu, view high scores, communicate with a friend, etc. Additionally, movement of the user <b>102</b> can be used and analyzed in any suitable manner to interact with applications other than games, for example to enter text, select icons or menu items, control media playback, browse websites or operate any other controllable aspect of an operating system or application.</p>
<p id="p-0036" num="0035">Reference is now made to <figref idref="DRAWINGS">FIG. 2</figref>, which illustrates a schematic diagram of the capture device <b>106</b> that can be used in the camera-based control system <b>100</b> of <figref idref="DRAWINGS">FIG. 1</figref>. In the example of <figref idref="DRAWINGS">FIG. 2</figref> the capture device <b>106</b> is configured to capture video images with depth information. Such a capture device can be referred to as a depth camera. The depth information can be in the form of a depth image that includes depth values, i.e. a value associated with each image element of the depth image that is related to the distance between the depth camera and an item or object located at that image element.</p>
<p id="p-0037" num="0036">The depth information can be obtained using any suitable technique including, for example, time-of-flight, structured light, stereo image, or the like. In some examples, the capture device <b>106</b> can organize the depth information into &#x201c;Z layers,&#x201d; or layers that may be perpendicular to a Z-axis extending from the depth camera along its line of sight.</p>
<p id="p-0038" num="0037">As shown in <figref idref="DRAWINGS">FIG. 2</figref>, the capture device <b>106</b> comprises at least one imaging sensor <b>200</b>. In the example shown in <figref idref="DRAWINGS">FIG. 2</figref>, the imaging sensor <b>200</b> comprises a depth camera <b>202</b> arranged to capture a depth image of a scene. The captured depth image can include a two-dimensional (2-D) area of the captured scene where each image element in the 2-D area represents a depth value such as a length or distance of an object in the captured scene from the depth camera <b>202</b>.</p>
<p id="p-0039" num="0038">The capture device can also include an emitter <b>204</b> arranged to illuminate the scene in such a manner that depth information can be ascertained by the depth camera <b>202</b>. For example, in the case that the depth camera <b>202</b> is an infra-red (IR) time-of-flight camera, the emitter <b>204</b> emits IR light onto the scene, and the depth camera <b>202</b> is arranged to detect backscattered light from the surface of one or more targets and objects in the scene. In some examples, pulsed infrared light can be emitted from the emitter <b>204</b> such that the time between an outgoing light pulse and a corresponding incoming light pulse can be detected by the depth camera and measured and used to determine a physical distance from the capture device <b>106</b> to a location on the targets or objects in the scene. Additionally, in some examples, the phase of the outgoing light wave from the emitter <b>204</b> can be compared to the phase of the incoming light wave at the depth camera <b>202</b> to determine a phase shift. The phase shift can then be used to determine a physical distance from the capture device <b>106</b> to a location on the targets or objects. In a further example, time-of-flight analysis can be used to indirectly determine a physical distance from the capture device <b>106</b> to a location on the targets or objects by analyzing the intensity of the reflected beam of light over time via various techniques including, for example, shuttered light pulse imaging.</p>
<p id="p-0040" num="0039">In another example, the capture device <b>106</b> can use structured light to capture depth information. In such a technique, patterned light (e.g., light displayed as a known pattern such as grid pattern or a stripe pattern) can be projected onto the scene using the emitter <b>204</b>. Upon striking the surface of one or more targets or objects in the scene, the pattern becomes deformed. Such a deformation of the pattern can be captured by the depth camera <b>202</b> and then be analyzed to determine a physical distance from the capture device <b>106</b> to a location on the targets or objects in the scene.</p>
<p id="p-0041" num="0040">In another example, the depth camera <b>202</b> can be in the form of two or more physically separated cameras that view a scene from different angles, such that visual stereo data is obtained that can be resolved to generate depth information. In this case the emitter <b>204</b> can be used to illuminate the scene or can be omitted.</p>
<p id="p-0042" num="0041">In some examples, in addition to the depth camera <b>202</b>, the capture device <b>106</b> can comprise a regular video camera, which is referred to as an RGB camera <b>206</b>. The RGB camera <b>206</b> is arranged to capture sequences of images of the scene at visible light frequencies, and can hence provide images that can be used to augment the depth images. In alternative examples, the RGB camera <b>206</b> can be used instead of the depth camera <b>202</b>.</p>
<p id="p-0043" num="0042">The capture device <b>106</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> further comprises at least one processor <b>208</b>, which is in communication with the imaging sensor <b>200</b> (i.e. depth camera <b>202</b> and RGB camera <b>206</b> in the example of <figref idref="DRAWINGS">FIG. 2</figref>) and the emitter <b>204</b>. The processor <b>208</b> can be a general purpose microprocessor, or a specialized signal/image processor. The processor <b>208</b> is arranged to execute instructions to control the imaging sensor <b>200</b> and emitter <b>204</b> to capture depth images and/or RGB images. The processor <b>208</b> can also optionally be arranged to perform processing on these images, as outlined in more detail hereinafter.</p>
<p id="p-0044" num="0043">The capture device <b>106</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> further includes a memory <b>210</b> arranged to store the instructions that for execution by the processor <b>208</b>, images or frames of images captured by the depth camera <b>202</b> or RGB camera <b>206</b>, or any other suitable information, images, or the like. In some examples, the memory <b>210</b> can include random access memory (RAM), read only memory (ROM), cache, Flash memory, a hard disk, or any other suitable storage component. The memory <b>210</b> can be a separate component in communication with the processor <b>208</b> or integrated into the processor <b>208</b>.</p>
<p id="p-0045" num="0044">The capture device <b>106</b> also comprises an output interface <b>212</b> in communication with the processor <b>208</b> and is arranged to provide data to the computing device <b>104</b> via a communication link. The communication link can be, for example, a wired connection (such as USB, Firewire, Ethernet or similar) and/or a wireless connection (such as WiFi, Bluetooth or similar). In other examples, the output interface <b>212</b> can interface with one or more communication networks (such as the internet) and provide data to the computing device <b>104</b> via these networks.</p>
<p id="p-0046" num="0045">Reference is now made to <figref idref="DRAWINGS">FIG. 3</figref>, which illustrates an example scene as captured by a depth camera such as that shown in <figref idref="DRAWINGS">FIG. 2</figref>. A first view <b>300</b> shows the scene from above, and includes the user <b>102</b> stood in front of the capture device <b>106</b> and the display device <b>108</b>. The user <b>102</b> is in a room having a floor, walls and a ceiling, and the room also contains a number of objects. A first object <b>302</b> is located at approximately the same distance from the capture device <b>106</b> as the user <b>102</b>, and is in proximity to the user <b>102</b>, but not in contact with the user <b>102</b>. A second object <b>304</b> is located behind the user <b>102</b>, such that the user <b>102</b> and the second object <b>304</b> overlap from the viewpoint of the capture device <b>104</b>. The second object <b>304</b> is, however, a further distance away from the capture device <b>104</b> than the user <b>102</b>. A third object <b>306</b> is located at the back of the room, and does not overlap with the user.</p>
<p id="p-0047" num="0046">The first, second, and third objects can be, for example, furniture such as tables, chairs, shelves, sofas, couches, and the like. The objects can also be animate objects such as other persons or pets.</p>
<p id="p-0048" num="0047">A second view <b>308</b> shows the scene from the point of view of the capture device <b>106</b>. The second view <b>308</b> shows: the first object <b>302</b> to the left of, but not overlapping with, the user <b>102</b>; the second object <b>304</b> overlapping an arm of the user <b>102</b>; and the third object not overlapping with, the user. The second view <b>308</b> represents an image of the form taken by the RGB camera <b>206</b> in the capture device <b>106</b>. Such an image can show information such as the color, texture or brightness of the objects in the scene. However, the information on the relative distances of the object in such an image is limited. Some distance information may be discernable through perspective, but this can be difficult to analyze using a computer. In other words, it is difficult for a computer to determine the relative depths of the objects from an image such as that shown in the second view <b>308</b>.</p>
<p id="p-0049" num="0048">On the other hand, a depth image, such as that captured by the depth camera <b>202</b>, conveys useful depth information. An example depth image <b>310</b> is shown in the third view of the scene. In a depth image, each image element has an associated value that represents the measured distance between the capture device <b>106</b> and an object located at that image element. The associated values can be represented visually as intensity values in an image. For example, image elements representing objects close to the capture device <b>106</b> (i.e. a small distance) can be represented as having low intensity (i.e. dark/black) and image elements representing objects far away to the capture device <b>106</b> (i.e. a large distance) can be represented as having high intensity (i.e. light/white). Image elements between these two extreme distances can be represented by a corresponding shade of gray.</p>
<p id="p-0050" num="0049">As shown in the depth image <b>310</b>, the user <b>102</b> and the first object <b>302</b> are both approximately the same distance from the capture device <b>106</b>, and are close to the capture device <b>106</b>, and hence have a similar intensity value (black in this example). The second object <b>304</b> has a lighter intensity value than the user <b>102</b>, due to being a greater distance from the capture device <b>106</b>. The third object has a lighter intensity value still, due to being a yet further away from the capture device <b>106</b>. The floor of the room has a graduated intensity value in the depth image <b>310</b>, getting lighter as it stretches away from the capture device <b>106</b>. The far wall of the room is shown in white, indicating it is the furthest distance shown in <figref idref="DRAWINGS">FIG. 3</figref> from the capture device <b>106</b>.</p>
<p id="p-0051" num="0050">Clearly, the example shown in <figref idref="DRAWINGS">FIG. 3</figref> is merely illustrative and for the purposes of explanation only. A real-life depth image comprises many more shades of intensity (even within one object), need not precisely extend between the intensity extremes of black for the user and white for the far wall, and does not show edges around objects.</p>
<p id="p-0052" num="0051">Depth images are useful in the context of camera-based control systems such as that shown in <figref idref="DRAWINGS">FIG. 1</figref> because they enable 3-D models of the user to be created, which reflect the user's movements in three dimensions. This enables the user to control the computing device using more realistic and accurate movements (as opposed to those simply seen face-on by a regular camera).</p>
<p id="p-0053" num="0052">However, to generate a 3-D model of the user, the depth information about the user is first isolated from the rest of the image. In other words, the foreground information (i.e. the user) is segmented from the background information (i.e. the objects and the rest of the room). Once the user has been segmented from the rest of the image, then the position and movements of the user can be analyzed and used to control the game or application.</p>
<p id="p-0054" num="0053">One technique for segmenting the user <b>102</b> from the rest of the image is to use a threshold filter. To do this, a portion of the user in the image is identified (for example using motion detection techniques described below) and the depth value for that portion is read. Then, all image elements in the image having a depth value within a predetermined threshold (which could be any value greater than or equal to zero) are selected as the foreground image elements, and the remaining image elements are selected as the background image elements.</p>
<p id="p-0055" num="0054">For example, with reference to the depth image <b>310</b> of <figref idref="DRAWINGS">FIG. 3</figref>, if all the image elements having a depth value corresponding to &#x201c;black&#x201d; are selected (i.e. a narrow threshold) then the user <b>102</b> can be successfully segmented from the second object <b>304</b> and third object <b>306</b>, as they are at a different depth and hence do not fall within the threshold filter. In other words, the image elements representing the second object <b>304</b> and third object <b>306</b> are not black. However, such a technique does not segment the first object <b>302</b>, because it is at the same depth as the user <b>102</b> (i.e. is also black).</p>
<p id="p-0056" num="0055">Therefore, a threshold filter technique suffers from the problem that when segmenting the user from the rest of the image, any objects that are at a depth within the predetermined threshold are not segmented. In addition, if the size of the predetermined threshold is too wide for the image in question, then additional objects are not segmented (e.g. the second object <b>304</b> and a lower portion of the floor could be included in the foreground if the threshold was &#x201c;black and dark grey&#x201d; in <figref idref="DRAWINGS">FIG. 3</figref>). Similarly, if the predetermined threshold is too narrow for the image in question, then portions of the user <b>102</b> could be segmented from the foreground in error, ultimately affecting the accuracy of the movement based control.</p>
<p id="p-0057" num="0056">Another technique for segmenting the user <b>102</b> from the rest of the image is to use a &#x201c;flood fill&#x201d; technique. With this technique a portion of the user in the depth image is selected, and then all image elements that are in contact with the selected portion and are within a predefined threshold of the depth value are selected. This includes image elements that are in contact with the selected portion via one or more other image elements that are within the predefined threshold of the depth value. In other words, the selection spreads from the selected portion across the image until image elements are reached that have a depth value outside the predefined threshold.</p>
<p id="p-0058" num="0057">For example, referring to the depth image <b>310</b> in <figref idref="DRAWINGS">FIG. 3</figref>, if an image element within the user <b>102</b> is selected, and the predefined threshold is set to encompass &#x201c;black&#x201d; only, then the segmentation selects all of the user's image elements, and stops at the edges of the user <b>102</b>. Therefore, the flood fill technique avoids including objects having the same depth value as the user (such as the first object <b>302</b>) in the foreground. In this respect, the flood fill technique therefore improves upon the threshold filter.</p>
<p id="p-0059" num="0058">However, the flood fill technique has problems when used with real-life depth images. Firstly, setting the predefined threshold accurately is difficult. If the predefined threshold is too large, then the flood fill will &#x201c;bleed&#x201d; from the user into other objects in contact with the user (in the image) that are at depths within the predefined threshold. For example, if the predefined threshold were such that it included the depth of the second object <b>304</b>, then the flood fill would bleed into the second object <b>304</b>, and this would be included in the foreground. In addition, because the flood fill spreads until image elements outside the predefined threshold are reached, this results in the whole of the second object <b>304</b> being included in the foreground. This clearly makes analyzing the user's pose and movement difficult.</p>
<p id="p-0060" num="0059">If the predefined threshold is too small, then regions of the user can become disconnected during the segmentation. For example, if the flood fill starts at the head of the user <b>102</b>, but the predefined threshold is so narrow that image elements representing the neck of the user are not included (due to being a small distance further away from the capture device than the head) then the flood fill stops at the neck, leaving only the head segmented, and not the rest of the body. A similar situation can occur with a bent or outstretched arm or leg, meaning that some body parts are not included in the foreground, making movement analysis difficult.</p>
<p id="p-0061" num="0060">Even if the predefined threshold is set at an appropriate value, then the flood fill technique can still result in portions of the user's body not being included in the foreground segmentation. For example, if a portion of the user's body in the depth image passes behind an object such that some of the user's body is shown one side of the object, and the remainder of the user's body is shown on the other side of the object, then the flood fill stops when it reaches the object. The flood fill does not progress any further to find the remainder of the user on the other side of the obstructing object. For example, if the user is holding an object in his hand, then in certain poses the object can lie over the user's arm in the image, dividing the arm in two. The flood fill does not pass the object, resulting in a part of the arm not being included in the foreground.</p>
<p id="p-0062" num="0061">Reference is now made to <figref idref="DRAWINGS">FIG. 4</figref>, which illustrates a flowchart of a further process for segmenting a foreground portion from a background portion of a depth image. The process in <figref idref="DRAWINGS">FIG. 4</figref> has several stages, each of which progressively improves the accuracy of the foreground segmentation, whilst maintaining computational efficiency.</p>
<p id="p-0063" num="0062">The process of <figref idref="DRAWINGS">FIG. 4</figref> can be performed at one or more processors of the computing device <b>104</b> (see <figref idref="DRAWINGS">FIG. 17</figref> hereinafter), or at the at least one processor <b>208</b> of the capture device <b>106</b>, or any combination thereof For example, some stages of the process can be performed at the capture device <b>106</b>, and others at the computing device <b>104</b>.</p>
<p id="p-0064" num="0063">Firstly, the depth image (such as depth image <b>310</b> shown in <figref idref="DRAWINGS">FIG. 3</figref>) is received <b>400</b> at a processor (be it at the capture device <b>106</b> or the computing device <b>104</b>). For example, the depth image can be captured by the depth camera <b>202</b>, and then provided to the computing device <b>104</b> via the output interface <b>212</b>. The received depth image is then compared <b>402</b> to a previously received depth image, which is stored at an image data store on a storage device <b>404</b>. The comparison between the received depth image and the previously received depth image detects any changes between the depth images. The detected change can be a result of movement of one or more objects between the capture times of the two images. As movements are most likely to be caused by motion of the user <b>102</b>, these changed regions are taken to indicate a foreground region of the depth image. This is discussed in more detail with reference to <figref idref="DRAWINGS">FIGS. 5 and 6</figref> below.</p>
<p id="p-0065" num="0064">The detected foreground region is then used as input to a geodesic background removal process <b>406</b>, which uses a geodesic distance transform to select image elements that are in the foreground of the image (i.e. are part of the user <b>102</b>), and remove those that are in the background (i.e. the background and other unwanted objects). This is discussed in detail with reference to <figref idref="DRAWINGS">FIG. 5 to 10</figref>.</p>
<p id="p-0066" num="0065">Following the geodesic background removal, the depth image has at least a portion of the background removed. However, some unwanted elements can still be present in the image. This is often the case where the user in direct contact with an object, for example when the user is sitting on a chair. In such cases, it is difficult for the geodesic background removal to distinguish between the image elements of the user and the object (e.g. chair). To counter this, a machine learning classifier is used to classify <b>408</b> each remaining image element as belonging to either the user or a foreground object. This is discussed in more detail with reference to <figref idref="DRAWINGS">FIG. 11 to 14</figref>. The initial removal of the at least a portion of the background reduces the computational complexity of the classifier, as it can be applied only to those image that remain following the geodesic background removal. Note, however, that machine learning classifier can also be used without the geodesic background removal, albeit slower due to the larger number of image elements.</p>
<p id="p-0067" num="0066">Once the image elements are classified, the wanted portion of the image (such as the user <b>102</b>) can be isolated <b>410</b> from other objects (such as a chair). This then leaves an image comprising only the image elements relating to the user <b>102</b>.</p>
<p id="p-0068" num="0067">In the case that the depth image shows more than one user, then additional steps can be performed to detect the presence of more than one user, and separate <b>412</b> the image elements relating to each user. This is discussed in more detail with reference to <figref idref="DRAWINGS">FIGS. 15 and 16</figref>, hereinafter. Following this, image element masks showing each user individually (with backgrounds removed) can be outputted <b>414</b>.</p>
<p id="p-0069" num="0068">Reference is now made to <figref idref="DRAWINGS">FIG. 5</figref>, which illustrates a flowchart of a process for segmenting a foreground portion from a background portion of an image using a geodesic distance transform, and which can be used to implement the first three blocks of <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0070" num="0069">Firstly, the depth image is received <b>500</b>, and the received depth image is compared <b>502</b> to at least one previously stored depth image, stored at the image data store on the storage device <b>404</b>. In one example, the received depth image is compared to the most recent previously received depth image (e.g. the previous frame in a video sequence of depth images). In another example, the received depth image is compared to several previously received depth images. In a further example, the image data store stores a depth image derived from a plurality of previously received depth images, in order to reduce storage requirements. For example, the image data store can store a depth image based on an exponential decay model, such as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i> <o ostyle="single">D</o></i><sub>t</sub>(<i>x</i>)=&#x3b1;<i>D</i><sub>t</sub>(<i>x</i>)+(1&#x2212;&#x3b1;)<i> <o ostyle="single">D</o></i><sub>t-1</sub>(<i>x</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0071" num="0070">Where D<sub>t</sub>(x) is a depth image received at time t, and &#x3b1; is a weighting value between 0 and 1. By using an exponential decay model, information from several depth images into the past can be taken into account, but more recent depth images are more prominently considered. In addition, the storage device <b>404</b> does not need to store many depth images, but can instead update <o ostyle="single">D</o><sub>t</sub>(x) each time a new depth image is received.</p>
<p id="p-0072" num="0071">It is then detected <b>504</b> whether changes are present between the newly received depth image and the previous depth image. If no changes are detected, then the process waits for another new depth image (updating the exponential decay model if required). If changes are detected, however, then the image elements that have changed are isolated <b>506</b> to create one or more foreground &#x201c;seed&#x201d; regions, which are used in the subsequent processing. The seed regions are denoted hereinafter using the symbol &#x3a9;. The seed regions are isolated by generating a binary seed mask (denoted M) having a value of 0 or 1 (other values can also be used), such that the mask has a value 0 for the image elements in the seed region of the depth image, and zero everywhere else in the image.</p>
<p id="p-0073" num="0072">Changes in the depth images are caused by the movement of at least one object between the capture times of the two images. In the case of a camera-based control system, where the user <b>102</b> moves or gestures in front of the capture device <b>106</b>, the detected changes are therefore are most likely to be caused by motion of the user <b>102</b>. The changed regions (i.e. the seed regions) are therefore located in the depth image on the moving part of the user's body. The seed region can hence be used as an example of a portion a foreground region of the depth image.</p>
<p id="p-0074" num="0073">An example of change detection and seed region generation is shown in <figref idref="DRAWINGS">FIG. 6</figref>. Binary seed mask <b>600</b> in <figref idref="DRAWINGS">FIG. 6</figref> shows the result of the change detection for the scene shown in <figref idref="DRAWINGS">FIG. 3</figref> for the illustrative case where the user <b>102</b> turns his head and moves his left hand. The outline of the user <b>102</b> and objects are shown with dotted lines for reference, but are not present in a real seed mask (as these items have not been moving). Seed mask <b>600</b> comprises two seed regions shown in black (e.g. having value 0). A first seed region <b>602</b> is shown at the side of the user's head, as a result of the turn of the head compared to the previous depth image, and a second seed region <b>604</b> is shown at the left hand of the user, as a result of the movement of this hand relative to the previous depth image. It should be noted that both of these seed regions are located on the body of the user, i.e. on the foreground part of the depth image that is to be segmented.</p>
<p id="p-0075" num="0074">Once the seed mask has been generated, then a geodesic distance can be calculated <b>508</b> from each image element in the depth image to the nearest image element having a value of zero in the seed mask (i.e. any image element in black in seed mask <b>600</b> of <figref idref="DRAWINGS">FIG. 6</figref>). The term &#x201c;geodesic distance&#x201d; is used herein to refer to a distance between two points in an image which takes into account image content. For example, consider the depth values of the image elements as a 3-D height map. The shortest geodesic distance between two points in an image may then be one that travels around a region with a large depth change rather than a Euclidean shortest path which travels &#x201c;as the crow flies&#x201d;. In the embodiments described herein the geodesic distances take into account the depth values in depth images. However, it is also possible to use geodesic distances which take into account gradients of other sources of information, such as intensity, texture gradients, color gradients, or gradients of probability maps.</p>
<p id="p-0076" num="0075">In other words, the shortest distance is calculated between every image element in the depth image and any image element in the seed region, whilst talking into account the depth values. This means that the shortest geodesic distance can be one that follows a non-straight path over the image if that non-straight line does not have significant depth variations, rather than the straight line (Euclidean) path that has significant depth variations.</p>
<p id="p-0077" num="0076">A geodesic distance transform can be used to determine the geodesic distances for the depth image. Any suitable geodesic distance transform can be used and an example of a known geodesic distance transform is set out below for ease of reference.</p>
<p id="p-0078" num="0077">Given an image I defined on a 2-D domain &#x3a8;, a binary seed mask M (with M(x)&#x3b5;{0,1}&#x2200;x) defining a seed region &#x3a9; with x&#x3b5;&#x3a9;<img id="CUSTOM-CHARACTER-00001" he="1.78mm" wi="3.13mm" file="US08625897-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>M(x)=0, the unsigned geodesic distance of each image element x from &#x3a9; is defined as:</p>
<p id="p-0079" num="0078">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <msub>
              <mi>D</mi>
              <mn>0</mn>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>x</mi>
                  <mo>;</mo>
                  <mi>M</mi>
                </mrow>
                <mo>,</mo>
                <mi>I</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <munder>
              <mi>min</mi>
              <mrow>
                <mo>{</mo>
                <mrow>
                  <mrow>
                    <msup>
                      <mi>x</mi>
                      <mi>&#x2032;</mi>
                    </msup>
                    <mo>&#x2758;</mo>
                    <mrow>
                      <mi>M</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <msup>
                          <mi>x</mi>
                          <mi>&#x2032;</mi>
                        </msup>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>=</mo>
                  <mn>0</mn>
                </mrow>
                <mo>}</mo>
              </mrow>
            </munder>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>d</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>x</mi>
                  <mo>,</mo>
                  <msup>
                    <mi>x</mi>
                    <mi>&#x2032;</mi>
                  </msup>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>,</mo>
        <mi>with</mi>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>d</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>a</mi>
              <mo>,</mo>
              <mi>b</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munder>
            <mi>inf</mi>
            <mrow>
              <mi>&#x393;</mi>
              <mo>&#x2208;</mo>
              <msub>
                <mi>P</mi>
                <mrow>
                  <mi>a</mi>
                  <mo>,</mo>
                  <mi>b</mi>
                </mrow>
              </msub>
            </mrow>
          </munder>
          <mo>&#x2062;</mo>
          <mrow>
            <msubsup>
              <mo>&#x222b;</mo>
              <mn>0</mn>
              <mrow>
                <mi>l</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>&#x393;</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </msubsup>
            <mo>&#x2062;</mo>
            <mrow>
              <msqrt>
                <mrow>
                  <mn>1</mn>
                  <mo>+</mo>
                  <msup>
                    <mrow>
                      <msup>
                        <mi>&#x3b3;</mi>
                        <mn>2</mn>
                      </msup>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mrow>
                            <mo>&#x2207;</mo>
                            <mrow>
                              <mi>I</mi>
                              <mo>&#x2061;</mo>
                              <mrow>
                                <mo>(</mo>
                                <mi>s</mi>
                                <mo>)</mo>
                              </mrow>
                            </mrow>
                          </mrow>
                          <mo>&#xb7;</mo>
                          <mrow>
                            <msup>
                              <mi>&#x393;</mi>
                              <mi>&#x2032;</mi>
                            </msup>
                            <mo>&#x2061;</mo>
                            <mrow>
                              <mo>(</mo>
                              <mi>s</mi>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mn>2</mn>
                  </msup>
                </mrow>
              </msqrt>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.2em" height="0.2ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <mo>&#x2146;</mo>
                <mi>s</mi>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0080" num="0079">Where P<sub>a,b </sub>is the set of all possible paths between the points a and b, and &#x393;(s):<img id="CUSTOM-CHARACTER-00002" he="2.46mm" wi="2.79mm" file="US08625897-20140107-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/>&#x2192;<img id="CUSTOM-CHARACTER-00003" he="3.13mm" wi="3.89mm" file="US08625897-20140107-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/> indicating one such path, parameterized by the scalar s&#x3b5;[0,1]. The spatial derivative &#x393;&#x2032;(s)=&#x2202;&#x393;(s)/&#x2202;s represents a vector tangent to the direction of the path. The dot-product in (2) ensures maximum influence for the gradient &#x2207;I (i.e. the change in depth value between image elements) when it is parallel to the direction of the path &#x393;. The geodesic factor &#x3b3; weighs the contribution of the image gradient versus the spatial distances. For &#x3b3;=0 equation (2) reduces to the Euclidean length of the path &#x393;.</p>
<p id="p-0081" num="0080">To calculate the geodesic distance for depth images in real-time, a fast geodesic transform algorithm can be used. An example of such an algorithm is a raster-scan algorithm, as described below. Other types of algorithm can also be used, such as a wave-front algorithm.</p>
<p id="p-0082" num="0081">As mentioned, a raster-scan process can be used in order to produce the distance transform. A raster scan algorithm is one which is based on a kernel operation applied sequentially over the image in multiple passes. Distance transforms with various different metrics can be computed using raster scan algorithms by using windows or kernels of various different types and sizes. <figref idref="DRAWINGS">FIG. 7</figref> shows an example pair of kernels (or windows) for use by a raster scan algorithm referred to herein as the Chamfer Distance algorithm. The pair comprises a forward pass kernel <b>700</b> and a backward pass kernel <b>702</b>. This pair can be referred to as a chamfer 3&#xd7;3 kernel. Many other different types of kernel can also be used including, but not limited to: city block, chessboard, chamfer 5&#xd7;5 and chamfer 7&#xd7;7. Larger kernels produce improved approximations to the exact distance with increasing accuracy.</p>
<p id="p-0083" num="0082">A distance transform engine (which can be implemented on either the capture device <b>106</b> or computing device <b>104</b>) selects a pair of windows for the raster scan, such as the forward pass kernel <b>700</b> and a backward pass kernel <b>702</b> of <figref idref="DRAWINGS">FIG. 7</figref>. In each window, &#x201c;0&#x201d; indicates the center of the window. The distance transform engine begins a forward raster scan over the depth image, as illustrated in example <b>704</b> in <figref idref="DRAWINGS">FIG. 7</figref>. It begins in the upper left corner of the image and places the forward pass kernel <b>700</b> on the depth image such that the center of the window is in registration with the first image element (in the top left corner). The kernel operation is carried out as described in more detail below. The window is then moved to the right so that the center of the window is in registration with the second image element of the top row and the kernel operation is carried out. This process repeats with the window moving from left to right along each row and from the top row to the bottom row of the depth image. When the forward raster scan ends the distance transform engine then carries out a backward raster scan over the depth image. The backward raster scan places the backward pass kernel <b>702</b> in the lower right corner of the depth image and moves from right to left and bottom to top.</p>
<p id="p-0084" num="0083">The kernel operation comprises replacing the image element which falls under the central image element of the window by the minimum of five distance values, those distance values being for the image element locations specified by the window. For a geodesic distance transform, this can be calculated as follows.</p>
<p id="p-0085" num="0084">Given a mask <b>706</b> M(x)&#x3b5;[0,1] (which can be, for example, a seed mask <b>600</b> as shown in <figref idref="DRAWINGS">FIG. 6</figref>), in the forward pass the mask <b>706</b> and the underlying depth image I is scanned from the top-left to the bottom-right corner and an intermediate function C is iteratively constructed as follows:</p>
<p id="p-0086" num="0085">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>C</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mi>min</mi>
    <mo>(</mo>
    <mtable>
      <mtr>
        <mtd>
          <mrow>
            <mrow>
              <mi>C</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>x</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>,</mo>
                  <mrow>
                    <mi>y</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <msqrt>
              <mrow>
                <msubsup>
                  <mi>&#x3c1;</mi>
                  <mn>2</mn>
                  <mn>2</mn>
                </msubsup>
                <mo>+</mo>
                <mrow>
                  <mi>&#x3b3;</mi>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <mo>&#x2207;</mo>
                    <msup>
                      <mrow>
                        <msub>
                          <mi>I</mi>
                          <mi>NW</mi>
                        </msub>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>x</mi>
                            <mo>,</mo>
                            <mi>y</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                </mrow>
              </mrow>
            </msqrt>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mrow>
            <mrow>
              <mi>C</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>x</mi>
                  <mo>,</mo>
                  <mrow>
                    <mi>y</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <msqrt>
              <mrow>
                <msubsup>
                  <mi>&#x3c1;</mi>
                  <mn>1</mn>
                  <mn>2</mn>
                </msubsup>
                <mo>+</mo>
                <mrow>
                  <mi>&#x3b3;</mi>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <mo>&#x2207;</mo>
                    <msup>
                      <mrow>
                        <msub>
                          <mi>I</mi>
                          <mi>N</mi>
                        </msub>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>x</mi>
                            <mo>,</mo>
                            <mi>y</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                </mrow>
              </mrow>
            </msqrt>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mrow>
            <mrow>
              <mi>C</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>x</mi>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>,</mo>
                  <mrow>
                    <mi>y</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <msqrt>
              <mrow>
                <msubsup>
                  <mi>&#x3c1;</mi>
                  <mn>2</mn>
                  <mn>2</mn>
                </msubsup>
                <mo>+</mo>
                <mrow>
                  <mi>&#x3b3;</mi>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <mo>&#x2207;</mo>
                    <msup>
                      <mrow>
                        <msub>
                          <mi>I</mi>
                          <mi>NE</mi>
                        </msub>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>x</mi>
                            <mo>,</mo>
                            <mi>y</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                </mrow>
              </mrow>
            </msqrt>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mrow>
            <mrow>
              <mi>C</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>x</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>,</mo>
                  <mi>y</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <msqrt>
              <mrow>
                <msubsup>
                  <mi>&#x3c1;</mi>
                  <mn>1</mn>
                  <mn>2</mn>
                </msubsup>
                <mo>+</mo>
                <mrow>
                  <mi>&#x3b3;</mi>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <mo>&#x2207;</mo>
                    <msup>
                      <mrow>
                        <msub>
                          <mi>I</mi>
                          <mi>W</mi>
                        </msub>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <mi>x</mi>
                            <mo>,</mo>
                            <mi>y</mi>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                </mrow>
              </mrow>
            </msqrt>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mrow>
            <mi>vM</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>x</mi>
                <mo>,</mo>
                <mi>y</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0087" num="0086">Where &#x2207;I<sub>NW </sub>denotes the image gradient (i.e. change in depth) between the image element at the centre of the window and the image element to the north-west, as illustrated in the forward pass kernel <b>700</b> of <figref idref="DRAWINGS">FIG. 7</figref>. Similar calculations are made for the north, north-east and west components. &#x3c1;<sub>1 </sub>and &#x3c1;<sub>2 </sub>are local distance values, and in one example are set to &#x3c1;<sub>1</sub>=1 and &#x3c1;<sub>2</sub>=&#x221a;{square root over (2)} or fixed-point approximations thereof. In other example, alternative values can be used. The symbol &#x3b3; represents the geodesic factor which weighs the contribution of the image gradient versus the spatial distances. The symbol v represents a scaling factor which can be set to any suitable value.</p>
<p id="p-0088" num="0087">Once the forward pass is completed, and the values for C(x,y) have been calculated, then the backward pass is performed. In the backward pass the algorithm proceeds right-to-left along scan-lines from the bottom-right to the top-left corner and applies the backward pass kernel <b>702</b> to the values for C(x,y) to produce the final geodesic distance D(x).</p>
<p id="p-0089" num="0088">Referring once more to <figref idref="DRAWINGS">FIG. 5</figref>, once the geodesic distance transform has been performed, then the output is a distance image in which each image element has a value that indicates its geodesic distance to the seed region. This can be represented graphically, such that short geodesic distances are given a low intensity (e.g. black for zero distance), long geodesic distances are given a high intensity (e.g. white for large distances), and geodesic distances between these extremes are gives a corresponding grey-scale value.</p>
<p id="p-0090" num="0089">Therefore, for image elements on the body of the user, the geodesic distance is short, as these elements are approximately the same depth from the seed regions (which were also on the body), and hence the geodesic path can simply travel along the body a short distance. Conversely, objects that are not connected to the user have a large geodesic distance, as the geodesic path takes into account the jump in depth off the user (e.g. to the background) and then onto the object. This is the case even if the object is at the same depth as (but not connected to) the user, as was the case with the first object <b>302</b> in <figref idref="DRAWINGS">FIG. 3</figref>. Therefore, ideally, the distance image shows the body of the user with a low intensity, and the remainder of the image with a high intensity. Limitations to this are discussed below.</p>
<p id="p-0091" num="0090">To remove the background from the image a threshold filter <b>510</b> is used on the distance image. In other words, all image elements having a geodesic distance greater than a predefined threshold are removed from the image. This leaves a set of image elements that predominantly correspond to the user <b>102</b>. The shape created by these image elements can be used to create an output user mask, which can then be used on the original depth image to segment the depth image into those image elements under the user mask (which are labeled as foreground image elements) and those outside the mask (which are labeled as background image elements). The segmented depth image can then be outputted <b>512</b>.</p>
<p id="p-0092" num="0091">An example of the result of the geodesic background removal process of <figref idref="DRAWINGS">FIG. 5</figref> being applied to the scene of <figref idref="DRAWINGS">FIG. 3</figref> is shown in <figref idref="DRAWINGS">FIG. 6</figref>. In this illustrative example, the seed mask <b>600</b> having seed regions at the head and hand of the user <b>102</b> (as discussed above) is input to a geodesic distance engine <b>606</b> along with the depth image <b>310</b>. The output from the geodesic distance engine <b>606</b> is a distance image. The distance image is then applied to a threshold filter <b>608</b>, and the output is a thresholded distance image <b>610</b>.</p>
<p id="p-0093" num="0092">The thresholded distance image <b>610</b> shows the body of the user <b>102</b> in black, due to the short geodesic distance of all image elements of the body to one of the seed regions. However, it should be noted that the thresholded distance image <b>610</b> also shows some &#x201c;bleeding&#x201d; <b>612</b> into the second object <b>304</b> and also some bleeding into the floor <b>614</b>. This is because the user is overlapping with the second object <b>304</b> in the image, and the depth gradient to the second object <b>304</b> is not that large. Similarly, for the floor, the user is in contact with the floor and hence there is a similar depth value, and thus short geodesic distance. Such bleeding can be mitigated to an extent with careful selection of the threshold filter parameter, and also with other enhancements described hereinafter.</p>
<p id="p-0094" num="0093">The geodesic background removal process of <figref idref="DRAWINGS">FIG. 5</figref> improves upon the flood fill technique described hereinbefore because it is tolerant of disconnected regions of foreground. For example, a bent or outstretched arm or leg does not result in parts of the user being skipped from the foreground, as the geodesic distance path travels along the &#x201c;surface&#x201d; of the user's body, and results in a short geodesic distance back to the seed region (provided the thresholding is sufficient). Similarly, bleeding effects are not as severe because a bleed will not always run across the whole of an object, as the geodesic distance gets larger as the image elements get further away from the seed region in the x and y as well as depth dimensions. Therefore, as a bleed spreads into an object the distance increases, and will at some point get large enough to be filtered by the threshold filter.</p>
<p id="p-0095" num="0094">Furthermore, the geodesic distance transform technique is also more computationally efficient than the flood-fill technique. The geodesic distance transform processing requirements increase linearly with the number of image elements (i.e. O(n) growth). The geodesic distance transform also accesses memory in a contiguous manner, and does not require complex recursion. Additionally, the geodesic distance transform is parallelizable, and hence can be efficiently implement on one or more GPUs.</p>
<p id="p-0096" num="0095">Reference is now made to <figref idref="DRAWINGS">FIG. 8</figref>, which illustrates a flowchart of a process for enhancing the geodesic distance transform technique above to further mitigate bleeding effects. This technique is denoted symmetric geodesic image segmentation. The flowchart of <figref idref="DRAWINGS">FIG. 8</figref> shares the same first five blocks as the process of <figref idref="DRAWINGS">FIG. 5</figref> described above. Summarizing, the depth image is received <b>500</b>, compared <b>502</b> to a previously received image, and if a change is detected <b>504</b> then the changed image elements are isolated <b>506</b> and the seed mask is created and a geodesic distance image calculated <b>508</b> in the manner described in detail above. In the flowchart of <figref idref="DRAWINGS">FIG. 8</figref>, this is denoted the foreground geodesic distance image.</p>
<p id="p-0097" num="0096">Once the foreground geodesic distance image has been calculated, a &#x201c;background seed&#x201d; is generated <b>800</b>. The background seed is a different seed to that calculated by the movement of the user, and is intended to map to the location of the background in the image. In one example, the background seed can be calculated by selecting a predefined shape that partially surrounds at least a central portion of the foreground geodesic distance image. An example of such a shape is shown illustrated in <figref idref="DRAWINGS">FIG. 9</figref>, where a background seed mask <b>900</b> comprises a first concave shape <b>902</b> and second concave <b>904</b> arranged to fit around the portion of the foreground geodesic distance image having the shortest geodesic distances (i.e. the user's body). Other shapes that lie in the background of the image can also be used, and the size of the selected shapes can be changed in dependence on the foreground geodesic distance image.</p>
<p id="p-0098" num="0097">The background seed mask is then used to calculate <b>802</b> geodesic distances from each image element in the depth image to the background seed. This is performed in the same manner as described above with reference to <figref idref="DRAWINGS">FIG. 5 to 7</figref>. The result of this operation is a background geodesic distance image. An example of this operation is shown illustrated in <figref idref="DRAWINGS">FIG. 9</figref>. The background seed mask <b>900</b> and the depth image <b>310</b> are input to the geodesic distance engine <b>606</b>, and a background geodesic distance image <b>906</b> is output. In contrast with the foreground geodesic distance image, the background and objects under the background seed have a low intensity, due to the short geodesic distance to the background seed, whereas the user <b>102</b> (which is not connected to the background seed) has a higher intensity due to the larger geodesic distance. Therefore, the generation of the background geodesic distance image can be seen as a type of negative or inverse of the foreground geodesic distance image.</p>
<p id="p-0099" num="0098">When the background geodesic distance image has been generated, the foreground geodesic distance image is subtracted <b>804</b> from the background geodesic distance image. The result of this is illustrated in <figref idref="DRAWINGS">FIG. 10</figref>, where the background geodesic distance image <b>906</b> and foreground geodesic distance image <b>610</b> (suffering from bleeding) are applied to a subtractor <b>1000</b>. The output is a final geodesic distance image <b>1010</b>, in which the bleeding has been reduced or removed. This occurs because the body of the user <b>102</b> has opposite values in the two distance images (high intensity in the background geodesic distance image <b>906</b> and low intensity in the foreground geodesic distance image <b>610</b>), resulting in a high difference value. Conversely, the bleed portions have low intensity in both distance images, resulting in a low difference result.</p>
<p id="p-0100" num="0099">Once the distance images have been subtracted, then the final geodesic distance image <b>1010</b> can be thresholded <b>806</b> using a threshold filter, and an output user mask generated. The output user mask can then be used on the original depth image to segment the depth image into those image elements under the user mask (which are labeled as foreground image elements) and those outside the mask (which are labeled as background image elements). The segmented depth image can then be outputted <b>808</b>.</p>
<p id="p-0101" num="0100">The symmetric geodesic image segmentation technique reduces bleeding effects significantly, and still operates in a computationally efficient manner due to the use of the efficient geodesic distance transforms. However, bleeding effects can still occur in some circumstances, for example where the user is in direct contact with an object. This can occur for example when the user is sitting on a chair. In these cases, the geodesic distance to the image elements of the chair is still short, and hence the chair is not segmented from the user. To counter this, a machine learning classifier is used to classify the image elements in the segmented image, and enable separation of the user from the object. This is now described with reference to <figref idref="DRAWINGS">FIG. 11 to 14</figref>.</p>
<p id="p-0102" num="0101">The machine learning classifier used herein is a random decision forest. However, in other examples, alternative classifiers could also be used. In further examples, rather than using a decision forest, a single trained decision tree can be used (this is equivalent to a forest with only one tree in the explanation below). Whilst some classifier algorithms can be computationally complex, the computational expense has been reduced in this example because the geodesic distance segmentation technique has already removed most of the background image elements from the image, and hence the number of image elements to be analyzed by the classifier is significantly reduced.</p>
<p id="p-0103" num="0102">Before a random decision forest classifier can be used to classify image elements, a set of decision trees that make up the forest are trained. The tree training process is described below with reference to <figref idref="DRAWINGS">FIGS. 11 and 12</figref>.</p>
<p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. 11</figref> illustrates a flowchart of a process for training a decision forest to identify features in an image. The decision forest is trained using a set of training images. The set of training images comprise a plurality images each showing at least one user and at least one object in proximity to the user. The objects in the training images are selected to be typical objects that are found in the home, and can be expected to be seen in the depth images in a camera-based control system. Examples of such objects are furniture, such as chairs, tables (e.g. coffee tables), couches/sofas and the like. The training images can also include floors and walls of a room. The users in the training images are in various different poses (such as sitting or standing). Each image element in each image in the training set is labeled as belonging to either the user (or optionally which body part of the user), a foreground object (the sofas/couches, chairs, coffee tables, other furniture, floors, walls etc), or background. Therefore, the training set forms a ground-truth database.</p>
<p id="p-0105" num="0104">In one example, rather than capturing depth images for many different examples of user poses, different furniture etc., the training set can comprise synthetic computer generated images. Such synthetic images realistically model the human body in different poses interacting with different objects, and can be generated to be viewed from any angle or position. However, they can be produced much more quickly than real depth images, and can provide a wider variety of training images.</p>
<p id="p-0106" num="0105">Referring to <figref idref="DRAWINGS">FIG. 11</figref>, to train the decision trees, the training set described above is first received <b>1100</b>. The number of decision trees to be used in a random decision forest is selected <b>1102</b>. A random decision forest is a collection of deterministic decision trees. Decision trees can be used in classification algorithms, but can suffer from over-fitting, which leads to poor generalization. However, an ensemble of many randomly trained decision trees (a random forest) yields improved generalization. During the training process, the number of trees is fixed.</p>
<p id="p-0107" num="0106">The following notation is used to describe the training process. An image element in a image I is defined by its coordinates x=(x, y). The forest is composed of T trees denoted &#x3a8;<sub>1</sub>, . . . , &#x3a8;<sub>t</sub>, . . . &#x3a8;<sub>T </sub>with t indexing each tree. An example random decision forest is shown illustrated in <figref idref="DRAWINGS">FIG. 12</figref>. The illustrative decision forest of <figref idref="DRAWINGS">FIG. 12</figref> comprises three decision trees: a first tree <b>1200</b> (denoted tree &#x3a8;<sub>1</sub>); a second tree <b>1202</b> (denoted tree &#x3a8;<sub>2</sub>); and a third tree <b>1204</b> (denoted tree &#x3a8;<sub>3</sub>). Each decision tree comprises a root node (e.g. root node <b>1206</b> of the first decision tree <b>1200</b>), a plurality of internal nodes, called split nodes (e.g. split node <b>1208</b> of the first decision tree <b>1200</b>), and a plurality of leaf nodes (e.g. leaf node <b>1210</b> of the first decision tree <b>1200</b>).</p>
<p id="p-0108" num="0107">In operation, each root and split node of each tree performs a binary test on the input data and based on the result directs the data to the left or right child node. The leaf nodes do not perform any action; they just store probability distributions (e.g. example probability distribution <b>1212</b> for a leaf node of the first decision tree <b>1200</b> of <figref idref="DRAWINGS">FIG. 12</figref>), as described hereinafter.</p>
<p id="p-0109" num="0108">The manner in which the parameters used by each of the split nodes are chosen and how the leaf node probabilities are computed is now described. A decision tree from the decision forest is selected <b>1104</b> (e.g. the first decision tree <b>1200</b>) and the root node <b>1206</b> is selected <b>1106</b>. All image elements from each of the training images are then selected <b>1108</b>. Each image element x of each training image is associated with a known class label, denoted Y(x). The class label indicates whether or not the point x belongs to the user (or optionally which body part of the user), a foreground object (the sofas/couches, chairs, coffee tables, other furniture, floors, walls etc), or background. Thus, for example, Y(x) indicates whether an image element x belongs to the class of head, foreground object, wall, left arm, right hand, floor, etc.</p>
<p id="p-0110" num="0109">A random set of test parameters are then generated <b>1110</b> for use by the binary test performed at the root node <b>1206</b>. In one example, the binary test is of the form: &#x3be;&#x3e;&#x192;(x;&#x3b8;)&#x3e;&#x3c4;, such that &#x192;(x;&#x3b8;) is a function applied to image element x with parameters &#x3b8;, and with the output of the function compared to threshold values &#x3be; and &#x3c4;. If the result of &#x192;(x;&#x3b8;) is in the range between &#x3be; and &#x3c4; then the result of the binary test is true. Otherwise, the result of the binary test is false. In other examples, only one of the threshold values &#x3be; and &#x3c4; can be used, such that the result of the binary test is true if the result of &#x192;(x;&#x3b8;) is greater than (or alternatively less than) a threshold value. In the example described here, the parameter &#x3b8; defines a visual feature of the image.</p>
<p id="p-0111" num="0110">An example function &#x192;(x;&#x3b8;) can make use of the relative position of users and objects in the depth images. The parameter &#x3b8; for the function &#x192;(x;&#x3b8;) is randomly generated during training. The process for generating the parameter &#x3b8; can comprise generating random spatial offset values in the form of a two-dimensional displacement (i.e. an angle and distance). The result of the function &#x192;(x;&#x3b8;) is then computed by observing the depth value for a test image element which is displaced from the image element of interest x in the image by the spatial offset. The depth value for the test image element can indicate whether the test image element is in the background of the image. The threshold values &#x3be; and &#x3c4; can be used to decide whether the test image element is at the background depth.</p>
<p id="p-0112" num="0111">This example function illustrates how the features in the images can be captured by considering the relative layout of visual patterns. For example, chair image elements tend to occur a certain distance away, in a certain direction, from the torso and upper legs of the body, and floor image elements tend to occur a certain distance away, in a certain direction, from the lower leg and foot image elements.</p>
<p id="p-0113" num="0112">The result of the binary test performed at a root node or split node determines which child node an image element is passed to. For example, if the result of the binary test is true, the image element is passed to a first child node, whereas if the result is false, the image element is passed to a second child node.</p>
<p id="p-0114" num="0113">The random set of test parameters generated comprise a plurality of random values for the function parameter &#x3b8; and the threshold values &#x3be; and &#x3c4;. In order to inject randomness into the decision trees, the function parameters &#x3b8; of each split node are optimized only over a randomly sampled subset &#x398; of all possible parameters. This is an effective and simple way of injecting randomness into the trees, and increases generalization.</p>
<p id="p-0115" num="0114">Then, every combination of test parameter is applied <b>1112</b> to each image element in the set of training images. In other words, all available values for &#x3b8; (i.e. &#x3b8;<sub>i</sub>&#x3b5;&#x398;) are tried one after the other, in combination with all available values of &#x3be; and &#x3c4; for each image element in each training image. For each combination, the information gain (also known as the relative entropy) is calculated. The combination of parameters that maximize the information gain (denoted &#x3b8;*, &#x3be;* and &#x3c4;*) is selected <b>1114</b> and stored at the current node for future use. This set of test parameters provides discrimination between the image element classifications. As an alternative to information gain, other criteria can be used, such as Gini entropy, or the &#x2018;two-ing&#x2019; criterion.</p>
<p id="p-0116" num="0115">It is then determined <b>1116</b> whether the value for the maximized information gain is less than a threshold. If the value for the information gain is less than the threshold, then this indicates that further expansion of the tree does not provide significant benefit. This gives rise to asymmetrical trees which naturally stop growing when no further nodes are beneficial. In such cases, the current node is set <b>1118</b> as a leaf node. Similarly, the current depth of the tree is determined <b>1116</b> (i.e. how many levels of nodes are between the root node and the current node). If this is greater than a predefined maximum value, then the current node is set <b>1118</b> as a leaf node.</p>
<p id="p-0117" num="0116">If the value for the maximized information gain is greater than or equal to the threshold, and the tree depth is less than the maximum value, then the current node is set <b>1120</b> as a split node. As the current node is a split node, it has child nodes, and the process then moves to training these child nodes. Each child node is trained using a subset of the training image elements at the current node. The subset of image elements sent to a child node is determined using the parameters &#x3b8;*, &#x3be;* and &#x3c4;* that maximized the information gain. These parameters are used in the binary test, and the binary test performed <b>1122</b> on all image elements at the current node. The image elements that pass the binary test form a first subset sent to a first child node, and the image elements that fail the binary test form a second subset sent to a second child node.</p>
<p id="p-0118" num="0117">For each of the child nodes, the process as outlined in blocks <b>1110</b> to <b>1122</b> of <figref idref="DRAWINGS">FIG. 11</figref> are recursively executed <b>1124</b> for the subset of image elements directed to the respective child node. In other words, for each child node, new random test parameters are generated <b>1110</b>, applied <b>1112</b> to the respective subset of image elements, parameters maximizing the information gain selected <b>1114</b>, and the type of node (split or leaf) determined <b>1116</b>. If it is a leaf node, then the current branch of recursion ceases. If it is a split node, binary tests are performed <b>1122</b> to determine further subsets of image elements and another branch of recursion starts. Therefore, this process recursively moves through the tree, training each node until leaf nodes are reached at each branch. As leaf nodes are reached, the process waits <b>1126</b> until the nodes in all branches have been trained. Note that, in other examples, the same functionality can be attained using alternative techniques to recursion.</p>
<p id="p-0119" num="0118">Once all the nodes in the tree have been trained to determine the parameters for the binary test maximizing the information gain at each split node, and leaf nodes have been selected to terminate each branch, then probability distributions can be determined for all the leaf nodes of the tree. This is achieved by counting <b>1128</b> the class labels of the training image elements that reach each of the leaf nodes. All the image elements from all of the training images end up at a leaf node of the tree. As each image element of the training images has a class label associated with it, a total number of image elements in each class can be counted at each leaf node. From the number of image elements in each class at a leaf node and the total number of image elements at that leaf node, a probability distribution for the classes at that leaf node can be generated <b>1130</b>. To generate the distribution, the histogram is normalized. Optionally, a small prior count can be added to all classes so that no class is assigned zero probability, which can improve generalization.</p>
<p id="p-0120" num="0119">An example probability distribution <b>1212</b> is shown illustrated in <figref idref="DRAWINGS">FIG. 12</figref> for leaf node <b>1210</b>. The probability distribution shows the classes c of image elements against the probability of an image element belonging to that class at that leaf node, denoted as P<sub>l</sub><sub><sub2>t</sub2></sub><sub>(x)</sub>(Y(x)=c), where l<sub>t </sub>indicates the leaf node l of the t<sup>th </sup>tree. In other words, the leaf nodes store the posterior probabilities over the classes being trained. Such a probability distribution can therefore be used to determine the likelihood of an image element reaching that leaf node belonging to a given classification, as described in more detail hereinafter.</p>
<p id="p-0121" num="0120">Returning to <figref idref="DRAWINGS">FIG. 11</figref>, once the probability distributions have been determined for the leaf nodes of the tree, then it is determined <b>1132</b> whether more trees are present in the decision forest. If so, then the next tree in the decision forest is selected, and the process repeats. If all the trees in the forest have been trained, and no others remain, then the training process is complete and the process terminates <b>1134</b>.</p>
<p id="p-0122" num="0121">Therefore, as a result of the training process, a plurality of decision trees are trained using synthesized training images. Each tree comprises a plurality of split nodes storing optimized test parameters, and leaf nodes storing associated probability distributions. Due to the random generation of parameters from a limited subset used at each node, the trees of the forest are distinct (i.e. different) from each other.</p>
<p id="p-0123" num="0122">The training process is performed in advance of using the classifier algorithm to segment a real depth image. The decision forest and the optimized test parameters are stored on a storage device for use in classifying depth images at a later time. <figref idref="DRAWINGS">FIG. 13</figref> illustrates a flowchart of a process for classifying image elements in a previously unseen depth image using a decision forest that has been trained as described hereinabove. Firstly, an unseen depth image is received <b>1300</b> at the classification algorithm. An image is referred to as &#x2018;unseen&#x2019; to distinguish it from a training image which has the image elements already classified. Note that the unseen depth image can be already segmented to an extent, for example by a geodesic transform process described above, which reduces the number of image elements to be classified. However, the classification process can also be used without the prior segmentation step in some examples.</p>
<p id="p-0124" num="0123">An image element from the unseen image is selected <b>1302</b> for classification. A trained decision tree from the decision forest is also selected <b>1304</b>. The selected image element is pushed <b>1306</b> through the selected decision tree (in a manner similar to that described above with reference to <figref idref="DRAWINGS">FIGS. 11 and 12</figref>), such that it is tested against the trained parameters at a node, and then passed to the appropriate child in dependence on the outcome of the test, and the process repeated until the image element reaches a leaf node. Once the image element reaches a leaf node, the probability distribution associated with this leaf node is stored <b>1308</b> for this image element.</p>
<p id="p-0125" num="0124">If it is determined <b>1310</b> that there are more decision trees in the forest, then a new decision tree is selected <b>1304</b>, the image element pushed <b>1306</b> through the tree and the probability distribution stored <b>1308</b>. This is repeated until it has been performed for all the decision trees in the forest. Note that the process for pushing an image element through the plurality of trees in the decision forest can also be performed in parallel, instead of in sequence as shown in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0126" num="0125">Once the image element has been pushed through all the trees in the decision forest, then a plurality of classification probability distributions have been stored for the image element (at least one from each tree). These probability distributions are then aggregated <b>1312</b> to form an overall probability distribution for the image element. In one example, the overall probability distribution is the mean of all the individual probability distributions from the T different decision trees. This is given by:</p>
<p id="p-0127" num="0126">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <mrow>
    <mi>P</mi>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mrow>
          <mi>Y</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mi>x</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mi>c</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mfrac>
      <mn>1</mn>
      <mi>T</mi>
    </mfrac>
    <mo>&#x2062;</mo>
    <mrow>
      <munderover>
        <mo>&#x2211;</mo>
        <mrow>
          <mi>t</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mi>T</mi>
      </munderover>
      <mo>&#x2062;</mo>
      <mrow>
        <msub>
          <mi>P</mi>
          <mrow>
            <msub>
              <mi>l</mi>
              <mi>t</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>x</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </msub>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow>
              <mi>Y</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>x</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>=</mo>
            <mi>c</mi>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0128" num="0127">Note that methods of combining the tree posterior probabilities other than averaging can also be used, such as multiplying the probabilities. Optionally, an analysis of the variability between the individual probability distributions can be performed (not shown in <figref idref="DRAWINGS">FIG. 13</figref>). Such an analysis can provide information about the uncertainty of the overall probability distribution. In one example, the entropy can be determined as a measure of the variability.</p>
<p id="p-0129" num="0128">Once the overall probability distribution is determined, the overall classification of the image element is calculated <b>1314</b> and stored. The calculated classification for the image element is assigned to the image element for future use (as outlined below). In one example, the calculation of a classification c for the image element can be performed by determining the maximum probability in the overall probability distribution (i.e. P<sub>c</sub>=max<sub>x </sub>P(Y(x)=c). In addition, the maximum probability can optionally be compared to a threshold minimum value, such that an image element having class c is considered to be present if the maximum probability is greater than the threshold. In one example, the threshold can be 0.5, i.e. the classification c is considered present if P<sub>c</sub>&#x3e;0.5. In a further example, a maximum a-posteriori (MAP) classification for an image element x can be obtained as c*=arg max<sub>c </sub>P(Y(x)=c).</p>
<p id="p-0130" num="0129">It is then determined <b>1316</b> whether further unanalyzed image elements are present in the unseen depth image, and if so another image element is selected and the process repeated. Once all the image elements in the unseen image have been analyzed, then classifications are obtained for all image elements.</p>
<p id="p-0131" num="0130">To perform the final segmentation of the depth image, the image element classifications are used to segment <b>1320</b> the image such that the image elements having classifications relating to the user are labeled as foreground portions of the image, and the remaining image elements (i.e. those classified as sofas/couches, chairs, coffee tables, other furniture, floors, walls, etc.) are classified as background portions of the image. The foreground portion having the user image elements can be used to generate a mask, and separate the user from the original depth image for processing to analyze pose and/or movement.</p>
<p id="p-0132" num="0131">An illustrative example of the overall operation of the segmentation process (as first discussed with reference to <figref idref="DRAWINGS">FIG. 4</figref> is shown in <figref idref="DRAWINGS">FIG. 14</figref>. In this example, a depth image <b>1400</b> that comprises a user <b>1402</b> sitting on a chair <b>1404</b> is captured, and provided to the image segmentation system. The depth image <b>1400</b> is then subjected to a first stage segmentation using the geodesic distance transform <b>1406</b> as described hereinabove. The result of this is a partially segmented image <b>1408</b> with a foreground mask <b>1410</b>, which has correctly removed the background from the depth image <b>1400</b>, but the foreground object (the chair <b>1404</b>) has not been removed as it was in direct contact with the user <b>1402</b>, and could not be distinguished by the geodesic distance transform.</p>
<p id="p-0133" num="0132">The second stage of the segmentation then uses the image classifier <b>1412</b>, as described hereinabove, to classify the image elements of the partially segmented image <b>1408</b>. The result of the image classifier <b>1412</b> is a classified image <b>1414</b>, in which each image element from the partially segmented image <b>1408</b> is given a label of &#x201c;user&#x201d; or &#x201c;foreground object&#x201d; (in this illustrative example only&#x2014;other examples can use more or different labels). In the classified image <b>1414</b> the image elements classified as user <b>1416</b> are given diagonal shading, and the image elements classified as foreground object <b>1418</b> are given check shading.</p>
<p id="p-0134" num="0133">Once the image is classified, objects (i.e. items other than the user) can be removed <b>1420</b>, which leaves a final segmented image <b>1422</b> comprising a user mask <b>1424</b> comprising (predominantly) only the image elements relating to the user <b>1402</b>. The user mask <b>1424</b> can then be used to extract the user's image elements from the original depth image <b>1400</b>, and the depth image comprising only the user can be processed further to control the operation of the computing device through movements, poses or gestures.</p>
<p id="p-0135" num="0134">It should be noted that whilst the geodesic image segmentation technique and the image classifier segmentation technique work well in combination, due to the fast geodesic transform performing a &#x201c;first pass&#x201d; to remove much of the background, thereby reducing the processing requirements for the image classifier, they can also each be used independently as image segmentation techniques. In addition, both of these techniques can also be used for images other than depth images, such as RGB images. It should also be appreciated that these techniques can be readily applied to videos, in that a video is merely a sequence of images.</p>
<p id="p-0136" num="0135">A final part of the image segmentation system is now described with reference to <figref idref="DRAWINGS">FIGS. 15 and 16</figref>. The previous segmentation stages have removed the background and any unwanted objects from the depth image showing the user. However, it is also possible that the depth image shows more than one user. Therefore, if the original depth image shows more than one user, then, following segmentation, the segmented image also contains image elements relating to more than one user. The process of <figref idref="DRAWINGS">FIG. 15</figref> identifies this circumstance, and separates the users.</p>
<p id="p-0137" num="0136">The user separation process of <figref idref="DRAWINGS">FIG. 15</figref> starts by receiving <b>1500</b> the segmented depth image. This segmented depth image can be segmented either by the geodesic segmentation process, or by the image classifier, or using the combination of the two shown in <figref idref="DRAWINGS">FIG. 14</figref>. For example, <figref idref="DRAWINGS">FIG. 16</figref> shows a segmented depth image <b>1600</b> comprising a first user <b>1602</b> and a second user <b>1604</b>.</p>
<p id="p-0138" num="0137">A line through the users in the segmented image is then selected <b>1502</b>. An example of such a line is shown by horizontal dashed line <b>1606</b> in <figref idref="DRAWINGS">FIG. 16</figref>. In some examples, the line can be horizontal and placed at a predetermined position in the image. Alternatively, the image can be analyzed to select an appropriate position and/or angle for the line (e g taking a horizontal line across the widest point of the image).</p>
<p id="p-0139" num="0138">The two opposing edge-most image elements intersecting with the line are then selected <b>1504</b>. For example, referring to <figref idref="DRAWINGS">FIG. 16</figref>, a first image element <b>1608</b> is selected at the left-most point where the depth image and line intersect, and a second image element <b>1610</b> is selected at the opposing right-most point where the depth image and line intersect.</p>
<p id="p-0140" num="0139">The depth information in the segmented depth image <b>1600</b> is then used to calculate <b>1506</b> geodesic distances from each image element to the first image element <b>1608</b>. In addition, geodesic distances are calculated <b>1508</b> from each image element to the second image element <b>1610</b>.</p>
<p id="p-0141" num="0140">The geodesic distances from each image element to the first image element <b>1608</b> are aggregated <b>1510</b> to form a left-hand side distance graph, and the geodesic distances from each image element to the second image element <b>1610</b> are aggregated <b>1512</b> to form a right-hand side distance graph. In one example, the aggregations can be performed by summing the geodesic distances from each image element over the y-axis at each point along the line <b>1606</b>. This can be expressed as follows:</p>
<p id="p-0142" num="0141">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
  <mrow>
    <mrow>
      <mrow>
        <msub>
          <mi>d</mi>
          <mi>l</mi>
        </msub>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mi>x</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
      <mo>=</mo>
      <mrow>
        <munderover>
          <mo>&#x2211;</mo>
          <mrow>
            <mi>y</mi>
            <mo>=</mo>
            <mn>0</mn>
          </mrow>
          <mi>H</mi>
        </munderover>
        <mo>&#x2062;</mo>
        <mrow>
          <mi>d</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>x</mi>
              <mo>,</mo>
              <msub>
                <mi>p</mi>
                <mi>l</mi>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
    <mo>,</mo>
    <mi>and</mi>
  </mrow>
</math>
</maths>
<maths id="MATH-US-00004-2" num="00004.2">
<math overflow="scroll">
  <mrow>
    <mrow>
      <msub>
        <mi>d</mi>
        <mi>r</mi>
      </msub>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mi>x</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <munderover>
        <mo>&#x2211;</mo>
        <mrow>
          <mi>y</mi>
          <mo>=</mo>
          <mn>0</mn>
        </mrow>
        <mi>H</mi>
      </munderover>
      <mo>&#x2062;</mo>
      <mrow>
        <mi>d</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mi>x</mi>
            <mo>,</mo>
            <msub>
              <mi>p</mi>
              <mi>r</mi>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</math>
</maths>
</p>
<p id="p-0143" num="0142">Where d<sub>l</sub>(x) is the left-hand side aggregated distance value for a point x on the x-axis of the image, d<sub>r</sub>(x) is the right-hand side aggregated distance value for a point x on the x-axis of the image, H is the height of the image (i.e. the maximum y-axis value), d(x,p<sub>l</sub>) is the geodesic distance between an image element x (having coordinates x, y) and the first image element <b>1608</b> (the left-most intersection), and d (x,p<sub>r</sub>) is the geodesic distance between image element x and the second image element <b>1610</b> (the right-most intersection). The values for d<sub>l</sub>(x) and d<sub>r</sub>(x) can optionally be normalized.</p>
<p id="p-0144" num="0143">An example of a left-hand side distance graph <b>1612</b> and right-hand side distance graph <b>1614</b> is shown in <figref idref="DRAWINGS">FIG. 16</figref>. The left-hand side distance graph <b>1612</b> and right-hand side distance graph <b>1614</b> are overlaid, as shown by graph <b>1616</b> in <figref idref="DRAWINGS">FIG. 16</figref>, and an intersection area <b>1618</b> (indicated by vertical hatching) is calculated <b>1514</b>.</p>
<p id="p-0145" num="0144">The value calculated for the intersection area <b>1618</b> is used to determine if there is more than one user present in the depth image. For example, if the value is 1 (or within a threshold of 1), then this means that either that there is only one user, or there is near complete overlap between more than one user. This is because a value close to or equal to 1 indicates that there is little difference between calculating the geodesic distances from either the left or right hand side, thereby indicating a single, uninterrupted body. Conversely, if the intersection area is less than 1 (or less than the threshold) then this indicates the presence of two, largely separated bodies. Note, however, that the users do not need to be completely disconnected in order to detect that there is more than one user.</p>
<p id="p-0146" num="0145">If it is determined <b>1516</b> that there is only one user present, then further segmentation does not need to be performed, and the segmented image can be outputted <b>1520</b> without further processing. If, however, the intersection area value indicates that more than one user is present, then the location of a soft, probabilistic boundary is calculated <b>1518</b> between the users. The boundary can be calculated by determining the location on the x-axis of the cross-over point of the overlaid left-hand side distance graph <b>1612</b> and right-hand side distance graph <b>1614</b>. Image elements to the one side of the cross-over point are labeled as belonging to the first user <b>1602</b>, and image elements to the other side of the of the cross-over point are labeled as belonging to the second user <b>1604</b>. Image elements in the region of the cross-over point can in some examples be labeled with a probabilistic value of belonging to one of the first or second user. The probabilistic value can be based upon the relative heights of the left- and right-hand side distance graphs.</p>
<p id="p-0147" num="0146">Once the boundary between the users has been calculated, the final separated depth image can be outputted <b>1520</b>, in which the users have been segmented from background objects and individually separated and labeled. An example of this is illustrated in <figref idref="DRAWINGS">FIG. 16</figref>, where a final segmented image <b>1620</b> shows the first user <b>1602</b> labeled with checked hatching, and the second user <b>1604</b> labeled with diagonal hatching. The two users depth images can then be separated and analyzed separately to independently monitor their movements and gestures, such that they can each provide input to the camera-based control system (e.g. each providing input to a multi-player game).</p>
<p id="p-0148" num="0147">Reference is now made to <figref idref="DRAWINGS">FIG. 17</figref>, which illustrates various components of an exemplary computing device <b>104</b> which may be implemented as any form of a computing and/or electronic device, and in which embodiments of the image segmentation techniques may be implemented.</p>
<p id="p-0149" num="0148">The computing device <b>104</b> comprises one or more processors <b>1700</b> which may be microprocessors, controllers, graphics processing units or any other suitable type of processors for processing computing executable instructions to control the operation of the device in order to perform the above-described image segmentation techniques.</p>
<p id="p-0150" num="0149">The computing device <b>104</b> also comprises one or more input interfaces <b>1702</b> arranged to receive and process input from one or more devices, such as user input devices (e.g. capture device <b>106</b>, a game controller <b>1704</b>, a keyboard <b>1706</b> and/or a mouse <b>1708</b>). This user input may be used to control software applications or games executed on the computing device <b>104</b>.</p>
<p id="p-0151" num="0150">The computing device <b>104</b> also comprises an output interface <b>1710</b> arranged to output display information to a display device <b>108</b> which can be separate from or integral to the computing device <b>104</b>. The display information may provide a graphical user interface. In an example, the display device <b>108</b> may also act as the user input device if it is a touch sensitive display device. The output interface may also output data to devices other than the display device, e.g. a locally connected printing device (not shown in <figref idref="DRAWINGS">FIG. 17</figref>).</p>
<p id="p-0152" num="0151">Computer executable instructions may be provided using any computer-readable media that is accessible by computing device <b>104</b>. Computer-readable media may include, for example, computer storage media such as memory <b>1712</b> and communications media. Computer storage media, such as memory <b>1712</b>, includes volatile and non-volatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EPROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to store information for access by a computing device.</p>
<p id="p-0153" num="0152">In contrast, communication media may embody computer readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave, or other transport mechanism. Although the computer storage media (memory <b>1712</b>) is shown within the computing device <b>104</b> it will be appreciated that the storage may be distributed or located remotely and accessed via a network or other communication link (e.g. using communication interface <b>1713</b>).</p>
<p id="p-0154" num="0153">Platform software comprising an operating system <b>1714</b> or any other suitable platform software may be provided at the computing device <b>104</b> to enable application software <b>1716</b> to be executed on the device. Other software that can be executed on the computing device <b>104</b> includes: motion detection logic <b>1718</b> (see for example <figref idref="DRAWINGS">FIG. 5-6</figref> and description above); geodesic transform logic (see for example <figref idref="DRAWINGS">FIGS. 5-7</figref> and <b>8</b>-<b>10</b> and description above); decision tree training logic (see for example <figref idref="DRAWINGS">FIG. 11-12</figref> and description above); image classification logic (see for example <figref idref="DRAWINGS">FIG. 13-14</figref> and description above); and user separation logic (see for example <figref idref="DRAWINGS">FIG. 15-16</figref> and description above). A data store <b>1728</b> is provided to store data such as previously received depth images; seed masks, intermediate function results; foreground masks, tree training parameters, probability distributions, classification labels, user masks, distance graphs and other results.</p>
<p id="p-0155" num="0154">The term &#x2018;computer&#x2019; is used herein to refer to any device with processing capability such that it can execute instructions. Those skilled in the art will realize that such processing capabilities are incorporated into many different devices and therefore the term &#x2018;computer&#x2019; includes PCs, servers, mobile telephones, personal digital assistants and many other devices.</p>
<p id="p-0156" num="0155">The methods described herein may be performed by software in machine readable form on a tangible storage medium. Examples of tangible (or non-transitory) storage media include disks, thumb drives, memory etc and do not include propagated signals. The software can be suitable for execution on a parallel processor or a serial processor such that the method steps may be carried out in any suitable order, or simultaneously.</p>
<p id="p-0157" num="0156">This acknowledges that software can be a valuable, separately tradable commodity. It is intended to encompass software, which runs on or controls &#x201c;dumb&#x201d; or standard hardware, to carry out the desired functions. It is also intended to encompass software which &#x201c;describes&#x201d; or defines the configuration of hardware, such as HDL (hardware description language) software, as is used for designing silicon chips, or for configuring universal programmable chips, to carry out desired functions.</p>
<p id="p-0158" num="0157">Those skilled in the art will realize that storage devices utilized to store program instructions can be distributed across a network. For example, a remote computer may store an example of the process described as software. A local or terminal computer may access the remote computer and download a part or all of the software to run the program. Alternatively, the local computer may download pieces of the software as needed, or execute some software instructions at the local terminal and some at the remote computer (or computer network). Those skilled in the art will also realize that by utilizing conventional techniques known to those skilled in the art that all, or a portion of the software instructions may be carried out by a dedicated circuit, such as a DSP, programmable logic array, or the like.</p>
<p id="p-0159" num="0158">Any range or device value given herein may be extended or altered without losing the effect sought, as will be apparent to the skilled person.</p>
<p id="p-0160" num="0159">It will be understood that the benefits and advantages described above may relate to one embodiment or may relate to several embodiments. The embodiments are not limited to those that solve any or all of the stated problems or those that have any or all of the stated benefits and advantages. It will further be understood that reference to &#x2018;an&#x2019; item refers to one or more of those items.</p>
<p id="p-0161" num="0160">The steps of the methods described herein may be carried out in any suitable order, or simultaneously where appropriate. Additionally, individual blocks may be deleted from any of the methods without departing from the spirit and scope of the subject matter described herein. Aspects of any of the examples described above may be combined with aspects of any of the other examples described to form further examples without losing the effect sought.</p>
<p id="p-0162" num="0161">The term &#x2018;comprising&#x2019; is used herein to mean including the method blocks or elements identified, but that such blocks or elements do not comprise an exclusive list and a method or apparatus may contain additional blocks or elements.</p>
<p id="p-0163" num="0162">It will be understood that the above description of a preferred embodiment is given by way of example only and that various modifications may be made by those skilled in the art. The above specification, examples and data provide a complete description of the structure and use of exemplary embodiments of the invention. Although various embodiments of the invention have been described above with a certain degree of particularity, or with reference to one or more individual embodiments, those skilled in the art could make numerous alterations to the disclosed embodiments without departing from the spirit or scope of this invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625897-20140107-M00001.NB">
<img id="EMI-M00001" he="15.16mm" wi="76.20mm" file="US08625897-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625897-20140107-M00002.NB">
<img id="EMI-M00002" he="26.08mm" wi="76.20mm" file="US08625897-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08625897-20140107-M00003.NB">
<img id="EMI-M00003" he="8.81mm" wi="76.20mm" file="US08625897-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004 MATH-US-00004-2" nb-file="US08625897-20140107-M00004.NB">
<img id="EMI-M00004" he="19.39mm" wi="76.20mm" file="US08625897-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method of segmenting a foreground portion from a background portion of an image, comprising:
<claim-text>receiving the image at a processor, wherein the image comprises a plurality of image elements, and the image represents at least one user, at least one foreground object in proximity to the at least one user, and a background;</claim-text>
<claim-text>accessing at least one trained decision tree stored on a memory;</claim-text>
<claim-text>selecting an image element from the image;</claim-text>
<claim-text>applying the image element to the or each trained decision tree to obtain one or more probabilities of the image element representing part of a user, a foreground object or background;</claim-text>
<claim-text>assigning a classification of user, foreground object or background to the image element in dependence on the one or more probabilities;</claim-text>
<claim-text>repeating the steps of selecting, applying and assigning for each image element in the image;</claim-text>
<claim-text>labeling the image elements having the classification of user as the foreground portion, and the image elements having the classification of foreground object and background as the background portion; and</claim-text>
<claim-text>subsequent to labeling the image elements:
<claim-text>selecting a first image element located at one side of the foreground portion and a second image element located at an opposing side of the foreground portion;</claim-text>
<claim-text>calculating a first set comprising a geodesic distance from each image element in the foreground portion to the first image element;</claim-text>
<claim-text>calculating a second set comprising a geodesic distance from each image element in the foreground portion to the second image element;</claim-text>
<claim-text>aggregating the geodesic distances in the first set over an axis of the image to form a first graph;</claim-text>
<claim-text>aggregating the geodesic distances in the second set over the axis of the image to form a second graph;</claim-text>
<claim-text>calculating an intersection area between the first graph and second graph; and</claim-text>
<claim-text>determining that two users are present in the image the intersection area is less than a predefined threshold.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image is a depth image and each image element represents a distance from the at least one user, the at least one foreground object, or the background to a capture device.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the step of, prior to accessing the at least one decision tree: partially segmenting the image using a geodesic distance transform to remove at least a portion of the background image elements.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one foreground object comprises at least one of: a sofa; a couch; a chair; a coffee table; furniture; a floor; and a wall.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising, in case that two users are present in the image:
<claim-text>overlaying the first graph and second graph;</claim-text>
<claim-text>labeling image elements to one side of a cross-over point of the first and second graph as representing a first user; and</claim-text>
<claim-text>labeling image elements to an opposing side of the cross-over point of the first and second graph as representing a second user.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein, in the case that the first and second user overlap in the image, labeling image elements in the cross-over point region with a probabilistic value of belonging to one of the first and second user.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising the step of training the at least one decision tree prior to receiving the image, wherein the step of training comprises:
<claim-text>receiving a plurality of training images, each comprising a plurality of image elements, wherein each image element has a classification representing one of a user, at least one foreground object in proximity to the user, or background;</claim-text>
<claim-text>selecting a decision tree from a random decision forest;</claim-text>
<claim-text>generating a random set of test parameters to apply at a node of the decision tree;</claim-text>
<claim-text>applying the set of test parameters to at least a portion of the image elements of the training images at the node;</claim-text>
<claim-text>selecting a subset of test parameters providing discrimination between the image element classifications and storing the subset in association with the node; and</claim-text>
<claim-text>repeating the steps of generating, applying, selecting and storing for each node in the or each decision tree.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the training images are computer-generated depth images each comprising at least one artificially synthesized user and foreground object. </claim-text>
</claim>
</claims>
</us-patent-grant>
