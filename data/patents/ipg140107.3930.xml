<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624998-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624998</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12479470</doc-number>
<date>20090605</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>473</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>76</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>225</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>3482312</main-classification>
<further-classification>348340</further-classification>
<further-classification>348 E5024</further-classification>
</classification-national>
<invention-title id="d2e53">Camera image selection based on detected device movement</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5471405</doc-number>
<kind>A</kind>
<name>Marsh</name>
<date>19951100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6013007</doc-number>
<kind>A</kind>
<name>Root et al.</name>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6032108</doc-number>
<kind>A</kind>
<name>Seiple et al.</name>
<date>20000200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6135951</doc-number>
<kind>A</kind>
<name>Richardson et al.</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6357147</doc-number>
<kind>B1</kind>
<name>Darley et al.</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6373481</doc-number>
<kind>B1</kind>
<name>Tan et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715788</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6463385</doc-number>
<kind>B1</kind>
<name>Fry</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6539336</doc-number>
<kind>B1</kind>
<name>Vock et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6560903</doc-number>
<kind>B1</kind>
<name>Darley</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6582342</doc-number>
<kind>B2</kind>
<name>Kaufman</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6619835</doc-number>
<kind>B2</kind>
<name>Kita</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6716139</doc-number>
<kind>B1</kind>
<name>Hosseinzadeh-Dolkhani et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6790178</doc-number>
<kind>B1</kind>
<name>Mault et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6793607</doc-number>
<kind>B2</kind>
<name>Neil</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6898550</doc-number>
<kind>B1</kind>
<name>Blackadar et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>7030735</doc-number>
<kind>B2</kind>
<name>Chen</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7062225</doc-number>
<kind>B2</kind>
<name>White</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7171331</doc-number>
<kind>B2</kind>
<name>Vock et al.</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7174227</doc-number>
<kind>B2</kind>
<name>Kobayashi et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7200517</doc-number>
<kind>B2</kind>
<name>Darley et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7251454</doc-number>
<kind>B2</kind>
<name>White</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7278966</doc-number>
<kind>B2</kind>
<name>Hjelt et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>7292867</doc-number>
<kind>B2</kind>
<name>Werner et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>7454002</doc-number>
<kind>B1</kind>
<name>Gardner et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>7519327</doc-number>
<kind>B2</kind>
<name>White</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>7596249</doc-number>
<kind>B2</kind>
<name>Bacus et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382128</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>7618345</doc-number>
<kind>B2</kind>
<name>Corbalis et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>7670263</doc-number>
<kind>B2</kind>
<name>Ellis et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>7856176</doc-number>
<kind>B2</kind>
<name>Yoon et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396 55</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2002/0077784</doc-number>
<kind>A1</kind>
<name>Vock et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2003/0097878</doc-number>
<kind>A1</kind>
<name>Farringdon et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2005/0172311</doc-number>
<kind>A1</kind>
<name>Hjelt et al.</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2005/0259888</doc-number>
<kind>A1</kind>
<name>Ozluturk</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2005/0280733</doc-number>
<kind>A1</kind>
<name>Imaizumi</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2006/0098106</doc-number>
<kind>A1</kind>
<name>Tanaka</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34823199</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2006/0132623</doc-number>
<kind>A1</kind>
<name>Nozaki et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34823199</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2007/0009244</doc-number>
<kind>A1</kind>
<name>Takahashi</name>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396 55</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2007/0040805</doc-number>
<kind>A1</kind>
<name>Mellot</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2007/0147814</doc-number>
<kind>A1</kind>
<name>Nomura et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396 55</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2008/0187181</doc-number>
<kind>A1</kind>
<name>Meadow et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2008/0259170</doc-number>
<kind>A1</kind>
<name>Hatanaka</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482086</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2008/0259176</doc-number>
<kind>A1</kind>
<name>Tamaru</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2008/0266406</doc-number>
<kind>A1</kind>
<name>McLeod et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2008/0304816</doc-number>
<kind>A1</kind>
<name>Ebato</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>396 53</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2009/0073286</doc-number>
<kind>A1</kind>
<name>Sugino et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34823199</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2009/0135264</doc-number>
<kind>A1</kind>
<name>John</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2009/0160959</doc-number>
<kind>A1</kind>
<name>Watanabe et al.</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34820899</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2010/0265357</doc-number>
<kind>A1</kind>
<name>Liu et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2010/0309335</doc-number>
<kind>A1</kind>
<name>Brunner et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2011/0181742</doc-number>
<kind>A1</kind>
<name>Nozaki et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482084</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>JP</country>
<doc-number>2006-140892</doc-number>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-cpc-text>H04N 5/91</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>WO</country>
<doc-number>02/093272</doc-number>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>10</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>3482312</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348340</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 E5024</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>7</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100309334</doc-number>
<kind>A1</kind>
<date>20101209</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>James</last-name>
<first-name>Bryan</first-name>
<address>
<city>Menlo Park</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hodge</last-name>
<first-name>Andrew</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lindahl</last-name>
<first-name>Aram</first-name>
<address>
<city>Menlo Park</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>James</last-name>
<first-name>Bryan</first-name>
<address>
<city>Menlo Park</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Hodge</last-name>
<first-name>Andrew</first-name>
<address>
<city>Palo Alto</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Lindahl</last-name>
<first-name>Aram</first-name>
<address>
<city>Menlo Park</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Wong, Cabello, Lutsch, Rutherford &#x26; Brucculeri, LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Apple Inc.</orgname>
<role>02</role>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Lam</last-name>
<first-name>Hung</first-name>
<department>2661</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Systems and methods are provided for selecting one or more of several images captured in sequence and stored in a buffer in response to receiving a user instruction to store a captured image. An electronic device can capture information describing the movement of the device at the time each of the several images was captured, such that each image can be associated with specific device movement information. The electronic device can then select the one of the captured and buffered images for which the movement information of the device satisfies particular criteria. This can ensure that the particular image stored is not blurry due to device movement at the time the image was captured.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="238.68mm" wi="185.25mm" file="US08624998-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="173.99mm" wi="119.97mm" file="US08624998-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="195.75mm" wi="183.56mm" file="US08624998-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="226.99mm" wi="170.01mm" file="US08624998-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="238.51mm" wi="184.15mm" file="US08624998-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="224.37mm" wi="172.30mm" file="US08624998-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">This is directed to capturing images using a lens of an electronic device, and selecting one of the captured images to store in memory. In particular, this is directed to capturing a sequence of images within a time period (e.g., within 1 or 2 seconds), and selecting an image from the sequence based on detected movements of the electronic device.</p>
<p id="p-0003" num="0002">Many electronic devices include the ability to capture images. In particular, many cellular telephones, personal assistants, and other portable electronic devices have integrated some or all of the functionality of cameras. Using a lens, the electronic devices can capture light and store images of the user's environment or of people or objects of interest to the user. Because of inherent processing limitations, lens limitations, previewing limitations, and other limitations of lens embedded in electronic devices, users may experience a lag between the time when the users provide an instruction to capture an image and when the device actually captures and stores an image. For example, an electronic device may be subject to an inherent lag as the device switches from a preview mode in which the lens resolution is low (e.g., and requires less power) to an image capture mode having a higher resolution.</p>
<p id="p-0004" num="0003">During the lag time, a user may shake or unintentionally move or tilt the electronic device, which may cause the image actually captured to be blurry or out of focus. In some cases, the act of pressing a button, touch screen, or other input interface of the electronic device can cause the electronic device, and thus the camera lens, to shake. Typical solutions to the device shake can include image stabilization or post-image capture sharpening and processing. In electronic devices that have smaller sensors however, image stabilization may not be possible. In addition, electronic devices with limited graphics processing capabilities or limited power may not be able to perform post-processing on an image in a resource efficient manner.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0005" num="0004">This is directed to systems and methods for capturing several images in sequence in response to receiving a user instruction to store a captured image. In particular, this is directed to comparing detected movement data associated with each of the several images in the sequence, and selecting the image associated with suitably low movement data.</p>
<p id="p-0006" num="0005">In some embodiments, an electronic device can include a lens for capturing images of an environment, and a preview interface such as a viewfinder or a display providing an indication of the image captured by the camera lens at a particular time. The electronic device can include a physical or virtual input interface (e.g., a physical button, or a selectable option displayed on a display) that a user can select to direct the lens to store a captured image (e.g., take a photograph). When the user provides the instruction (e.g., presses a button), the electronic device can move slightly due to the force applied by the user on the device. In some cases, the electronic device may have a lag between the time the user provides the instruction and the time the device actually records a captured image, during which time a user may not hold the device perfectly immobile (e.g., the device can move relative to the initial position when the user provided the instruction). In other cases, the lens may require light to be captured for an extended period during which a user can inadvertently move the device (e.g., in low light conditions). This movement in the device can cause the lens to capture images that are not sharp or that are blurry or out of focus.</p>
<p id="p-0007" num="0006">The device can measure or quantify the device movement using any suitable approach. In some embodiments, the electronic device can include a motion sensing component, such as one or more accelerometers providing outputs characterizing the device movement. The electronic device can receive measurements of device movement at any suitable time. For example, the motion sensing component can detect device movements at regular intervals. As another example, the motion sensing component can detect movements in response to a particular electronic device operation or function being enabled (e.g., detect motion when a camera mode is enabled). The detected motion sensing component output can be time stamped, such that a particular output is associated with particular inputs or information detected by sensors at the same time.</p>
<p id="p-0008" num="0007">To ensure that an image is not blurry, the electronic device can continuously buffer images captured by the lens. The buffer can have any suitable length, including for example a buffer large enough to hold 10, 20 or 50 images. In some embodiments, the buffer length may vary based on user inputs. For example, a buffer may have an initial length of 10 images that expands to 20 images in response to receiving a user instruction to store a captured image. As each image is captured by the lens and stored in the buffer, the motion sensing component can provide an output depicting the device movement at the time the image is captured. Thus, each image in the buffer can be associated with motion information describing the movement of the electronic device at the time the image was captured.</p>
<p id="p-0009" num="0008">In response to receiving a user instruction to store an image captured by a lens, the electronic device can mark the particular image in the buffer that corresponds to the moment at which the instruction was received. In addition, the electronic device can continue to store captured images in the buffer, up to a predetermined maximum buffer length (e.g., store 10, 15 or 20 additional images). The buffer can then include a series of images captured before and after the instruction was received. To select a picture that will not be blurry, the electronic device can analyze the motion sensing information associated with each of the images in the buffer, and identify the one or more images for which the motion sensing information is less than a maximum threshold or satisfies other criteria. The electronic device can then select one of the one or more images, and store the selected one or more images in memory. In some embodiments, the electronic device can present the one or more images to the user for selection.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0010" num="0009">The above and other features of the present invention, its nature and various advantages will be more apparent upon consideration of the following detailed description, taken in conjunction with the accompanying drawings in which:</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic view of an illustrative electronic device for capturing images in accordance with one embodiment of the invention;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIGS. 2A</figref> and B are schematic views of an illustrative electronic device in accordance with one embodiment of the invention;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 3</figref> is a schematic view of an illustrative image buffer in accordance with one embodiment of the invention;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 4</figref> is a schematic diagram illustrating how to select a particular buffered image in response to receiving a corresponding instruction in accordance with one embodiment of the invention;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart of an illustrative process for selecting an image from a buffer of stored images based on detected movement data in accordance with one embodiment of the invention; and</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart of an illustrative process for applying predetermined criteria to select from several captured images in accordance with one embodiment of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0017" num="0016">This is directed to systems and methods for selecting one of several images captured by an electronic device lens in response to receiving a user instruction to store a captured image. In particular, this is directed to associating a series of sequentially captured images with information describing the motion of the electronic device at the time each image was captured. Other approaches for selecting images are described in commonly assigned U.S. Patent Application Publication No. 2010/0309335, entitled &#x201c;Image Capturing Device Having Continuous Image Capture,&#x201d; filed Jun. 5, 2009, which is incorporated by reference herein in its entirety.</p>
<p id="p-0018" num="0017">An electronic device can be operative to capture images of a user's environment. For example, an electronic device can include an optical or digital lens operative to capture light reflected from the user's environment. The electronic device may be operative to store particular images captured by the lens for playback (e.g., to print or to send electronically for others to view). The electronic device can store images at any suitable time, including for example in response to receiving a user instruction (e.g., in response to a user providing an input using an input interface), after a delay (e.g., in a timer mode), or automatically (e.g., at particular moments in time or when the electronic device detects movement). In some cases, however, a user may move as the device captures an image. This may cause images to be blurry, or otherwise lack definition.</p>
<p id="p-0019" num="0018">The electronic device movement can be measured and quantified using any suitable approach. In some embodiments, the electronic device can include a motion detection or motion sensing component for detecting and measuring motion of the device. Such components can include, for example, accelerometers and gyroscopes, or other sensors for detecting linear and rotational acceleration. The motion sensing component can operate at any suitable time, and detect device movement at any suitable rate or frequency. In some embodiments, the frequency at which the motion sensing component detects device movement can be related to a frame rate or image capture rate of the lens.</p>
<p id="p-0020" num="0019">To reduce the effect of unintentional movement or shaking of the device on captured images, the electronic device can continuously store sequentially captured images in a buffer when in an image capture (e.g., camera) mode. The image buffer can include any suitable number of images, and can in some embodiments have a varying length. For example, the image buffer can initially store 5, 10 or 20 consecutive images, where images are captured and buffered at a preset rate (e.g., 6 or 10 images per second). As the image buffer is filled with captured images, the electronic device can receive motion information from the motion sensing component at rates substantially corresponding to the rate at which captured images are buffered. This can allow the electronic device to associate the motion information and image that are detected and captured, respectively, at the same time. Thus each of the buffered images can be associated with motion information describing the particular movement of the device at the time the buffered image was captured.</p>
<p id="p-0021" num="0020">In response to receiving a user instruction to store a captured image, the electronic device can save the captured images stored in the buffer just prior to receiving the user instruction, and continue to buffer additional images captured after receiving the user instruction. Any suitable number of images can be stored in the buffer, including a different number of images in the portions of the buffer corresponding to before and after the instruction was received. The electronic device can in addition associate movement information provided by the motion sensing component with each of the images stored in the buffer.</p>
<p id="p-0022" num="0021">The electronic device can use any suitable approach for selecting one or more of the images stored in the buffer to store in memory. In some embodiments, the electronic device can compare the movement information associated with each of the buffered images, and select the image with the least movement information. In some embodiments, the electronic device can instead or in addition select one or more images for which the movement information is less than a maximum threshold. If several images satisfy the threshold requirement, the electronic device can select an image that is closest in time to the moment when the user instruction was received. Any other suitable approach can be used to select an image based on the movement information.</p>
<p id="p-0023" num="0022">In some embodiments, the electronic device can augment the criteria for selecting a buffered image using additional information or attributes of each image. For example, the electronic device can analyze the composition, color palette or distribution, noise or other artifacts, or other attributes of each image to select one of the buffered images. As another example, the electronic device can select an image based on the presence, absence, or characteristics of a specific object or face in a picture (e.g., select the picture that has the most faces). In some embodiments, the electronic device can provide a preliminary filter and display a reduced number of images from which the user may select.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic view of an illustrative electronic device for changing the display of information based on device movement in accordance with one embodiment of the invention. Electronic device <b>100</b> can include any suitable type of electronic device having a lens. For example, electronic device <b>100</b> can include a media player such as an iPod&#xae; available by Apple Inc., of Cupertino, Calif., a cellular telephone, a personal e-mail or messaging device (e.g., a Blackberry&#xae; or a Sidekick&#xae;), an iPhone&#xae; available from Apple Inc., pocket-sized personal computers, personal digital assistants (PDAs), a laptop computer, a music recorder, a video recorder, a camera, or any other portable electronic device capable of being moved by the user. The electronic device can include or be coupled to positioning circuitry to determine the current position of the electronic device.</p>
<p id="p-0025" num="0024">Electronic device <b>100</b> can include a processor or control circuitry <b>102</b>, storage <b>104</b>, memory <b>106</b> input/output circuitry <b>108</b>, motion sensing component <b>110</b>, and camera lens <b>112</b> as typically found in an electronic device of the type of electronic device <b>100</b>. In some embodiments, one or more of electronic device components <b>100</b> can be combined or omitted (e.g., combine storage <b>104</b> and memory <b>106</b>), or electronic device <b>100</b> can include other components not combined or included in those shown in <figref idref="DRAWINGS">FIG. 1</figref> (e.g., communications circuitry, motion detection or sensing components, or positioning circuitry), or several instances of the components shown in <figref idref="DRAWINGS">FIG. 1</figref>. For the sake of simplicity, only one of each of the components is shown in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0026" num="0025">Motion sensing component <b>110</b> may be operative to detect movements of electronic device <b>100</b>. For example, a motion sensor can be operative to detect a user's movements of electronic device <b>100</b> and thus determine a change in orientation or movement along a path of the device. In some embodiments, a motion sensor can include one or more three-axes acceleration motion sensors (e.g., an accelerometer) operative to detect linear acceleration in three directions (i.e., the x or left/right direction, the y or up/down direction, and the z or forward/backward direction). As another example, a motion sensor can include one or more two-axis acceleration motion sensors which can be operative to detect linear acceleration only along each of x or left/right and y or up/down directions (or any other pair of directions). In some embodiments, a motion sensor can include an electrostatic capacitance (capacitance-coupling) accelerometer that is based on silicon micro-machined MEMS (Micro Electro Mechanical Systems) technology, a piezoelectric type accelerometer, a piezoresistance type accelerometer, or any other suitable accelerometer.</p>
<p id="p-0027" num="0026">Camera lens <b>112</b> can include any suitable lens operative to capture images or video of the electronic device environment. For example, the electronic device can include an optical or digital lens for capturing light reflected from the user's environment. The captured light can be recorded as individual distinct images, or as consecutive video frames of a recording (e.g., several video frames constituting a primary frame and subsequent frames indicating the difference between the primary frame and the subsequent frames). The control circuitry may associate different metadata with the recorded images, including for example positioning information, device movement information, a time code, a device identifier, or any other suitable metadata. As used in this application, the term camera lens will be understood to mean a lens for capturing light from a device environment, or a lens and appropriate circuitry for converting captured light into an image that can be previewed or stored by the device.</p>
<p id="p-0028" num="0027">In some embodiments, electronic device <b>100</b> can include a bus operative to provide a data transfer path for transferring data to, from, or between control processor <b>102</b>, storage <b>104</b>, memory <b>106</b>, input/output circuitry <b>108</b>, motion sensing component <b>110</b>, lens <b>112</b>, and any other component included in the electronic device.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. 2A</figref> and B are schematic views of an illustrative electronic device in accordance with one embodiment of the invention. Electronic device <b>200</b> can include some or all of the features of electronic device <b>100</b> (<figref idref="DRAWINGS">FIG. 1</figref>). In particular, electronic device <b>200</b> can include display <b>210</b> and camera lens <b>212</b> extending from the device. When a camera mode is enabled, the electronic device can provide a preview of the image captured by lens <b>212</b> on display <b>210</b>. The display can include option <b>214</b>, which the user can select to store the captured image (e.g., take the photograph).</p>
<p id="p-0030" num="0029">In the absence of an instruction to store a captured image, the electronic device can store images captured by the lens using any suitable approach. In some embodiments, the captured images can be passed directly to display circuitry for previewing. Once a more recent image is captured by the lens, prior images can be discarded and released from memory. Alternatively, the electronic device can temporarily or permanently store several captured images, for example in a buffer. The buffer can have any suitable length or size, including, for example, a length defined by an amount of memory (e.g., 5 MB), a number of images stored (e.g., 10 images), a duration during which images are captured (e.g., images captured by the lens during 1 second), or any other length. In response to capturing an image, the electronic device can pass the image to display circuitry for the preview display, and place the image in buffer. As new images are captured, the new images can be displayed and stored in the buffer, replacing the oldest captured image in the buffer when the buffer is full.</p>
<p id="p-0031" num="0030">Each image in the buffer can be time-stamped to determine the exact time at which the image was captured. Alternatively, the buffer can be defined such that a known duration exists between adjacent buffered images (e.g., images are buffered at different rates). This can allow the electronic device to determine timing information for each stored image relative to the most recently captured image (e.g., each image was captured 1/10 second before the following image). <figref idref="DRAWINGS">FIG. 3</figref> is a schematic view of an illustrative image buffer in accordance with one embodiment of the invention. Buffer <b>300</b> can include several elements <b>302</b> for storing captured images and related information. For example, each element <b>302</b> can include captured image <b>310</b> and related time stamp <b>312</b>. Some or all of the consecutive image captured by the lens can be stored in buffer <b>300</b>. For example, buffer <b>300</b> can include every consecutively captured image. As another example, buffer <b>300</b> can include a subset of the captured images. The electronic device can select the subset using any suitable approach, including for example based on a predetermined interval between images (e.g., buffer every fourth captured image, or buffer an image every 500 ms), or based on a detected environmental condition (e.g., buffer images more closely in time in low light conditions, or in response to detecting substantial device movement).</p>
<p id="p-0032" num="0031">Time stamp <b>312</b> can provide the absolute or relative timing information for the image <b>310</b> of the corresponding buffer element <b>302</b>. For example, time stamp <b>312</b> can define the absolute or system time at which the corresponding image was captured. As another example, time stamp <b>312</b> can define a duration between the current buffer element and another buffer element of buffer <b>300</b> (e.g., the duration relative to the next or previous buffer element <b>302</b>). In some embodiments, time stamp <b>312</b> can include a value that changes as additional images are placed in buffer <b>300</b> (e.g., if time stamp <b>312</b> defines timing information relative to the first or last buffer element <b>302</b> of buffer <b>300</b>). In some embodiments, time stamp <b>312</b> can be used to associate particular images <b>310</b> with other time or image-specific data, such as movement data detected by a motion-sensing component. Time stamp <b>312</b> can then not specifically provide timing information, but instead provide a pointer or other reference information for associating the data with the image (e.g., time stamp <b>312</b> points to specific elements of a buffer of movement information populated by the output of a motion sensing component or include the movement information provided by the motion sensing component).</p>
<p id="p-0033" num="0032">When a user provides an instruction to capture an image, the electronic device can lag before an image is actually captured and stored. During this delay, a user can unintentionally move or shake the device. In addition, the act of pressing a button, touching a screen, or otherwise providing an input using the input interface of the device can case can cause the user to shake or move the device. When the device, and thus the lens moves, the captured image can be blurry or lack definition. To avoid storing a blurry image, the electronic device can instead select one of the images buffered around the time the instruction to store the image was received.</p>
<p id="p-0034" num="0033">In response to receiving an instruction to store an image from a user (e.g., in response to receiving an instruction to take a photograph), the electronic device can save the buffered images that were captured shortly before and/or after the instruction was received. For example, the electronic device can save some or all of the buffered images when an instruction is received, and continue to buffer a predetermined number of images captured after the instruction was received. Based on the buffer length, the electronic device can retain some or all of the more recently captured images prior to receiving the instruction. For example, the electronic device can retain images captured within a predetermined duration from the instruction receipt (e.g., retain all images captured within 2 seconds from the instruction), or a particular number of images (e.g., retain 10 prior images).</p>
<p id="p-0035" num="0034">The electronic device can retain images captured after receiving the instruction using any suitable approach. For example, the electronic device can identify the oldest buffered image to retain, and replace all older images with images captured after receiving the instruction in the same buffer. As another example, the electronic device can define a second buffer for capturing images received after the receiving the user instruction. The electronic device can store images captured after receiving the instruction at any suitable rate, including for example at a rate different from the rate used for images captured prior to receiving the instruction. The second buffer can have any suitable length measured in any suitable manner (e.g., in any of the manners discussed above), including for example the same length as the buffer of images captured before receiving the instruction, or a different length (e.g., having more images).</p>
<p id="p-0036" num="0035">The electronic device can select any of the buffered images for storage in response to receiving a user instruction to store a captured image. In some embodiments, the electronic device can select an image to capture based on data describing the movement of the electronic device, so as to select an image that was captured while the device was substantially motionless. Using a motion sensing component, the electronic device can measure the device movement at different moments in time. The electronic device can process the motion sensing component output using any suitable approach. In some embodiments, the electronic device can determine the linear or rotational acceleration to which the device is subject based on the motion sensing component output (e.g., an accelerometer output). Alternatively, or in addition, the electronic device can process the output to determine a different measurement for the motion, such as integrating acceleration information over several motion sensing component outputs to determine velocity or displacement measurements for the device. The selected movement information can then be analyzed to determine when the device movement is satisfactory.</p>
<p id="p-0037" num="0036">The electronic device can retain past motions sensing component outputs or movement quantities in a buffer, where each particular output is associated with a time stamp. The time stamps can then be used to determine which of the buffered images to associate with the different movement information. For example, the electronic device can retrieve the time stamps associated with each buffered image (e.g., time stamps <b>312</b>, <figref idref="DRAWINGS">FIG. 3</figref>), and identify the particular movement information measured or processed at a time corresponding to the time stamp, and associated the image with the identified movement information. In some embodiments, the identified movement information can be specifically linked to the images in buffer <b>300</b> (<figref idref="DRAWINGS">FIG. 3</figref>), added to buffer elements <b>302</b> of buffer <b>300</b>, incorporated as metadata in each image, or associated with captured images using any other suitable approach.</p>
<p id="p-0038" num="0037">The electronic device can analyze the movement information (e.g., the processed or raw output of the motion sensing component associated with each image) of each buffered image to select an image that both includes what the user wanted to capture from the environment, but that also is not blurry due to device movement as the image was captured. The electronic device can use any suitable criteria for determining whether movement information is suitable or adequate. For example, the electronic device can select the image in the buffer that is associated with the lowest movement information. As another example, the electronic device can identify all images that are associated with movement information that is less than a maximum threshold, and select the one of the identified images that is closest to when the instruction to store an image was received (e.g., closest in time or in number of buffered images from the image associated with the time at which the instruction was received). In some embodiments, the maximum threshold can change for different sets of captured images. For example, the maximum threshold can be selected based on conditions of the environment (e.g., light conditions), attributes of the captured image (e.g., color palette, F-stop, aperture), lens settings (e.g., optical or digital zoom), or any other external conditions or settings that can be set or detected by the device. In some embodiments, the electronic device can give preference to images captured after the instruction to store an image was received (e.g., when people are more likely to be posing properly).</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 4</figref> is a schematic diagram illustrating how to select a particular buffered image in response to receiving a corresponding instruction in accordance with one embodiment of the invention. Diagram <b>400</b> includes movement axis <b>402</b> and time axis <b>404</b>. Each image stored in the buffer for which movement information is available is depicted by a mark <b>420</b>. The maximum threshold for movement information is depicted by line <b>410</b>, and the instant of time at which an instruction to store an image was received is depicted by line <b>406</b>. As can be seen in diagram <b>400</b>, of all of the images stored in the buffer, only a subset are associated with movement information that is less than the maximum threshold. The electronic device can then disregard all images having movement information that exceeds the threshold, and select one of the images represented by a mark <b>420</b> below line <b>410</b>. In some embodiments, the electronic device can select the image having the lowest movement information (e.g., the image associated with mark <b>422</b>). As another example, the electronic device can select the image having movement information below the threshold and that is the closest in time to the time at which the instruction was received (e.g., the image associated with mark <b>424</b>). As still another example, the electronic device can select the image having movement information below the threshold and that is the closest in time to and after the time at which the instruction was received (e.g., the image associated with mark <b>426</b>).</p>
<p id="p-0040" num="0039">In some embodiments, a combination of approaches can be used to select a particular image. For example, the electronic device can display several images selected using different approaches to the user, and allow the user to select one or more of the images to store. As another example, the electronic device can analyze characteristics of the images themselves to select a particular image. Suitable characteristics can include, for example, the composition of the image, lighting characteristics, a color palette, specific objects or faces detected in the image, zoom level, aperture, F-stop, or other characteristics of the captured image.</p>
<p id="p-0041" num="0040">The following flowcharts illustrate steps of various processes used in different embodiments of the invention. <figref idref="DRAWINGS">FIG. 5</figref> is a flowchart of an illustrative process for selecting an image from a buffer of stored images based on detected movement data in accordance with one embodiment of the invention. Process <b>500</b> can begin at step <b>502</b>. At step <b>504</b>, the electronic device can determine whether a camera mode is enabled. For example, the electronic device can determine whether the user provided an instruction to operate a camera application or turn on a camera lens. If the electronic device determines that the camera mode is not enabled, process <b>500</b> can move to step <b>506</b> and end. If, at step <b>504</b>, the electronic device instead determines that the camera mode is enabled, process <b>500</b> can move to step <b>508</b>.</p>
<p id="p-0042" num="0041">At step <b>508</b>, the electronic device can continuously capture images using a camera lens. For example, the electronic device can enable a camera lens to capture light reflected from the user's environment. The electronic device can display the captured images in sequence on a preview screen for the user, and temporarily store some or all of the captured images in memory. At step <b>510</b>, the electronic device can continuously identify device movement information for the device. For example, the electronic device can receive the successive output of a motion sensing component and process the output, if necessary, to define comparable quantities describing the device movement. In some embodiments, the electronic device can perform integration operations on acceleration data to define velocity or displacement quantities describing the device movement. At step <b>512</b>, the electronic device can store the captured images and the device movement information in buffers. For example, the electronic device can store captured images in a first buffer, and movement information in a second buffer. As another example, both images and movement information can be stored in the same buffer. In some embodiments, device movement information describing the device movement at a particular moment in time can be associated with the image captured at that same moment in time (e.g., using pointers, metadata, buffer elements, or any other suitable approach).</p>
<p id="p-0043" num="0042">At step <b>514</b>, the electronic device can determine whether an instruction to store a captured image was received. For example, the electronic device can determine whether a user provided an input using an input interface to store an image (e.g., take a photograph). If the electronic device determines that no such instruction was provided, process <b>500</b> can return to step <b>508</b> and continue to capture images. If, at step <b>514</b>, the electronic device instead determines that an instruction to store a captured image was received, process <b>500</b> can move to step <b>516</b>. At step <b>516</b>, the electronic device can identify the movement information associated with each image in the image buffer. For example, the electronic device can retrieve a movement information buffer associated with the image buffer. As another example, the electronic device can extract movement information from specific images in the image buffer, or from data stored in the image buffer. In some embodiments, the electronic device can continue to store captured images and identified movement information for a duration after having received the instruction at step <b>514</b>.</p>
<p id="p-0044" num="0043">At step <b>518</b>, the electronic device can select a buffered image that is associated with movement information that satisfies predetermined criteria. The predetermined criteria can include any criteria suitable for selecting images captured when the device movement was small, including for example the image associated with the smallest identified movement, an image associated with movement information that is less than a maximum threshold, an image associated with appropriate movement information and close in time to when the instruction was received, or any other suitable criteria. In some embodiments, the criteria can be supplemented by analysis of characteristics of each captured image, or by providing a subset of captured images from which the user can select (e.g., if several captured images satisfy the initial predetermined criteria). In some embodiments, several images can be selected. At step <b>520</b>, the selected image can be stored in memory. For example, the electronic device can add the one or more selected images to a photo library. Process <b>500</b> can then end at step <b>506</b>.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart of an illustrative process for applying predetermined criteria to select from several captured images in response to an instruction to store a captured image in accordance with one embodiment of the invention. Process <b>600</b> begins at step <b>602</b>. At step <b>604</b>, the electronic device can identify movement information associated with buffered images. For example, the electronic device can determine associations of buffered movement information with buffered images captured by a device lens. The electronic device can use any suitable approach to determine the associations, including for example using on time stamps associated with both the images and movement information. At step <b>606</b>, the electronic device can determine whether at least one value of the identified movement information is less than a predetermined threshold. For example, the electronic device can compare the values quantifying the identified movement with a threshold and determine whether the values are less than the threshold value. If the electronic device determines that none of the values of the identified movement information is less than the threshold, process <b>600</b> can move to step <b>608</b>. At step <b>608</b>, the electronic device can select and store the image captured at the time the instruction was received. Process <b>600</b> can then move to step <b>612</b>.</p>
<p id="p-0046" num="0045">If, at step <b>606</b>, the electronic device instead determines that at least one value of identified movement information is less than the threshold, process <b>600</b> can move to step <b>610</b>. At step <b>610</b>, the electronic device can identify a particular value of movement information that is less than the threshold and that is also closest in time to the time at which the instruction was received. For example, the electronic device can determine the particular value of movement information indicating that the device was sufficiently stable and that is as close as possible to the time at which the instruction was received. At step <b>612</b>, the electronic device can store the image associated with the identified particular value of movement information. By this process, the selected image can be the image captured when the device was sufficiently stable and close in time to when the instruction was received. Process <b>600</b> can then end at step <b>614</b>.</p>
<p id="p-0047" num="0046">The above described embodiments of the invention are presented for purposes of illustration and not of limitation, and the present invention is limited only by the claims which follow.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for storing an image captured by a lens of an electronic device, comprising:
<claim-text>capturing a plurality of images using the lens, each of the images having a corresponding capture time;</claim-text>
<claim-text>storing the captured images in a buffer;</claim-text>
<claim-text>detecting movement information describing movement of the electronic device using a motion sensing component;</claim-text>
<claim-text>associating each image in the buffer with movement information detected at a time corresponding to the image's capture time, wherein the act of associating further comprises:
<claim-text>storing the detected movement information as movement information values in a second buffer;</claim-text>
<claim-text>associating a first time stamp with each of the captured images in the buffer;</claim-text>
<claim-text>associating a second time stamp with each of the movement information values stored in the second buffer; and</claim-text>
<claim-text>associating captured images and movement information values based, at least in part, on a relationship between the first time stamps and the second time stamps;</claim-text>
</claim-text>
<claim-text>receiving an instruction to store a captured image;</claim-text>
<claim-text>identifying one or more images in the buffer having associated movement information that satisfies a movement threshold; and</claim-text>
<claim-text>selecting one of the identified one or more images for storage, wherein the act of selecting one of the identified one or more images for storage comprises selecting an image from the identified one or more images having a corresponding capture time that is closest in time to the time that the instruction to store the captured image was received.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the act of storing the captured images in a buffer further comprises:
<claim-text>determining that the buffer is full; and</claim-text>
<claim-text>replacing an oldest captured image in the buffer with a newest image captured by the lens.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the act of associating each image in the buffer with movement information further comprises embedding as metadata, for each captured image in the buffer, the movement information detected at the time corresponding to the image's capture time.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the act of detecting movement information describing movement of the electronic device using a motion sensing component comprises processing acceleration information from the motion sensing component to identify at least one of velocity and displacement information.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the act of selecting one of the identified one or more images for storage further comprises determining whether each of the identified one or more images was captured before or after the instruction to store the captured image was received.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a value of the movement threshold is selected based on at least one of: conditions of an environment in which the plurality of images are captured, attributes of the captured images, and lens settings.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. An electronic device operative to capture and store images, comprising:
<claim-text>a lens operative to capture images;</claim-text>
<claim-text>a memory operative to store captured images in a buffer;</claim-text>
<claim-text>a motion sensing component operative to quantify movement of the electronic device; and</claim-text>
<claim-text>a processor operative to execute instructions stored in the memory to cause the processor to:
<claim-text>direct the lens to sequentially capture images;</claim-text>
<claim-text>direct the motion sensing component to quantify device movement during a time period that the lens captures images;</claim-text>
<claim-text>direct the memory to store each of the sequentially captured images in the buffer, wherein the most recently captured images replace older captured images stored in the buffer when the buffer is full;</claim-text>
<claim-text>associate each image in the buffer with a quantified device movement value, wherein the instructions to associate further comprise instructions to:
<claim-text>store the quantified device movement values as movement information values in a second buffer;</claim-text>
<claim-text>associate a first time stamp with each of the captured images in the buffer;</claim-text>
<claim-text>associate a second time stamp with each of the movement information values stored in the second buffer; and</claim-text>
<claim-text>associate captured images and movement information values based, at least in part, on a relationship between the first time stamps and the second time stamps;</claim-text>
</claim-text>
<claim-text>receive an instruction to store a captured image;</claim-text>
<claim-text>identify one or more images in the buffer having an associated device movement value that satisfies a movement threshold, wherein the movement threshold is selected based on at least one of:
<claim-text>environmental conditions, attributes of the sequentially captured images, and lens settings; and</claim-text>
</claim-text>
<claim-text>select one of the identified one or more images for storage in the memory, wherein the instructions to cause the processor to select one of the identified one or more images for storage in the memory comprise instructions to select an image from the identified one or more images having a corresponding capture time that is closest in time to the time that the instruction to store the captured image was received.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the images stored in the buffer are captured both before and after the instruction to store a captured image is received.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The electronic device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising instructions stored in the memory to cause the processor to determine a buffer size based on at least one of:
<claim-text>an amount of memory available;</claim-text>
<claim-text>a number of captured images to store; and</claim-text>
<claim-text>a duration from the current time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. Non-transitory computer readable media for storing an image captured by a lens of an electronic device, comprising computer-readable instructions recorded thereon to cause a processor to:
<claim-text>capture a plurality of images using the lens;</claim-text>
<claim-text>store the captured images in a first buffer;</claim-text>
<claim-text>detect movement information values describing movement of the electronic device using a motion sensing component;</claim-text>
<claim-text>store the detected movement information values in a second buffer;</claim-text>
<claim-text>associate each captured image in the first buffer with detected movement information values detected at a time the image was captured, wherein the instructions to associate further comprise instructions to:
<claim-text>associate a first time stamp with each of the captured images in the first buffer;</claim-text>
<claim-text>associate a second time stamp with each of the movement information values stored in the second buffer; and</claim-text>
<claim-text>associate captured images and movement information values based, at least in part, on a relationship between the first time stamps and the second time stamps;</claim-text>
</claim-text>
<claim-text>receive an instruction to store a captured image;</claim-text>
<claim-text>identify one or more images in the first buffer having associated movement information that satisfies a movement threshold; and</claim-text>
<claim-text>select one of the identified one or more images, wherein the instructions to cause the processor to select one of the identified one or more images comprise instructions to select an image from the identified one or more images having a corresponding capture time that is closest in time to the time that the instruction to store the captured image was received. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
