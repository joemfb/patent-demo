<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624926-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624926</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12423334</doc-number>
<date>20090414</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>885</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>G</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>3</main-group>
<subgroup>048</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345653</main-classification>
<further-classification>715848</further-classification>
<further-classification>715850</further-classification>
<further-classification>715858</further-classification>
</classification-national>
<invention-title id="d2e53">Panning using virtual surfaces</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5557714</doc-number>
<kind>A</kind>
<name>Lines et al.</name>
<date>19960900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345653</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2005/0187015</doc-number>
<kind>A1</kind>
<name>Suzuki et al.</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2007/0206030</doc-number>
<kind>A1</kind>
<name>Lukis</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2009/0292989</doc-number>
<kind>A1</kind>
<name>Matthews et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>EP</country>
<doc-number>1 369 822</doc-number>
<kind>A2</kind>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00006">
<othercit>Ji &#x201c;Dynamic View Selection for Time-Varying Volumes&#x201d;, IEEE Transaction on Visualization and Computer Graphics, vol. 12, No. 5, Sep./Oct. 2006.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00007">
<othercit>Henriksen et al., &#x201c;Virtual Trackballs Revisited,&#x201d; IEEE Transactions on Visualization and Computer Graphics, IEEE Service Center, Los Alamitos, CA, vol. 10, No. 2, Mar. 1, 2004, pp. 206-216.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00008">
<othercit>Chu-Yu Tang et al., &#x201c;A Virtual Reality-Based Surgical Simulation System for Virtual Neuroendoscopy,&#x201d; IEEE International Conference on Integration Technology Integration Technology, Mar. 1, 2007, pp. 253-258.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00009">
<othercit>Hachet, Martin et al., &#x201c;Navidget for Easy 3D Camera Positioning From 2D Inputs,&#x201d;, 2008 IEEE Symposium on 3D User Interfaces, Mar. 8, 2008, pp. 83-89.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>Zeleznik, R. C. et al., &#x201c;Two Pointer Input for 3D Interaction,&#x201d; Proceedings of the 1997 Symposium on Interactive 3D Graphics, 1997, pp. 115-119.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>PCT, &#x201c;Notification of Transmittal of the International Search Report and the Written Opinion of the International Search Authority, or the Declaration,&#x201d; International Application, PCT/US2009/002311, mailed Aug. 14, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00012">
<othercit>PCT, Notification of Transmittal of the International Search Report and the Written Opinion of the International Searching Authority, or the Declaration and Written Opinion of the International Search Authority; mailed Sep. 23, 2010; International Appln. No. PCT/US2009/054727, International Filing Date: Aug. 24, 2009; 14 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Matsushita et al., &#x201c;Dual Touch: A Two-Handed Interface for Pen-Based PDAs,&#x201d; Interaction Laboratory, Sony Computer Science Laboratories Inc., Tokyo, Japan, UIST '00 Proceedings of the 13th annual ACM symposium on User interface software and technology, pp. 211-212.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>15</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345563</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345173</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345653</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345419</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715850</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715856-859</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>11</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61044754</doc-number>
<date>20080414</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20090256840</doc-number>
<kind>A1</kind>
<date>20091015</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Varadhan</last-name>
<first-name>Gokul</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Barcay</last-name>
<first-name>Daniel</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Varadhan</last-name>
<first-name>Gokul</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Barcay</last-name>
<first-name>Daniel</first-name>
<address>
<city>San Francisco</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Sterne, Kessler, Goldstein &#x26; Fox P.L.L.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Google Inc.</orgname>
<role>02</role>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Ma</last-name>
<first-name>Tize</first-name>
<department>2678</department>
</primary-examiner>
<assistant-examiner>
<last-name>He</last-name>
<first-name>Yingchun</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">The present invention relates to panning a virtual camera in a three dimensional environment. In an embodiment of the present invention, a computer-implemented method pans a virtual camera in a three dimensional environment. In the method embodiment, a first point is determined on a three dimensional model in the three dimensional environment. According to the first point, the three dimensional model, and a position of a virtual camera in the three dimensional environment, a virtual surface is determined. A second point is determined on the virtual surface. Finally, a location of the three dimensional model is changed according to the first point and the second point.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="165.86mm" wi="240.11mm" file="US08624926-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="234.27mm" wi="178.90mm" file="US08624926-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="144.95mm" wi="177.88mm" file="US08624926-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="170.35mm" wi="161.12mm" file="US08624926-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="212.77mm" wi="167.72mm" orientation="landscape" file="US08624926-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="216.24mm" wi="111.59mm" file="US08624926-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="218.10mm" wi="178.22mm" file="US08624926-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="256.96mm" wi="166.71mm" orientation="landscape" file="US08624926-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="129.46mm" wi="170.69mm" file="US08624926-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="128.86mm" wi="151.89mm" file="US08624926-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0002" num="0001">The present invention relates to moving a virtual camera in a three dimensional environment.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">Systems exist for navigating through a three dimensional environment to display three dimensional data. The three dimensional environment includes a virtual camera. The virtual camera has a perspective that defines what three dimensional data to display to a user. The user may input data that pans the virtual camera. By panning the virtual camera, the user navigates in the three dimensional environment.</p>
<p id="p-0004" num="0003">One type of system that pans a virtual camera to navigate in a three dimensional environment is a geographic information system. A geographic information system is a system for storing, retrieving, manipulating, and displaying a three dimensional model of the Earth. The three dimensional model may include satellite images texture mapped to terrain, such as mountains, valleys, and canyons. Further, the three dimensional model may include buildings and other three dimensional features.</p>
<p id="p-0005" num="0004">As the virtual camera's perspective becomes tangential to the Earth, panning becomes increasingly unstable. A small change to the user's input pans the virtual camera large distances. This can be disorienting for the user.</p>
<p id="p-0006" num="0005">Systems and methods are needed for more stable panning in a three dimensional environment.</p>
<heading id="h-0003" level="1">BRIEF SUMMARY</heading>
<p id="p-0007" num="0006">The present invention relates to panning in a three dimensional environment. In an embodiment of the present invention, a computer-implemented method pans a virtual camera in a three dimensional environment. In the method embodiment, a first point is determined on a three dimensional model in the three dimensional environment. According to the first point, the three dimensional model, and a position of a virtual camera in the three dimensional environment, a virtual surface is determined. A second point is determined on the virtual surface. Finally, a location of the three dimensional model is changed according to the first point and the second point.</p>
<p id="p-0008" num="0007">In a second embodiment, a system pans a virtual camera in a three dimensional environment. The system includes a panning module that determines a first point on a three dimensional model in the three dimensional environment. The panning module also determines a second point on a virtual surface. A virtual surface calculator module determines the virtual surface according to the first point, the three dimensional model, and a position of a virtual camera in the three dimensional environment. A rotator module that changes a location of the three dimensional model according to the first and second points.</p>
<p id="p-0009" num="0008">In a third embodiment, a computer-implemented method pans a virtual camera in a three dimensional environment that includes a three dimensional model of the Earth. The method includes: displaying in a display area the three dimensional model from a perspective of the virtual camera, enabling a user to select a first point on the display area with a cursor, and determining a first screen ray according to the first point. The method further includes: enabling the user to move the cursor to a second point in the display area, determining a second screen ray according to the second point, and rotating the three dimensional model around an axis through a rotation point (e.g., a center) of the three dimensional model to follow the cursor at a rate defined by a panning velocity. The panning velocity is a ratio of an angle of rotation of the three dimensional model to an angle between the first screen ray and the second screen ray. The panning velocity does not exceed a threshold value.</p>
<p id="p-0010" num="0009">Employing a virtual surface to pan in a three dimensional environment reduces movement of the three dimensional model when the virtual camera's perspective is nearly tangential to the three dimensional model. By reducing movement of the three dimensional model, embodiments of the present invention make panning more stable.</p>
<p id="p-0011" num="0010">Further embodiments, features, and advantages of the invention, as well as the structure and operation of the various embodiments of the invention are described in detail below with reference to accompanying drawings.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading>
<p id="p-0012" num="0011">The accompanying drawings, which are incorporated herein and form a part of the specification, illustrate the present invention and, together with the description, further serve to explain the principles of the invention and to enable a person skilled in the pertinent art to make and use the invention.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 1A</figref> is a diagram illustrating panning without a virtual surface.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 1B</figref> is a diagram illustrating panning with a virtual surface according to an embodiment of the present invention.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 2A</figref> is a screenshot of a user interface of a geographic information system.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2B</figref> is a diagram illustrating various regions of a display area used for panning with the virtual surface.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3</figref> is an architecture diagram of a geographic information system client that pans using virtual surfaces according to an embodiment of the present invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart of a method for panning using virtual surfaces, which may be used in operation of the system in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 5A</figref> is a diagram that illustrates determining a surrogate ray according to a step in the method in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 5B</figref> is a diagram that illustrates determining a hit point according to a step in the method in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram illustrating calculations for constructing a virtual surface according to a step in the method in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. 7A-B</figref> are diagrams that illustrate determining an intersection with a virtual surface and rotating a three dimensional model according to the method in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0023" num="0022">The drawing in which an element first appears is typically indicated by the leftmost digit or digits in the corresponding reference number. In the drawings, like reference numbers may indicate identical or functionally similar elements.</p>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading>
<p id="p-0024" num="0023">Embodiments of the present invention relate to stable panning using virtual surfaces. In the detailed description of the invention that follows, references to &#x201c;one embodiment&#x201d;, &#x201c;an embodiment&#x201d;, &#x201c;an example embodiment&#x201d;, etc., indicate that the embodiment described may include a particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it is submitted that it is within the knowledge of one skilled in the art to effect such feature, structure, or characteristic in connection with other embodiments whether or not explicitly described.</p>
<p id="p-0025" num="0024">As used herein, the term &#x201c;panning&#x201d; a virtual camera refers to rotating a three dimensional model about an axis through a rotation point, such as its center. It is important to note that moving the three dimensional model relative to the camera is equivalent to moving a camera relative to a three dimensional model.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 1A</figref> shows a diagram <b>100</b> illustrating trackball panning without a virtual surface. Diagram <b>100</b> shows a virtual camera <b>106</b> viewing a three dimensional geometry <b>102</b>. Three dimensional geometry <b>102</b> includes terrain <b>116</b>. As is described in detail below, three dimensional geometry <b>102</b> may, for example, may be a three-dimensional model of the Earth having a center origin at a point <b>112</b>, and virtual camera <b>106</b> may define what portion of the three dimensional model to display to a user on a display area.</p>
<p id="p-0027" num="0026">The user may select a first point on the display area. As result, a ray <b>114</b> is extended according to the point. In an example, ray <b>114</b> may extend from a focal point of the virtual camera through a point on the viewport of the virtual camera corresponding to the point selected on the display area. How ray <b>114</b> may be determined is described in more detail with respect to <figref idref="DRAWINGS">FIG. 5B</figref>. Once ray <b>114</b> is determined, an intersection <b>108</b> between ray <b>114</b> and three dimensional model <b>102</b> is determined.</p>
<p id="p-0028" num="0027">A panning sphere <b>104</b> is determined corresponding to point <b>108</b> and three dimensional model <b>102</b>. Panning sphere <b>104</b> is a three dimensional surface used to control panning. In an example, panning sphere <b>104</b> may be a sphere with a center origin at point <b>112</b> and a radius equal to a distance between intersection <b>108</b> and point <b>112</b>.</p>
<p id="p-0029" num="0028">Once panning sphere <b>104</b> is determined, a user selects a second point on the display area, and three dimensional model <b>102</b> is rotated accordingly. The user may select the second point on the display area by, for example, moving a mouse to a new location. As result of the user selection, a ray <b>120</b> corresponding to the second point is determined. Once ray <b>120</b> is determined, an intersection <b>110</b> between ray <b>120</b> and panning sphere <b>104</b> is determined. A line segment <b>118</b> connects point <b>112</b> with intersection <b>108</b>, and a line segment <b>122</b> connects point <b>112</b> with intersection <b>110</b>. Three dimensional model <b>102</b> is rotated by an angle <b>114</b> between line segment <b>118</b> and line segment <b>122</b>. In this way, by selecting a point on a display area with, for example, a mouse and moving the mouse, the user rotates the model to follow the movement of the mouse.</p>
<p id="p-0030" num="0029">While standard trackball panning has advantages, it can be unstable. As ray <b>120</b> becomes increasing tangential to three dimensional model <b>102</b> and panning sphere <b>104</b>, small changes to the user's selection results in large rotations of model <b>102</b>. Embodiments of the present invention make panning more stable by introducing a virtual surface as described with respect to <figref idref="DRAWINGS">FIG. 1B</figref>.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 1B</figref> shows a diagram <b>150</b> illustrating panning with a virtual surface according to an embodiment of the present invention. As in diagram <b>100</b>, in diagram <b>150</b> the user selects a first and second point on a display area and, as result, the model rotates. However, the virtual surface reduces the model's rotation, thus increasing stability.</p>
<p id="p-0032" num="0031">When the user selects the first point on the display area, ray <b>114</b>, intersection <b>108</b>, and panning sphere <b>104</b> are determined as described for diagram <b>100</b>. Once panning sphere <b>104</b> is determined, a virtual surface <b>152</b> is determined. Virtual surface <b>152</b> and panning sphere <b>104</b> together form a continuous, smooth surface. Virtual surface <b>152</b> may be, for example, a concave surface opposing a portion of panning sphere <b>104</b>. In one embodiment, virtual surface <b>152</b> may be a mirror image of a portion of panning sphere <b>104</b>.</p>
<p id="p-0033" num="0032">When the user selects the second point on the display area, ray <b>120</b> is extended according to the user selection, and three dimensional model <b>102</b> is rotated. An intersection <b>154</b> is determined between ray <b>120</b> and virtual surface <b>152</b>. A line segment <b>158</b> connects point <b>112</b> and intersection <b>154</b>. Model <b>102</b> is rotated by an angle <b>156</b> between line segment <b>158</b> and line segment <b>118</b>. Thus, an intersection is determined using virtual surface <b>152</b>, instead of panning sphere <b>104</b>. In this way, the rotation of model <b>102</b> does not occur as abruptly when ray <b>120</b> becomes tangential to sphere <b>104</b>.</p>
<p id="p-0034" num="0033">Embodiments of the present invention are described in greater detail below with reference to the remaining figures. In particular, <figref idref="DRAWINGS">FIGS. 2-3</figref> describe a geographic information system that pans using virtual surfaces. <figref idref="DRAWINGS">FIG. 4</figref> describes a method that may be used in operation of the geographic information system. Finally, <figref idref="DRAWINGS">FIGS. 5</figref>, <b>6</b>, and <b>7</b>A-B include diagrams elaborating on the method.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 2A</figref> is a screenshot of a user interface <b>200</b> of a geographic information system, which may pan using virtual surfaces. User interface <b>200</b> includes a display area <b>202</b> for displaying geographic data. As mentioned above, the data displayed in display area <b>202</b> is from the perspective of a virtual camera. In an embodiment, the perspective is defined by a frustum such as, for example, a three dimensional pyramid with the top spliced off. Geographic data within the frustum can be displayed at varying levels of detail depending on its distance from the virtual camera.</p>
<p id="p-0036" num="0035">Example geographic data displayed in display area <b>202</b> include images of the Earth. These images can be rendered onto a geometry representing the Earth's terrain creating a three dimensional model of the Earth. Other data that may be displayed include three dimensional models of buildings.</p>
<p id="p-0037" num="0036">User interface <b>200</b> includes controls <b>204</b> for changing the virtual camera's orientation. Controls <b>204</b> enable a user to change, for example, the virtual camera's altitude, latitude, longitude, pitch, yaw, and roll. In an embodiment, controls <b>104</b> are manipulated using a computer pointing device such as a mouse. As the virtual camera's orientation changes, the virtual camera's frustum and the geographic data displayed also change. In addition to controls <b>204</b>, a user can also control the virtual camera's orientation using other computer input devices such as, for example, a computer keyboard or a joystick.</p>
<p id="p-0038" num="0037">User interface <b>200</b> may also enable a user to pan the virtual camera. A user may pan the virtual camera by selecting a first point on display area <b>202</b> with, for example, a computer pointing device. Then, the user may move the computer pointing device to select a second point on display area <b>202</b>. As is described below, the geographic information system rotates the model of the Earth according to the first and second points selected by the user.</p>
<p id="p-0039" num="0038">The geographic information system may operate using a client-server computer architecture. In such a configuration, user interface <b>200</b> resides on a client machine. The client machine can be a general-purpose computer with a processor, local memory, display, and one or more computer input devices such as a keyboard, a mouse and/or a joystick. Alternatively, the client machine can be a specialized computing device such as, for example, a mobile handset. The client machine communicates with one or more servers over one or more networks, such as the Internet. Similar to the client machine, the server can be implemented using any general-purpose computer capable of serving data to the client. The architecture of the geographic information system client is described in more detail with respect to <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 2B</figref> shows a diagram <b>250</b> illustrating various regions of a display area <b>260</b> used for panning with the virtual surface. As described above with respect to display area <b>202</b> in <figref idref="DRAWINGS">FIG. 2A</figref>, display area <b>260</b> may display a three dimensional model of the Earth from the perspective of a virtual camera. A user may pan the virtual camera by selecting a first point on display area <b>260</b> with, for example, a cursor controlled by computer pointing device such as a mouse. By extending a screen ray according to the point selected by the user, a hit point in the three dimensional model may be determined. Then, the user may move the cursor to select a second point on display area <b>260</b>. The three dimensional model of the Earth may rotate to follow the second point. Display area <b>260</b> includes regions <b>256</b>, <b>254</b>, and <b>252</b>. How the model rotates to follow the second point varies depending which region includes the second point.</p>
<p id="p-0041" num="0040">If the user moves the cursor to select a second point in region <b>256</b>, the model rotates so that the hit point stays under the cursor. In this way, the model rotates to follow the cursor. Referring back to <figref idref="DRAWINGS">FIG. 1B</figref>, in region <b>256</b> a screen ray extended according to the cursor position would intersect with panning sphere <b>104</b>. As the cursor moves closer to region <b>254</b>, the velocity of rotation increases. When the cursor enters region <b>254</b>, a screen ray extended according to the cursor position would intersect with virtual surface <b>152</b>.</p>
<p id="p-0042" num="0041">In an embodiment, a virtual surface may be constructed such that a velocity of rotation stays constant regardless of the position of the cursor in region <b>254</b>. In other words, for any movement of a cursor by X pixels, the model rotates by a constant Y degrees. Further, that velocity may be set the velocity of rotation at a point <b>258</b> at which region <b>256</b> connects to region <b>254</b> (e.g. where the panning surface meets the virtual surface). By setting the velocity of rotation on the virtual surface equal to the velocity on the panning sphere at the intersection between the panning surface and the virtual surface, there is a smooth transition between the two surfaces. The smooth transition reduces the likelihood that the user would experience jerk in panning. Ultimately, reduced jerk results in a smoother panning experience.</p>
<p id="p-0043" num="0042">In an embodiment, a virtual surface may be constructed such that panning is reversible. The model starts at a first location. A user selects a first position on display area <b>260</b>. Then, the user moves the cursor to a second position on display area <b>260</b>. Accordingly, the model rotates to a second location. If the user moves the cursor back to the first position on display area <b>260</b>, the model rotates back to the first location. In this way, the virtual surface may be constructed such that panning is reversible.</p>
<p id="p-0044" num="0043">Finally, display area <b>260</b> may optionally have a region <b>252</b> where little or no rotation occurs. In a geographic information system, region <b>252</b> may correspond to the sky or outer space. If a user moves a cursor into region <b>252</b>, panning may stop. Further, a screen ray extended according to a cursor location in region <b>252</b> may not intersect with a panning sphere or a virtual surface.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 3</figref> is an architecture diagram of an exemplary client <b>300</b> of a GIS according to an embodiment of the invention. In an embodiment, client <b>300</b> includes a user interaction module <b>310</b>, local memory <b>330</b>, cache node manager <b>340</b>, renderer module <b>350</b>, network loader <b>365</b>, and display interface <b>380</b>. As shown in <figref idref="DRAWINGS">FIG. 3</figref>, user interaction module <b>310</b> includes a graphical user interface (GUI) <b>312</b> and motion model <b>318</b>. Local memory <b>330</b> includes a view specification <b>332</b> and quad node tree <b>334</b>. Cache node manager <b>340</b> includes a retrieval list <b>345</b>.</p>
<p id="p-0046" num="0045">In an embodiment, the components of client <b>300</b> can be implemented, for example, as software running on a client machine. Client <b>300</b> interacts with a GIS server (not shown) to bring images of the Earth and other geospatial data to client <b>300</b> for viewing by a user. Together, the images of the Earth and other geospatial data form a three dimensional model in a three dimensional environment. In an embodiment, software objects are grouped according to functions that can run asynchronously (e.g., time independently) from one another.</p>
<p id="p-0047" num="0046">In general, client <b>300</b> operates as follows. User interaction module <b>310</b> receives user input regarding a location that a user desires to view and, through motion model <b>318</b>, constructs view specification <b>332</b>. Renderer module <b>350</b> uses view specification <b>332</b> to decide what data is to be drawn and draws the data. Cache node manager <b>340</b> runs in an asynchronous thread of control and builds a quad node tree <b>334</b> by populating it with quad nodes retrieved from a remote server via a network.</p>
<p id="p-0048" num="0047">In an embodiment of user interface module <b>310</b>, a user inputs location information using GUI <b>312</b>. This results, for example, in the generation of view specification <b>332</b>. View specification <b>332</b> is placed in local memory <b>330</b>, where it is used by renderer module <b>350</b>.</p>
<p id="p-0049" num="0048">Motion model <b>318</b> uses location information received via GUI <b>312</b> to adjust the position or orientation of a virtual camera. The camera is used, for example, for viewing a displayed three dimensional model of the Earth. A user sees a displayed three dimensional model on his or her computer monitor from the standpoint of the virtual camera. In an embodiment, motion model <b>318</b> also determines view specification <b>332</b> based on the position of the virtual camera, the orientation of the virtual camera, and the horizontal and vertical fields of view of the virtual camera.</p>
<p id="p-0050" num="0049">View specification <b>332</b> defines the virtual camera's viewable volume within a three dimensional space, known as a frustum, and the position and orientation of the frustum with respect, for example, to a three dimensional map. In an embodiment, the frustum is in the shape of a truncated pyramid. The frustum has minimum and maximum view distances that can change depending on the viewing circumstances. As a user's view of a three dimensional map is manipulated using GUI <b>312</b>, the orientation and position of the frustum changes with respect to the three dimensional map. Thus, as user input is received, view specification <b>332</b> changes. View specification <b>332</b> is placed in local memory <b>330</b>, where it is used by renderer module <b>350</b>. View specification <b>332</b> and the other components of GIS client <b>300</b> are described in greater detail below.</p>
<p id="p-0051" num="0050">Motion model <b>318</b> uses several sub-modules to control panning using virtual surfaces. The sub-modules include a panning module <b>390</b>, virtual surface calculator module <b>392</b>, and rotator module <b>394</b>. Motion model <b>318</b> accepts a user input from GUI <b>312</b>. If the user input is a mouse click, motion model <b>318</b> may activate panning module <b>390</b>, for example, by a function call.</p>
<p id="p-0052" num="0051">Panning module <b>390</b> extends a ray according to the user input and determines an intersection between the three dimensional model and the ray. Panning module <b>390</b> also may construct a panning sphere. Once the panning sphere is constructed, virtual surface calculator module <b>392</b> may fit a virtual surface to the panning sphere.</p>
<p id="p-0053" num="0052">Motion model <b>318</b> accepts a second user input from GUI <b>312</b>. Panning module <b>390</b> extends another ray according to the second user input. Panning module <b>390</b> also calculates an intersection between the virtual surface and the ray. Finally, rotator module <b>394</b> rotates the three dimensional model according to the intersection. The operation of panning module <b>390</b>, virtual surface calculator module <b>392</b>, and rotator module <b>394</b> is described in more detail below with respect to <figref idref="DRAWINGS">FIGS. 4-6</figref> and <b>7</b>A-B.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 4</figref> is a flowchart of a method <b>400</b> for panning using virtual surfaces, which may be used in operation of motion model <b>318</b> in <figref idref="DRAWINGS">FIG. 3</figref>. Although method <b>400</b> is described with respect to motion model <b>318</b>, it is not meant to be limited to motion model <b>318</b>.</p>
<p id="p-0055" num="0054">Method <b>400</b> begins by receiving a user event at step <b>402</b>. If the user event is a click event, a surrogate ray may be calculated at a step <b>418</b>. Then, a hit point is calculated by intersecting a screen ray or the surrogate ray with the three dimensional model at step <b>404</b>. At step <b>406</b>, a panning sphere is fit to the hit point, and, at step <b>408</b>, a virtual surface is fit to the panning sphere. The panning sphere may be generated according to the hit point as described with respect to <figref idref="DRAWINGS">FIG. 1B</figref>. The virtual surface may be generated according to position of the virtual camera. A mouse move event is received, and method <b>400</b> proceeds to step <b>410</b>. At step <b>410</b>, a screen ray is intersected with both the panning surface and the virtual surface. One of the intersection points is selected at step <b>412</b>. If the selected intersection point is on the virtual surface, then it is projected onto the panning sphere in step <b>414</b>. Finally, at step <b>416</b>, the three dimensional model is rotated by an angle subtended by a line segment connecting the hit point determined in step <b>404</b> and the intersection point selected in step <b>416</b>. Each of the steps is described in more detail below with respect to motion model <b>318</b> in <figref idref="DRAWINGS">FIG. 3</figref> and diagrams in <figref idref="DRAWINGS">FIGS. 5A-B</figref>, <b>6</b>, and <b>7</b>A-B.</p>
<p id="p-0056" num="0055">At step <b>402</b>, GUI <b>312</b> may receive one of two user events&#x2014;a click event or a move event. GUI <b>312</b> may receive the click event in response to the user clicking a mouse or other input device at a position in display area <b>202</b> in <figref idref="DRAWINGS">FIG. 2</figref>. GUI <b>312</b> may receive the move event when the user holds down an input device (such as a mouse) and moves the input device to a new position on display area <b>202</b>. These events may result in GUI <b>312</b> making callback function calls to panning module <b>390</b>. During the function call, GUI <b>312</b> may pass position information pertaining to the event to panning module <b>390</b>.</p>
<p id="p-0057" num="0056">If the user event is a click event, panning module <b>390</b> may determine a surrogate screen ray at step <b>418</b> as illustrated in <figref idref="DRAWINGS">FIG. 5A</figref>. <figref idref="DRAWINGS">FIG. 5A</figref> shows a virtual camera with a focal point <b>506</b>. Based on the click event, a screen ray <b>510</b> is determined. Determining a screen ray is described in more detail with respect to <figref idref="DRAWINGS">FIG. 5B</figref>. A line segment <b>514</b> connects center origin <b>112</b> and focal point <b>506</b>. An angle <b>516</b> between line segment <b>514</b> and screen ray <b>510</b> exceeds a threshold value. Because angle <b>516</b> exceeds the threshold value, angle <b>516</b> is fixed to the threshold value at an angle <b>518</b>. Finally, a surrogate ray <b>508</b> is extended from focal point <b>506</b> at angle <b>518</b> with respect to line segment <b>514</b>.</p>
<p id="p-0058" num="0057">At step <b>404</b>, panning module <b>390</b> calculates a hit point by intersecting a screen ray or a surrogate ray with the three dimensional model. Step <b>404</b> is described in detail with respect to a diagram <b>500</b> in <figref idref="DRAWINGS">FIG. 5B</figref>. Diagram <b>500</b> shows a model of the Earth <b>552</b>. Diagram <b>500</b> also shows focal point <b>506</b> of a virtual camera. The virtual camera is used to capture and to display information as described with respect to <figref idref="DRAWINGS">FIG. 2</figref>. The virtual camera has a focal length <b>558</b> and a viewport <b>560</b>. Viewport <b>560</b> corresponds to display area <b>202</b> in <figref idref="DRAWINGS">FIG. 2</figref>. A user selects a position on display area <b>202</b>, and the position corresponds to a point <b>562</b> on viewport <b>560</b>.</p>
<p id="p-0059" num="0058">Panning module determines a hit point by extending a screen ray from the virtual camera to determine an intersection with the model. In diagram <b>500</b>, a ray <b>564</b> extends from focal point <b>506</b> through point <b>562</b>. In another embodiment, ray <b>564</b> may be a surrogate ray computed as described with respect to <figref idref="DRAWINGS">FIG. 5A</figref>. Ray <b>564</b> intersects with model <b>552</b> at a location <b>554</b>. Thus, the hit point is at location <b>554</b>. Ray <b>564</b> may intersect with three dimensional model <b>552</b> at buildings or terrain.</p>
<p id="p-0060" num="0059">Referring to <figref idref="DRAWINGS">FIGS. 3 and 4</figref>, if a ray intersects with buildings or terrain, panning module <b>390</b> may create a panning sphere at step <b>406</b>. The center origin of the panning sphere may be located at the center of the three dimensional model, and the radius of the panning sphere may be the distance between the center of the model and the hit point determined in step <b>404</b>.</p>
<p id="p-0061" num="0060">Once the panning surface is determined at step <b>406</b>, virtual surface calculator module <b>392</b> calculates a virtual surface at step <b>408</b>. The virtual surface is fitted to the panning surface such that the virtual surface and the panning surface together form a continuous surface. The virtual surface may be a concave surface from the perspective of the virtual camera. In an example, the virtual surface is a mirror image of a portion of the panning surface.</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 6</figref> shows a diagram <b>600</b> illustrating how to calculate a virtual surface according to one embodiment of the present invention. Diagram <b>600</b> illustrates one way to calculate a virtual surface, however there are other ways to calculate a virtual surface as well. As described above, the virtual surface may also be calculated as a concave surface or a mirror image of the Earth. Diagram <b>600</b> shows a virtual camera <b>602</b> viewing a three dimensional model <b>606</b> having a center origin <b>614</b>. Diagram <b>600</b> also shows a panning sphere <b>604</b>.</p>
<p id="p-0063" num="0062">Panning velocity is the rate at which movement of a cursor rotates three dimensional model <b>606</b>. In one embodiment, a virtual surface may be constructed such that the panning velocity reaches a maximum at the intersection between the panning surface and stays constant across the virtual surface. In this way, there is a smooth transition from the panning surface to the virtual surface.</p>
<p id="p-0064" num="0063">To determine the virtual surface, virtual surface calculator module <b>392</b> first determines a point where the panning surface connects to the virtual surface. In diagram <b>600</b>, the panning surface connects to the virtual surface at a point <b>616</b>. Point <b>616</b> may correspond to a parameter defining how tangential the camera has to be to before the virtual surface is used. In an alternative embodiment, point <b>616</b> may be the point at which the panning velocity reaches a maximum tolerable threshold. Using point <b>616</b>, several angles are determined.</p>
<p id="p-0065" num="0064">A first angle, labeled do, is an angle between a screen ray <b>624</b> from the user's mouse click and a line segment <b>618</b> connecting camera <b>602</b> with point <b>616</b>. A line segment <b>622</b> connects center origin <b>614</b> with an intersection <b>626</b> of the screen ray from the user's mouse click and the panning model. A line segment <b>620</b> connects center origin <b>614</b> and point <b>616</b>. A second angle, label d&#x3a6;, is an angle between line segment <b>622</b> and line segment <b>620</b>. The angles d&#x3b8; and d&#x3a6; may be small offsets. The ratio of the first angle d&#x3a6; to the second angle d&#x3b8; (e.g., d&#x3a6;/d&#x3b8;) may be the panning velocity.</p>
<p id="p-0066" num="0065">The ratio between the first angle d&#x3b8; and the second angle d&#x3a6; is then used to determine a virtual surface. The virtual surface may be constructed such that the panning velocity d&#x3a6;/d&#x3b8; stays constant across the entire surface. To do this, a line segment <b>628</b> is determined that connects virtual camera <b>602</b> and center origin <b>614</b>. An angle &#x3b8; is an angle between line segment <b>628</b> and a line segment <b>630</b> connecting virtual camera <b>602</b> and a variable point <b>632</b> on a virtual surface. An angle &#x3a6; is an angle between line segment <b>628</b> and a line segment <b>634</b> connecting center origin <b>614</b> and variable point <b>632</b> on the virtual surface. For each variable point on the virtual surface, angles &#x3b8; and &#x3a6; satisfy the equation:</p>
<p id="p-0067" num="0066">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mfrac>
            <mrow>
              <mi>&#x3a6;</mi>
              <mo>-</mo>
              <msub>
                <mi>&#x3a6;</mi>
                <mn>0</mn>
              </msub>
            </mrow>
            <mrow>
              <mi>d</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>&#x3a6;</mi>
            </mrow>
          </mfrac>
          <mo>=</mo>
          <mfrac>
            <mrow>
              <mi>&#x3b8;</mi>
              <mo>-</mo>
              <msub>
                <mi>&#x3b8;</mi>
                <mn>0</mn>
              </msub>
            </mrow>
            <mrow>
              <mi>d</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>&#x3b8;</mi>
            </mrow>
          </mfrac>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0068" num="0067">where &#x3b8;<sub>0 </sub>is the angle between screen ray <b>624</b> generated using to the user's mouse click and line segment <b>628</b> and &#x3a6;<sub>0 </sub>is an angle between line segment <b>622</b> and line segment <b>628</b>.</p>
<p id="p-0069" num="0068">In other words, the virtual surface is defined such that if a ray were extended from a virtual camera at angle &#x3b8;, the ray would intersect the virtual surface at a spherical coordinate having the angle &#x3a6; and origin <b>614</b>. Rearranging the equation, the spherical coordinate value &#x3a6; can be determined for each angle &#x3b8; from the virtual camera as follows:</p>
<p id="p-0070" num="0069">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>&#x3a6;</mi>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>&#x3b8;</mi>
                <mo>-</mo>
                <msub>
                  <mi>&#x3b8;</mi>
                  <mn>0</mn>
                </msub>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mo>&#x2062;</mo>
            <mfrac>
              <mrow>
                <mo>&#x2146;</mo>
                <mi>&#x3a6;</mi>
              </mrow>
              <mrow>
                <mo>&#x2146;</mo>
                <mi>&#x3b8;</mi>
              </mrow>
            </mfrac>
          </mrow>
          <mo>+</mo>
          <mrow>
            <msub>
              <mi>&#x3a6;</mi>
              <mn>0</mn>
            </msub>
            <mo>.</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0071" num="0070">Using this equation, virtual surface calculator module <b>392</b> of <figref idref="DRAWINGS">FIG. 3</figref> may construct a virtual surface.</p>
<p id="p-0072" num="0071">In embodiments, a virtual surface may be calculated using other equations that do not substantially differ from equations (1) and (2). For example, coefficients and offsets may be incorporated into the equations.</p>
<p id="p-0073" num="0072">Referring to <figref idref="DRAWINGS">FIGS. 3 and 4</figref>, GUI <b>312</b> may receive a user event. If the user event is a move event, panning module <b>390</b> intersects a screen ray with both the panning sphere and virtual surface at step <b>410</b>. Step <b>410</b> is illustrated in <figref idref="DRAWINGS">FIGS. 7A-B</figref>.</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 7A</figref> shows a diagram <b>700</b> illustrating extending a screen ray to intersect with a virtual surface. Diagram <b>700</b> shows a panning sphere <b>704</b> and a virtual surface <b>702</b> constructed as described with respect to steps <b>406</b> and <b>408</b>. Diagram <b>700</b> also shows a virtual camera with focal point <b>506</b>, focal length <b>558</b> and viewpoint <b>560</b>. The user may select a new point by, for example, moving a mouse. The point selected by the user corresponds to a point <b>706</b> in viewpoint <b>560</b>. Panning module <b>390</b> extends a ray <b>708</b> from focal point <b>506</b> through point <b>706</b>. Ray <b>708</b> intersects with virtual surface <b>702</b> at a point <b>710</b>.</p>
<p id="p-0075" num="0074">Ray <b>708</b> may also intersect with panning sphere <b>704</b> as shown in <figref idref="DRAWINGS">FIG. 7B</figref>. <figref idref="DRAWINGS">FIG. 7B</figref> includes a diagram <b>750</b>. Diagram <b>750</b> shows ray <b>708</b> intersecting with panning sphere <b>704</b> at a point <b>764</b>.</p>
<p id="p-0076" num="0075">Again referring to <figref idref="DRAWINGS">FIGS. 3 and 4</figref>, once intersection points <b>764</b> and <b>710</b> are determined, panning module <b>390</b> may select one of the points at step <b>412</b>. Panning module <b>390</b> may select the closest of the points to the virtual camera. In diagram <b>750</b>, point <b>710</b> is closer to focal point <b>506</b> than point <b>764</b>. As point <b>710</b> on virtual surface <b>702</b> is selected, panning module <b>390</b> may project point <b>710</b> onto panning sphere <b>704</b> at step <b>414</b>. Panning module <b>390</b> may project point <b>710</b> onto panning sphere <b>704</b> by determining a line segment <b>766</b> between point <b>710</b> and a center origin <b>758</b> of the three dimensional model. Then, panning module <b>390</b> determines an intersection between line segment <b>766</b> and panning sphere <b>704</b> at a point <b>762</b>.</p>
<p id="p-0077" num="0076">In an embodiment, point <b>710</b> intersecting screen ray <b>708</b> and virtual surface <b>702</b> may be determined using the sine rule for a triangle. To apply the sine rule, several parameters must be determined. First, a line segment <b>768</b> connecting virtual camera <b>506</b> and center origin <b>758</b> is determined. Second, an angle &#x3b8; between line segment <b>768</b> and screen ray <b>708</b> is determined. Intersection point <b>710</b> intersection satisfies the equation:</p>
<p id="p-0078" num="0077">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mfrac>
            <mrow>
              <mi>sin</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>&#x3a6;</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mi>A</mi>
          </mfrac>
          <mo>=</mo>
          <mfrac>
            <mrow>
              <mi>sin</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>&#x3b8;</mi>
                  <mo>+</mo>
                  <mi>&#x3a6;</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mi>B</mi>
          </mfrac>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0079" num="0078">where B is the length of line segment <b>768</b>, A is the distance between virtual camera <b>506</b> and intersection point <b>710</b>, and &#x3a6; is an angle between line segment <b>768</b> and a line segment connecting virtual camera <b>506</b> and intersection point <b>710</b>. Because the distance A is less then a distance between virtual camera <b>506</b> and intersection point <b>764</b>, the rotation occurs based on point <b>710</b> on virtual surface <b>702</b>. Otherwise, the model would rotate based on intersection point <b>764</b> with the panning sphere.</p>
<p id="p-0080" num="0079">After the intersection point is determined at step <b>416</b>, rotator module <b>394</b> changes a location of three dimensional model according to the points determined in steps <b>404</b> and <b>412</b>. In an embodiment, rotator module <b>394</b> changes the location of the model by rotating model according to the points. In <figref idref="DRAWINGS">FIG. 7B</figref>, diagram <b>750</b> shows a ray <b>752</b> and a hit point <b>754</b> determined in response to the mouse click event in step <b>404</b>. Diagram <b>750</b> also shows point <b>710</b> determined in response to the move event as described above. A line segment <b>756</b> connects hit point <b>754</b> with center origin <b>758</b>, and a line segment <b>766</b> connects center origin <b>758</b> with point <b>710</b>. Rotator module <b>394</b> rotates the three dimensional model by an angle <b>760</b> between line segment <b>756</b> and line segment <b>766</b>.</p>
<p id="p-0081" num="0080">Referring back to <figref idref="DRAWINGS">FIG. 3</figref>, various components of geographic information system client <b>300</b> are described in greater detail.</p>
<p id="p-0082" num="0081">As described earlier view specification <b>332</b> defines the virtual camera's viewable volume within a three dimensional space, known as a fiustum, and the position and orientation of the frustum with respect, for example, to a three dimensional map. In accordance with one embodiment of the present invention, view specification <b>332</b> specifies three main parameter sets for the virtual camera: the camera tripod, the camera lens, and the camera focus capability. The camera tripod parameter set specifies the following: the virtual camera position: X, Y, Z (three coordinates); which way the virtual camera is oriented relative to a default orientation, such as heading angle (e.g., north?, south?, in-between?); pitch (e.g., level?, down?, up?, in-between?); and yaw/roll (e.g., level?, clockwise?, anti-clockwise?, in-between?). The lens parameter set specifies the following: horizontal field of view (e.g., telephoto?, normal human eye&#x2014;about 55 degrees?, or wide-angle?); and vertical field of view (e.g., telephoto?, normal human eye&#x2014;about 55 degrees?, or wide-angle?). The focus parameter set specifies the following: distance to the near-clip plane (e.g., how close to the &#x201c;lens&#x201d; can the virtual camera see, where objects closer are not drawn); and distance to the far-clip plane (e.g., how far from the lens can the virtual camera see, where objects further are not drawn).</p>
<p id="p-0083" num="0082">In one example operation, and with the above camera parameters in mind, assume the user presses the left-arrow (or right-arrow) key. This would signal motion model <b>318</b> that the view should move left (or right). Motion model <b>318</b> implements such a ground level &#x201c;pan the camera&#x201d; type of control by adding (or subtracting) a small value (e.g., 1 degree per arrow key press) to the heading angle. Similarly, to move the virtual camera forward, the motion model <b>318</b> would change the X, Y, Z coordinates of the virtual camera's position by first computing a unit-length vector along the view direction (HPR) and adding the X, Y, Z sub-components of this vector to the camera's position after scaling each sub-component by the desired speed of motion. In these and similar ways, motion model <b>318</b> adjusts view specification <b>332</b> by incrementally updating XYZ and HPR to define the &#x201c;just after a move&#x201d; new view position. In this way, motion model <b>318</b> is responsible for navigating the virtual camera through the three dimensional environment.</p>
<p id="p-0084" num="0083">Renderer module <b>350</b> has cycles corresponding to the display device's video refresh rate (e.g., 60 cycles per second). In one particular embodiment, renderer module <b>350</b> performs a cycle of (i) waking up, (ii) reading the view specification <b>332</b> that has been placed by motion model <b>318</b> in a data structure accessed by a renderer, (iii) traversing quad node tree <b>334</b> in local memory <b>330</b>, and (iv) drawing drawable data contained in the quad nodes residing in quad node tree <b>334</b>. The drawable data may be associated with a bounding box (e.g., a volume that contains the data or other such identifier). If present, the bounding box is inspected to see if the drawable data is potentially visible within view specification <b>332</b>. Potentially visible data is drawn, while data known not to be visible is ignored. Thus, the renderer uses view specification <b>332</b> to determine whether the drawable payload of a quad node resident in quad node tree <b>334</b> is not to be drawn, as will now be more fully explained.</p>
<p id="p-0085" num="0084">Initially, and in accordance with one embodiment of the present invention, there is no data within quad node tree <b>334</b> to draw, and renderer module <b>350</b> draws a star field by default (or other suitable default display imagery). Quad node tree <b>334</b> is the data source for the drawing that renderer <b>350</b> does except for this star field. Renderer module <b>350</b> traverses quad node tree <b>334</b> by attempting to access each quad node resident in quad node tree <b>334</b>. Each quad node is a data structure that has up to four references and an optional payload of data. If a quad node's payload is drawable data, renderer module <b>350</b> will compare the bounding box of the payload (if any) against view specification <b>332</b>, drawing it so long as the drawable data is not wholly outside the frustum and is not considered inappropriate to draw based on other factors. These other factors may include, for example, distance from the camera, tilt, or other such considerations. If the payload is not wholly outside the frustum and is not considered inappropriate to draw, renderer module <b>350</b> also attempts to access each of the up to four references in the quad node. If a reference is to another quad node in local memory (e.g., memory <b>330</b> or other local memory), renderer module <b>350</b> will attempt to access any drawable data in that other quad node and also potentially attempt to access any of the up to four references in that other quad node. The renderer module's attempts to access each of the up to four references of a quad node are detected by the quad node itself.</p>
<p id="p-0086" num="0085">As previously explained, a quad node is a data structure that may have a payload of data and up to four references to other files, each of which in turn may be a quad node. The files referenced by a quad node are referred to herein as the children of that quad node, and the referencing quad node is referred to herein as the parent. In some cases, a file contains not only the referenced child, but descendants of that child as well. These aggregates are known as cache nodes and may include several quad nodes. Such aggregation takes place in the course of database construction. In some instances, the payload of data is empty. Each of the references to other files comprises, for instance, a filename and a corresponding address in local memory for that file, if any. Initially, the referenced files are all stored on one or more remote servers (e.g., on server(s) of the GIS), and there is no drawable data present on the user's computer.</p>
<p id="p-0087" num="0086">Quad nodes and cache nodes have built-in accessor functions. As previously explained, the renderer module's attempts to access each of the up to four references of a quad node are detected by the quad node itself. Upon the renderer module's attempt to access a child quad node that has a filename but no corresponding address, the parent quad node places (e.g., by operation of its accessor function) that filename onto a cache node retrieval list <b>345</b>. The cache node retrieval list comprises a list of information identifying cache nodes to be downloaded from a GIS server. If a child of a quad node has a local address that is not null, the renderer module <b>350</b> uses that address in local memory <b>330</b> to access the child quad node.</p>
<p id="p-0088" num="0087">Quad nodes are configured so that those with drawable payloads may include within their payload a bounding box or other location identifier. Renderer module <b>350</b> performs a view frustum cull, which compares the bounding box/location identifier of the quad node payload (if present) with view specification <b>332</b>. If the bounding box is completely disjoint from view specification <b>332</b> (e.g., none of the drawable data is within the frustum), the payload of drawable data will not be drawn, even though it was already retrieved from a GIS server and stored on the user's computer. Otherwise, the drawable data is drawn.</p>
<p id="p-0089" num="0088">The view frustum cull determines whether or not the bounding box (if any) of the quad node payload is completely disjoint from view specification <b>332</b> before renderer module <b>350</b> traverses the children of that quad node. If the bounding box of the quad node is completely disjoint from view specification <b>332</b>, renderer module <b>350</b> does not attempt to access the children of that quad node. A child quad node never extends beyond the bounding box of its parent quad node. Thus, once the view frustum cull determines that a parent quad node is completely disjoint from the view specification, it can be assumed that all progeny of that quad node are also completely disjoint from view specification <b>332</b>.</p>
<p id="p-0090" num="0089">Quad node and cache node payloads may contain data of various types. For example, cache node payloads can contain satellite images, text labels, political boundaries, 3 dimensional vertices along with point, line or polygon connectivity for rendering roads, and other types of data. The amount of data in any quad node payload is limited to a maximum value. However, in some cases, the amount of data needed to describe an area at a particular resolution exceeds this maximum value. In those cases, such as processing vector data, some of the data is contained in the parent payload and the rest of the data at the same resolution is contained in the payloads of the children (and possibly even within the children's descendents). There also may be cases in which children may contain data of either higher resolution or the same resolution as their parent. For example, a parent node might have two children of the same resolution as that parent, and two additional children of different resolutions (e.g., higher) than that parent.</p>
<p id="p-0091" num="0090">The cache node manager <b>340</b> thread, and each of one or more network loader <b>365</b> threads, operate asynchronously from renderer module <b>350</b> and user interaction module <b>310</b>. Renderer module <b>350</b> and user interaction module <b>310</b> can also operate asynchronously from each other. In some embodiments, as many as eight network loader <b>365</b> threads are independently executed, each operating asynchronously from renderer module <b>350</b> and user interaction module <b>310</b>. The cache node manager <b>340</b> thread builds quad node tree <b>334</b> in local memory <b>330</b> by populating it with quad nodes retrieved from GIS server(s). Quad node tree <b>334</b> begins with a root node when the client system is launched or otherwise started. The root node contains a filename (but no corresponding address) and no data payload. As previously described, this root node uses a built-in accessor function to self-report to the cache node retrieval list <b>345</b> after it has been traversed by renderer module <b>350</b> for the first time.</p>
<p id="p-0092" num="0091">In each network loader <b>365</b> thread, a network loader traverses the cache node retrieval list <b>345</b> (which in the embodiment shown in <figref idref="DRAWINGS">FIG. 3</figref> is included in cache node manager <b>340</b>, but can also be located in other places, such as the local memory <b>330</b> or other storage facility) and requests the next cache node from the GIS server(s) using the cache node's filename. The network loader only requests files that appear on the cache node retrieval list. Cache node manager <b>340</b> allocates space in local memory <b>330</b> (or other suitable storage facility) for the returned file, which is organized into one or more new quad nodes that are descendents of the parent quad node. Cache node manager <b>340</b> can also decrypt or decompress the data file returned from the GIS server(s), if necessary (e.g., to complement any encryption or compression on the server-side). Cache node manager <b>340</b> updates the parent quad node in quad node tree <b>334</b> with the address corresponding to the local memory <b>330</b> address for each newly constructed child quad node.</p>
<p id="p-0093" num="0092">Separately and asynchronously in renderer module <b>350</b>, upon its next traversal of quad node tree <b>334</b> and traversal of the updated parent quad node, renderer module <b>350</b> finds the address in local memory corresponding to the child quad node and can access the child quad node. The renderer's traversal of the child quad node progresses according to the same steps that are followed for the parent quad node. This continues through quad node tree <b>334</b> until a node is reached that is completely disjoint from view specification <b>332</b> or is considered inappropriate to draw based on other factors as previously explained.</p>
<p id="p-0094" num="0093">In this particular embodiment, note that there is no communication between the cache node manager thread and renderer module <b>350</b> other than the renderer module's reading of the quad nodes written or otherwise provided by the cache node manager thread. Further note that, in this particular embodiment, cache nodes and thereby quad nodes continue to be downloaded until the children returned contain only payloads that are completely disjoint from view specification <b>332</b> or are otherwise unsuitable for drawing, as previously explained. Display interface <b>380</b> (e.g., a display interface card) is configured to allow data from a mapping module to be sent to a display associated with the user's computer, so that the user can view the data. Display interface <b>380</b> can be implemented with conventional technology.</p>
<p id="p-0095" num="0094">The summary and abstract sections may set forth one or more but not all exemplary embodiments of the present invention as contemplated by the inventor(s), and thus, are not intended to limit the present invention and the appended claims in any way.</p>
<p id="p-0096" num="0095">The present invention has been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed.</p>
<p id="p-0097" num="0096">The foregoing description of the specific embodiments will so fully reveal the general nature of the invention that others can, by applying knowledge within the skill of the art, readily modify and/or adapt for various applications such specific embodiments, without undue experimentation, without departing from the general concept of the present invention. Therefore, such adaptations and modifications are intended to be within the meaning and range of equivalents of the disclosed embodiments, based on the teaching and guidance presented herein. It is to be understood that the phraseology or terminology herein is for the purpose of description and not of limitation, such that the terminology or phraseology of the present specification is to be interpreted by the skilled artisan in light of the teachings and guidance.</p>
<p id="p-0098" num="0097">The breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08624926-20140107-M00001.NB">
<img id="EMI-M00001" he="6.35mm" wi="76.20mm" file="US08624926-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08624926-20140107-M00002.NB">
<img id="EMI-M00002" he="6.35mm" wi="76.20mm" file="US08624926-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08624926-20140107-M00003.NB">
<img id="EMI-M00003" he="6.35mm" wi="76.20mm" file="US08624926-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08624926-20140107-M00004.NB">
<img id="EMI-M00004" he="6.01mm" wi="76.20mm" file="US08624926-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08624926-20140107-M00005.NB">
<img id="EMI-M00005" he="6.01mm" wi="76.20mm" file="US08624926-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method for panning a virtual camera in a three dimensional environment, comprising:
<claim-text>determining a first point located on a panning sphere in a three dimensional model in the three dimensional environment;</claim-text>
<claim-text>determining a virtual surface according to the first point, the three dimensional model, and a position of the virtual camera in the three dimensional environment;</claim-text>
<claim-text>determining a second point located on the panning sphere;</claim-text>
<claim-text>determining a third point located on the virtual surface, wherein the second point on the panning sphere is associated with the third point on the virtual surface, and wherein the distance between the virtual camera and the third point is less than the distance between the virtual camera and the second point; and</claim-text>
<claim-text>changing a location of the three dimensional model by rotating the three dimensional model according to the third point, wherein the rotation of the three dimensional model according to the third point is less than a rotation of the three dimensional model according to the second point, and</claim-text>
<claim-text>wherein the determining of the virtual surface comprises fitting the virtual surface to the panning sphere so that the virtual surface is a concave surface from a perspective of the virtual camera.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining of the virtual surface further comprises:
<claim-text>configuring the panning sphere, wherein a center of the panning sphere is located at the center of the three dimensional model, and wherein the panning sphere has a radius equal to a distance between the center of the three dimensional model and the first point; and</claim-text>
<claim-text>fitting the virtual surface and the panning surface together form a continuous surface that fits smoothly with the panning sphere.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the three dimensional model includes a model of the Earth.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining of the virtual surface further comprises that the virtual surface is a mirror image of a portion of the panning sphere.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining of a first point comprises:
<claim-text>extending a first ray from a focal point of the virtual camera through a fourth point on a viewport of the virtual camera, the fourth point corresponding to a first user selection; and</claim-text>
<claim-text>determining the first point at an intersection between the first ray and the three dimensional model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the determining of the virtual surface comprises,
<claim-text>determining a panning sphere, wherein a center of the panning sphere is located at a center of the three dimensional model and wherein the panning sphere has a radius equal to a distance between the center of the three dimensional model and the first point; and</claim-text>
<claim-text>determining a virtual surface connected to the panning sphere at a connecting point, wherein the virtual surface substantially satisfies an equation</claim-text>
</claim-text>
<claim-text>
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mrow>
  <mrow>
    <mfrac>
      <mrow>
        <mi>&#x3a6;</mi>
        <mo>-</mo>
        <msub>
          <mi>&#x3a6;</mi>
          <mn>0</mn>
        </msub>
      </mrow>
      <mrow>
        <mi>d</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>&#x3a6;</mi>
      </mrow>
    </mfrac>
    <mo>=</mo>
    <mfrac>
      <mrow>
        <mi>&#x3b8;</mi>
        <mo>-</mo>
        <msub>
          <mi>&#x3b8;</mi>
          <mn>0</mn>
        </msub>
      </mrow>
      <mrow>
        <mi>d</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>&#x3b8;</mi>
      </mrow>
    </mfrac>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<claim-text>wherein &#x3b8;<sub>0 </sub>is an angle between the first ray and a line segment connecting the virtual camera and a center of the three dimensional model,</claim-text>
<claim-text>wherein &#x3a6;<sub>0 </sub>is an angle between the first line segment connecting the virtual camera and the center of the three dimensional model and a second line segment connecting the center of the three dimensional model and the first point,</claim-text>
<claim-text>wherein d&#x3a6; is an angle between the second line segment connecting the center of the three dimensional model and the first point and a third line segment connecting the center of the three dimensional model and the connecting point,</claim-text>
<claim-text>wherein d&#x3b8; is an angle between the first ray and a third line segment connecting the virtual camera and the connecting point,</claim-text>
<claim-text>wherein &#x3b8; is an angle between the second line segment connecting the virtual camera and the center of the three dimensional model and a fourth line segment connecting the virtual camera and a variable point on the virtual surface, and</claim-text>
<claim-text>wherein &#x3a6; is an angle between the second line segment connecting the virtual camera and the center of the three dimensional model and a fifth line segment connecting the center of the three dimensional model and the variable point on the virtual surface.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining a third point on the virtual surface comprises:
<claim-text>extending a second ray from the focal point of the virtual camera through the second point on the panning sphere; and</claim-text>
<claim-text>determining the third point at an intersection between the second ray and the virtual surface.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A system for panning a virtual camera in a three dimensional environment, comprising:
<claim-text>a panning module, utilizing a computing device, configured to determine a first point and a second point located on a panning sphere in a three dimensional model in the three dimensional environment, the panning module further configured to determine a third point on a virtual surface, wherein the second point is associated with the third point,</claim-text>
<claim-text>a virtual surface calculator module, utilizing the computing device, configured to determine the virtual surface according to the first point, the three dimensional model, and a position of the virtual camera in the three dimensional environment; and</claim-text>
<claim-text>a rotator module, utilizing the computing device, configured to change a location of the three dimensional model, by rotating, according to the third point,</claim-text>
<claim-text>wherein the distance between the virtual camera and the third point is less than the distance between the virtual camera and the second point,</claim-text>
<claim-text>wherein the rotation of the three dimensional model according to the third point is less than a rotation of the three dimensional model according to the second point, and</claim-text>
<claim-text>wherein the determining of the virtual surface comprises fitting the virtual surface to the panning sphere so that the virtual surface is a concave surface from a perspective of the virtual camera.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein a center of the panning sphere is located at the center of the three dimensional model, and wherein the panning sphere has a radius equal to a distance between the center of the three dimensional model and the first point, and
<claim-text>wherein the panning module is configured to fitting the virtual surface to the panning surface such that the virtual surface and the panning surface together form a continuous surface that fits smoothly with the panning sphere.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the three dimensional model includes a model of the Earth.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the virtual surface is a mirror image of a portion of the panning sphere.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the panning module is configured to extend a first ray from a focal point of the virtual camera through a fourth point on a viewport of the virtual camera, the fourth point corresponding to a first user selection, and
<claim-text>wherein the panning module is configured to determine the first point at an intersection between the first ray and the three dimensional model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the panning module is configured to determine a panning sphere, wherein a center of the panning sphere is located at a center of the three dimensional model and wherein the panning sphere has a radius equal to a distance between the center of the three dimensional model and the first point, and
<claim-text>wherein the virtual surface calculator module is configured to determine a virtual surface connected to the panning sphere at a connecting point, wherein the virtual surface substantially satisfies an equation</claim-text>
</claim-text>
<claim-text>
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mrow>
  <mrow>
    <mfrac>
      <mrow>
        <mi>&#x3a6;</mi>
        <mo>-</mo>
        <msub>
          <mi>&#x3a6;</mi>
          <mn>0</mn>
        </msub>
      </mrow>
      <mrow>
        <mi>d</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>&#x3a6;</mi>
      </mrow>
    </mfrac>
    <mo>=</mo>
    <mfrac>
      <mrow>
        <mi>&#x3b8;</mi>
        <mo>-</mo>
        <msub>
          <mi>&#x3b8;</mi>
          <mn>0</mn>
        </msub>
      </mrow>
      <mrow>
        <mi>d</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>&#x3b8;</mi>
      </mrow>
    </mfrac>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
<claim-text>wherein &#x3b8;<sub>0 </sub>is an angle between the first ray and a line segment connecting the virtual camera and a center of the three dimensional model,</claim-text>
<claim-text>wherein &#x3a6;<sub>0 </sub>is an angle between the first line segment connecting the virtual camera and the center of the three dimensional model and a second line segment connecting the center of the three dimensional model and the first point,</claim-text>
<claim-text>wherein d&#x3a6; is an angle between the second line segment connecting the center of the three dimensional model and the first point and a third line segment connecting the center of the three dimensional model and the connecting point,</claim-text>
<claim-text>wherein d&#x3b8; is an angle between the first ray and a third line segment connecting the virtual camera and the connecting point,</claim-text>
<claim-text>wherein &#x3b8; is an angle between the second line segment connecting the virtual camera and the center of the three dimensional model and a fourth line segment connecting the virtual camera and a variable point on the virtual surface, and</claim-text>
<claim-text>wherein &#x3a6; is an angle between the second line segment connecting the virtual camera and the center of the three dimensional model and a fifth line segment connecting the center of the three dimensional model and the variable point on the virtual surface.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the panning module is configured to extend a second ray from the focal point of the virtual camera through the second point on the panning sphere; and
<claim-text>determine the third point at an intersection between the second ray and the virtual surface.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A computer-implemented method for panning a virtual camera in a three dimensional environment including a three dimensional model of the Earth, comprising:
<claim-text>displaying in a display area the three dimensional model from a perspective of the virtual camera;</claim-text>
<claim-text>enabling a user to select a first point on a panning sphere in the three dimensional model within the display area with a cursor;</claim-text>
<claim-text>determining a screen ray between the virtual camera and the first point;</claim-text>
<claim-text>determining an intersection point between the screen ray and a virtual surface; and</claim-text>
<claim-text>rotating the three dimensional model around an axis through a rotation point of the three dimensional modal until the virtual camera is aligned with the intersection point on the virtual surface and the rotation point of the three dimensional model,</claim-text>
<claim-text>wherein the rotation of the three dimensional model according to the intersection point is less than a rotation of the three dimensional model according to the first point and</claim-text>
<claim-text>wherein the determining, of the virtual surface comprises fitting the virtual surface to the panning sphere so that the virtual surface is a concave surface from a perspective of the virtual camera. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
