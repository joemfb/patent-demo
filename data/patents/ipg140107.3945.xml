<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625013-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625013</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13215002</doc-number>
<date>20110822</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>294</us-term-extension>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>228</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>235</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>3</main-group>
<subgroup>14</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>335</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>03</class>
<subclass>B</subclass>
<main-group>7</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>348296</main-classification>
<further-classification>3482221</further-classification>
<further-classification>3482291</further-classification>
<further-classification>348362</further-classification>
</classification-national>
<invention-title id="d2e55">Multi-exposure imaging</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4647975</doc-number>
<kind>A</kind>
<name>Alston et al.</name>
<date>19870300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5247366</doc-number>
<kind>A</kind>
<name>Ginosar et al.</name>
<date>19930900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5420635</doc-number>
<kind>A</kind>
<name>Konishi et al.</name>
<date>19950500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5517242</doc-number>
<kind>A</kind>
<name>Yamada et al.</name>
<date>19960500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5929908</doc-number>
<kind>A</kind>
<name>Takahashi et al.</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6204881</doc-number>
<kind>B1</kind>
<name>Ikeda et al.</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6278490</doc-number>
<kind>B1</kind>
<name>Fukuda et al.</name>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6418245</doc-number>
<kind>B1</kind>
<name>Udagawa</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6496226</doc-number>
<kind>B2</kind>
<name>Takahashi et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6593970</doc-number>
<kind>B1</kind>
<name>Serizawa et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348362</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6670993</doc-number>
<kind>B1</kind>
<name>Yamamoto et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6677992</doc-number>
<kind>B1</kind>
<name>Matsumoto et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6707492</doc-number>
<kind>B1</kind>
<name>Itani</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6720993</doc-number>
<kind>B1</kind>
<name>Hwang et al.</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6847398</doc-number>
<kind>B1</kind>
<name>Fossum</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6952234</doc-number>
<kind>B2</kind>
<name>Hatano</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6985185</doc-number>
<kind>B1</kind>
<name>Crawford et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7061524</doc-number>
<kind>B2</kind>
<name>Liu et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7106913</doc-number>
<kind>B2</kind>
<name>Castorina et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7133069</doc-number>
<kind>B2</kind>
<name>Wallach et al.</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7193652</doc-number>
<kind>B2</kind>
<name>Hori et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7239757</doc-number>
<kind>B2</kind>
<name>Kang et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>8068140</doc-number>
<kind>B2</kind>
<name>Helbing</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>8089530</doc-number>
<kind>B2</kind>
<name>Mabuchi</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>8159579</doc-number>
<kind>B2</kind>
<name>Jannard et al.</name>
<date>20120400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348296</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2005/0057674</doc-number>
<kind>A1</kind>
<name>Krymski et al.</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2006/0109373</doc-number>
<kind>A1</kind>
<name>Kurane</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2008/0094482</doc-number>
<kind>A1</kind>
<name>Yoshimura</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2008/0211950</doc-number>
<kind>A1</kind>
<name>Ono et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2009/0153710</doc-number>
<kind>A1</kind>
<name>John</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2010/0002112</doc-number>
<kind>A1</kind>
<name>Tay</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2010/0165122</doc-number>
<kind>A1</kind>
<name>Castorina et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2010/0187401</doc-number>
<kind>A1</kind>
<name>Kawahito</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2011/0109754</doc-number>
<kind>A1</kind>
<name>Matsunaga et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>International Search Report and Written Opinion for Application No. PCT/US2011/048670 mailed Nov. 7, 2011.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Unger et al, <i>High Dynamic Range Video for Photometric Measurement of Illumination</i>. Link&#xf6;ping University, Sweden, The URL of the website on which the document is available indicates that the document may have been available sometime in 2007, http://staffwww.itn.liu.se/&#x2dc;jonun/web/papers/2007-El/UngerEl07.pdf.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Wang et al, <i>High Dynamic Range Video Using Split Aperture Camera</i>. University of Illinois, IL, USA, the Internet Archive: Wayback Machine website indicates that the document was available on-line on Apr. 24, 2009, http://web.archive.org/web/20090424070953/http://vision.ai.uiuc.edu/&#x2dc;wandhc/papers/OMNIVIS05<sub>&#x2014;</sub>Hongcheng<sub>&#x2014;</sub>3192.pdf.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>International Search Report and Written Opinion for Application No. PCT/US2011/048670 dated Feb. 26, 2013.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>31</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>3482161</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482221</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482291</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482301</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34833312</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348362</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>11</number-of-drawing-sheets>
<number-of-figures>15</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61376172</doc-number>
<date>20100823</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61473711</doc-number>
<date>20110408</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120069213</doc-number>
<kind>A1</kind>
<date>20120322</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Jannard</last-name>
<first-name>James H.</first-name>
<address>
<city>Las Vegas</city>
<state>NV</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Nattress</last-name>
<first-name>Graeme</first-name>
<address>
<city>Acton</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Mathur</last-name>
<first-name>Bimal</first-name>
<address>
<city>Thousand Oaks</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Mathur</last-name>
<first-name>Uday</first-name>
<address>
<city>Los Angeles</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>DaSilva</last-name>
<first-name>Deanan</first-name>
<address>
<city>Hollywood</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="006" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Land</last-name>
<first-name>Peter Jarred</first-name>
<address>
<city>Los Angeles</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Jannard</last-name>
<first-name>James H.</first-name>
<address>
<city>Las Vegas</city>
<state>NV</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Nattress</last-name>
<first-name>Graeme</first-name>
<address>
<city>Acton</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Mathur</last-name>
<first-name>Bimal</first-name>
<address>
<city>Thousand Oaks</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Mathur</last-name>
<first-name>Uday</first-name>
<address>
<city>Los Angeles</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>DaSilva</last-name>
<first-name>Deanan</first-name>
<address>
<city>Hollywood</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="006" designation="us-only">
<addressbook>
<last-name>Land</last-name>
<first-name>Peter Jarred</first-name>
<address>
<city>Los Angeles</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Knobbe Martens Olson &#x26; Bear LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Red.com, Inc.</orgname>
<role>02</role>
<address>
<city>Irvine</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Pasiewicz</last-name>
<first-name>Daniel M</first-name>
<department>2661</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Certain cameras and systems described herein produce enhanced dynamic range still or video images. The images can also have controlled or reduced motion artifacts. Moreover, the cameras and systems in some cases allow the dynamic range and/or motion artifacts to be tuned to achieve a desired cinematic effect.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="153.08mm" wi="257.05mm" file="US08625013-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="239.61mm" wi="188.04mm" orientation="landscape" file="US08625013-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="189.82mm" wi="175.09mm" file="US08625013-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="261.62mm" wi="175.85mm" orientation="landscape" file="US08625013-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="258.49mm" wi="181.78mm" orientation="landscape" file="US08625013-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="258.83mm" wi="178.99mm" file="US08625013-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="183.22mm" wi="145.71mm" file="US08625013-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="245.19mm" wi="180.76mm" file="US08625013-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="207.69mm" wi="176.53mm" file="US08625013-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="230.80mm" wi="183.90mm" file="US08625013-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="217.51mm" wi="181.44mm" file="US08625013-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="189.15mm" wi="139.36mm" file="US08625013-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims the benefit of priority from U.S. Provisional Application Nos. 61/376,172 filed Aug. 23, 2010, and 61/473,711 filed Apr. 8, 2011, both of which are incorporated in their entirety by reference herein.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">The disclosure herein relates to devices for creating improved dynamic range images, such as from images of still or moving scenes detected with a digital camera.</p>
<p id="p-0004" num="0003">The dynamic range of a digital image can be characterized as the ratio in intensity between the brightest and the darkest measured values in the image. Thus, an image having a wide dynamic range represents more accurately the wide range of intensity levels found in real scenes.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">Conventional digital cameras and techniques typically produce relatively low dynamic range images and provide limited flexibility in controlling dynamic range. Dynamic range can be limited by factors including the level of brightness detectable by each sensor element (e.g., the bit-width of each pixel) and the level of exposure. For example, a captured image may accurately represent the intensity of relatively dark regions of the scene, but not relatively bright regions of the same scene, or vice versa.</p>
<p id="p-0006" num="0005">Different techniques can be used to enhance the dynamic range of images. However, particularly for moving images, existing techniques are generally overly complex, provide sub-optimal image quality, or allow limited creative flexibility. For example, some techniques produce relatively large, un-controlled motion artifacts (e.g., blurring or flickering). For these and other reasons, certain cameras and systems described herein produce enhanced dynamic range images also having controlled or reduced motion artifacts. Moreover, according to some aspects, the system provides an interface allowing users to tune the dynamic range and/or motion artifacts to achieve a desired cinematic effect.</p>
<p id="p-0007" num="0006">According to certain aspects, a method is provided of obtaining image data using a digital imaging sensor comprising a picture element array of rows and columns. The method can include using a plurality of picture elements in the picture element array to capture light corresponding to a first image at a first exposure level and using the plurality of picture elements in the picture element array to capture light corresponding to a second image at a second exposure level different than the first exposure level. The method can further include converting the light captured by the plurality of picture elements for the first image to digital measurements and converting the light captured by the plurality of picture elements for the second image to digital measurements. In some embodiments, the step of using the plurality of picture elements in the picture element array to capture light corresponding to the second image at the second exposure level begins prior to the completion of the step of converting the light captured by the plurality of picture elements for the first image to digital measurements. Moreover, in certain embodiments, the step of converting the light captured by the plurality of picture elements for the first image to digital measurements substantially completes prior to the beginning of the step of converting light captured by the plurality of picture elements for the second image to digital measurements.</p>
<p id="p-0008" num="0007">According to additional aspects, an imaging system is provided, comprising a plurality of picture elements arranged in an array. The imaging system can further comprise control circuitry configured to capture light corresponding to a first image at a first exposure level with the plurality of picture elements in the picture element array and capture light corresponding to a second image at a second exposure level different than the first exposure level with the plurality of picture elements in the picture element array. The control circuitry can further be configured to convert the light captured by the plurality of picture elements for the first image to digital measurements and convert light captured by the plurality of picture elements for the second image to digital measurements. According to some embodiments, the control circuitry begins the capturing of the light corresponding to the second image at the second exposure level prior to the completion of the converting of the light captured for the first image to digital measurements. The control circuitry may additionally substantially complete the converting of the light for the first image to digital measurements prior to the beginning of the converting of the light for the second image to digital measurements.</p>
<p id="p-0009" num="0008">According to further aspects, a method is provided of processing image data. The method can include receiving first image data corresponding to a plurality of picture elements in a picture element array used to capture light corresponding to a first image at a first exposure level. The method may further include receiving second image data corresponding to the plurality of picture elements in the picture element array used to capture light corresponding to a second image at a second exposure level. The method can additionally include selectively combining the first image data and the second image data to create combined image data corresponding to a combined image having a wider dynamic range than either of the first image or the second image. In some cases, the using the plurality of picture elements in the picture element array to capture light corresponding to the second image at the second exposure level began prior to the completion of converting the light captured by the plurality of picture elements for the first image to digital measurements. Moreover, the converting the light captured by the plurality of picture elements for the first image to digital measurements substantially completed prior to the beginning of the step of converting light captured by the plurality of picture elements for the second image to digital measurements.</p>
<p id="p-0010" num="0009">In yet further embodiments, a method is provided of obtaining image data using a digital imaging sensor. The method can include capturing a first image with the sensor at a first exposure level and capturing a second image with the sensor at a second exposure level different from the first exposure level. The method may additionally include outputting the first image and outputting the second image. The capturing of the first image in some cases begins before said capturing of the second image begins. Additionally, the capturing of the first image and said capturing of the second image overlap in time in some cases. Further, the outputting of the first image and said outputting of the second image do not overlap in time in some embodiments.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an example imaging system according to embodiments described herein.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating an example sensor architecture capable of generating multiple, closely-timed exposures according to some embodiments.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. 3A-3D</figref> are timing diagrams showing operation of example sensors recording multiple, closely-timed exposures for multiple frames of recorded video in accordance with certain embodiments.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3E</figref> is a timing diagram showing operation of a sensor operating without recording closely-timed exposures.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIGS. 4A-4C</figref> are timing diagrams showing operation of embodiments of sensors recording closely-timed exposures for several rows of an image sensor.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. 5</figref> is a block diagram illustrating an example process for blending tracks of closely timed exposures having different exposure levels in accordance with some embodiments.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. 6A-6B</figref> illustrate example operations for blending tracks of closely-timed exposures having different exposure levels in accordance with certain embodiments.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7</figref> illustrates another example process for blending tracks having different exposure levels in accordance with some embodiments.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an embodiment of a process for blending tracks of closely-timed exposures having different levels of exposure to control motion artifacts.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0020" num="0019">According to certain aspects, a camera system is provided having a sensor capable of sequentially capturing and outputting first and second image(s) having different exposure levels. Moreover, the system begins capturing the first and second (or more) images within a single frame period, providing controllable or reduced motion artifacts, among other advantages.</p>
<p id="p-0021" num="0020">For instance, the first and second images are closely timed with respect to one another. Moreover, while there can be some separation in time between when the system captures pixels for the first image and captures corresponding pixels for the second image, the first and second images may nonetheless be referred to as or &#x201c;conjoined&#x201d; with one another in time because the separation does not result in visually significant artifacts when the images are combined. To achieve this effect, the exposure times for at least some portions of the first and second images overlap with one another, in some configurations. When recording video images, some sensors disclosed herein output at least first and second closely-timed (or &#x201c;conjoined&#x201d;) image streams (also referred to herein as &#x201c;tracks&#x201d;). According to additional aspects, systems and methods described herein process the first and second image streams (e.g., in-camera or during post-processing) to create a combined output stream.</p>
<p id="p-0022" num="0021">For example, for each video frame, the imaging system captures individual exposures in a second (e.g., relatively high exposure) track very soon after the capturing corresponding exposures in the first (e.g., relatively low exposure) track. In this manner, certain techniques described herein reduce or remove the potential for visual separation between objects in an image track having a first exposure level as compared to the same objects in an image track(s) having a different exposure level. Undesirable levels of visual separation between objects could otherwise arise between the objects in the first image and corresponding objects in the second image, due either to movement of objects in scene or of the camera, for example.</p>
<p id="p-0023" num="0022">Moreover, the system acquires the individual images from the sensor relatively quickly. Where the system incorporates a rolling shutter, for example, the period of delay from between beginning exposure for the first rows of the sensor for a particular image and beginning the exposure for the last rows of the sensor for the image is relatively short. Similarly, the period of delay from between reading out the first rows for an image to reading out the last rows of the sensor for the image is also relatively short. In this manner, the system limits the amount of motion artifact (e.g., wobble, skew, smear) introduced by the rolling shutter. This is in contrast to configurations where the amount of time it takes to capture and read out an entire exposure consumes a larger period of time.</p>
<p id="p-0024" num="0023">Moreover, systems described herein according to some aspects exploit the minimal gap between first and second images, limited rolling shutter artifacts, and other aspects of the techniques described herein to achieve a variety of desirable creative effects. For example, the system can selectively blend or combine the closely-timed image streams to adjust the dynamic range, the character of the motion effect (e.g., amount or quality of blur), or other characteristic.</p>
<p id="p-0025" num="0024">In some cases, the blending of the image streams is tailored to achieve a motion effect substantially similar to how motion is perceived by the human eye. In some instances, the blending is adjusted such that the blurring effect is similar to that of a traditional camera. A user may advantageously control the amount or quality of the image stream blending, providing additionally creative power and flexibility. Such processes can be partially or fully automated, as well.</p>
<p id="p-0026" num="0025">Thus, the techniques described herein provide a variety of benefits. Particularly in the area of recording motion video, certain embodiments provide recordings having improved-dynamic range, customized motion or other visual effects, and combinations thereof</p>
<p id="p-0027" num="0026">While the techniques described herein are discussed primarily with respect to digital motion cameras recording continuous video, it will be understood that various aspects of the inventions described herein are compatible with still cameras, as well as digital still and motion cameras (DSMC's).</p>
<p id="h-0006" num="0000">System Overview</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram an example system <b>100</b> that detects optical data and processes the detected data. The system <b>100</b> can include an imaging device <b>101</b> that can include an optics module <b>110</b>, and an image sensor <b>112</b>, which may further include a multiple exposure generator <b>113</b>. The device <b>101</b> can also include a memory <b>114</b> and an image processing module <b>116</b>. The image processing module <b>116</b> may in turn include a blending module <b>117</b> that is generally configured to adjust the dynamic range of an output image or stream of images. The blending module <b>117</b> may further be configured to adjust the motion effect in the output stream.</p>
<p id="p-0029" num="0028">The imaging device <b>101</b> may be a camera system, for example, and the optics module <b>110</b>, image sensor <b>112</b>, memory <b>114</b>, and the image processing module <b>116</b> or portions thereof may be housed in or supported by the camera housing. Moreover, portions of the system <b>100</b> may be included in a separate device. For example, as shown, the blending module <b>117</b> or portions thereof in some configurations resides in a separate computing device <b>102</b>, such as a laptop or other computer.</p>
<p id="p-0030" num="0029">The optics module <b>110</b> focuses an image on the image sensor <b>112</b>. Sensors <b>112</b> may include, for example, an array of charge-coupled devices (CCD) or Complementary Metal-Oxide-Semiconductor (CMOS) image sensor cells such as active-pixel sensor cells. Such image sensors are typically built on silicon chips and may contain millions of image sensor cells. Each sensor cell detects light reaching its surface and outputs a signal that corresponds to the intensity of the light detected. The detected light is then digitized.</p>
<p id="p-0031" num="0030">Because these image sensors are sensitive to a broad spectrum of wavelengths of light, a color filter array can be disposed over the light sensitive surface of such sensors. One type of color filter array is a Bayer pattern color filter array, which selectively passes red, blue, or green wavelengths to sensor elements. The output of such a sensor, however, is a mosaic image. This mosaic image is formed by the overlapping matrices of red, green, and blue pixels. The mosaic image is usually then demosaiced, so that each picture element has a full set of color image data. The color image data may be expressed in the RGB color format or any other color format.</p>
<p id="p-0032" num="0031">Some of the embodiments disclosed herein are described in the context of a video camera having a single sensor device with a Bayer pattern filter. However, the embodiments and inventions herein can also be applied to cameras having other types of image sensors (e.g., CMY Bayer as well as other non-Bayer patterns), other numbers of image sensors, operating on different image format types, and being configured for still and/or moving pictures. It is to be understood that the embodiments disclosed herein are exemplary but non-limiting embodiments, and the inventions disclosed herein are not limited to the disclosed exemplary embodiments.</p>
<p id="p-0033" num="0032">The optics hardware <b>110</b> can be in the form of a lens system having at least one lens configured to focus an incoming image onto the image sensor <b>112</b>. The optics hardware <b>110</b>, optionally, can be in the form of a multi-lens system providing variable zoom, aperture, and focus. Additionally, the optics hardware <b>110</b> can be in the form of a lens socket supported by a camera housing and configured to receive a plurality of different types of lens systems for example, but without limitation, the optics hardware <b>110</b> include a socket configured to receive various sizes of lens systems including a 50-100 millimeter (T3) zoom lens, a 50-150 millimeter (T3) zoom lens, an 18-50 millimeter (T3) zoom lens, an 18-85 millimeter (T2.9) zoom lens, a 300 millimeter (T2.8) lens, 18 millimeter (T2.9) lens, 25 millimeter (T1.8) lens, 35 millimeter (T1.8) lens, 50 millimeter (T1.8) lens, 85 millimeter (T1.8) lens, 85 millimeter (T1.8) lens, 100 millimeter (T1.8) and/or any other lens or any other lens. In certain embodiments, a 50-100 millimeter (F2.8) zoom lens, an 18-50 millimeter (F2.8) zoom lens, a 300 millimeter (F2.8) lens, 15 millimeter (F2.8) lens, 25 millimeter (F1.9) lens, 35 millimeter (F1.9) lens, 50 millimeter (F1.9) lens, 85 millimeter (F1.9) lens, and/or any other lens. As noted above, the optics hardware <b>110</b> can be configured such that despite which lens is attached thereto, images can be focused upon a light-sensitive surface of the image sensor <b>112</b>.</p>
<p id="p-0034" num="0033">The image sensor <b>112</b> can be any type of image sensing device, including, for example, but without limitation, CCD, CMOS, vertically-stacked CMOS devices such as the Foveon&#xae; sensor, or a multi-sensor array using a prism to divide light between the sensors. In some embodiments, the image sensor <b>112</b> can include a CMOS device having about 12 million photocells. However, other size sensors can also be used. In some configurations, camera 10 can be configured to output video at &#x201c;5 k&#x201d; (e.g., 5120&#xd7;2700), Quad HD (e.g., 3840&#xd7;2160 pixels), &#x201c;4.5 k&#x201d; resolution (e.g., 4,520&#xd7;2540), &#x201c;4 k&#x201d; (e.g., 4,096&#xd7;2,540 pixels), &#x201c;2 k&#x201d; (e.g., 2048&#xd7;1152 pixels) or other resolutions. As used herein, in the terms expressed in the format of xk (such as 2 k and 4 k noted above), the &#x201c;x&#x201d; quantity refers to the approximate horizontal resolution. As such, &#x201c;4 k&#x201d; resolution corresponds to about 4000 or more horizontal pixels and &#x201c;2 k&#x201d; corresponds to about 2000 or more pixels.</p>
<p id="p-0035" num="0034">The camera can also be configured to downsample and subsequently process the output of the sensor <b>112</b> to yield video output at 2K, 1080p, 720p, or any other resolution. For example, the image data from the sensor <b>112</b> can be &#x201c;windowed&#x201d;, thereby reducing the size of the output image and allowing for higher readout speeds. However, other size sensors can also be used. Additionally, the camera can be configured to upsample the output of the sensor <b>112</b> to yield video output at higher resolutions. Moreover, the camera can be configured to capture and record video at 10, 20, 24, 30, 60, and 120 frames per second, or any other frame rate. Additionally, the blending techniques described herein are generally compatible with a variety resolutions and frame rates, including those listed above.</p>
<p id="p-0036" num="0035">The memory <b>114</b> can be in the form of any type of digital storage, such as, for example, but without limitation, hard drives, solid-state drives, flash memory, optical discs, or any other type of memory device. In some embodiments, the size of the memory <b>114</b> can be sufficiently large to store image data from the compression module corresponding to at least about 30 minutes of video at 12 mega pixel resolution, 12-bit color resolution, and at 60 frames per second. However, the memory <b>114</b> can have any size.</p>
<p id="p-0037" num="0036">In some embodiments, the memory <b>114</b> can be mounted on an exterior of a camera housing. Further, in some embodiments, the memory can be connected to the other components through standard or custom communication ports, including, for example, but without limitation, Ethernet, USB, USB2, USB3, IEEE 1394 (including but not limited to FireWire 400, FireWire 800, FireWire S3200, FireWire S800T, i.LINK, DV), SATA and SCSI. Further, in some embodiments, the memory <b>114</b> can comprise a plurality of hard drives, such as those operating under a RAID protocol. However, any type of storage device can be used.</p>
<p id="p-0038" num="0037">The image processing module <b>116</b> may, for example, operate on data that is stored in the memory <b>114</b>. Alternatively, the image processing module may operate on data as it comes from the image sensor <b>112</b>, and the processed data may then be stored in the memory <b>114</b>. The image processing module <b>116</b> may perform a variety of operations on the data. Depending on the operation, the processing module <b>116</b> may perform the operation on the individual first and second image streams, prior to blending, or alternatively, on the blended HDR output stream.</p>
<p id="p-0039" num="0038">For example, the image processing module <b>116</b> may compress the data from the image sensor, perform pre-compression data-preparation (e.g., pre-emphasis and/or entropy reduction), format the compressed data, and the like. Examples of such techniques are described in greater detail in U.S. Pat. No. 7,830,967 entitled &#x201c;VIDEO CAMERA&#x201d; (the '967 patent), which is incorporated by reference herein in its entirety. In some instances, the image processing module <b>116</b> processes the individual image stream (or combined image stream) to tune the image streams to perceptual space, as will be described in detail below with respect to <figref idref="DRAWINGS">FIG. 8A</figref>. Additional compatible techniques include green data modification and pre-emphasis processes shown and described throughout the '967 patent (e.g., with respect to <figref idref="DRAWINGS">FIGS. 8-11</figref> and columns 11-13). In general, certain embodiments described herein are compatible with and/or are components of embodiments described in the '967 patent. Thus, some or all of the features described herein can be used or otherwise combined with various features described in the '967 patent, including features described in the '967 patent which are not explicitly discussed herein.</p>
<p id="p-0040" num="0039">The sensor <b>112</b> can be configured to sequentially capture and output at least first and second image streams (or tracks) having different exposure levels. For example, the multiple exposure generator <b>113</b> may comprise specially designed timing control logic that controls the sensor <b>112</b> to output the image streams. In some cases, the sensor <b>112</b> can output more than two tracks (e.g., totaling 3, 4, 5, 10 or more) having various exposure levels.</p>
<p id="p-0041" num="0040">Where there is a significant amount of delay between exposures or portions of exposures in a first track and corresponding exposures or portions of exposures in a second track, undesirable motion artifacts can arise. For example, such artifacts can arise where an object in the recorded scene (e.g., an actor or other photographic subject) moves a visually significant distance between the time the first image stream frame is captured and the time the second image stream frame is captured. In such cases, visual separation, or &#x201c;gaps,&#x201d; can arise between objects in one track as compared to the same object in another track. Such issues can be difficult to correct for or control in post-processing.</p>
<p id="p-0042" num="0041">For these and other reasons, the sensor <b>112</b> according to certain aspects is configured to sequentially output at least first and second image streams having different exposure levels and which are closely timed with respect to one another. For example, the multiple exposure generator <b>113</b> may control a pixel array and output circuitry of the sensor <b>112</b> to cause the sensor <b>112</b> to output first and second image streams. Each image stream can have multiple frames which are closely-timed and/or at least partially overlapping in time with corresponding frames from the other image stream. For instance, the amount of time from between when the system converts captured data for picture elements in the images from the first stream to digital values and begins to expose the same picture elements for corresponding images in the second image stream is relatively short. Moreover, in some cases, the multiple exposure generator <b>113</b> captures at least some of the picture elements for the images in the second stream while capturing other picture elements for the corresponding images in the first stream, during overlapping time periods. In this manner, the visual separation between moving objects in the first image stream and the same moving objects in the second image stream may be relatively insignificant, or may be otherwise substantially reduced. The multiple exposure generator <b>113</b> may generally be a specialized timing generator coupled to the sensor <b>112</b>, and is described in greater detail with respect to <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0043" num="0042">Because there may be no significant visual separation between objects in a frame in the first image track frame and the corresponding objects in a subsequently captured frame in the second image track, the first and second image tracks may also be referred to as having no &#x201c;gaps&#x201d; with respect to one another, or as substantially &#x201c;touching&#x201d; one another. For example, while there may be some minor delay between the capture of picture elements for first images and the capture of picture elements in the second images, the delay may be so small that there is reduced or substantially no visual object separation at full sensor resolution, at one or more scaled-down resolutions, or a combination thereof. Due to the closely-timed nature of the tracks, the motion artifacts in the combined output track are substantially reduced, removed, or can be controlled in a desired fashion.</p>
<p id="p-0044" num="0043">Sensors described herein are referred to herein as having &#x201c;adjacent read&#x201d; or &#x201c;conjoined exposure&#x201d; capability. Additionally, the images tracks may be referred to as being &#x201c;adjacent,&#x201d; &#x201c;closely timed,&#x201d; &#x201c;conjoined,&#x201d; or the like.</p>
<p id="p-0045" num="0044">Moreover, as described above, the individual exposures according to some embodiments are captured and/or output in a compact manner, making efficient use of the frame period.</p>
<p id="p-0046" num="0045">The blending module <b>117</b> according to certain embodiments selectively combines the closely-timed tracks to produce an output track having a desired visual effect. For instance, the output video stream can have an enhanced dynamic range relative to the dynamic range of the individual first and second image tracks. As an example, the first image stream may have a relatively low exposure level (e.g., short integration time), providing useful data in relatively bright regions of a captured scene, such as highlight detail. The second image stream may have a relatively higher exposure level (e.g., longer integration time), providing useful data in relatively darker regions of the scene, such as detail in the shadows.</p>
<p id="p-0047" num="0046">Moreover, in some cases where the sensor <b>112</b> outputs more than two tracks, the blending module <b>117</b> is capable of combining a corresponding number of tracks. Where the blending module <b>117</b> or portions thereof reside in the external computing device <b>102</b>, the dynamic range and/or motion processing can be done in post-processing. Additionally, depending on the implementation, the blending module <b>117</b> may operate on data stored in the memory <b>114</b>, on data as it is output from the image sensor <b>112</b>, or on data stored in the external computing device <b>102</b>.</p>
<p id="p-0048" num="0047">As discussed, the combined output stream may be created on camera or alternatively off camera, during post-processing. In one configuration, the user can select on-camera or off camera creation of the output stream, as desired. Moreover, as will be described in further detail below with respect to <figref idref="DRAWINGS">FIGS. 5-9</figref>, in various embodiments the blending module <b>117</b> can combine the closely-timed tracks according to a variety of algorithms. The algorithms can be fixed or user-selectable or adjustable. For example, in one embodiment, the image streams are combined on-camera according to a fixed algorithm. In another configuration, the user selects from a variety of algorithms, depending on the desired creative effect.</p>
<p id="h-0007" num="0000">Generating Tracks of Closely-Timed Images Having Multiple Exposure Levels</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating example image sensor <b>200</b> capable of generating closely-timed exposures according to embodiments described herein. Referring to <figref idref="DRAWINGS">FIG. 2</figref>, the image sensor <b>200</b> includes a pixel array <b>202</b>, output circuitry <b>204</b>, and a multiple exposure generator <b>206</b>. As will be described in greater detail, the multiple exposure generator <b>206</b> generally controls the various components of the sensor <b>200</b> to provide closely-timed exposures on the output bus <b>212</b> of the sensor <b>200</b> for further storage and processing.</p>
<p id="p-0050" num="0049">The image sensor <b>200</b> may be similar to the image sensor <b>12</b> described above with respect to <figref idref="DRAWINGS">FIG. 1</figref>, for example, and the pixel array <b>202</b> includes a plurality of pixels (e.g., photocells) arranged in a matrix including &#x201c;M&#x201d; rows and &#x201c;N&#x201d; columns. While a wide variety of values are possible for &#x201c;M&#x201d; and &#x201c;N&#x201d;, an example &#x201c;5 k&#x201d; sensor includes 5,120 rows and 2,700 columns, an example Quad HD sensor includes 3,840 and 2,160 pixels, an example &#x201c;4.5 k&#x201d; resolution sensor includes 4,520 rows and 2,540 columns, an example&#x201c;4 k&#x201d; sensor includes 4,096 rows and 2,540 columns, and an example &#x201c;2 k&#x201d; sensor includes 2,048 and 1,152 columns.</p>
<p id="p-0051" num="0050">In the illustrated example, sensor <b>200</b> is configured to output a single row at a given time, and the sensor <b>200</b> includes one instance of output circuitry <b>204</b> that is configured to process and output image information for a single row. In another embodiment, such as where a Bayer pattern sensor is used, the sensor outputs two rows at a time and can include two instances of the output circuitry <b>204</b>. In yet other configurations, the sensor <b>200</b> outputs other numbers of rows during a given time slot (e.g., 2, 3, 4, 5, 10 or more rows), and can include a corresponding number of instances of the output circuitry <b>204</b>.</p>
<p id="p-0052" num="0051">During operation, the multiple exposure generator <b>206</b> provides row select information via input bus <b>208</b> to row decode logic (not shown) of the pixel array <b>202</b>. For each of the selected rows, the pixel array <b>202</b> provides the stored values for the N pixels corresponding to the columns of the selected row (or subset of rows) to the output circuitry <b>204</b>.</p>
<p id="p-0053" num="0052">The output circuitry <b>204</b> of the example sensor <b>200</b> also receives timing control signals on the bus <b>210</b> from the multiple exposure generator <b>206</b>, and is generally configured to process and digitize the analog pixel values received from the pixel array. The output circuitry <b>204</b> of the example sensor <b>200</b> includes sets of programmable-gain amplifiers (PGAs) and analog-to-digital converters (ADCs), although a variety components may be used in various implementations. The output circuitry <b>204</b> in turn presents the digitized, processed values of the currently selected row on the output bus <b>212</b>. For example, the sensor <b>200</b> may transmit the values to the memory <b>114</b>, image processing module <b>116</b>, or other components of the system of <figref idref="DRAWINGS">FIG. 1</figref> for storage and processing. In some instances, the sensor buffers the values for one or more rows before transmission on the output bus <b>212</b>.</p>
<p id="p-0054" num="0053">As shown in <figref idref="DRAWINGS">FIG. 2</figref>, the multiple exposure generator <b>206</b> may reside outside of the packaging <b>214</b>. For example, the packaging <b>214</b> is an integrated circuit packaging and the pixel array <b>202</b> and output circuitry <b>204</b> are formed on an integrated circuit housed in the packaging <b>214</b>. The multiple exposure generator <b>206</b>, on the other hand, may form a part of a separate integrated circuit that is housed in a separate package. In another embodiment, the multiple exposure generator <b>206</b> is included on the same integrated circuit as the other components, and is housed in the sensor package <b>214</b>.</p>
<p id="p-0055" num="0054">In some cases, the multiple exposure generator <b>206</b> comprises logic implemented in a field-programmable gate array (FPGA), although the multiple exposure generator <b>206</b> can be implemented in a variety of ways, and can include hardware, software, analog and/or digital circuitry, such as custom circuitry, a microprocessor, an application-specific integrated circuit (ASIC), combinations thereof and the like.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 3A</figref> is an example timing diagram <b>300</b> illustrating two consecutive frames of video for a camera that is recording two closely-timed image tracks (also referred to as image streams) in accordance with an embodiment. Two closely-timed exposures <b>302</b>, <b>304</b> are taken within each video frame, one for each image track. Thus, a first image track corresponds to the first exposures <b>302</b> taken over a series of recorded video frames and a second image track <b>304</b> corresponds to second exposures <b>304</b> taken over the same series of video frames.</p>
<p id="p-0057" num="0056">The camera exposes the first exposure <b>302</b> for each frame for a first integration period <b>306</b>, and exposes the second exposure <b>304</b> for each frame for a second integration period <b>308</b>. The timing diagram <b>300</b> may be representative of the operation of the sensors <b>112</b>, <b>200</b> of <figref idref="DRAWINGS">FIGS. 1 and 2</figref>, for example. Moreover, while the diagram shows operation for only two frames for the purposes of illustration, it will be appreciated that the camera may record any number of frames, depending on the length of the recording.</p>
<p id="p-0058" num="0057">As shown, the camera of the illustrated example exposes the rows 0&#x2192;M of the sensor in an overlapping fashion. In this manner, the first row (or subset of rows) is exposed for a first integration period beginning at time t<sub>0 </sub>and ending at time t<sub>1</sub>. The remaining rows (or subsets of rows) are exposed in successive, overlapping time slots, and the first integration period for the last row extends from time t<sub>2 </sub>to time t<sub>3</sub>. This general exposure scheme, where successive rows (or subsets of rows) are exposed in successive, overlapping time slots, may generally be referred to as a &#x201c;rolling shutter&#x201d; technique.</p>
<p id="p-0059" num="0058">Picture elements for the second exposure <b>304</b> are advantageously captured relatively shortly after capturing the same picture elements for the first exposure <b>302</b>. For example, the inter-exposure delay <b>312</b> between the end of the first exposure period <b>306</b> for each row (or subset of rows) and the beginning of the second exposure period <b>308</b> for the same row is relatively minimal. For example, as discussed in greater detail with respect to <figref idref="DRAWINGS">FIG. 4A</figref> below, the inter-exposure delay <b>312</b> may depend in one instance on the reset time for one or more rows of the sensor. In other cases, the inter-exposure delay <b>312</b> can depend on some other parameter(s). Referring again to the sensor <b>200</b> of <figref idref="DRAWINGS">FIG. 2</figref>, the multiple exposure generator <b>206</b> is configured to generate control signals for operating the pixel array <b>202</b> and output circuitry <b>204</b> to generate the closely-timed exposures <b>302</b>, <b>304</b>.</p>
<p id="p-0060" num="0059">Moreover, the inter-row skew for the individual exposures <b>302</b>, <b>304</b> introduced by the rolling shutter is relatively minimal. This results in a shorter overall image acquisition time, and helps to control any motion artifacts that may otherwise be introduced by the rolling shutter. As will be discussed further with respect to <figref idref="DRAWINGS">FIG. 4A</figref>, the inter-row skew and image acquisition time can be related to the readout time for a row (or subset rows).</p>
<p id="p-0061" num="0060">As shown, the system additionally makes efficient use of the sensor hardware and frame period by overlapping certain portions of the first and second exposures in time. For instance, the system begins to expose at least some of the rows (or subsets of rows) for the second exposure <b>304</b> before all of the rows have been converted and/or read out for the first exposure <b>302</b>. Moreover, at least some rows are exposed for the first exposure <b>302</b> concurrently with other rows for the second exposure <b>304</b>.</p>
<p id="p-0062" num="0061">The system in certain embodiments additionally substantially completes the conversion and/or readout of the sensor rows (or subsets of rows) for the first exposure <b>302</b> before beginning to convert and/or readout the rows for the second exposure <b>304</b>. By dedicating the conversion and readout circuitry to each of the respective exposures until they are complete, the system helps maintain a compact overall acquisition time for the individual exposures <b>302</b>, <b>304</b>. In the illustrated embodiment, the system completes converting and/or reading out 100 percent of the sensor rows for first exposure <b>302</b> before beginning to convert and/or readout any of the rows for the second exposure <b>304</b>. In other cases, the system completes converting and/or reading out at least 80 percent, at least 90 percent, at least 95 percent, at least 96 percent, at least 97 percent, at least 98 percent or at least 99 percent of the sensor rows for the first exposure <b>302</b> before beginning to convert and/or read out the rows for the second exposure <b>304</b>. In yet further cases, the system substantially completes conversion and/or readout of the rows for the first exposure <b>302</b> before reading out any of the rows for the second exposure <b>304</b>, or before reading out more than 1 percent, more than 2 percent, more than 3 percent, more than 4 percent, more than 5 percent, more than 10 percent, or more than 20 percent of the rows for the second exposure <b>304</b>, depending on the embodiment. Further details regarding the specifics of the sensor timing are provided below with respect to <figref idref="DRAWINGS">FIGS. 4A-4C</figref>.</p>
<p id="p-0063" num="0062">Depending on the particular configuration, the duration of the various time periods shown in <figref idref="DRAWINGS">FIG. 3A</figref> can vary greatly, including the frame period <b>312</b>, first and second exposure integration times <b>302</b>, <b>304</b>, inter-exposure delay <b>312</b> and sensor inactive <b>310</b>. The sensor inactive time <b>310</b> may refer to a period where sensor elements are substantially inactive and not being exposed. For example, when capturing the first and second exposures, the sensor electronically activates the appropriate picture elements according to the rolling shutter technique to be sensitive to and capture incident light. In contrast, the appropriate pixel elements are electronically deactivated and therefore not sensitive to incident light during the sensor inactive period <b>310</b>. In other implementations, where a physical shutter is used, the physical shutter is opened/closed in place of the electronic activation/deactivation. The sensor inactive period can also instead be defined as the period of time from between t<sub>7</sub>, when exposure completes for the last row in the second exposure <b>304</b>, to the end of the frame period, immediately before the first row in the first exposure <b>302</b> begins to expose for the next frame.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIGS. 3B-3D</figref> illustrate timing diagrams for several example configurations. For clarity, the inter-exposure delay <b>312</b> is not shown in <figref idref="DRAWINGS">FIGS. 3B-3D</figref>. In one embodiment, <figref idref="DRAWINGS">FIG. 3B</figref> illustrates a scenario where the frame period is about 41.7 milliseconds (24 fps), the first integration time <b>306</b> is about 2.6 milliseconds, the second integration time <b>308</b> is about 20.8 milliseconds, the inter-exposure delay <b>312</b> is about 15.6 microseconds, and the shutter closed time is about 18.2 milliseconds. The second integration time <b>308</b> is about half the frame period, and thus generally corresponds to an integration time for a sensor implementing a 180 degree shutter. In another example case, <figref idref="DRAWINGS">FIG. 3C</figref> shows a scenario where the frame period <b>312</b> is about 41.7 milliseconds (24 fps), the first integration time <b>306</b> is about 2.6 milliseconds, the second integration time <b>308</b> is about 31.6 milliseconds, the inter-exposure delay <b>312</b> is about 15.6 microseconds, and the shutter closed time <b>310</b> is about 7.4 milliseconds. In this case, the second integration <b>308</b> time corresponds to an integration time for a sensor implementing a 273 degree shutter. In yet another instance, <figref idref="DRAWINGS">FIG. 3D</figref> illustrates an case where the frame period <b>312</b> is about 41.7 milliseconds (24 fps), the first integration time <b>306</b> is about 0.65 milliseconds, the second integration time <b>308</b> is about 5.2 milliseconds, the inter-exposure delay <b>312</b> is about 15.6 microseconds, and the shutter closed time <b>310</b> is about 35.75 milliseconds. Here, the second integration time <b>308</b> corresponds to an integration time for a 45 degree shutter.</p>
<p id="p-0065" num="0064">For comparison, <figref idref="DRAWINGS">FIG. 3E</figref> illustrates a timing diagram for another example camera capturing multiple exposures having different exposure levels. However, unlike the embodiments shown in <figref idref="DRAWINGS">FIGS. 3A-3D</figref>, the camera of <figref idref="DRAWINGS">FIG. 3E</figref> captures the long exposures <b>304</b> and short exposures <b>306</b> on different, alternating frames. Thus, the exposures <b>304</b> and <b>306</b> are not closely-timed, and significant undesirable motion artifacts may be present. For example, relatively large gaps of visual separation may exist between image scene objects in the long exposures <b>304</b> as compared to image scene objects in the short exposure <b>306</b> in the next frame. Thus, it can be difficult in such cases to combine the short and long exposures <b>304</b>, <b>306</b> to create high-dynamic range images with controlled motion artifacts.</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 4A</figref> shows a timing diagram <b>400</b> illustrating closely-timed first and second exposures <b>402</b>, <b>404</b> for one frame of recorded video. For clarity, the timing diagram is shown for only the first three rows R<b>1</b>, R<b>2</b>, R<b>3</b> (or subsets of rows) of an image sensor. For example, the diagram <b>400</b> may correspond to one embodiment of a more detailed depiction of the exposure timing for the first three rows of the sensor whose operation is depicted in any of <figref idref="DRAWINGS">FIGS. 3A-3D</figref>. Referring to the first row R<b>1</b>, before the first exposure <b>402</b> begins, the row R<b>1</b> is initially reset, as indicated by the vertical arrow labeled &#x201c;RST.&#x201d;</p>
<p id="p-0067" num="0066">After a reset period <b>410</b> passes corresponding to the amount of time it takes to reset the pixels in the row R<b>1</b>, the photocells in the row R<b>1</b> begin to accumulate charge during the first integration period <b>406</b>. Following the first integration period <b>406</b>, the pixels in the first row R<b>1</b> have accumulated charge corresponding to the first exposure level. A readout cycle is then initiated for the row R<b>1</b>, along with another reset cycle, as indicated by the vertical arrow labeled &#x201c;RO/RST&#x201d;. Referring to <figref idref="DRAWINGS">FIG. 2</figref>, at the end of the readout period <b>412</b>, the row R<b>1</b> is readout of the pixel array <b>202</b> by the output circuitry <b>204</b>, and is provided on the output <b>212</b> for storage and/or processing.</p>
<p id="p-0068" num="0067">Additionally, once the row R<b>1</b> has completed the second reset cycle, the second exposure <b>404</b> begins. As such, the inter-exposure delay <b>410</b> between the first and second exposures in one embodiment depends on the reset time <b>410</b> for a row of pixels (or subset of rows where more than one row are read out at once). For example, the inter-exposure delay <b>410</b> may correspond to, substantially correspond, or be approximately equal to the reset time <b>410</b> for one row (or subset of rows). In other instances, the inter-exposure delay <b>410</b> is less than or equal to approximately one, approximately 2, approximately 5, approximately 10, approximately 20, approximately 50 or approximately <b>100</b> reset times <b>410</b> for a row (or subset of rows). In yet further cases, the inter-exposure delay <b>410</b> is some value between approximately one reset time <b>410</b> for a row (or subset of rows) and approximately 2, approximately 5, approximately 10, approximately 20, approximately 50 or approximately 100 reset times, depending on the embodiment. As is described in greater detail below, the reset time <b>410</b> can advantageously be relatively short, leading to closely timed first and second exposures <b>402</b>, <b>404</b>.</p>
<p id="p-0069" num="0068">In other configurations, the delay between the first and second exposures <b>402</b>, <b>404</b> can correspond to a variety of values other than the reset time <b>410</b>. For example, the inter-exposure delay <b>410</b> may correspond to, substantially correspond to, or otherwise depend on the readout time <b>412</b>, such as where the readout time is greater than or equal to the reset time. In one such embodiment, the inter-exposure delay <b>410</b> is approximately equal to the readout time <b>412</b>. In other cases, depending on the specific implementation, the inter-exposure delay <b>410</b> is less than or equal to approximately one, approximately 2, approximately 5, approximately 10, approximately 20, approximately 50 or approximately 100 readout times <b>412</b> for a row (or subset of rows). According to yet further embodiments, the inter-exposure delay <b>410</b> is be some value between approximately one readout time <b>412</b> for a row (or subset of rows) and approximately 2, approximately 5, approximately 10, approximately 20, approximately 50 or approximately 100 readout times <b>412</b>. In further cases, the delay <b>410</b> between the first and second exposures <b>402</b>, <b>404</b> depends on both the readout time and the reset time. For example, the reset operation in one embodiment is initiated only after completion of the readout operation. In this case, the delay between the first and second exposures <b>402</b>, <b>404</b> may correspond to or substantially correspond to the sum of the readout time <b>412</b> and the reset time. In such cases, the inter-exposure delay <b>410</b> may be less than or equal to approximately the sum of the reset time <b>410</b> and the readout time <b>412</b> for a row (or subset of rows), or may alternatively be less than or equal to approximately the sum of the reset time <b>410</b> and the readout time <b>412</b>, multiplied by a factor of 2, 5, 10, 20, 50 or 100. In further embodiments, the inter-exposure delay <b>410</b> can be some value between approximately the sum of the reset time <b>410</b> and the readout time <b>412</b> for a row (or subset of rows) and approximately the sum of the reset time <b>410</b> and the readout time <b>412</b>, multiplied by a factor of 2, 5, 10, 20, 50 or 100.</p>
<p id="p-0070" num="0069">As shown, the sensor initiates a similar sequence of operations for the other rows R<b>2</b>, R<b>3</b> . . . RM. However, as described above, with respect to <figref idref="DRAWINGS">FIG. 2</figref>, the sensor may be configured to output only one row (or a subset of two or more rows) at a time. Thus, for each subsequent row, the multiple exposure generator <b>206</b> waits at least one readout period <b>412</b> before initiating the initial reset cycle. As such, the exposure timing for each row is staggered in time with respect to the other rows. In the illustrated example, each subsequent row is delayed by the readout period <b>412</b> with respect to the preceding row. As such, when an integration period <b>402</b>, <b>404</b> is complete for a particular row, the output circuitry <b>204</b> will be generally immediately available. In this manner, the utilization of the readout circuitry (e.g., the output circuitry <b>204</b> of <figref idref="DRAWINGS">FIG. 2</figref>) is relatively high. Referring again to <figref idref="DRAWINGS">FIG. 2</figref>, the multiple exposure generator <b>206</b> may send the appropriate control signals to the pixel array via the bus <b>208</b> and to the output circuitry <b>204</b> via the bus <b>210</b>, initiating the reset and readout of the rows R<b>1</b>, R<b>2</b>, R<b>3</b> . . . RM in the manner described.</p>
<p id="p-0071" num="0070">Although a variety of values are possible, one example sensor <b>200</b> has 2740 rows of pixels configured to be readout two rows at a time, and the readout time <b>412</b> for the two rows of the example sensor <b>200</b> is about 7.4 microseconds. In this example, the reset time <b>410</b> for resetting a subset of two rows is about 15.6 microseconds. Thus, the time between the first and second exposures <b>402</b>, <b>404</b> and the time between the closely-timed image streams is about 15.6 microseconds, or about 0.03 percent of the frame period for a frame rate of 24 fps, and about 0.05, 0.08, 0.09, and 0.16 percent of the frame period for frame rates of 30, 50, 59.94, and 100 fps, respectively. In another configuration, for example, the sensor <b>200</b> produces first and second image streams which are temporally spaced by no more than about 7 microseconds. As such, for each frame the sensor <b>200</b> captures the a subsequent <b>404</b> (corresponding to the second image stream) no more than about 7 microseconds after capturing the preceding exposure <b>402</b> for that frame (corresponding to the first image stream). In other embodiments, the first and second exposures <b>402</b>, <b>404</b> and corresponding image streams are temporally spaced by no more than about 4 microseconds, no more than about 2 microseconds, or no more than about 1 microsecond. In yet other embodiments, the first and second exposures <b>402</b>, <b>404</b> are spaced by no more than about 1 millisecond, no more than about 100 microseconds, no more than about 50 microseconds, no more than about 25 microseconds, no more than about 20 microseconds, no more than about 16 microseconds, no more than 15.6 microseconds, no more than about 15 microseconds, or no more than about 10 microseconds. As indicated above, in some embodiments there are more than two exposures within each frame. In such cases, the delay between any two adjacent exposures may correspond to any of the above listed values or ranges of values. In various embodiments, the inter-exposure delay is less than about 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, or 30 percent of the frame period.</p>
<p id="p-0072" num="0071">Additionally, as mentioned previously, because acquisition of each row (or subset of rows) is staggered in time from the immediately preceding row by a relatively minimal amount, the skew and resulting motion artifacts introduced by the rolling shutter are reduced. For example, the inter-row acquisition skew between the time the system begins to acquire image data for a row (or subset of rows) and begins to acquire image data for the immediately subsequent row corresponds to the readout time <b>412</b> for a row (or subset of rows) in the illustrated embodiment. Likewise, an inter-row readout skew between the time the system completes reading out acquired image data for a row (or subset of rows) and completes reading out acquired image data for the subsequent row (or subset of rows) corresponds to the readout time <b>412</b> for a row (or subset of rows). In various embodiments, the inter-row acquisition skew and/or the inter-row readout skew is approximately equal to one readout time <b>412</b> for a row or subset of rows, or is less than or equal to approximately one, approximately 1.5, approximately 2, approximately 2.5, or approximately 3 readout times <b>412</b>.</p>
<p id="p-0073" num="0072">Extending the inter-row skew across all of the rows for <figref idref="DRAWINGS">FIG. 4A</figref>, the total intra-exposure acquisition skew across the entire image from between the time the system begins to acquire a first of the rows (or subset of rows) and begins to acquire all of the rows or substantially all (e.g., at least 90, 95 or 99 percent) of the rows is given by approximately:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(M&#x2212;1)* read_out_time, &#x2003;&#x2003;(Eq. 1)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0074" num="0073">Similarly, the intra-exposure readout skew between the time the system completes reading out a first of the rows and completes reading out substantially all of the rows may correspond to the same amount. As a specific example, for the 2740 row sensor described above, the intra-exposure acquisition and readout skews are both equal to about 10.1 milliseconds ((2740/2&#x2212;1)*7.4 microseconds).</p>
<p id="p-0075" num="0074">Further, the acquisition time for an entire exposure, from between the time when the system begins to acquire a first of the rows (or subsets of rows) and completes reading out substantially all of the rows for the exposure is approximately given by:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>exposure_time+[(M&#x2212;1)*read_out_time]. &#x2003;&#x2003;(Eq. 2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0076" num="0075">In further compatible embodiments, the image acquisition time is less than or equal to one of the following:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>exposure_time+[(M&#x2212;1)*read_out_time]&#x2003;&#x2003;(Eq. 3)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>exposure_time+[(M&#x2212;1)*1.5*read_out_time], &#x2003;&#x2003;(Eq. 4)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>exposure_time+[(M&#x2212;1)*2*read_out_time], &#x2003;&#x2003;(Eq. 5)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>exposure_time+[(M&#x2212;1)*2.5*read_out_time], or &#x2003;&#x2003;(Eq. 6)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>exposure_time+[(M&#x2212;1)*3*read_out_time]. &#x2003;&#x2003;(Eq. 7)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0077" num="0076">The systems in each of the above-described specific examples record a first relatively lower exposure (e.g., short exposure) followed by a second relatively higher exposure (e.g., long exposure) for each frame. However, the number and order of the inter-frame exposures can vary, and can be customized as desired. For example, in one instance, each frame includes a long track followed by a short track. Additionally, the device can be configured to capture more than two exposures per frame, such as 3, 4, 5, 10 or more exposures. As just a few illustrative examples, the following capture patterns are possible including more than two exposures per frame: (1) long, short, long; (2) short, long, short; (3) short, short+x, short+x*2, . . . , short+x*k; (4) long, long-x, long-x*2, . . . long-x*k; (4) short, medium, long; (5) long, medium, short; (6) medium, long, short.</p>
<p id="p-0078" num="0077">The diagrams <b>300</b>, of <figref idref="DRAWINGS">FIGS. 3A-3D</figref> and <b>4</b>A correspond to configurations in which the sensor outputs the exposures in the same order for each row. For example, referring to <figref idref="DRAWINGS">FIG. 4</figref>, each of the rows R<b>1</b> . . . RN outputs the short exposure <b>402</b> followed by the long exposure <b>404</b> for each frame. However, in some cases, the multiple exposure generator <b>206</b> is configured to cause the sensor <b>200</b> to output the first and second exposures in a different order depending on the row, providing greater creative flexibility.</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 4B</figref> shows one such example, where the order of the short and long exposures <b>402</b>, <b>404</b> alternates from row to row (or from subset of rows to subset of rows). Accordingly, the first row R<b>1</b> outputs the short exposure <b>402</b> followed by the long exposure 404, the second row R<b>2</b> outputs the long exposure <b>404</b> followed by the short exposure, the third row outputs the short exposure <b>402</b> followed by the long exposure <b>404</b>, and so on. In another similar embodiment, one or more of the integration times for the short exposure and the long exposure for the odd rows are different than the short exposure and the long exposure for the even rows.</p>
<p id="p-0080" num="0079">In yet other instances, one or more of the rows capture only a single exposure per frame, while the remaining rows capture multiple exposures. For example, <figref idref="DRAWINGS">FIG. 4C</figref> shows one such case where the odd rows output a single long exposure <b>402</b> per frame, while the even rows output three closely-timed exposures (e.g., short, medium, short) <b>404</b>, <b>406</b>, <b>408</b> per frame. Thus, it can be appreciated that a wide variety of possibilities exist for creating closely-timed exposures.</p>
<p id="p-0081" num="0080">For the purposes of clarity, the terms &#x201c;short&#x201d; and &#x201c;long&#x201d; are used herein to distinguish different exposures, such as where one exposure has an integration time relatively shorter than another exposure(s), or shutter speed that is thus relatively longer than the other exposure(s). However, the above-described techniques are compatible with embodiments where other parameters instead of, or in addition to integration time, are manipulated and which affect the degree of exposure. Such parameters can include gain (e.g., analog gain), aperture, and ISO, for example. Thus, depending on the embodiment, where exposures are referred to as being &#x201c;short&#x201d; or a &#x201c;long,&#x201d; it will be appreciated that they may actually be more accurately referred to as &#x201c;low&#x201d; or &#x201c;high.&#x201d; As used herein, the terms &#x201c;picture element reset time&#x201d; and &#x201c;picture element readout time&#x201d; can refer to one or more of the amount of time it takes to read out a row of picture elements (e.g., pixels), a subset of more than one rows of picture elements, a single picture element, or some other non-row grouping of pixel elements, such as one or more columns of picture elements. In some cases, such as where multiple pixel elements are readout in parallel, for example, the readout time for a particular group (e.g., row or subset of rows of pixel elements) corresponds to the same amount of time as that of a single picture element.</p>
<p id="h-0008" num="0000">Generating Tracks of Closely-Timed Images Having Multiple Exposure Levels</p>
<p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. 5</figref> is a flow diagram illustrating an example process <b>500</b> for creating a blended output track from multiple tracks of closely-timed exposure frames. As will be described, according to the process <b>500</b> and other techniques described herein, the blending module <b>117</b> generally receives and processes tracks of closely-timed exposure frames. The blending module <b>117</b> may receive and or process the tracks on a frame-by-frame basis, or may operate on any portion of the tracks (e.g., 5, 10, 100, 1000 or more frames, or entire tracks) at any given time.</p>
<p id="p-0083" num="0082">At block <b>502</b>, the blending module <b>117</b> receives multiple tracks (or portions thereof) of video frames having closely timed exposures (e.g., spaced apart by no more than about 20, 16, 7, 4, or 2 microseconds). The blending module <b>117</b> can receive the tracks in a variety of ways. As one example, the blending module <b>117</b> resides in software executing on an external computing device. The camera <b>100</b> records the closely-timed exposure tracks to the memory device <b>114</b> and the tracks are transferred from the memory device <b>114</b> to the external computing device, which forwards the recorded tracks to the blending module <b>117</b> for processing. As another example, the blending module <b>117</b> resides in software or hardware on the camera <b>100</b> and receives the recorded tracks directly from the memory device <b>114</b> or sensor <b>112</b>.</p>
<p id="p-0084" num="0083">At block <b>504</b>, the blending module <b>117</b> processes the received exposures according to the desired (e.g., fixed or user-selectable) algorithm. The blending module <b>117</b> may blend together the first and second tracks according to the algorithm, generating the output track based on the first and second (or more) tracks. Some example blending techniques are shown and described with respect to <figref idref="DRAWINGS">FIGS. 6A-9</figref>.</p>
<p id="p-0085" num="0084">At block <b>506</b>, the blending module <b>117</b> provides the blended output track. For example, where the blending module <b>117</b> resides on-camera, it may provide the output image stream frame to the memory <b>114</b> for storage. Alternatively, the camera may stream the blended image track to the computing device <b>102</b> for external processing and/or storage. Where the blending module <b>117</b> operates on only a frame or other portion of each track at a time, the process <b>500</b> may then return to block <b>502</b>, where the blending module <b>117</b> receives the first and second exposures for the next frame or group of frames in the respective track.</p>
<p id="p-0086" num="0085"><figref idref="DRAWINGS">FIGS. 6A-6B</figref> illustrate example blending operations. For example, the blending module <b>117</b> may be capable of implementing the blending techniques shown in <figref idref="DRAWINGS">FIGS. 6A-6B</figref>. Generally, using these and other blending techniques, the blending module <b>117</b> is capable of providing an output track that incorporates desired content from the respective images while discarding undesired image content. The discarded image content can include noisy portions of one or more of the tracks, as well as other portions that are not needed to provide the desired aesthetic effect. Moreover, in some cases the user can select which content to include in the output track, providing improved creative flexibility.</p>
<p id="p-0087" num="0086">Referring to <figref idref="DRAWINGS">FIG. 6A</figref>, the blending operation <b>600</b> represents a scenario in which the camera captures first (e.g., long) <b>604</b> and second (e.g., short) <b>606</b> closely timed exposures, respectively, for each video frame. The width of the bars representing the long and short exposures <b>604</b>, <b>606</b> generally corresponds to the dynamic range of the sensor, which in turn can correspond to the range of possible pixel values for the sensor <b>112</b> (e.g., 0 to 1023 for a 10-bit sensor, 0 to 4095 for a 12-bit sensor, 0 to 65,535 for a 16-bit sensor, etc.). <figref idref="DRAWINGS">FIG. 6A</figref> corresponds to a configuration in which the output track has an expanded bit-depth as compared to the sensor <b>112</b>, and has an improved dynamic range as compared to either of the individual tracks <b>604</b>, <b>606</b> due to the selective inclusion of content from the two tracks <b>604</b>, <b>606</b>.</p>
<p id="p-0088" num="0087">As shown, for each of the long and short exposures <b>604</b>, <b>606</b> a certain number of captured values will be relatively noisy, having a relatively lower signal-to-noise ratio (SNR). For example, the SNR is relatively low for lower pixel values and gradually increases as the measured pixel values increase. Moreover, the subject scene may include highlights in certain brighter regions. As indicated, the short exposure <b>606</b> can capture a significant portion of the highlights because of the shorter integration time, reducing the chance that the highlights will become washed out. On the other hand, as illustrated, the long exposure <b>604</b> in some cases does not capture much highlight detail due to the relatively longer integration time, which can lead to wash out.</p>
<p id="p-0089" num="0088">The scene may also include darker regions including shadows, which can often be juxtaposed with brighter regions. As shown, the long exposure <b>604</b> may capture relatively more detail in these regions due to the long integration time. Conversely, the short exposure <b>606</b> may capture less detail in these regions due to the shorter integration time.</p>
<p id="p-0090" num="0089">Also, as shown, the long and short exposures <b>604</b>, <b>606</b> can also include content that corresponds to &#x201c;normal&#x201d; portions of the image scene. These portions may not be particularly dark or bright, for example, and can have a generally normal or medium brightness level. The labels used with respect to <figref idref="DRAWINGS">FIGS. 6A-6B</figref> (&#x201c;shadows,&#x201d; &#x201c;normal,&#x201d;&#x201c;highlights&#x201d;) are used only for the purposes of illustrating the blending concepts, and are not limiting. For example, it will be appreciated that the shorter exposure may in some cases capture some amount of shadow content from the image scene. Likewise, the longer exposure in some instances will record some highlight detail.</p>
<p id="p-0091" num="0090">In the illustrated example, the blending module <b>117</b> matches the light levels between the two exposures <b>604</b>, <b>606</b>. For example, the blending module <b>117</b> can shift the exposures with respect to one another by N bits (e.g., 1, 2, 4, 8, or more bits) prior to blending, or perform some other mathematical operation on one or more of the exposures. In the illustrated example, the short exposure <b>606</b> is shifted up by N bits, and is therefore multiplied by a factor of 2<sup>N</sup>. In such cases, the long exposure <b>604</b> may be more exposed than the short exposure <b>606</b> by a factor of 2<sup>N</sup>. As such, after the shifting, the light levels between the two exposures <b>604</b>, <b>606</b> for corresponding image scene regions more closely match. Also, as shown, the noisiest content in the short exposure <b>606</b> is discarded in some cases.</p>
<p id="p-0092" num="0091">A variety of other processes are possible for adjusting the exposures <b>604</b>, <b>606</b> with respect to one another prior to blending. For example, in some cases, the long exposure <b>604</b> is shifted or otherwise adjusted instead of the shorter exposure <b>606</b>. In one instance, the long exposure <b>604</b> is shifted down by N bits instead of shifting the shorter exposure <b>606</b> up by N bits. <figref idref="DRAWINGS">FIG. 3B</figref>, discussed below, shows one such embodiment. In other embodiments, the exposures <b>604</b>, <b>606</b> are both shifted. For example, the exposures <b>604</b>, <b>606</b> may be shifted in equal and opposite directions. In yet other cases, the system shifts the exposures <b>604</b>, <b>606</b> in opposite directions but by different values. For example, where the first exposure <b>604</b> is exposed by a factor of 2<sup>N </sup>greater than the second exposure <b>606</b>, and where N=X+Y, the first exposure <b>604</b> may be shifted down by X bits, and the second exposure may be shifted up by Y bits. A variety of other embodiments are possible. For instance, instead of multiplying or dividing the pixel values for the exposures <b>604</b>, <b>606</b>, a subtraction, addition or other appropriate operation can be used.</p>
<p id="p-0093" num="0092">After matching the light levels, the blending module blends together the exposures <b>604</b>, <b>606</b>. Three blending algorithms (i)-(iv) are shown for the purposes of illustration.</p>
<p id="p-0094" num="0093">Referring to the first algorithm (i), the blending module <b>117</b> in one embodiment selects the image content of the long exposure <b>604</b> that corresponds to the line segment A-B for inclusion in the output track. In one example, the sensor is a 12-bit sensor, the short exposure is shifted up by 2 bits, and the line segment A-B corresponds to pixel values between 0 and 2048. The blending module <b>117</b> uses the measured values from long exposure as a reference to determine which pixel values to include in the blended output track. For example, the blending module <b>117</b> determines which measured pixels in the long exposure <b>604</b> have values from 0 to 2048, and incorporates those pixel values into the corresponding pixels in the blended output image. The values from the long exposure <b>604</b> are incorporated substantially directly, unmodified, into the output track while, in some alternative configurations, the blending module <b>117</b> performs some mathematical operation on the values prior to incorporation, such as a scaling, shifting or rounding.</p>
<p id="p-0095" num="0094">The charts <b>608</b>, <b>610</b> shown for the long and short exposures <b>604</b>, <b>606</b>, respectively, indicate the percentage of each track included in the output track according to the blending algorithm (i). For example, for pixel values between points A and B, 100% of the output track is composed of content from the long exposure 604.</p>
<p id="p-0096" num="0095">The blending module <b>117</b> may alternatively use the short exposure <b>606</b> as a reference to determine which measured pixel values from the long exposure <b>604</b> to include in the output track for the portion A-B. In the example case, for instance, the blending module <b>117</b> determines which pixels in the short exposure <b>606</b> have measured values between 0 and 2048, and selects the measured values for the corresponding pixels in the long exposure <b>604</b> to include in the output track. In yet other instances, the blending module <b>117</b> uses other types of references. For example, the blending module <b>117</b> may calculate some combination (e.g., an average) of the measured values for the short and long exposure <b>606</b>, <b>604</b> for each pixel, and use the calculated value as a reference to determine which pixels from the long exposure <b>604</b> to include in the output track. Similarly, while the remaining blending operations (e.g., those along the lines B-C, C-D, and those for the other example blending schemes (ii)-(iv)) are described as using the long exposure <b>604</b> as a reference, the short exposure <b>606</b>, some combination of the short and long exposure <b>606</b>, <b>604</b>, or some other type of reference may be employed in those cases as well.</p>
<p id="p-0097" num="0096">This image content from the long exposure <b>604</b> along the line segment A-B may include relatively good detail in the shadows. In some other cases, the blending module <b>117</b> blends together the short and long exposures <b>604</b>, <b>606</b> to some degree for values along the line A-B. The amount of blending may be relatively constant, regardless of the input pixel values. Additionally, the blending may be biased in favor of one of the tracks, e.g., the long track <b>604</b>. For example, the blending module <b>117</b> may perform a weighted average of input values from the long track <b>604</b> and the short track <b>606</b>, where the long track has a higher weighting than the short track <b>606</b>. In some cases, the degree of bias or weighting is controllable by a user, or automatically adjusts depending on the particular mode of operation. Blending algorithm (iv) described below shows an example where weighted averages are used to generate some of the image content in the blended output track.</p>
<p id="p-0098" num="0097">Referring still to algorithm (i), the next portion of the output track corresponds to a combination of the image content from both the long and short exposures <b>604</b>, <b>606</b> along the slanted line B-C. Moreover, the degree to which the respective exposures <b>604</b>, <b>606</b> are included in the output exposure varies based on the measured input values. This can be in contrast to the portions of the output image corresponding to the line segments A-B and C-D (discussed below). For example, for the segments A-B and C-D, pixels from only one track are selected for inclusion in the output track, or the amount of blending is generally constant (e.g., weighted average), and irrespective of the measured input values.</p>
<p id="p-0099" num="0098">A variety of operations are possible for the blending of the exposures <b>604</b>, <b>606</b> for the segment B-C. For example, in some embodiments, the amount of the long track <b>604</b> included in the output track is greatest at the point B. Conversely, the amount of the short track <b>606</b> used in the blending operation is least at the point B. Moreover, for increasing pixel values x between B and C, the amount of the long track <b>604</b> included in the output track gradually decreases. Conversely, the amount of the second track <b>606</b> included in the output track increases, reaching a maximum at the point C. This is illustrated in the chart <b>608</b>, which shows the percentage of the long exposure <b>604</b> in the output track decreasing linearly from 100 percent to 0 percent for increasing pixel values B to C. Conversely, the chart <b>610</b> shows the percentage of the short exposure <b>606</b> in the output track increasing linearly from 0 percent to 100 percent from B to C. In an example case, the sensor is a 12-bit sensor, the short exposure <b>606</b> is shifted up by 2 bits, B corresponds to a pixel value of 2048, and C corresponds to a pixel value of 4094. Taking as an example a point midway between B and C corresponding to a pixel value of 3071, the blending module <b>117</b> determines which pixels in the long exposure <b>604</b> (the reference exposure) have a value of 3071. Because the point is midway between B and C, the blending module <b>117</b> calculates the values for corresponding pixels in the output track according to the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>out_pixel_val=0.5*long_pixel_val*0.5short_pixel_val. &#x2003;&#x2003;(Eq. 8)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0100" num="0099">A linear interpolation is used for the portion B-C in some instances, and in one case the interpolant is given by:</p>
<p id="p-0101" num="0100">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>y</mi>
          <mo>=</mo>
          <mrow>
            <mn>1</mn>
            <mo>+</mo>
            <mfrac>
              <mrow>
                <mi>x</mi>
                <mo>-</mo>
                <mi>B</mi>
              </mrow>
              <mrow>
                <mi>B</mi>
                <mo>-</mo>
                <mi>C</mi>
              </mrow>
            </mfrac>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>Eq</mi>
          <mo>.</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>9</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where x corresponds to measured input pixel values between B and C, such that B&#x2266;x&#x2266;C, for example. In turn, the interpolant may be used to calculate the interpolated output pixel value for each value of x. For instance, the output pixel values may be calculated according to the following equation:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>out_pixel_val=<i>y</i>*long_pixel_val+(1<i>&#x2212;y</i>)*short_pixel_val &#x2003;&#x2003;(Eq. 10)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0102" num="0101">Other types of blending can be used for pixel values corresponding to the line segment B-C. In some cases, and unlike the linear interpolation technique, the amount of the respective exposures <b>604</b>, <b>606</b> included in the blended exposure may not depend on the input values. For example, for this portion of the output track, the blending module <b>117</b> may calculate an average of the image content from the long exposure <b>604</b> and the image content of the short exposure <b>606</b>, and include the average in the output track. A variety of other operations can be used instead of an average, including, without limitation, a sum, difference, weighted average, or the like. Blending algorithm (iii) described below incorporates a curved blending function.</p>
<p id="p-0103" num="0102">In one embodiment, the blending module <b>117</b> selects the image content of the short exposure <b>606</b> corresponding to the line C-D to complete the output track, and does not blend the exposures for this portion of the output track. For example, referring to 12-bit sensor example, the point C may correspond to a pixel value of 4095, which corresponds to a saturated value for the long exposure <b>604</b>. The blending module <b>117</b> may therefore determine which pixels in the long exposure <b>604</b> have a saturated value of 4095 and incorporate into the output image the values from the corresponding pixels in the short exposure <b>606</b>. As indicated, the image content along the line C-D includes relatively good highlight detail. In other cases, some degree of blending may be used for the portion of the output track corresponding to the segment C-D. For example, for the portion C-D, the blending module <b>117</b> may perform a weighted average or other operation that is similar to those described above with respect to the segment A-B. However, the weighted average may be biased in favor of the short track <b>606</b> instead of the long track <b>604</b> for the portion C-D, unlike for the portion A-B. Algorithm (iv) described below incorporates a blending function along these lines.</p>
<p id="p-0104" num="0103">Thus, according to such blending techniques, the blending module <b>117</b> uses the short exposure <b>606</b> to preserve the highlights in the resulting output track, correcting for any possible loss of highlight detail. The track that is made up of the short exposure frames may therefore be referred to as a &#x201c;highlight protection track&#x201d; or &#x201c;highlight correction track,&#x201d; for example.</p>
<p id="p-0105" num="0104">As indicated by the arrows, one or more of the points A, B, C and D can be adjusted to adjust the blending profile. This can greatly affect both the dynamic range and the motion characteristic of the resulting output track. For example, the lengths of the segments A-B and C-D can significantly alter the dynamic range of the image. Moreover, the motion effect can be tuned by adjusting length of the segment B-C or the particular blending operation (e.g., linear interpolation, weighted average, etc.) that is used on this portion. Specific motion control operations are described in greater detail below with respect to <figref idref="DRAWINGS">FIG. 8</figref>.</p>
<p id="p-0106" num="0105">Blending algorithm (ii) is similar to blending algorithm (i), except that, as indicated by the shorter line segment B-C, the blending module <b>117</b> blends together the long and short exposures 604, 606 for a much smaller portion of the output track. Moreover, as indicated by the relatively long line segment A-B, a larger portion of the long track <b>604</b> is included directly in the output track as compared as compared to algorithm (i).</p>
<p id="p-0107" num="0106">As shown by the charts <b>608</b>, <b>610</b> for algorithm (iii), a blending module <b>117</b> implementing algorithm (iii) applies a curve on the measured input values to generate the output track. While a variety of different curves are possible, the blending module <b>117</b> in the illustrated example applies an &#x201c;S&#x201d; curve over the portion of the output track corresponding to the pixel values between the points B and C. The curve may be a sigmoid curve, or some other type of curve. As with the previous examples, one or more of the points A, B, C, and D can be adjusted to change length of the various segments and corresponding blending profile. In one instance, the curve is applied over the entire output track.</p>
<p id="p-0108" num="0107">There may additionally be more than three portions (A-B, B-C, C-D) of the output track having discrete blending profiles. Algorithm (iv) shows such a case where there are separate blending profiles for five different portions of the output track: (1) A-B&#x2014;direct incorporation of the long track <b>604</b>; (2) B-C&#x2014;weighted average, where the long track <b>604</b> is weighted heavier than short track <b>606</b>; (3) C-D&#x2014;linear interpolation between long track <b>604</b> and short track <b>606</b>; (4) D-E&#x2014;weighted average, wherein the short track <b>606</b> is weighted heavier than the long track <b>604</b>; and (5) E-F&#x2014;direct incorporation of the short track <b>606</b>.</p>
<p id="p-0109" num="0108">Referring now to <figref idref="DRAWINGS">FIG. 6B</figref>, the illustrated blending operation is similar to that of <figref idref="DRAWINGS">FIG. 6A</figref> in that system again captures and blends together first (e.g., long) and second (e.g., long) closely timed exposures <b>604</b>, <b>606</b>. However, unlike the operation shown in <figref idref="DRAWINGS">FIG. 6A</figref>, <figref idref="DRAWINGS">FIG. 6B</figref> shows a configuration where the blending module <b>117</b> does not expand the bit-width of the output track as compared to the input tracks. Rather, the bit-width remains the same. As one example, the sensor <b>112</b> is a 12-bit sensor and thus outputs 12-bit long and short exposures <b>604</b>, <b>606</b>, although other bit-widths are possible. The blending module <b>117</b> performs light level matching by shifting the long exposure <b>604</b> down by a number (e.g., 2, 4, 8, etc.) of bits with respect to the long exposure <b>604</b>. However, unlike the operation shown in <figref idref="DRAWINGS">FIG. 4A</figref>, the shifted portion is preserved in the embodiment of <figref idref="DRAWINGS">FIG. 6B</figref>. The first and second tracks <b>604</b>, <b>606</b> are then blended together according to the selected algorithm, creating a 16-bit output track. Three example blending algorithms (i), (ii) and (iii) are shown, although a wide variety of other possibilities exist. As shown, the noisiest content in the long exposure <b>604</b> is discarded in some cases, while still preserving some detail in the shadows.</p>
<p id="p-0110" num="0109">While described with respect to a number of specific examples in <figref idref="DRAWINGS">FIGS. 6A-6B</figref>, the blending can occur in a variety of additional manners. For example, the blending can be based on contrast levels, as described below with respect to <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. 7</figref> shows a blending process <b>700</b> for creating an output exposure from multiple closely-timed exposures. Although described with respect to single first and second exposures for the purposes of illustration, the operations described below with respect to the process <b>700</b> may instead be performed on entire tracks of first and second exposures or portions thereof.</p>
<p id="p-0112" num="0111">At blocks <b>702</b>A, <b>702</b>B, the sensor <b>112</b> captures first (e.g., long) and second (e.g., long) exposures. At block <b>704</b>A, <b>704</b>B, the blending module <b>117</b> or other component processes the captured exposures.</p>
<p id="p-0113" num="0112">For example, referring to <figref idref="DRAWINGS">FIGS. 6A-6B</figref>, in some cases one or more of the exposures 604, 606 is adjusted before blending, e.g., to match the light levels between the two exposures. In some of these cases, the blending module <b>117</b> shifts one or both exposures <b>604</b>, <b>606</b> and therefore multiplies or divides the pixel values by a corresponding factor of two. However, sensor pixels in some cases output non-zero values even when no light is incident on the pixels (e.g., when the lens cap is on the sensor), creating &#x201c;non-zero black&#x201d; values. It can desirable to take this into account prior to the blending. For example, multiplying these &#x201c;non-zero black&#x201d; values in the light matching process can complicate later processing stages or produce other undesirable results.</p>
<p id="p-0114" num="0113">Thus, in one embodiment the module <b>117</b> subtracts an offset (e.g., a black offset) from the first and/or second exposures prior to the light matching. The module <b>117</b> then performs the light matching or other mathematical operation(s) on the exposures, e.g., multiplying each of the exposures by a predetermined factor. Finally, the blending module <b>117</b> can add an offset to the first and second exposures, such as a black offset having the same magnitude as the value that was subtracted prior to the light matching.</p>
<p id="p-0115" num="0114">In some configurations, at block <b>704</b>A, <b>704</b>B the blending module performs one or more operations on the first and second exposures to improve the compression of the image data. For example, in one instance, the image processing module <b>116</b> combines select (e.g., low) frequency components of wavelets from both the first and second exposures to improve compression efficiency. The image processing module <b>116</b> may also implement a pre-emphasis curve one or more of the exposures (e.g., the darker, shorter exposure). Example pre-emphasis functions are described in greater detail in the '967 patent (e.g., with respect to <figref idref="DRAWINGS">FIGS. 8</figref>, <b>8</b>A, <b>11</b> and columns 11-12 of the '967 patent) and are incorporated by reference herein. Such techniques can improve the efficiency or other quality of the compression. For example, in some cases, pre-emphasis or other techniques can avoid or reduce the amount of compression that occurs on lower bits of the particular exposure(s).</p>
<p id="p-0116" num="0115">At block <b>706</b>, the blending module <b>117</b> receives parameters for use in the blending operation. For example, the blending parameters generally define how the first and second exposures will be combined. In some cases, for example, the system <b>100</b> provides an interface to set one or more of the blending parameters. For example, the system <b>100</b> displays a graphical user interface (GUI) to the user that the user can interact with to set the parameter(s). The GUI or other interface can be provided via software on a display of an external computer, or may instead be provided on the camera, depending on the configuration.</p>
<p id="p-0117" num="0116">In one instance, the GUI includes a slider or one or more other icons that the user can manipulate to adjust the blending parameters. The graphic <b>707</b> is similar to those shown with respect to the algorithms of <figref idref="DRAWINGS">FIGS. 6A-6B</figref>. Accordingly, the positions of &#x201c;A,&#x201d; &#x201c;B,&#x201d; &#x201c;C,&#x201d; and &#x201c;D&#x201d; generally define how to blend the first and second exposures (&#x201c;EXP. 1,&#x201d; &#x201c;EXP. 2&#x201d;). As indicated in <figref idref="DRAWINGS">FIG. 7</figref>, the user in some cases be able to adjust the position of &#x201c;B&#x201d; and &#x201c;C&#x201d; using the interface, thereby setting how much of the first and second (or more) exposures to include in the resulting output track, and what portions of the tracks should be blended together. In some cases the user can adjust the position of &#x201c;A&#x201d; and/or &#x201c;D&#x201d; instead of or in addition to &#x201c;B&#x201d; and &#x201c;C.&#x201d; In addition to the illustrated example, a number of other compatible interfaces are possible. Moreover, the interface can allow users to adjust a variety of other parameters.</p>
<p id="p-0118" num="0117">In another configuration, the user can select a &#x201c;highlight protection&#x201d; value, which determines the amount of highlight protection to employ. Referring to <figref idref="DRAWINGS">FIGS. 6A-6B</figref>, the selected &#x201c;highlight protection&#x201d; value may generally the exposure (e.g., integration time) of the exposures <b>606</b> in the short track. In one embodiment, the user can select a number of stops (e.g., 2, 3, 4, 5, or 6 stops) of highlight protection to include. For example, if the user selects 2 stops of highlight protection, the exposure for the short exposure <b>606</b> track will be 2 stops less than that of the long track <b>604</b>. For example, where the long track <b>604</b> exposure value is 1/48 seconds, the exposure for the short track <b>606</b> will be set to 1/192 seconds.</p>
<p id="p-0119" num="0118">In another configuration, the system provides the user with a menu of pre-set algorithms which each have a particular creative effect. Each of the pre-set algorithms have pre-determined blending parameters (e.g, &#x201c;B&#x201d; and &#x201c;C&#x201d; values). The particular pre-set algorithms may also determine what type of processing occurs in blocks <b>704</b>A, <b>704</b>B (e.g., linear interpolation, weighted average, &#x201c;S&#x201d; curve, etc.) in some cases.</p>
<p id="p-0120" num="0119">At block <b>708</b>, the blending module <b>117</b> performs the blending operation based on the selected blending parameters. For example, the blending module <b>117</b> can perform the blending generally in the manner described above with respect to <figref idref="DRAWINGS">FIGS. 5-6B</figref>. Finally, at block <b>710</b>, the blending module <b>117</b> outputs the blended exposure (or track of exposures). For example, the blended exposures are provided for storage and/or playback to the user.</p>
<p id="p-0121" num="0120">In some embodiments, the system <b>100</b> provides the user with generally real-time feedback. For example, as the user changes one or more blending parameters or selects a different pre-set blending algorithm, the system plays back the blended output track or a portion thereof for user review on a camera or computer monitor. Thus, the user can make adjustments on the fly to achieve the desired visual effect.</p>
<p id="p-0122" num="0121">Returning to block <b>708</b>, in some embodiments the blending module <b>117</b> blends the closely-timed tracks by determining, according to the selected algorithm, which portions of each track are properly exposed. For example, for each exposure, the blending module <b>117</b> flags the properly exposed pixels for inclusion in the combined output image. According to another embodiment, the blending module <b>117</b> compares pixels from one exposure to corresponding pixel(s) in the other exposure(s), and according to a predetermined algorithm, calculates the corresponding pixel value in the blended output.</p>
<p id="p-0123" num="0122">As described, any of the blending techniques can generally be implemented on a frame-by-frame basis, for example, or at some other granularity, and a wide variety of compatible blending techniques are possible. In certain configurations, more than one frame from the first and/or second (or more) tracks are compared or otherwise analyzed in generating the output track. For example, in such cases the blending module <b>117</b> compares frames n&#x2212;w, . . . , n+x from the first track to frames n&#x2212;y, . . . n+z from the second track to generate frame n in the output image stream, where n is the current output track frame.</p>
<p id="h-0009" num="0000">Controlling Motion Effect</p>
<p id="p-0124" num="0123">As discussed, the image capture techniques described herein efficiently use the frame period to provide tracks having different exposures levels within individual frames. The inter-exposure delay is relatively small, resulting in substantially reduced or no temporal gap between the differently exposed tracks. For example, there may be substantially no visual separation between objects in one track as compared to the same objects in another track. Moreover, as discussed, the inter-exposure skew for the individual tracks is relatively minimal. According to certain aspects, the system exploits these and other aspects to control the motion effect in the combined output track, such as the amount or quality of blur, in addition to providing the dynamic range benefits described herein. For example, motion artifacts in the combined output stream can be substantially reduced, removed, or controlled in a desired fashion. Additionally, some techniques described herein reduce or eliminate the amount of motion artifact detection and processing that can be necessary with some conventional approaches.</p>
<p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. 8</figref> shows a process <b>800</b> of providing an output image track having controlled motion effect. At block <b>802</b>, the blending module <b>117</b> receives at least two closely-timed tracks, which can be recorded according to any of the techniques described herein. For example, the tracks may be any of those shown and described in <figref idref="DRAWINGS">FIGS. 3A-6B</figref>. At block <b>804</b>, the blending module <b>117</b> receives a set of motion control parameters. These parameters generally define the motion control algorithm that the blending module <b>117</b> performs when selectively combining tracks. As with the dynamic range blending parameters (e.g., amount of &#x201c;highlight protection&#x201d;) described above, the motion control parameters may be user-selectable or fixed, depending on the implementation. In some cases, the user is provided a menu having a list of various motion control techniques to choose from. These may be any of the techniques described below with respect to block <b>806</b>, or some other technique(s).</p>
<p id="p-0126" num="0125">At block <b>806</b>, the process <b>800</b> combines the tracks of closely-timed images to create an output image stream having the desired motion effect. A variety of motion control techniques are possible.</p>
<p id="p-0127" num="0126">Where multiple, tracks of closely-timed images are recorded at different exposure levels, one of the tracks may be relatively blurry, similar to conventional video recordings. For example, significant blurring may be present in relatively long integration time tracks, such as those shown in <figref idref="DRAWINGS">FIGS. 3A-4C</figref> and <b>6</b>A-<b>6</b>B. However, other tracks may be sharper and include reduced or minimal blurring. For example, the short integration time exposures shown in <figref idref="DRAWINGS">FIGS. 3A-4C</figref> and <b>6</b>A-<b>6</b>B may be relatively sharper. Typical video recordings (e.g., 24 fps with a 180 degree shutter) show a generally constant blurring for moving objects, without sharper references. However, what the human eye sees when viewing the scene in person is closer to a combination of both the blurred references to the objects as well as discrete, sharper references to the moving objects.</p>
<p id="p-0128" num="0127">To mimic this effect, in some embodiments, the blending module <b>117</b> uses both the more blurry track(s) and the sharper track(s) to control the motion effect, create an output track that includes both the blurred and sharp references to moving objects. For instance, referring again to <figref idref="DRAWINGS">FIGS. 6A-6B</figref>, the amount or quality of the motion effect included in the output track can be controlled by adjusting the blending parameters, such as the positions of A, B, C, D or the type of blending operations used. As one example, a user can control the amount of blurred and sharp references represented in the output track by adjusting the position of B and C, thereby adjusting the length and position of the line segments A-B, B-C, and C-D. In particular, referring to algorithm (i) shown in <figref idref="DRAWINGS">FIG. 6A</figref>, for increasing measured pixel values along the line B-C where the blending module <b>117</b> implements an interpolation function, the output track will tend to include a varying mixture of blurred references and sharper references. Specifically, the output track along the line B-C includes an increasingly significant portion of image content from the short track <b>606</b> and a correspondingly lesser portion of the long track <b>604</b>. As such, portions of the output track corresponding to the line segment B-C will include a varying mixture of relatively more blurred references to image scene objects from the long track <b>604</b> and relatively sharper references to the same image scene objects from the short track <b>606</b>. In this manner, the blending module <b>117</b> can create an output track that more closely mimics what the human eye would see if watching the scene in person. The system or a user can additionally adjust the positions of B and C to modify the effect.</p>
<p id="p-0129" num="0128">In addition, gradually shifting the relative contributions of the larger track <b>604</b> and the shorter track <b>606</b> (or vice versa) in an incremental manner, such as by using linear interpolation or a curve function, can help limit or prevent any banding or other undesirable visual artifacts that may occur from a more abrupt change in the relative contributions of the two tracks. In some cases, the gradual shifting can also help to reduce the amount of lower SNR image content that is incorporated in the output track (e.g., from the shorter track <b>606</b>), improving image quality.</p>
<p id="p-0130" num="0129">In another example, the blending module <b>117</b> implements a weighted average of the long and short exposures <b>604</b>, <b>606</b> for portions of the output track along the line B-C. For example, a 50/50 weighted average may result in an output track having a generally even mixture of sharper content from the short track <b>606</b> and blurred content from the longer track <b>604</b>. The weightings can be adjusted to tailor the effect. In one embodiment, the system provides the user with an interface to control how much of the motion effect from each track to include in the resulting output track. Generally, the system can employ a wide variety of other functions instead of or in combination with the illustrated ones.</p>
<p id="p-0131" num="0130">In yet further configurations, the blending module <b>117</b> can set the relative weighting of the short and long tracks <b>604</b>, <b>606</b> in the output track based on a contrast level. For instance, the blending module <b>117</b> processes measured input values of the long track <b>604</b>, the short track <b>606</b>, or a combination thereof to determine a contrast level in particular image regions, and uses the determined contrast level to select the relative weighting between the long track <b>604</b> and the short track <b>606</b> for pixels within those image regions. For example, the blending module <b>117</b> applies a high-pass filter on the image data for the short track <b>606</b> (and/or long track <b>606</b>) to determine high contrast regions. For relatively high contrast regions, the blending module <b>117</b> weights the short track <b>606</b> heavier than the long track <b>604</b> in the output track. For relatively lower contrast regions, the blending module <b>117</b> may weight the long track <b>604</b> higher, or weight the two tracks equally. In this manner, the resulting output track can include both sharp references to moving objects (from the high contrast regions of the short track <b>606</b>) and blurred references to moving objects (from adjacent, relatively lower contrast regions of the long track <b>604</b>). The blending module <b>117</b> can identify lower contrast regions instead of, or in addition to, higher contrast regions, by performing a low pass filter on the long and/or short exposures <b>604</b>, <b>606</b>, for example.</p>
<p id="p-0132" num="0131">Moreover, for portions of the output track corresponding to the line segments A-B and/or C-D, a user in some embodiments adjusts the motion effect by controlling the amount of each image to include in the output track. For example, the user can adjust the bias value in a weighted average blend operation to control the weight each input track is given in the blending operation. For example, the system may provide a slider, other GUI, or another input mechanism to allow the user tailor the effect.</p>
<p id="p-0133" num="0132">In another embodiment, the blending module <b>117</b> is configured to blend the tracks together in a manner that more closely matches the motion effect of a traditional camera (e.g., 24 fps with a 180 degree shutter). For example, a customized motion estimation algorithm may be used to match the motion blur of one or more of the tracks to that of a traditional camera. For example, referring to <figref idref="DRAWINGS">FIG. 3A</figref>, the blur in the shorter integration time track <b>302</b> may be increased to match that of the longer integration time track <b>304</b>. In one embodiment, the blur matching is done before the blending of the exposures, although it can be done afterwards in other implementations.</p>
<p id="p-0134" num="0133">At block <b>908</b>, the blending module <b>117</b> outputs the motion-adjusted track for playback or further processing.</p>
<p id="h-0010" num="0000">Terminology/Additional Embodiments</p>
<p id="p-0135" num="0134">Embodiments have been described in connection with the accompanying drawings. However, it should be understood that the figures are not drawn to scale. Distances, angles, etc. are merely illustrative and do not necessarily bear an exact relationship to actual dimensions and layout of the devices illustrated. In addition, the foregoing embodiments have been described at a level of detail to allow one of ordinary skill in the art to make and use the devices, systems, etc. described herein. A wide variety of variation is possible. Components, elements, and/or steps can be altered, added, removed, or rearranged. While certain embodiments have been explicitly described, other embodiments will become apparent to those of ordinary skill in the art based on this disclosure.</p>
<p id="p-0136" num="0135">Conditional language used herein, such as, among others, &#x201c;can,&#x201d; &#x201c;could,&#x201d; &#x201c;might,&#x201d; &#x201c;may,&#x201d; &#x201c;e.g.,&#x201d; and the like, unless specifically stated otherwise, or otherwise understood within the context as used, is generally intended to convey that certain embodiments include, while other embodiments do not include, certain features, elements and/or states. Thus, such conditional language is not generally intended to imply that features, elements and/or states are in any way required for one or more embodiments or that one or more embodiments necessarily include logic for deciding, with or without author input or prompting, whether these features, elements and/or states are included or are to be performed in any particular embodiment.</p>
<p id="p-0137" num="0136">Depending on the embodiment, certain acts, events, or functions of any of the methods described herein can be performed in a different sequence, can be added, merged, or left out all together (e.g., not all described acts or events are necessary for the practice of the method). Moreover, in certain embodiments, acts or events can be performed concurrently, e.g., through multi-threaded processing, interrupt processing, or multiple processors or processor cores, rather than sequentially. In some embodiments, the algorithms disclosed herein can be implemented as routines stored in a memory device. Additionally, a processor can be configured to execute the routines. In some embodiments, custom circuitry may be used.</p>
<p id="p-0138" num="0137">The various illustrative logical blocks, modules, circuits, and algorithm steps described in connection with the embodiments disclosed herein can be implemented as electronic hardware, computer software, or combinations of both. To clearly illustrate this interchangeability of hardware and software, various illustrative components, blocks, modules, circuits, and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. The described functionality can be implemented in varying ways for each particular application, but such implementation decisions should not be interpreted as causing a departure from the scope of the disclosure.</p>
<p id="p-0139" num="0138">The various illustrative logical blocks, modules, and circuits described in connection with the embodiments disclosed herein can be implemented or performed with a general purpose processor, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein. A general purpose processor can be a microprocessor, but in the alternative, the processor can be any conventional processor, controller, microcontroller, or state machine. A processor can also be implemented as a combination of computing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration.</p>
<p id="p-0140" num="0139">The blocks of the methods and algorithms described in connection with the embodiments disclosed herein can be embodied directly in hardware, in a software module executed by a processor, or in a combination of the two. A software module can reside in RAM memory, flash memory, ROM memory, EPROM memory, EEPROM memory, registers, a hard disk, a removable disk, a CD-ROM, or any other form of computer-readable storage medium known in the art. An exemplary storage medium is coupled to a processor such that the processor can read information from, and write information to, the storage medium. In the alternative, the storage medium can be integral to the processor. The processor and the storage medium can reside in an ASIC. The ASIC can reside in a user terminal. In the alternative, the processor and the storage medium can reside as discrete components in a user terminal.</p>
<p id="p-0141" num="0140">While the above detailed description has shown, described, and pointed out novel features as applied to various embodiments, it will be understood that various omissions, substitutions, and changes in the form and details of the devices or algorithms illustrated can be made without departing from the spirit of the disclosure. As will be recognized, certain embodiments of the inventions described herein can be embodied within a form that does not provide all of the features and benefits set forth herein, as some features can be used or practiced separately from others. The scope of certain inventions disclosed herein is indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625013-20140107-M00001.NB">
<img id="EMI-M00001" he="6.35mm" wi="76.20mm" file="US08625013-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is;</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of obtaining image data using at least one picture element array, the method comprising:
<claim-text>using at least one picture element array comprising a plurality of picture elements, capturing light corresponding to a first image at a first exposure level;</claim-text>
<claim-text>using the picture element array, capturing light corresponding to a second image at a second exposure level different than the first exposure level, wherein individual ones of the picture elements complete capturing the light corresponding to the first image prior to when those individual picture elements begin to capture the light corresponding to the second image;</claim-text>
<claim-text>reading out first digital measurements corresponding to the light captured by the picture element array for the first image; and</claim-text>
<claim-text>reading out second digital measurements corresponding to the light captured by the picture element array for the second image,</claim-text>
<claim-text>wherein the step of capturing light corresponding to the second image begins prior to the completion of the step of reading out the first digital measurements,</claim-text>
<claim-text>further wherein the amount of time that elapses from between substantial completion of said reading out the first digital measurements to substantial completion of said reading out the second digital measurements is greater than the amount of time from between completing readout of a first of the picture elements for the first image to completing readout of substantially all of the picture elements for the first image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of picture elements form a set of rows including subsets of one or more rows, and wherein said capturing light corresponding to a first image and said capturing light corresponding to a second image occur for the picture elements in each subset together in time as a group.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, for respective individual picture elements of the plurality of picture elements, there is an inter-exposure delay of less than approximately 100 picture element reset times between reading out a digital measurement of the first digital measurements captured by the individual picture element for the first image and beginning to capture light corresponding to the second image at the second exposure level by the individual picture element.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the inter-exposure delay is less than approximately 10 picture element reset times.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the inter-exposure delay is approximately one picture element reset time.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first and second images are captured using a rolling shutter technique.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein an inter-picture element delay from when said capturing light corresponding to the first image by a particular subset of one or more rows of picture elements begins and when the capturing of light corresponding to the first image by the subsequent subset of one or more rows of picture elements begins is less than approximately two picture element read out times.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the inter-picture element delay is approximately one picture element readout time.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first image and the second image correspond to a first frame in a sequence of motion video frames, the method further comprising:
<claim-text>using the picture element array, capturing light corresponding to a third image at the first exposure level;</claim-text>
<claim-text>using the the picture element array, capturing light corresponding to a fourth image at the second exposure level, wherein individual ones of the picture elements complete capturing the light corresponding to the third image prior to when those individual picture elements begin to capture the light corresponding to the fourth image, wherein the third image and the fourth image correspond to a second frame in the sequence of motion video frames;</claim-text>
<claim-text>reading out third digital measurements corresponding to the light captured by the picture element array for the third image; and</claim-text>
<claim-text>reading out fourth digital measurements corresponding to the light captured by the picture element array for the fourth image,</claim-text>
<claim-text>wherein the step of capturing light corresponding to the fourth image begins prior to the completion of the step of reading out the third digital measurements,</claim-text>
<claim-text>further wherein the amount of time that elapses from between substantial completion of said reading out the third digital measurements to substantial completion of said reading out the fourth digital measurements is greater than the amount of time from between completing readout of a first of the picture elements for the third image to completing readout of substantially all of the picture elements for the third image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the first and second images are selectively combined to create a first combined image having a different dynamic range than either of the first and second images and wherein the third and fourth images are combined together to create a second combined image having a different dynamic range than either of the third and fourth images.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. An imaging system, comprising:
<claim-text>at least one picture element array comprising a plurality of picture elements; and</claim-text>
<claim-text>a controller configured to:
<claim-text>capture light corresponding to a first image at a first exposure level with the picture element array;</claim-text>
<claim-text>capture light corresponding to a second image at a second exposure level different than the first exposure level with the picture element array, wherein individual ones of the picture elements complete capturing the light corresponding to the first image prior to when those individual picture elements begin to capture the light corresponding to the second image;</claim-text>
<claim-text>read out first digital measurements corresponding to the light captured by the picture element array for the first image; and</claim-text>
<claim-text>read out second digital measurements corresponding to the light captured by the picture element array for the second image,</claim-text>
<claim-text>wherein the controller begins the capturing of the light corresponding to the second image prior to the completion of the reading out the first digital measurements,</claim-text>
<claim-text>further wherein the amount of time that elapses from between substantial completion of the reading out of the first digital measurements to substantial completion of the reading out of the second digital measurements is greater than the amount of time from between completing readout of a first of the picture elements for the first image to completing readout of substantially all of the picture elements for the first image.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The imaging system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the plurality of picture elements form a set of rows including subsets of one or more rows, and wherein the capturing light corresponding to the first image and the capturing light corresponding to the second image occur for the picture elements in each subset together in time as a group.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The imaging system of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein, for respective individual picture elements of the plurality of picture elements, there is an inter-exposure delay of less than approximately 100 picture element reset times between when the control circuitry reads out a digital measurement of the first digital measurements captured by the individual picture element for the first image and begins to capture light corresponding to the second image at the second exposure level by the individual picture element.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The imaging system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the inter-exposure delay is less than approximately 10 picture element reset times.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The imaging system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the inter-exposure delay is approximately one picture element reset time.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The imaging system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the control circuitry is configured to capture the first and second images using a rolling shutter technique.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The imaging system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein an inter-picture element delay from when the capturing of light corresponding to the first image by a particular subset of one or more rows of picture elements begins and when the capturing of light corresponding to the first image by the subsequent subset of one or more rows of picture elements begins is less than approximately two picture element read out times.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The imaging system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the inter-picture element delay is approximately one picture element readout time.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The imaging system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first image and the second image correspond to a first frame in a sequence of motion video frames, the controller further configured to:
<claim-text>capture light corresponding to a third image at the first exposure level with the picture element array;</claim-text>
<claim-text>capture light corresponding to a fourth image at the second exposure level with the picture element array, wherein individual ones of the picture elements complete capturing the light corresponding to the third image prior to when those individual picture elements begin to capture the light corresponding to the fourth image, wherein the third image and the fourth image correspond to a second frame in the sequence of motion video frames;</claim-text>
<claim-text>read out third digital measurements corresponding to the light captured by the picture element array for the third image; and</claim-text>
<claim-text>read out fourth digital measurements corresponding to the light captured by the picture element array for the fourth image,</claim-text>
<claim-text>wherein the control circuitry begins the capturing of the light corresponding to the fourth image prior to the completion of the reading out the third digital measurements,</claim-text>
<claim-text>further wherein the amount of time that elapses from between substantial completion of the reading out of the third digital measurements to substantial completion of the reading out of the fourth digital measurements is greater than the amount of time from between completing readout of a first of the picture elements for the third image to completing readout of substantially all of the picture elements for the third image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The imaging system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the first and second images are selectively combined to create a first combined image having a different dynamic range than either of the first and second images and wherein the third and fourth images are combined together to create a second combined image having a different dynamic range than either of the third and fourth images.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. A method of obtaining image data using at least one digital imaging sensor, the method comprising:
<claim-text>capturing a first image with at least one digital imaging sensor at a first exposure level using electronic rolling shutter technique, the sensor comprising a plurality of picture elements;</claim-text>
<claim-text>capturing a second image with the sensor at a second exposure level different from the first exposure level electronic rolling shutter technique;</claim-text>
<claim-text>outputting from the sensor digital measurements corresponding to the first image; and</claim-text>
<claim-text>outputting from the sensor digital measurements corresponding to the second image,</claim-text>
<claim-text>wherein said capturing of the first image begins before said capturing of the second image begins,</claim-text>
<claim-text>further wherein said capturing of the first image and said capturing of the second image overlap in time, and</claim-text>
<claim-text>further wherein the amount of time that elapses from between when said outputting from the sensor digital measurements corresponding to the first image substantially completes to when said outputting from the sensor digital measurements corresponding to the second image substantially completes is greater than the amount of time from between completing output of a digital measurement corresponding to a first of the picture elements for the first image to completing output of the digital measurements of substantially all of the picture elements for the first image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. A method of combining image data obtained using at least one picture element array, the method comprising:
<claim-text>as implemented by one or more hardware processors,
<claim-text>obtaining first image data corresponding to a first image and derived from first digital measurements corresponding to light captured by at least one picture element array comprising a plurality of picture elements at a first exposure level;</claim-text>
<claim-text>obtaining second image data corresponding to a second image and derived from second digital measurements corresponding to light captured by the picture element array at a second exposure level different than the first exposure level, wherein individual ones of the picture elements completed capturing the light corresponding to the first image prior to when those individual picture elements begin to capture the light corresponding to the second image; and</claim-text>
<claim-text>selectively combining the first image data and the second image data to create combined image data corresponding to a combined image;</claim-text>
</claim-text>
<claim-text>wherein the step of capturing light corresponding to the second image began prior to the completion of reading out the first digital measurements,</claim-text>
<claim-text>further wherein the amount of time that elapsed from between substantial completion of reading out the first digital measurements to substantial completion of reading out the second digital measurements is greater than the amount of time from between completing readout of a first of the picture elements for the first image to completing readout of substantially all of the picture elements for the first image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the first image and the second image correspond to a first frame in a sequence of motion video frames, the method further comprising:
<claim-text>obtaining third image data corresponding to a third image and derived from third digital measurements corresponding to light captured by the picture element array at the first exposure level;</claim-text>
<claim-text>obtaining fourth image data corresponding to a fourth image and derived from fourth digital measurements corresponding to light captured by the picture element array at the second exposure level, wherein individual ones of the picture elements completed capturing the light corresponding to the third image prior to when those individual picture elements began to capture the light corresponding to the fourth image, wherein the third image and the fourth image correspond to a second frame in the sequence of motion video frames; and</claim-text>
<claim-text>selectively combining the third image data and the fourth image data to create combined image data corresponding to a second combined image;</claim-text>
<claim-text>wherein the step of capturing light corresponding to the fourth image began prior to the completion of reading out the third digital measurements,</claim-text>
<claim-text>further wherein the amount of time that elapsed from between substantial completion of reading out the third digital measurements to substantial completion of reading out the fourth digital measurements is greater than the amount of time from between completing readout of a first of the picture elements for the third image to completing readout of substantially all of the picture elements for the third image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, further comprising blending together content from the first image with content from the second image for at least some of the combined image by varying the relative amounts of first image content and second image content included in the blended content in response to picture element values corresponding to one or more of the first image and the second image.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, further comprising adjusting the amount of blended content to include in the combined image in response to user input.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein said blending together content comprises discarding relatively low signal-to-noise content from one or more of the first and second images.</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. A system for blending image data obtained using at least one picture element array, the system comprising:
<claim-text>one or more hardware processors;</claim-text>
<claim-text>a blending module executing on the one or more hardware processors and configured to:
<claim-text>obtain first image data corresponding to a first image and derived from first digital measurements corresponding to light captured by at least one picture element array comprising a plurality of picture elements at a first exposure level;</claim-text>
<claim-text>obtain second image data corresponding to a second image and derived from second digital measurements corresponding to light captured by the picture element array at a second exposure level different than the first exposure level, wherein individual ones of the picture elements completed capturing the light corresponding to the first image prior to when those individual picture elements began to capture the light corresponding to the second image; and</claim-text>
<claim-text>selectively combine the first image data and the second image data to create combined image data corresponding to a combined image;</claim-text>
</claim-text>
<claim-text>wherein the capturing of light corresponding to the second image began prior to the completion of reading out the first digital measurements,</claim-text>
<claim-text>further wherein the amount of time that elapsed from between substantial completion of reading out the first digital measurements to substantial completion of reading out the second digital measurements is greater than the amount of time from between completing readout of a first of the picture elements for the first image to completing readout of substantially all of the picture elements for the first image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the first image and the second image correspond to a first frame in a sequence of motion video frames, the blending module further configured to:
<claim-text>obtain third image data corresponding to a third image and derived from third digital measurements corresponding to light captured by the picture element array at the first exposure level;</claim-text>
<claim-text>obtain fourth image data corresponding to a fourth image and derived from fourth digital measurements corresponding to light captured by the picture element array at the second exposure level, wherein individual ones of the picture elements completed capturing the light corresponding to the third image prior to when those individual picture elements begin to capture the light corresponding to the fourth image, wherein the third image and the fourth image correspond to a second frame in the sequence of motion video frames; and</claim-text>
<claim-text>selectively combine the third image data and the fourth image data to create combined image data corresponding to a second combined image;</claim-text>
<claim-text>wherein capturing light corresponding to the fourth image began prior to the completion of reading out the third digital measurements,</claim-text>
<claim-text>further wherein the amount of time that elapsed from between substantial completion of reading out the third digital measurements to substantial completion of reading out the fourth digital measurements is greater than the amount of time from between completing readout of a first of the picture elements for the third image to completing readout of substantially all of the picture elements for the third image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the blending module is further configured to blend together content from the first image with content from the second image for at least some of the combined image by varying the relative amounts of first image content and second image content included in the blended content in response to picture element values corresponding to one or more of the first image and the second image.</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the blending module is further configured to adjust the amount of blended content to include in the combined image in response to user input.</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the blending module performs the blending together of the content at least partly by discarding relatively low signal-to-noise content from one or more of the first and second images. </claim-text>
</claim>
</claims>
</us-patent-grant>
