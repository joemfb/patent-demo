<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626685-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626685</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12497813</doc-number>
<date>20090706</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>P2008-183019</doc-number>
<date>20080714</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>662</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>E</subclass>
<main-group>1</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>E</subclass>
<main-group>3</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>15</main-group>
<subgroup>18</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>G</subclass>
<main-group>7</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>706 19</main-classification>
<further-classification>704207</further-classification>
<further-classification>706 45</further-classification>
<further-classification>706 22</further-classification>
<further-classification>706 15</further-classification>
</classification-national>
<invention-title id="d2e71">Information processsing apparatus, information processing method, and program</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>2002/0078429</doc-number>
<kind>A1</kind>
<name>Yoshida</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>716 21</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2005/0007159</doc-number>
<kind>A1</kind>
<name>Kondo et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>327100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2005/0246165</doc-number>
<kind>A1</kind>
<name>Pettinelli et al.</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704207</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>JP</country>
<doc-number>2007-122186</doc-number>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>WO</country>
<doc-number>WO 2007/049641</doc-number>
<kind>A1</kind>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00006">
<othercit>&#x201c;A New Facial Feature Extraction Method Based on Linear Combination Model&#x201d; Yongli Hu, Baocai Yin, Dehui Kong Multimedia and Intelligent Software Technology Lab Beijing University of Technology, Beijing, China 100022 &#xa9; 2003 IEEE.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00007">
<othercit>Notification of Reasons for Refusal of Japanese Patent Application No. JP 2008-183019, (May 25, 2010).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>16</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>58</number-of-drawing-sheets>
<number-of-figures>65</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100010947</doc-number>
<kind>A1</kind>
<date>20100114</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kobayashi</last-name>
<first-name>Yoshiyuki</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Kobayashi</last-name>
<first-name>Yoshiyuki</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Finnegan, Henderson, Farabow, Garrett &#x26; Dunner, L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Sony Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Chaki</last-name>
<first-name>Kakali</first-name>
<department>2122</department>
</primary-examiner>
<assistant-examiner>
<last-name>Sitiriche</last-name>
<first-name>Luis</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An information processing apparatus for configuring algorithms is disclosed. The information processing apparatus includes an algorithm configuring section that configures an algorithm for performing discrimination on an input signal by using a genetic search technique. The algorithm includes feature extraction expressions and an information estimation expression represented by a combination of the feature extraction expressions. The information processing apparatus also includes a tradeoff analyzing section that determines pareto optimal solutions by optimizing the algorithm with respect to evaluation indices by performing tradeoff analysis on the basis of the algorithm. In addition, the information processing apparatus includes a storage for storing the algorithm.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="90.68mm" wi="252.22mm" file="US08626685-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="227.25mm" wi="159.09mm" orientation="landscape" file="US08626685-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="222.59mm" wi="92.96mm" orientation="landscape" file="US08626685-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="249.43mm" wi="122.43mm" orientation="landscape" file="US08626685-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="156.46mm" wi="94.57mm" orientation="landscape" file="US08626685-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="252.31mm" wi="155.45mm" orientation="landscape" file="US08626685-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="250.36mm" wi="152.48mm" orientation="landscape" file="US08626685-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="178.56mm" wi="111.17mm" orientation="landscape" file="US08626685-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="213.70mm" wi="102.53mm" orientation="landscape" file="US08626685-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="193.80mm" wi="172.97mm" file="US08626685-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="240.11mm" wi="181.53mm" file="US08626685-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="238.76mm" wi="136.65mm" orientation="landscape" file="US08626685-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="190.50mm" wi="111.51mm" orientation="landscape" file="US08626685-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="96.86mm" wi="149.52mm" file="US08626685-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="211.33mm" wi="111.76mm" orientation="landscape" file="US08626685-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="251.38mm" wi="116.76mm" orientation="landscape" file="US08626685-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="202.10mm" wi="111.76mm" orientation="landscape" file="US08626685-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="223.27mm" wi="157.48mm" orientation="landscape" file="US08626685-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="251.04mm" wi="130.30mm" orientation="landscape" file="US08626685-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="229.87mm" wi="121.41mm" orientation="landscape" file="US08626685-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="252.73mm" wi="142.24mm" orientation="landscape" file="US08626685-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="190.16mm" wi="167.05mm" file="US08626685-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="236.47mm" wi="172.30mm" file="US08626685-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="188.55mm" wi="166.03mm" file="US08626685-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="200.74mm" wi="125.65mm" orientation="landscape" file="US08626685-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="240.37mm" wi="119.13mm" orientation="landscape" file="US08626685-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="244.77mm" wi="117.35mm" orientation="landscape" file="US08626685-20140107-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="246.46mm" wi="147.57mm" orientation="landscape" file="US08626685-20140107-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="225.55mm" wi="125.31mm" orientation="landscape" file="US08626685-20140107-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="208.03mm" wi="94.57mm" orientation="landscape" file="US08626685-20140107-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="199.73mm" wi="129.37mm" orientation="landscape" file="US08626685-20140107-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00031" num="00031">
<img id="EMI-D00031" he="203.79mm" wi="129.96mm" orientation="landscape" file="US08626685-20140107-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00032" num="00032">
<img id="EMI-D00032" he="215.65mm" wi="139.53mm" orientation="landscape" file="US08626685-20140107-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00033" num="00033">
<img id="EMI-D00033" he="207.35mm" wi="136.57mm" orientation="landscape" file="US08626685-20140107-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00034" num="00034">
<img id="EMI-D00034" he="239.44mm" wi="112.10mm" orientation="landscape" file="US08626685-20140107-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00035" num="00035">
<img id="EMI-D00035" he="209.97mm" wi="136.57mm" file="US08626685-20140107-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00036" num="00036">
<img id="EMI-D00036" he="171.37mm" wi="164.08mm" file="US08626685-20140107-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00037" num="00037">
<img id="EMI-D00037" he="188.89mm" wi="120.65mm" file="US08626685-20140107-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00038" num="00038">
<img id="EMI-D00038" he="199.39mm" wi="129.96mm" file="US08626685-20140107-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00039" num="00039">
<img id="EMI-D00039" he="160.10mm" wi="125.98mm" file="US08626685-20140107-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00040" num="00040">
<img id="EMI-D00040" he="234.87mm" wi="169.33mm" file="US08626685-20140107-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00041" num="00041">
<img id="EMI-D00041" he="192.53mm" wi="159.09mm" file="US08626685-20140107-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00042" num="00042">
<img id="EMI-D00042" he="212.01mm" wi="122.68mm" file="US08626685-20140107-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00043" num="00043">
<img id="EMI-D00043" he="189.23mm" wi="126.66mm" file="US08626685-20140107-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00044" num="00044">
<img id="EMI-D00044" he="246.04mm" wi="176.28mm" file="US08626685-20140107-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00045" num="00045">
<img id="EMI-D00045" he="234.10mm" wi="170.26mm" file="US08626685-20140107-D00045.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00046" num="00046">
<img id="EMI-D00046" he="249.43mm" wi="179.32mm" file="US08626685-20140107-D00046.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00047" num="00047">
<img id="EMI-D00047" he="236.14mm" wi="155.45mm" orientation="landscape" file="US08626685-20140107-D00047.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00048" num="00048">
<img id="EMI-D00048" he="120.06mm" wi="136.57mm" file="US08626685-20140107-D00048.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00049" num="00049">
<img id="EMI-D00049" he="244.77mm" wi="139.28mm" orientation="landscape" file="US08626685-20140107-D00049.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00050" num="00050">
<img id="EMI-D00050" he="242.74mm" wi="137.24mm" orientation="landscape" file="US08626685-20140107-D00050.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00051" num="00051">
<img id="EMI-D00051" he="258.57mm" wi="155.11mm" orientation="landscape" file="US08626685-20140107-D00051.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00052" num="00052">
<img id="EMI-D00052" he="250.36mm" wi="143.85mm" orientation="landscape" file="US08626685-20140107-D00052.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00053" num="00053">
<img id="EMI-D00053" he="245.45mm" wi="126.66mm" orientation="landscape" file="US08626685-20140107-D00053.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00054" num="00054">
<img id="EMI-D00054" he="193.12mm" wi="129.62mm" file="US08626685-20140107-D00054.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00055" num="00055">
<img id="EMI-D00055" he="196.77mm" wi="147.83mm" file="US08626685-20140107-D00055.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00056" num="00056">
<img id="EMI-D00056" he="233.17mm" wi="175.94mm" file="US08626685-20140107-D00056.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00057" num="00057">
<img id="EMI-D00057" he="231.14mm" wi="144.86mm" file="US08626685-20140107-D00057.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00058" num="00058">
<img id="EMI-D00058" he="215.65mm" wi="152.15mm" orientation="landscape" file="US08626685-20140107-D00058.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">The present invention relates to an information processing apparatus, an information processing method, and a program. More particularly, the present invention relates to an information processing apparatus, an information processing method, and a program which can present multiple algorithms having a tradeoff relationship in evaluation indices.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">There has been proposed a technology (automatic algorithm configuration technology) for automatically configuring an amount-of-feature extraction algorithm for extracting the amount of features representing features of given data by using a genetic search technique. In the automatic algorithm configuration technology, an algorithm for speech recognition, image recognition, and so on can be automatically configured.</p>
<p id="p-0006" num="0005">For example, the present assignee has proposed a technology for configuring an algorithm that can quickly extract the amount of features with high accuracy from content data, such as music data (e.g., refer to International Patent Publication No. WO2007/049641).</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0007" num="0006">In the automatic algorithm-configuration technology, only a highest performance (a highest accuracy) algorithm is output. Thus, for example, when two evaluation indices such as performance (accuracy) and speed (processing time) exist, it is difficult to obtain an algorithm that operates at high speed even though the performance may decline somewhat. Although one of the evaluation indices may be reduced once an algorithm is configured, the technology of the related art typically may not deal with changes, such as a desire to increase one of the evaluation indices.</p>
<p id="p-0008" num="0007">The present invention has been conceived in view of such a situation, and it is desirable to make it possible to present multiple algorithms having a tradeoff relationship in evaluation indices.</p>
<p id="p-0009" num="0008">According to one embodiment of the present invention, there is provided an information processing apparatus. The information processing apparatus includes: algorithm configuring means for configuring an algorithm for performing discrimination on an input signal by using a genetic search technique; and tradeoff analyzing means for determining Pareto optimal solutions of the algorithm with respect to evaluation indices by performing tradeoff analysis on the basis of the algorithm.</p>
<p id="p-0010" num="0009">The information processing apparatus may further include optimum-algorithm determining means for determining, of the Pareto optimal solutions, an optimum algorithm that matches a requested condition of the evaluation indices.</p>
<p id="p-0011" num="0010">The algorithm configured by the algorithm configuring means may have feature extraction expressions and an information estimation expression that is a combination of the feature extraction expressions.</p>
<p id="p-0012" num="0011">The tradeoff analyzing means may generate Pareto optimal solutions in an initial state by using algorithms having different numbers of feature extraction expressions, the algorithms being created by deleting the feature extraction expressions one by one from the algorithm configured by the algorithm configuring means.</p>
<p id="p-0013" num="0012">The tradeoff analyzing means may use, as a Pareto optimal solution in an initial state, the algorithm configured by the algorithm configuring means.</p>
<p id="p-0014" num="0013">The tradeoff analyzing means may update the Pareto optimal solutions by randomly changing use or not-use of each feature extraction expression in the Pareto optimal solutions in the initial state.</p>
<p id="p-0015" num="0014">The optimum-algorithm determining means may determine, of the Pareto optimal solutions, the optimum algorithm on the basis of requested processing time and accuracy.</p>
<p id="p-0016" num="0015">The information processing apparatus may further includes evaluation-value calculating means for determining evaluation values of the feature extraction expressions in the information estimation expression, and the algorithm configuring means may update the feature extraction expressions in the information estimation expression on the basis of the determined evaluation values of the feature extraction expressions.</p>
<p id="p-0017" num="0016">When the algorithm configuring means configures multiple information estimation expressions as algorithms, a total value of contribution rates of the same feature extraction expressions in the information estimation expressions may be used as the evaluation value of the corresponding feature extraction expression.</p>
<p id="p-0018" num="0017">According to another embodiment of the present invention, there is provided an information processing method for an information processing apparatus having algorithm configuring means for configuring an algorithm and tradeoff analyzing means for determining Pareto optimal solutions of the algorithm. The information processing method includes the steps of: configuring the algorithm for performing discrimination on an input signal by a genetic search technique; and determining Pareto optimal solutions of the algorithm with respect to evaluation indices by performing tradeoff analysis on the basis of the algorithm.</p>
<p id="p-0019" num="0018">According to still another embodiment of the present invention, there is provided a program. The program causes a computer to function as: algorithm configuring means for configuring an algorithm for performing discrimination on an input signal by using a genetic search technique; and tradeoff analyzing means for determining Pareto optimal solutions of the algorithm with respect to evaluation indices by performing tradeoff analysis on the basis of the algorithm.</p>
<p id="p-0020" num="0019">According to the embodiment of the present invention, an algorithm for performing discrimination on an input signal is configured, and tradeoff analysis is performed on the basis of the configured algorithm. As a result, multiple Pareto optimal solutions of the algorithm with respect to evaluation indices are determined.</p>
<p id="p-0021" num="0020">According to the embodiment of the present invention, it is possible to present multiple algorithms having a tradeoff relationship in evaluation indices.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing an example of the configuration of an information processing apparatus according to an embodiment of the present invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 2</figref> shows an example of a sample signal input to the information processing apparatus;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram illustrating a structure of an algorithm to be generated;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 4</figref> shows the structure of an abnormal-sound discrimination expression;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram illustrating another structure of the algorithm to be generated;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 6</figref> is a block diagram showing an example of the configuration of an information extractor;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 7</figref> shows examples of feature extraction expressions;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 8</figref> shows an example of one feature extraction expression;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 9</figref> illustrates a type of input signal;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 10</figref> illustrates a type of input signal;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 11</figref> illustrates types of input signal;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 12</figref> illustrates types of operator;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 13</figref> is a diagram showing an example of an automatic algorithm configuration performed by an automatic algorithm-configuring section;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 14</figref> illustrates selection generation;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 15</figref> illustrates cross generation;</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 16</figref> illustrates mutation generation;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 17</figref> illustrates an example of generation of next-generation feature extraction expressions;</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 18</figref> illustrates next-generation feature extraction expressions;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 19</figref> is a block diagram showing an example of a detailed configuration of the automatic algorithm-configuring section;</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 20</figref> illustrates processing performed by a learning-data cutout section;</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 21</figref> shows an example of the amounts of features calculated by an amount-of-feature calculating section;</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 22</figref> shows an example of data supplied to a machine learning section;</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIGS. 23A and 23B</figref> illustrate processing performed by the machine learning section;</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 24</figref> illustrates processing performed by the machine learning section;</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 25</figref> illustrates processing performed by the machine learning section;</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIGS. 26A and 26B</figref> illustrate processing performed by the machine learning section;</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 27</figref> illustrates Pareto optimal solutions;</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 28</figref> illustrates Pareto-optimal-solution initialization;</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 29</figref> illustrates the Pareto-optimal-solution initialization;</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 30</figref> illustrates the Pareto-optimal-solution initialization;</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 31</figref> illustrates the Pareto-optimal-solution initialization;</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 32</figref> illustrates Pareto-optimal-solution update;</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 33</figref> is a graph illustrating the Pareto-optimal-solution update;</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 34</figref> is a graph illustrating the Pareto-optimal-solution update;</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 35</figref> is a graph illustrating the Pareto-optimal-solution update;</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 36</figref> illustrates optimum algorithm determination;</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 37</figref> illustrates abnormal-sound discrimination processing;</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 38</figref> is a flowchart illustrating automatic algorithm-configuration processing;</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 39</figref> is a flowchart illustrating feature-extraction-expression list generation processing;</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 40</figref> is a flowchart illustrating random generation processing;</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 41</figref> is a flowchart illustrating next-generation list generation processing;</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 42</figref> is a flowchart illustrating selection generation processing;</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 43</figref> is a flowchart-illustrating cross generation processing;</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 44</figref> is a flowchart illustrating mutation generation processing;</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 45</figref> is a flowchart illustrating random generation processing;</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 46</figref> is a flowchart illustrating amount-of-feature calculation processing;</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 47</figref> is a flowchart illustrating machine learning processing;</p>
<p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. 48</figref> is a flowchart illustrating tradeoff analysis processing;</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 49</figref> is a flowchart illustrating Pareto-optimal-solution initialization processing;</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 50</figref> is a block diagram showing an automatic algorithm-configuring system according to another embodiment of the present invention;</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 51</figref> shows an example of teacher data;</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 52</figref> shows an example of the amount of features and an average time which are supplied to an evaluation-value calculating section;</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIGS. 53A and 53B</figref> show examples of use/not-use of each amount of features;</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 54</figref> shows examples of calculated amounts of features;</p>
<p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. 55</figref> shows an example of calculation of a total calculation time;</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 56</figref> shows an example of a speed evaluation reference value, a brightness evaluation reference value, and a total calculation time;</p>
<p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. 57</figref> illustrates Pareto-optimal-solution update;</p>
<p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. 58</figref> is a flowchart illustrating Pareto-optimal-solution search processing;</p>
<p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. 59</figref> is a flowchart illustrating amount-of-feature calculation processing;</p>
<p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. 60</figref> is a flowchart illustrating evaluation-value calculation processing;</p>
<p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. 61</figref> is a flowchart illustrating evaluation-value determination processing; and</p>
<p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. 62</figref> is a block diagram showing an example of the configuration of a computer according to one embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading>
<p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. 1</figref> shows an example of the confirmation of an information processing apparatus according to one embodiment of the present invention.</p>
<p id="p-0085" num="0084">An information processing apparatus <b>1</b> performs processing for detecting abnormal sounds, automatically configures algorithms on the basis of a sample signal with which whether or an abnormal sound or a normal sound is known, the sample signal being input as learning data. The information processing apparatus <b>1</b> determines whether or not a new input signal (hereinafter referred to as a &#x201c;new signal&#x201d;) has an abnormal sound or a normal sound in accordance with the automatically configured algorithm.</p>
<p id="p-0086" num="0085">The information processing apparatus <b>1</b> includes a sample-signal input section <b>11</b>, a sample-signal storage section <b>12</b>, an automatic algorithm-configuring section <b>13</b>, a tradeoff analyzing section <b>14</b>, a Pareto-optimal-solution storage section <b>15</b>, a use-algorithm determining section <b>16</b>, a new-signal input section <b>17</b>, an automatic abnormal-sound-discrimination section <b>18</b>, an abnormal-sound discrimination result output section <b>19</b>, and a correction-information input section <b>20</b>.</p>
<p id="p-0087" num="0086">The sample signal for algorithm configuration is input from another block (not shown) in the information processing apparatus <b>1</b> or from another apparatus to the sample-signal input section <b>11</b>. In response to the sample signal for algorithm configuration, the sample-signal input section <b>11</b> supplies the sample signal to the sample-signal storage section <b>12</b>.</p>
<p id="p-0088" num="0087">The sample-signal storage section <b>12</b> stores, as teacher data, the sample signal supplied from the sample-signal input section <b>11</b>. The automatic abnormal-sound-discrimination section <b>18</b> and the correction-information input section <b>20</b> supplies signals (described below) to the sample-signal storage section <b>12</b>.</p>
<p id="p-0089" num="0088">Data indicating whether or not the sample signal input to the sample-signal input section <b>11</b> has a normal sound or an abnormal sound is attached, as a label, to the sample signal.</p>
<p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. 2</figref> is an example of a sample signal input to the sample-signal input section <b>11</b>. In <figref idref="DRAWINGS">FIG. 2</figref>, the horizontal axis indicates time, and the sample signal and the label has a one-to-one correspondence at each point in time. The label indicating whether the sample signal is a normal sound or an abnormal sound can be given as a signal having a high level or low level.</p>
<p id="p-0091" num="0090">By performing learning based on a genetic search technique, the automatic algorithm-configuring section <b>13</b> configures (generates) an algorithm for discriminating between an abnormal sound or a normal sound with respect to a new signal with which whether an abnormal sound or a normal sound is unknown. The algorithm configured is an algorithm (an amount-of-feature extraction algorithm) for extracting the amount of features indicating predetermined features of an input signal and making it possible to discriminate between an abnormal sound and a normal sound on the basis of whether (the value of) the amount of features extracted by the algorithm is larger than or equal to a predetermined threshold. Details of the algorithm configuration are described below with reference to <figref idref="DRAWINGS">FIG. 3</figref> and subsequent figures.</p>
<p id="p-0092" num="0091">The tradeoff analyzing section <b>14</b> determines Pareto optimal solutions, as algorithms, by performing tradeoff analysis on the basis of the algorithm configured by the automatic algorithm-configuring section <b>13</b>. The tradeoff analyzing section <b>14</b> then supplies the multiple algorithms, which are the Pareto optimal solutions, to the Pareto-optimal-solution storage section <b>15</b>. The Pareto-optimal-solution storage section <b>15</b> stores the multiple algorithms (which are the Pareto optimal solutions) supplied from the tradeoff analyzing section <b>14</b>.</p>
<p id="p-0093" num="0092">The use-algorithm determining section <b>16</b> determines, of the multiple algorithms stored in the Pareto-optimal-solution storage section <b>15</b>, an optimum algorithm that matches requested conditions for evaluation indices, and supplies the determined optimum algorithm to the automatic abnormal-sound-discrimination section <b>18</b>. The evaluation indices include, for example, a currently available resource status and requested performance of the information processing apparatus <b>1</b>.</p>
<p id="p-0094" num="0093">The new-signal input section <b>17</b> receives an input new signal that is to be subjected to abnormal-sound discrimination and supplies the received new signal to the automatic abnormal-sound-discrimination section <b>18</b>.</p>
<p id="p-0095" num="0094">The automatic abnormal-sound-discrimination section <b>18</b> uses the algorithm, supplied from the use-algorithm determining section <b>16</b>, to discriminate between an abnormal sound and a normal sound with respect to the new signal supplied from the new-signal input section <b>17</b>, and supplies a discrimination result to the abnormal-sound discrimination result output section <b>19</b>. The automatic abnormal-sound-discrimination section <b>18</b> also adds, as a label, the discrimination result to the new signal supplied from the new-signal input section <b>17</b>, and supplies the resulting signal to the sample-signal storage section <b>12</b> for storage. Thus, the new signal supplied from the new-signal input section <b>17</b> is stored in the sample-signal storage section <b>12</b> as a new sample signal.</p>
<p id="p-0096" num="0095">The abnormal-sound discrimination result output section <b>19</b> outputs the discrimination result, supplied from the automatic abnormal-sound-discrimination section <b>18</b>, in the form of sound or image. For example, when the new signal has an abnormal sound, the abnormal-sound discrimination result output section <b>19</b> outputs a sound indicating an abnormal sound or displays, on a screen, video showing character &#x201c;normal sound&#x201d; or &#x201c;abnormal sound&#x201d;.</p>
<p id="p-0097" num="0096">When the label of the sample signal stored in the sample-signal storage section <b>12</b> has error, the correction-information input section <b>20</b> corrects the label. That is, the correction-information input section <b>20</b> re-writes the label of the sample signal in response to a user operation.</p>
<p id="p-0098" num="0097">Thus, not only the label-added sample signal from the sample-signal input section <b>11</b> but also a label-added sample signal from the automatic abnormal-sound-discrimination section <b>18</b> is input to the sample-signal storage section <b>12</b>. When the label of the sample signal supplied from the automatic abnormal-sound-discrimination section <b>18</b> has error, the correction-information input section <b>20</b> corrects the label.</p>
<p id="p-0099" num="0098">The algorithm configuration performed by the automatic algorithm-configuring section <b>13</b> will be described next in detail.</p>
<p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram illustrating the structure of an algorithm to be configured.</p>
<p id="p-0101" num="0100">The configured algorithm includes m feature extraction expressions 1 to m (m is an integer greater than 1) and an information estimation expression represented by a linear combination expression of the m feature extraction expressions 1 to m. Each feature extraction expressions perform predetermined computations on the input signal and output first-order values as computation results. The information estimation expression uses the first-order values output from the feature extraction expressions to estimate information indicating features of the input signal. The automatic algorithm-configuring section <b>13</b> determines an abnormal-sound discrimination expression U as the information estimation expression.</p>
<p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. 4</figref> shows a specific example of the abnormal-sound discrimination expression U. The discrimination result to be output is a binary indicating an abnormal sound or a normal sound. Thus, whether the input signal has an abnormal sound or a normal sound is discriminated based on whether the computation result of the abnormal-sound discrimination expression U is smaller than or equal to a predetermined threshold Th or is larger than the predetermined threshold Th. For example, as shown in <figref idref="DRAWINGS">FIG. 4</figref>, when the computation result of the abnormal-sound discrimination expression U is smaller than or equal to the predetermined threshold Th, the discrimination result to be output indicates that the input signal has a normal sound, and when the computation result of the abnormal-sound discrimination expression U is larger than the predetermined threshold Th, the discrimination result to be output indicates that the input signal has an abnormal sound. In the example of the abnormal-sound discrimination expression U shown in <figref idref="DRAWINGS">FIG. 4</figref>, only feature extraction expressions 1, 2, 5, and 15 of the feature extraction expressions 1 to m are used. This is because linear combination coefficients of the other feature extraction expressions are zero.</p>
<p id="p-0103" num="0102">The algorithm to be configured may be an algorithm for estimating, for example, numeric values in predetermined ranges, such as the degrees (levels) of music speed and brightness, instead of the binary indicating an abnormal sound or a normal sound. In such a case, as shown in <figref idref="DRAWINGS">FIG. 5</figref>, a value indicating the speed of the input signal is determined from a computation result of a speed estimation expression represented by a linear combination expression of m feature extraction expressions 1 to m, and a value indicating the brightness of the input signal is determined from a computation result of a brightness estimation expression represented by a linear combination expression of m feature extraction expressions 1 to m.</p>
<p id="p-0104" num="0103">An example in which an algorithm used as a feature extractor for extracting the values of speed and brightness when a new signal is input is configured as shown in <figref idref="DRAWINGS">FIG. 6</figref> is described in a next embodiment describe below with reference to <figref idref="DRAWINGS">FIG. 50</figref> and subsequent figures.</p>
<p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. 7</figref> shows examples of the feature extraction expressions 1 to m.</p>
<p id="p-0106" num="0105">The feature extraction expressions can process an input signal, which can be expressed in multiple order values, until it has one value, that is, it has a scalar quantity. <figref idref="DRAWINGS">FIG. 8</figref> shows one example of the feature extraction expressions.</p>
<p id="p-0107" num="0106">This feature extraction expression shown in <figref idref="DRAWINGS">FIG. 8</figref> is constituted by information indicating the type of an input signal and operators that provide information indicating signal processing to be performed on the input signal. Each operator includes a processing-target axis and a processing description and optionally includes a parameter for the processing. In the example shown in <figref idref="DRAWINGS">FIG. 8</figref>, &#x201c;12TonesM&#x201d; is information indicating the type of an input signal and is followed by four operators &#x201c;F#Differential&#x201d;, &#x201c;F#MaxIndex&#x201d;, &#x201c;T#LPF_<b>1</b>;0.861&#x201d;, and &#x201c;T#UVariance&#x201d;.</p>
<p id="p-0108" num="0107">The information indicating the type of an input signal includes, for example, &#x201c;wav&#x201d; resenting a waveform and &#x201c;Spectrum&#x201d; representing a two-dimensional image (which shows a sound spectrum), in addition to &#x201c;12TonesM&#x201d; (shown in the example in <figref idref="DRAWINGS">FIG. 8</figref>) indicating that monaural PCM (pulse coded modulation) sound source waveform data is subjected to interval-analysis along the time axis.</p>
<p id="p-0109" num="0108">The feature extraction expression indicates that the signal processing expressed by the operators is sequentially performed on the input signal, starting from the information closer to the information indicating the type of the input signal. The beginning of each operator indicates an axis to be processed (a processing-target axis). In the axis to be processed, T indicates a time direction and F indicates a frequency (an interval) direction.</p>
<p id="p-0110" num="0109">With regard to the processing description that follows the processing-target axis of the operator, &#x201c;Differential&#x201d; indicates differentiation, &#x201c;MaxIndex&#x201d; indicates acquiring a maximum-value index, &#x201c;LPF_<b>1</b>&#x201d; indicates low-pass filtering, and &#x201c;UVariance&#x201d; indicates computing an unbiased variance. Each operator may be given a parameter, as appropriate. In the example shown in <figref idref="DRAWINGS">FIG. 8</figref>, the processing of &#x201c;LPF_<b>1</b>&#x201d; (low-pass filtering) is given a parameter &#x201c;0.861&#x201d;.</p>
<p id="p-0111" num="0110">Thus, the feature extraction expression shown in <figref idref="DRAWINGS">FIG. 8</figref> is to differentiate data, obtained by performing interval analysis on PCM waveform data in a time axis, in an interval direction; to calculate &#x201c;Index&#x201d; indicating a maximum value in the interval direction (i.e., to obtain the number of an interval having the strongest sound&#x201d;); to apply a low pass filter in the time axis direction; and to lastly calculate an unbiased variance. The result of the calculation is output.</p>
<p id="p-0112" num="0111">In the present embodiment, &#x201c;wav&#x201d; indicating a waveform in a sound-representing signal is employed for the input signal, but the feature extraction expression may have not only a sound signal but also any signal that can be expressed in a matrix form (in a form of a multi-dimensional matrix having values). Examples include a signal for a still image and a signal for a moving image.</p>
<p id="p-0113" num="0112">For example, the input signal may be an audio-waveform signal regarded as a matrix of amplitude, time, and channel, as shown in <figref idref="DRAWINGS">FIG. 9</figref>, may be an image signal regarded as a matrix of X and Y axes of a frame and RGB (R: red, G: green, and B: blue), as shown in <figref idref="DRAWINGS">FIG. 10</figref>, or may be a moving-image signal regarded as a matrix of X and Y axes of a frame, RGB, and time, as shown in <figref idref="DRAWINGS">FIG. 11</figref>. In addition, the input signal may be, for example, binary data that can be represented by a matrix.</p>
<p id="p-0114" num="0113">The types of operators are shown in <figref idref="DRAWINGS">FIG. 12</figref> by way of example. For example, the types of operators include mean value (Mean), fast Fourier transform (FFT), standard deviation (StDev), appearance ratio (Ratio), low-pass filter (LPF_<b>1</b>), high-pass filter (HPF_<b>1</b>), absolute value (ABS), square (Sqr), square root (Sqrt), normalization (Normalize), differentiation (Differential), integration (Integrate), maximum value (MaxIndex), unbiased variance (UVariance), and down sampling (DownSampling). The processing-target axis may be fixed depending on the determined operator. In such a case, the fixed processing-target axis is used for the operator. When an operator that uses a parameter is determined, the parameter is also determined to have a random or preset value.</p>
<p id="p-0115" num="0114">Next, a flow of automatic algorithm generation that the automatic algorithm-configuring section <b>13</b> performs on the basis of a genetic search technique will be described with reference to <figref idref="DRAWINGS">FIGS. 13 to 18</figref>.</p>
<p id="p-0116" num="0115">As shown in <figref idref="DRAWINGS">FIG. 13</figref>, with respect to the current-generation feature extraction expressions 1 to m, the automatic algorithm-configuring section <b>13</b> determines evaluation values for evaluating the feature extraction expressions. The automatic algorithm-configuring section <b>13</b> rearranges the current-generation feature extraction expressions 1 to m in descending order of the evaluation values, and then performs selection generation, cross generation, mutation generation, and random generation to generate next-generation feature extraction expressions 1 to m. The reason why the current-generation feature extraction expressions 1 to m are rearranged in descending order of the evaluation values is to allow feature extraction expressions for generating the next-generation feature extraction expressions to be selected in descending order of the evaluation values. Thus, processing in which the next-generation feature extraction expressions 1 to m become current-generation feature extraction expressions 1 to m and next-generation feature extraction expressions 1 to m are generated based on the current-generation feature extraction expressions 1 to m is repeated.</p>
<p id="p-0117" num="0116">The selection generation, the cross generation, the mutation generation, and the random generation will now be described in detail.</p>
<p id="p-0118" num="0117">The term &#x201c;selection generation&#x201d; refers to generating new feature extraction expressions by selecting, from the current-generation feature extraction expressions, a predetermined number of feature extraction expressions in descending order of the evaluation values and directly using the selected feature extraction expressions as the next-generation feature extraction expressions.</p>
<p id="p-0119" num="0118">For example, as shown in <figref idref="DRAWINGS">FIG. 14</figref>, when the evaluation values of the current-generation feature extraction expressions 1, 2, and 3 are 0.53, 0.85, and 0.74, respectively, the current-generation feature extraction expression 2 having the highest-evaluation value is selected and is used as a next-generation feature extraction expression 1.</p>
<p id="p-0120" num="0119">The term &#x201c;cross generation&#x201d; refers to generating a new feature extraction expression by randomly selecting two feature extraction expressions with a weight being assigned so as to facilitate selection of a feature extraction expression having a higher-evaluation value and exchanging (crossing) the selected feature extraction expressions at random positions.</p>
<p id="p-0121" num="0120">For example, as shown in <figref idref="DRAWINGS">FIG. 15</figref>, a first feature extraction expression &#x201c;Wav,T#Differential,T#IndexLR<b>0</b>&#x201d; and a second feature extraction expression &#x201c;Wav,T#HPF_<b>1</b>;0.262544,T#Mean&#x201d;, the first and second feature extraction expressions having high-evaluation values, are selected and portion &#x201c;Wav,T#Differential&#x201d; in the first feature extraction expression and portion &#x201c;T#Mean&#x201d; in the second feature extraction expression are combined, in other words, portion&#x201c;T#IndexLR<b>0</b>&#x201d; in the first feature extraction expression is exchanged with the portion &#x201c;T#Mean&#x201d; in the second feature extraction expression, to thereby generate a new feature extraction expression.</p>
<p id="p-0122" num="0121">The term &#x201c;mutation generation&#x201d; refers to generating a new feature extraction expression by randomly selecting a feature extraction expression with a weight being assigned so as to facilitate selection of a feature extraction expression having a higher-evaluation value and randomly changing a part of the selected feature extraction expression.</p>
<p id="p-0123" num="0122">For example, as shown in <figref idref="DRAWINGS">FIG. 16</figref>, a feature extraction expression &#x201c;Wav,T#LPF_<b>1</b>;0.3,T#IndexLR<b>0</b>&#x201d; having a high-evaluation value is selected as a mutation source, and one operator &#x201c;T#LPF_<b>1</b>;0.3&#x201d; in the selected feature extraction expression is deleted (mutation example 1), an operator &#x201c;Sqr&#x201d; is added (mutation example 2), or a parameter of one operator &#x201c;T#LPF_<b>1</b>;0.3&#x201d; in the feature extraction expression is changed to 0.7 (mutation example 3) to thereby generate a new feature extraction expression.</p>
<p id="p-0124" num="0123">The term &#x201c;random generation&#x201d; refers to generating a new feature extraction expression by randomly combining operators so that an input signal has one value (a scalar quantity). First-generation feature extraction expressions 1 to m have no previous-generation feature extraction expressions that become sources thereof, and thus are all generated through random generation.</p>
<p id="p-0125" num="0124">For example, as shown in <figref idref="DRAWINGS">FIG. 17</figref>, the automatic algorithm-configuring section <b>13</b> uses the selection generation to determine next-generation feature extraction expressions 1 to 3, uses the cross generation to determine next-generation feature extraction expressions 4 to 7, uses the mutation generation to determine next-generation feature extraction expressions 8 to 13, and uses the random generation to determine next-generation feature extraction expressions 14 to m.</p>
<p id="p-0126" num="0125"><figref idref="DRAWINGS">FIG. 18</figref> shows an example in which next-generation feature extraction expressions 1 to m are generated from current-generation feature extraction expressions 1 to m.</p>
<p id="p-0127" num="0126"><figref idref="DRAWINGS">FIG. 19</figref> is a block diagram showing an example of a detailed configuration of an automatic algorithm-configuring section <b>13</b>.</p>
<p id="p-0128" num="0127">The automatic algorithm-configuring section <b>13</b> includes a learning-data cutout section <b>31</b>, a feature-extraction-expression list generating section <b>32</b>, an amount-of-feature calculating section <b>33</b>, a machine learning section <b>34</b>, and an amount-of-feature extraction algorithm output section <b>35</b>.</p>
<p id="p-0129" num="0128">The learning-data cutout section <b>31</b> cuts out learning data stored in the sample-signal storage section <b>12</b> and supplies the learning data to the amount-of-feature calculating section <b>33</b> and the machine learning section <b>34</b>. More specifically, the learning-data cutout section <b>31</b> divides the sample signal, stored in the sample-signal storage section <b>12</b>, and the label of the sample signal into portions in predetermined unit times and supplies the divided sample signals in the respective unit times to the amount-of-feature calculating section <b>33</b> and also supplies the divided labels in the unit times to the machine learning section <b>34</b>.</p>
<p id="p-0130" num="0129">It is now assumed that, as shown in <figref idref="DRAWINGS">FIG. 20</figref>, the learning-data cutout section <b>31</b> divides a sample signal, stored in the sample-signal storage section <b>12</b>, and the label of the sample signal into n portions in n unit times. The first to nth sample signals in the unit times are referred to as data D<sub>1 </sub>to D<sub>n</sub>, respectively, and the labels associated with the data D<sub>1 </sub>to D<sub>n </sub>are referred to as label data. In this case, the learning-data cutout section <b>31</b> supplies the data D<sub>1 </sub>to D<sub>n </sub>to the amount-of-feature calculating section <b>33</b> and supplies the label data of the data D<sub>1 </sub>to D<sub>n </sub>to the machine learning section <b>34</b>.</p>
<p id="p-0131" num="0130">The machine learning section <b>34</b> supplies evaluation values of current-generation feature extraction expressions 1 to m to the feature-extraction-expression list generating section <b>32</b>. On the basis of the evaluation values, the feature-extraction-expression list generating section <b>32</b> generates next-generation feature extraction expressions 1 to m and supplies the generated next-generation feature extraction expressions 1 to m to the amount-of-feature calculating section <b>33</b>. More specifically, the feature-extraction-expression list generating section <b>32</b> rearranges the current-generation feature extraction expressions 1 to m in descending order of the evaluation values, and then performs the selection generation, cross generation, mutation generation, and random generation on predetermined feature extraction expressions in the current generation to thereby generate next-generation feature extraction expressions 1 to m.</p>
<p id="p-0132" num="0131">With respect to the respective data D<sub>1 </sub>to D<sub>n </sub>supplied from the learning-data cutout section <b>31</b>, the amount-of-feature calculating section <b>33</b> performs calculation of the feature extraction expressions 1 to m supplied from the feature-extraction-expression list generating section <b>32</b> and supplies calculation results, i.e., the amounts of features of the data D<sub>1 </sub>to D<sub>n</sub>, to the machine learning section <b>34</b>.</p>
<p id="p-0133" num="0132"><figref idref="DRAWINGS">FIG. 21</figref> shows an example of the amounts of features calculated by the amount-of-feature calculating section <b>33</b> and supplied to the machine learning section <b>34</b>.</p>
<p id="p-0134" num="0133">By using the amounts of features of the data D<sub>1 </sub>to D<sub>n </sub>and the label data thereof, the amounts being supplied from the amount-of-feature calculating section <b>33</b> and the label data being supplied from the learning-data cutout section <b>31</b>, the machine learning section <b>34</b> performs linear discrimination with feature selection to estimate an abnormal-sound discrimination expression U. In other words, by using the amounts of features of the data D<sub>1 </sub>to D<sub>n</sub>, the amounts being supplied from the amount-of-feature calculating section <b>33</b>, the machine learning section <b>34</b> estimates an abnormal-sound discrimination expression U that uses feature extraction expressions and that best matches the label data of the data D<sub>1 </sub>to D<sub>n</sub>, the label data being supplied from the learning-data cutout section <b>31</b>. The machine learning section <b>34</b> determines evaluation values of the respective feature extraction expressions when the estimated abnormal-sound discrimination expression U is used and supplies the determined evaluation values to the feature-extraction-expression list generating section <b>32</b>.</p>
<p id="p-0135" num="0134">The processing performed by the machine learning section <b>34</b> will now be described in more detail.</p>
<p id="p-0136" num="0135"><figref idref="DRAWINGS">FIG. 22</figref> shows an example of data supplied from the amount-of-feature calculating section <b>33</b> and the learning-data cutout section <b>31</b> to the machine learning section <b>34</b>.</p>
<p id="p-0137" num="0136">The machine learning section <b>34</b> evaluates which of the current-generation feature extraction generations 1 to m are to be used to best match the label data of the data D<sub>1 </sub>to D<sub>n</sub>, and determines an optimum combination of the feature extraction expressions to be used.</p>
<p id="p-0138" num="0137">First, on the basis of a reference state (shown in <figref idref="DRAWINGS">FIG. 23A</figref>) in which any of the current-generation feature extraction expressions 1 to m is not used (i.e., the feature extraction expressions 1 to m are not in use), the machine learning section <b>34</b> determines a use combination of m feature extraction expressions (shown in <figref idref="DRAWINGS">FIG. 23B</figref>), use/not-use of each feature extraction expression being sequentially inverted. In <figref idref="DRAWINGS">FIGS. 23A and 23B</figref>, in the arrangement of 0s and 1 in one row, &#x201c;1&#x201d; indicates a case in which each of the feature extraction expressions 1 to m is used and &#x201c;0&#x201d; indicates a case in which each of the feature extraction expressions 1 to m is not used, and use or not-use of the feature extraction expressions 1, 2, 3, . . . , and m is represented sequentially from the left side in one row.</p>
<p id="p-0139" num="0138">Next, the machine learning section <b>34</b> generates (estimates) an abnormal-sound discrimination expression U with respect to each of the use combinations of the m feature extraction expressions. The abnormal-sound discrimination expression U is given by:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>U=b</i><sub>1</sub><i>X</i><sub>1</sub><i>+b</i><sub>2</sub><i>X</i><sub>2</sub><i>+b</i><sub>3</sub><i>X</i><sub>3</sub><i>+, . . . , +b</i><sub>m</sub><i>X</i><sub>m</sub><i>+b</i><sub>0</sub>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where X<sub>1 </sub>to X<sub>m </sub>indicate the amounts of features resulting from the feature extraction expressions 1 to m, b<sub>1 </sub>to b<sub>m </sub>indicate linear combination coefficients, and b<sub>0 </sub>indicates an intercept.
</p>
<p id="p-0140" num="0139">In expression (1), the value of the linear combination coefficient for the amount of features of each feature extraction expression that is not used is zero.</p>
<p id="p-0141" num="0140">A least-squares method can be used in order to determine the linear combination coefficients b<sub>1 </sub>to b<sub>m </sub>and the intercept b<sub>0</sub>. That is, when the kth one (true value) of the label data of the data D<sub>1 </sub>to D<sub>n</sub>, the label data being supplied from the amount-of-feature calculating section <b>33</b>, is represented by U<sub>k </sub>and an estimation value of the true value U<sub>k </sub>obtained from expression (1) is represented by U<sub>k</sub>&#x2032;, an estimation error e<sub>k </sub>therefor can be given by: e<sub>k</sub>=(U<sub>k</sub>&#x2212;U<sub>k</sub>&#x2032;). Determining the linear combination coefficients b<sub>1 </sub>to b<sub>m </sub>and the intercept b<sub>0 </sub>with which squared error &#x201c;E=&#x3a3;e<sub>k</sub><sup>2</sup>&#x201d; of the estimation errors e<sub>k </sub>with respect to each label data of the data D<sub>1 </sub>to D<sub>n </sub>becomes the smallest makes it possible to determine an optimum linear combination coefficients b<sub>1 </sub>to b<sub>m </sub>and an optimum intercept b<sub>0</sub>.</p>
<p id="p-0142" num="0141">For example, the determined abnormal-sound discrimination expression U is given by:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>U=</i>0.5<i>&#xd7;X</i><sub>1</sub>+0.3<i>&#xd7;X</i><sub>2</sub>&#x2212;0.0<i>&#xd7;X</i><sub>3</sub>+, . . . , +0.7<i>X</i><sub>m</sub>+0.2.<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0143" num="0142">In this example of the abnormal-sound discrimination expression U, the linear combination coefficients, b<sub>1</sub>, b<sub>2</sub>, b<sub>3</sub>, . . . , and b<sub>m </sub>are 0.5, 0.3, 0.0, . . . , and 0.7, respectively, the intercept b<sub>0 </sub>is 0.2, and the amount of features resulting from the feature extraction expression 3 for which the linear combination coefficient is zero is not used.</p>
<p id="p-0144" num="0143">Next, the machine learning section <b>34</b> calculates an evaluation value of the abnormal-sound discrimination expression U generated with respect to each of the use combinations of the m feature extraction expressions. Hereinafter, the evaluation value of the abnormal-sound discrimination expression U is referred to as an &#x201c;evaluation reference value&#x201d; so that it is distinguished from the evaluation value of each feature extraction expression.</p>
<p id="p-0145" num="0144">For example, AIC (Akaike Information Criterion) can be employed for an evaluation function for determining the evaluation reference value. AIC is a function expressing that a smaller value is better (i.e., has a higher evaluation) and is given by:</p>
<p id="p-0146" num="0145">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>A</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>I</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>C</mi>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mi>n</mi>
            <mo>&#xd7;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>log</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mn>2</mn>
                      <mo>&#xd7;</mo>
                      <mi>PI</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>+</mo>
                <mn>1</mn>
                <mo>+</mo>
                <mrow>
                  <mi>log</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msup>
                        <mrow>
                          <mo>&#xf605;</mo>
                          <mi>E</mi>
                          <mo>&#xf606;</mo>
                        </mrow>
                        <mn>2</mn>
                      </msup>
                      <mo>&#xf7;</mo>
                      <mi>n</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>+</mo>
          <mrow>
            <mn>2</mn>
            <mo>&#xd7;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <msup>
                  <mi>m</mi>
                  <mi>&#x2032;</mi>
                </msup>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where n indicates the number of pieces of teacher data, i.e., the number of pieces of data supplied from the learning-data cutout section <b>31</b>, and pI indicates the circumference of a circle. &#x201c;&#x2225;E&#x2225;<sup>2</sup>&#xf7;n&#x201d; indicates a mean square error for each piece of data D<sub>1 </sub>to D<sub>n </sub>and m&#x2032; indicates the number of feature extraction expressions used.
</p>
<p id="p-0147" num="0146"><figref idref="DRAWINGS">FIG. 24</figref> shows evaluation reference values of respective abnormal-sound discrimination expressions U generated with respect to the use combinations of the m feature extraction expressions shown in <figref idref="DRAWINGS">FIG. 23B</figref>.</p>
<p id="p-0148" num="0147">The machine learning section <b>34</b> selects, as a next reference state, the use-combination of feature extraction expressions which has a highest evaluation (which has a smallest evaluation reference value). In the example shown in <figref idref="DRAWINGS">FIG. 24</figref>, the evaluation reference value of the abnormal-sound discrimination expression U for the use combination in which only the feature extraction expression 4 is used is the smallest. Thus, as shown in <figref idref="DRAWINGS">FIG. 25</figref>, the machine learning section <b>34</b> selects, as a next reference state, the use combination in which only the feature extraction expression 4 is used.</p>
<p id="p-0149" num="0148">With respect to the use combination (the reference state shown in <figref idref="DRAWINGS">FIG. 26A</figref>) in which only the feature extraction expression 4 is used, the machine learning section <b>34</b> determines use combinations of m feature extraction expressions, the use/not-use of each feature extraction expression being sequentially inverted, as shown in <figref idref="DRAWINGS">FIG. 26B</figref>.</p>
<p id="p-0150" num="0149">The machine learning section <b>34</b> repeats processing for determining a next reference state by generating (estimating) abnormal-sound discrimination expressions U for the determined use combinations of the m feature extraction expressions and calculating evaluation reference values of the abnormal-sound discrimination expressions U. The above-described determination of the reference state, generation of the abnormal-sound discrimination expressions U, and calculation of the evaluation reference values thereof are repeated until no more evaluation reference value is updated, that is, until a smaller evaluation reference value is not calculated.</p>
<p id="p-0151" num="0150">When an abnormal-sound discrimination expression U for which no more evaluation reference value is updated is obtained, the machine learning section <b>34</b> determines evaluation values of the feature extraction expressions 1 to m, on the basis of the linear combination coefficients for the obtained abnormal-sound discrimination expression U.</p>
<p id="p-0152" num="0151">Specifically, the machine learning section <b>34</b> calculates a contribution rate C<sub>i </sub>of the ith amount of features as an evaluation value of the feature extraction expression i, as given by:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>C</i><sub>i</sub><i>=b</i><sub>i</sub>&#xf7;StDev(<i>X</i><sub>i</sub>)&#xd7;StDev(<i>T</i>)&#xd7;Correl(<i>X</i><sub>i</sub><i>,T</i>)&#x2003;&#x2003;(3).<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where StDev(X<sub>i</sub>) indicates a standard deviation of the value (the amount of features X<sub>i</sub>) of the feature extraction expression i for the data D<sub>1 </sub>to D<sub>n </sub>and StDev(T) indicates standard deviations of label data T of the data D<sub>1 </sub>to data D<sub>n</sub>. Correl(T) indicates a Pearson's correlation coefficient between the value (the amount of features X<sub>i</sub>) of the feature extraction expression i for the data D<sub>1 </sub>to D<sub>n </sub>and the label data T thereof.
</p>
<p id="p-0153" num="0152">The evaluation values of the feature extraction expressions 1 to m calculated as described above are supplied, together with the generated abnormal-sound discrimination expression U, to the feature-extraction-expression list generating section <b>32</b>.</p>
<p id="p-0154" num="0153">When a predetermined completion condition is satisfied, for example, when processing for generating a next-generation feature extraction expression is performed a predetermined number of times, the machine learning section <b>34</b> supplies the last-generation feature extraction expressions and the abnormal-sound discrimination expression U to the amount-of-feature extraction algorithm output section <b>35</b>. The last-generation feature extraction expressions are obtained from the feature-extraction-expression list generating section <b>32</b> or the amount-of-feature calculating section <b>33</b>.</p>
<p id="p-0155" num="0154">The amount-of-feature extraction algorithm output section <b>35</b> supplies the last-generation feature extraction expressions and the abnormal-sound discrimination expression U, supplied from the machine learning section <b>34</b>, to the tradeoff analyzing section <b>14</b> (shown in <figref idref="DRAWINGS">FIG. 1</figref>).</p>
<p id="p-0156" num="0155">Detailed processing of the tradeoff analyzing section <b>14</b> will be described next.</p>
<p id="p-0157" num="0156">By using the algorithm (the abnormal-sound evaluation expression U) configured by the automatic algorithm-configuring section <b>13</b> as the first Pareto optimum solution, the tradeoff analyzing section <b>14</b> determines new Pareto optimal solutions by performing tradeoff analysis using two evaluation indices, i.e., an algorithm execution speed and an accuracy. For search of the Pareto optimal solutions to determine the new Pareto optimal solutions, the tradeoff analyzing section <b>14</b> employs a simple genetic algorithm for generating a next Pareto optimal solution candidate through only mutation from an amount-of-feature extraction algorithm that is a current Pareto optimum solution.</p>
<p id="p-0158" num="0157">The Pareto optimal solutions will now be described with reference to <figref idref="DRAWINGS">FIG. 27</figref>.</p>
<p id="p-0159" num="0158">For evaluating an algorithm by using two evaluation indices, when another solution having a high evaluation in all evaluation indices does not exist, the solution in question can be regarded as a Pareto optimal solution. In contrast, when another solution having a high evaluation in all evaluation indices exists, the solution in question is not a Pareto optimal solution. In <figref idref="DRAWINGS">FIG. 27</figref>, black circles represent Pareto optimal solutions and white circles represent non-Pareto optimal solutions.</p>
<p id="p-0160" num="0159">In order to determine abnormal-sound discrimination expressions U as Pareto optimal solutions, as shown in <figref idref="DRAWINGS">FIG. 27</figref>, the tradeoff analyzing section <b>14</b> first initializes Pareto optimal solutions. That is, the tradeoff analyzing section <b>14</b> sequentially sets the feature extraction expressions, included in the abnormal-sound discrimination expression U configured by the automatic algorithm-configuring section <b>13</b>, one by one in ascending order of label-data correlations (Correl(T) noted above) of the feature extraction expressions so that the feature extraction expressions are not used. By doing so, the tradeoff analyzing section <b>14</b> generates multiple abnormal-sound discrimination expressions U in which the number of feature extraction expressions used are different from each other by one. In this case, the maximum number of feature extraction expressions is the number of feature extraction expressions included in the abnormal-sound determination expression U configured by the automatic algorithm-configuring section <b>13</b> and the minimum number of feature extraction expressions is one.</p>
<p id="p-0161" num="0160">Generating abnormal-sound discrimination expressions U, each having a different number of feature extraction expressions to be used, on the basis of the abnormal-sound discrimination expression U configured by the automatic algorithm-configuring section <b>13</b> is referred to as Pareto-optimal-solution initialization. Since the abnormal-sound discrimination expression U supplied from the automatic algorithm-configuring section <b>13</b> does not necessarily use all feature extraction expressions, the number of feature extraction expressions included in the abnormal-sound discrimination expression U configured by the automatic algorithm-configuring section <b>13</b> may be smaller than m. In the present embodiment, however, for simplification of description, a description will be given assuming that the abnormal-sound discrimination expression U supplied from the automatic algorithm-configuring section <b>13</b> uses all feature extraction expressions.</p>
<p id="p-0162" num="0161">The Pareto-optimal-solution initialization will be further described with reference to <figref idref="DRAWINGS">FIGS. 28 to 31</figref>.</p>
<p id="p-0163" num="0162">As shown in <figref idref="DRAWINGS">FIG. 28</figref>, the tradeoff analyzing section <b>14</b> creates a list including average times (average calculation times) taken for calculating the respective feature extraction expressions and label-data correlation coefficients calculated using the respective feature extraction expressions. The average calculation times can be determined by calculating the input data D<sub>1 </sub>to D<sub>n </sub>and the correlation coefficients can be determined by Correl(T) in expression (3).</p>
<p id="p-0164" num="0163">Next, the tradeoff analyzing section <b>14</b> sets, of feature extraction expressions in use, a feature extraction expression having a smallest correlation coefficient so that the feature extraction expression is not used, and uses only the amounts of features extracted using the remaining feature extraction expressions to determine linear combination coefficients b<sub>1 </sub>to b<sub>m </sub>and intercept b<sub>0 </sub>and determines a new abnormal-sound discrimination expression U. The tradeoff analyzing section <b>14</b> also calculates a time (a total calculation time) taken and an accuracy (estimated accuracy) obtained when the newly determined abnormal-sound discrimination expression U is used to perform abnormal-sound discrimination.</p>
<p id="p-0165" num="0164">In the example of the data shown in <figref idref="DRAWINGS">FIG. 28</figref>, the correlation coefficient of the feature extraction expression 3 is 0.05, which is the lowest, and thus, the feature extraction expression 3 is set so that it is not used, as shown in <figref idref="DRAWINGS">FIG. 29</figref>. Subsequently, a new abnormal-sound discrimination expression U and an accuracy thereof when the feature extraction expressions 1 to m except the feature extraction expression 3 are used are determined, and a total calculation time excluding the calculation time (0.11 ms) taken for the feature extraction expression 3 is calculated.</p>
<p id="p-0166" num="0165">Similarly, the feature extraction expressions are sequentially set one by one in ascending order of the correlation coefficients so that the feature extraction expressions are not used, new abnormal-sound discrimination expressions U are determined, and total calculation times and accuracies are calculated. A list of Pareto optimal solution candidates shown in <figref idref="DRAWINGS">FIG. 30</figref> is then created.</p>
<p id="p-0167" num="0166">The tradeoff analyzing section <b>14</b> extracts only Pareto optimal solutions (i.e., deletes non-Pareto optimal solutions) from the Pareto-optimal-solution candidate list shown in <figref idref="DRAWINGS">FIG. 30</figref>. More specifically, of the abnormal-sound discrimination expressions U including a solution <b>1</b> using m feature extraction expressions to a solution m using only one feature extraction expression (each solution is a Pareto optimal solution candidate), solutions 3 to 5 each having another solution with a higher accuracy and a shorter total calculation time are deleted as non-Pareto optimal solutions and the remaining solutions are used as Pareto optimal solutions.</p>
<p id="p-0168" num="0167">In general, when the number of feature extraction expressions decreases, the accuracy tends to decrease although the amount of the total calculation time becomes small. Thus, the total calculation times and the accuracies of the abnormal-sound discrimination expressions U generated by the Pareto-optimal-solution initialization can be plotted onto a graph as shown in <figref idref="DRAWINGS">FIG. 31</figref>. The horizontal axis on the graph in <figref idref="DRAWINGS">FIG. 31</figref> indicates a total calculation time (ms) and the vertical axis indicates an accuracy in F value (FMs: F-measures).</p>
<p id="p-0169" num="0168">As a result of the Pareto-optimal-solution initialization described above, Pareto optimal solutions in an initial state are determined.</p>
<p id="p-0170" num="0169">Next, the tradeoff analyzing section <b>14</b> causes mutation of the Pareto optimal solutions in the initial state to generate new solutions to thereby perform Pareto-optimal-solution update for searching for better Pareto optimal solutions. When the Pareto optimal solutions are updated to better Pareto optimal solutions, the line connecting the Pareto optimal solutions shifts in the upper left direction, as shown by an arrow in <figref idref="DRAWINGS">FIG. 31</figref>.</p>
<p id="p-0171" num="0170">More specifically, the tradeoff analyzing section <b>14</b> randomly selects one of the Pareto optimal solutions in the initial state, and generates a new solution in which use or not-use of (e.g., one to three) feature extraction expressions of the selected solution is randomly changed.</p>
<p id="p-0172" num="0171"><figref idref="DRAWINGS">FIG. 32</figref> shows an example of mutation in which the feature extraction expression 7 in the selected solution is changed from &#x201c;use&#x201d; to &#x201c;not-use&#x201d; and the feature extraction expression 8 is changed from &#x201c;not-use&#x201d; to &#x201c;use&#x201d; to thereby generate a new solution.</p>
<p id="p-0173" num="0172">The tradeoff analyzing section <b>14</b> then calculates a total calculation time and an accuracy with respect to the new solution, recreates the list shown in <figref idref="DRAWINGS">FIG. 30</figref>, and deletes non-Pareto optimal solutions.</p>
<p id="p-0174" num="0173">The tradeoff analyzing section <b>14</b> executes processing for searching for better Pareto optimal solutions by randomly reselecting one of the remaining Pareto solutions and generating a new solution through mutation on the basis of the selected solution, until a completion condition is satisfied, for example, until the processing is repeated a predetermined number of times.</p>
<p id="p-0175" num="0174"><figref idref="DRAWINGS">FIG. 33</figref> shows Pareto optimal solutions obtained after the processing for searching for Pareto optimal solutions is repeated 200 times from the Pareto optimal solutions in the initial state.</p>
<p id="p-0176" num="0175">In <figref idref="DRAWINGS">FIG. 33</figref>, a solid line connecting black circles represents Pareto optimal solutions obtained after the processing is repeatedly executed 200 times and a dotted line represents the Pareto optimal solutions in the initial state. White circles (&#x25ef;) represent non-Pareto optimal solutions calculated by the Pareto-optimal-solution search processing.</p>
<p id="p-0177" num="0176">Similarly, <figref idref="DRAWINGS">FIG. 34</figref> shows Pareto optimal solutions after the Pareto-optimal-solution search processing is repeatedly executed 1000 times and <figref idref="DRAWINGS">FIG. 35</figref> shows Pareto optimal solutions after the search processing is repeatedly executed 5000 times. The larger number of times the search processing is repeated, the better Pareto optimal solutions can be obtained. However, a significant difference is not recognized between the case in which the processing is executed 1000 times and the case in which the processing is executed 5000 times, and it can thus be understood that satisfactory Pareto optimal solutions can be obtained by executing the processing a predetermined number of times.</p>
<p id="p-0178" num="0177">Next, a description will be given of processing of the use-algorithm determining section <b>16</b> and the automatic abnormal-sound-discrimination section <b>18</b>.</p>
<p id="p-0179" num="0178">On the basis of the requested processing time and the accuracy, the use-algorithm determining section <b>16</b> determines an optimum algorithm out of the algorithms (the abnormal-sound discrimination expressions U) serving as Pareto optimal solutions determined by the tradeoff analyzing section <b>14</b> and stored in the Pareto-optimal-solution storage section <b>15</b>, as described above. The use-algorithm determining section <b>16</b> then supplies the determined optimum algorithm to the automatic abnormal-sound-discrimination section <b>18</b>.</p>
<p id="p-0180" num="0179">For example, when there are sufficient resources and there is a request for a highest performance (a highest accuracy) even with an increased amount of processing time, the use-algorithm determining section <b>16</b> determines an algorithm (an abnormal-sound discrimination expression U) of a Pareto optimal solution <b>51</b> having a highest accuracy, as shown in <figref idref="DRAWINGS">FIG. 36</figref>, and supplies the determined algorithm to the automatic abnormal-sound-discrimination section <b>18</b>.</p>
<p id="p-0181" num="0180">For example, when it is requested that the accuracy be 0.76 (FMs) or more, the use-algorithm determining section <b>16</b> determines an algorithm (an abnormal-sound discrimination expression U) of a Pareto optimal solution <b>52</b> having a shortest total calculation time (processing time) while satisfying the request, and supplies the determined algorithm to the automatic abnormal-sound-discrimination section <b>18</b>.</p>
<p id="p-0182" num="0181">In addition, when it is requested that the processing time per piece of data be 2 ms or less as the total calculation time (processing time), the use-algorithm determining section <b>16</b> determines an algorithm (an abnormal-sound discrimination expression U) of a Pareto optimal solution <b>53</b> and supplies the determined algorithm to the automatic abnormal-sound-discrimination section <b>18</b>.</p>
<p id="p-0183" num="0182">As shown in <figref idref="DRAWINGS">FIG. 37</figref>, the automatic abnormal-sound-discrimination section <b>18</b> divides a new signal, supplied from the new-signal input section <b>17</b>, into signals in unit times which are similar to those in the learning-data cutout section <b>31</b>. The automatic abnormal-sound-discrimination section <b>18</b> then discriminates whether or not each divided new signal per unit time is an abnormal sound or a normal sound by using the algorithm supplied from the use-algorithm determining section <b>16</b>, and supplies a discrimination result to the abnormal-sound discrimination result output section <b>19</b>. As described above, the discrimination result of the new signal per unit time is also supplied to the sample-signal storage section <b>12</b> for storage.</p>
<p id="p-0184" num="0183">The automatic algorithm-configuration processing performed by the automatic algorithm-configuring section <b>13</b> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 38</figref>.</p>
<p id="p-0185" num="0184">First, in step S<b>1</b>, the learning-data cutout section <b>31</b> cuts out learning data stored in the sample-signal storage section <b>12</b> and supplies the learning data to the amount-of-feature calculating section <b>33</b> and the machine learning section <b>34</b>. More specifically, the learning-data cutout section <b>31</b> cuts out data D<sub>1 </sub>to D<sub>n </sub>from learning data, supplies the data D<sub>1 </sub>to D<sub>n </sub>to the amount-of-feature calculating section <b>33</b>, and supplies the label data of the data D<sub>1 </sub>to D<sub>n </sub>to the machine learning section <b>34</b>.</p>
<p id="p-0186" num="0185">In step S<b>2</b>, on the basis of evaluation values of current-generation feature extraction expressions 1 to m, the evaluation values being supplied from the machine learning section <b>34</b>, the feature-extraction-expression list generating section <b>32</b> executes feature-extraction-expression list generation processing for generating next-generation feature extraction expressions 1 to m (a feature-extraction-expression list). The processing in step S<b>2</b> is repeatedly performed. When the processing in step S<b>2</b> is performed for the first time, the current-generation feature extraction expressions 1 to m do not exist and thus next-generation feature extraction expressions 1 to m are generated through the random generation. Details of the feature-extraction-expression list generation processing are described below with reference to <figref idref="DRAWINGS">FIG. 39</figref>.</p>
<p id="p-0187" num="0186">In step S<b>3</b>, the amount-of-feature calculating section <b>33</b> executes amount-of-feature calculation processing. That is, with respect to the respective data D<sub>1 </sub>to D<sub>n </sub>supplied from the learning-data cutout section <b>31</b>, the amount-of-feature calculating section <b>33</b> performs calculation in accordance with the feature extraction expressions 1 to m supplied from the feature-extraction-expression list generating section <b>32</b>, and supplies calculation results, i.e., the amounts of features of the data D<sub>1 </sub>to D<sub>n</sub>, to the machine learning section <b>34</b>. Details of the amount-of-feature calculation processing are described below with reference to <figref idref="DRAWINGS">FIG. 46</figref>.</p>
<p id="p-0188" num="0187">In step S<b>4</b>, the machine learning section <b>34</b> executes machine learning processing. That is, by using the amounts of features of the data D<sub>1 </sub>to D<sub>n</sub>, the amounts being supplied from the amount-of-feature calculating section <b>33</b>, the machine learning section <b>34</b> estimates an abnormal-sound discrimination expression U that uses feature extraction expressions and that best matches the label data of the data D<sub>1 </sub>to D<sub>n </sub>supplied from the learning-data cutout section <b>31</b>. The machine learning section <b>34</b> determines evaluation values of the respective feature extraction expressions when the estimated abnormal-sound discrimination expression U is used and supplies the determined evaluation values to the feature-extraction-expression list generating section <b>32</b>. Details of the machine learning processing are described below with reference to <figref idref="DRAWINGS">FIG. 47</figref>.</p>
<p id="p-0189" num="0188">In step S<b>5</b>, the machine learning section <b>34</b> determines whether or not a completion condition is satisfied. More specifically, for example, when the processing in step S<b>2</b> to S<b>4</b> is repeatedly executed a predetermined number of times or when an operation for stopping is detected, the machine learning section <b>34</b> determines that the completion condition is satisfied.</p>
<p id="p-0190" num="0189">When it is determined in step S<b>5</b> that the completion condition is not satisfied, the process returns to step S<b>2</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0191" num="0190">On the other hand, when it is determined in step S<b>5</b> that the completion condition is satisfied, the process proceeds to step S<b>6</b>. In step S<b>6</b>, the machine learning section <b>34</b> supplies the last-generation feature extraction expressions and the abnormal-sound discrimination expression U to the amount-of-feature extraction algorithm output section <b>35</b>. The amount-of-feature extraction algorithm output section <b>35</b> then outputs the last-generation feature extraction expressions and the abnormal-sound discrimination expression U to the tradeoff analyzing section <b>14</b>, thereby ending the processing.</p>
<p id="p-0192" num="0191">The feature-extraction-expression list generation processing executed in step S<b>2</b> shown in <figref idref="DRAWINGS">FIG. 38</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 39</figref>.</p>
<p id="p-0193" num="0192">In step S<b>21</b>, the feature-extraction-expression list generating section <b>32</b> determines whether or not a feature-extraction-expression list to be generated is a second generation or later.</p>
<p id="p-0194" num="0193">When it is determined in step S<b>21</b> that the feature-extraction-expression list to be generated is not a second generation or later, that is, is a first generation, the process proceeds to step S<b>22</b>. In step S<b>22</b>, the feature-extraction-expression list generating section <b>32</b> executes random generation processing, which is described below with reference to <figref idref="DRAWINGS">FIG. 40</figref>.</p>
<p id="p-0195" num="0194">When it is determined in step S<b>21</b> that the feature-extraction-expression list is a second generation or later, the process proceeds to step S<b>23</b>. In step S<b>23</b>, the feature-extraction-expression list generating section <b>32</b> executes next-generation list generation processing, which is described below with reference to <figref idref="DRAWINGS">FIG. 41</figref>.</p>
<p id="p-0196" num="0195">After the processing in step S<b>22</b> or S<b>23</b> is completed, the process proceeds to step S<b>24</b> in which the feature-extraction-expression list generating section <b>32</b> supplies the feature-extraction-expression list, generated by the processing in step S<b>22</b> or S<b>23</b>, to the amount-of-feature calculating section <b>33</b>. The process then returns to step S<b>2</b> shown in <figref idref="DRAWINGS">FIG. 38</figref> and proceeds to step S<b>3</b>.</p>
<p id="p-0197" num="0196">As described above, for the first generation, all feature extraction expressions are randomly generated, and for the second generation or later, a next-generation list generation processing is performed using the genetic search technique.</p>
<p id="p-0198" num="0197">The random generation processing executed in step S<b>22</b> shown in <figref idref="DRAWINGS">FIG. 39</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 40</figref>.</p>
<p id="p-0199" num="0198">In step S<b>41</b>, the feature-extraction-expression list generating section <b>32</b> initializes an expression loop parameter M to 1 and starts an expression loop. The expression loop is repeated a number of times corresponding to the number &#x201c;m&#x201d; of feature extraction expressions included in the feature-extraction-expression list.</p>
<p id="p-0200" num="0199">In step S<b>42</b>, the feature-extraction-expression list generating section <b>32</b> determines a type of an input signal of the Mth feature extraction expression (hereinafter, also referred to as &#x201c;feature extraction expression [M]&#x201d;). In the present embodiment, it is determined that the type of the input signal is &#x201c;wav&#x201d; representing a waveform.</p>
<p id="p-0201" num="0200">In step S<b>43</b>, the feature-extraction-expression list generating section <b>32</b> randomly determines a processing-target axis and one operator for the feature extraction expression [M] to be generated.</p>
<p id="p-0202" num="0201">In step S<b>44</b>, the feature-extraction-expression list generating section <b>32</b> determines whether or not a computation result of the feature-extraction expression [M] that has been generated up to the current point in time is a scalar quantity (a first order value). When it is determined in step S<b>44</b> that the computation result is not a scalar quantity, the process returns to step S<b>43</b> and the processing subsequent thereto is repeated, so that one operator is added.</p>
<p id="p-0203" num="0202">When it is determined in step S<b>44</b> that the computation result is a scalar quantity, the process proceeds to step S<b>45</b> in which the feature-extraction-expression list generating section <b>32</b> determines whether or not the expression loop parameter M is smaller than the maximum value m. When it is determined in step S<b>45</b> that the expression-loop parameter M is smaller than the maximum value m, the expression loop parameter M is incremented by 1. The process then returns to step S<b>42</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0204" num="0203">On the other hand, when it is determined in step S<b>45</b> that the expression loop parameter M is not smaller than the maximum value m (i.e., is equal to the maximum value m), the expression loop is exited. The process then returns to step S<b>22</b> in <figref idref="DRAWINGS">FIG. 39</figref> and proceeds to step S<b>24</b>.</p>
<p id="p-0205" num="0204">As a result of the above-described processing, the first-generation feature-extraction-expression list is generated.</p>
<p id="p-0206" num="0205">The next-generation list generation processing executed in step S<b>23</b> shown in <figref idref="DRAWINGS">FIG. 39</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 41</figref>.</p>
<p id="p-0207" num="0206">In step S<b>61</b>, the feature-extraction-expression list generating section <b>32</b> determines values so as to satisfy ms+mx+mm+mr=m (the total number of feature extraction expressions), where ms indicates the number of feature extraction expressions (the number of selections) generated by the selection generation processing, mx indicates the number of feature extraction expressions (the number of crosses) generated by the cross generation processing, mm indicates the number of feature extraction expressions (the number of mutations) generated by the mutation generation processing, and mr indicates the number of feature extraction expressions (the number of random generations) generated by the random generation processing.</p>
<p id="p-0208" num="0207">The ratios of the values may be predetermined or the values may be randomly determined so as to satisfy ms+mx+mm+mr=m.</p>
<p id="p-0209" num="0208">In step S<b>62</b>, the feature-extraction-expression list generating section <b>32</b> executes selection generation processing, which is described below with reference to <figref idref="DRAWINGS">FIG. 42</figref>.</p>
<p id="p-0210" num="0209">In step S<b>63</b>, the feature-extraction-expression list generating section <b>32</b> executes cross generation processing, which is described below with reference to <figref idref="DRAWINGS">FIG. 43</figref>.</p>
<p id="p-0211" num="0210">In step S<b>64</b>, the feature-extraction-expression list generating section <b>32</b> executes mutation generation processing, which is described below with reference to <figref idref="DRAWINGS">FIG. 44</figref>.</p>
<p id="p-0212" num="0211">In step S<b>65</b>, the feature-extraction-expression list generating section <b>32</b> executes random generation processing, which is described below with reference to <figref idref="DRAWINGS">FIG. 45</figref>. After the random generation processing is competed, the process returns to step S<b>23</b> in <figref idref="DRAWINGS">FIG. 39</figref> and proceeds to step S<b>24</b>.</p>
<p id="p-0213" num="0212">As a result of the above-described processing, the second-generation feature-extraction expression list or later is generated based on the genetic search technique.</p>
<p id="p-0214" num="0213">The selection generation processing executed in step S<b>62</b> shown in <figref idref="DRAWINGS">FIG. 41</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 42</figref>.</p>
<p id="p-0215" num="0214">In step S<b>91</b>, the feature-extraction-expression list generating section <b>32</b> sorts the current-generation feature extraction expressions in descending order of the evaluation values.</p>
<p id="p-0216" num="0215">In step S<b>92</b>, the feature-extraction-expression list generating section <b>32</b> uses top ms feature extraction expressions as next-generation feature extraction expressions. The process then returns to step S<b>62</b> in <figref idref="DRAWINGS">FIG. 41</figref> and proceeds to step S<b>63</b>.</p>
<p id="p-0217" num="0216">As a result of the processing, the feature extraction expressions having higher-evaluation values can be selected and can be copied to the next-generation feature-extraction-expression list.</p>
<p id="p-0218" num="0217">The cross generation processing executed in step S<b>63</b> shown in <figref idref="DRAWINGS">FIG. 41</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 43</figref>.</p>
<p id="p-0219" num="0218">In step S<b>121</b>, the feature-extraction-expression list generating section <b>32</b> initializes a cross loop parameter MX to 1 and starts a cross loop. The cross loop is repeated a number of times corresponding to the number &#x201c;mx&#x201d; of crosses.</p>
<p id="p-0220" num="0219">In step S<b>122</b>, the feature-extraction-expression list generating section <b>32</b> randomly selects one expression (referred to as &#x201c;expression A&#x201d;) from all feature extraction expressions included in the current-generation feature-extraction-expression list while assigning a weight so as to facilitate selection of a feature extraction expression having a higher-evaluation value.</p>
<p id="p-0221" num="0220">In step S<b>123</b>, the feature-extraction-expression list generating section <b>32</b> randomly selects one expression (referred to as &#x201c;expression B&#x201d;) from all feature extraction expressions included in the current-generation feature-extraction-expression list while assigning a weight so as to facilitate selection of a feature extraction expression having a higher-evaluation value.</p>
<p id="p-0222" num="0221">In step S<b>124</b>, the feature-extraction-expression list generating section <b>32</b> determines whether or not expression B is different from expression A. When it is determined in step S<b>124</b> that expression B is not different from expression A, the process returns to step S<b>123</b> and the processing subsequent thereto is repeated, so that expression B is newly selected until it is determined that expression B is different from expression A.</p>
<p id="p-0223" num="0222">When it is determined in step S<b>124</b> that expression B is different from expression A, the process proceeds to step S<b>125</b> in which the feature-extraction-expression list generating section <b>32</b> exchanges parts of expressions A and B to create a new feature extraction expression.</p>
<p id="p-0224" num="0223">In this case, the feature-extraction-expression list generating section <b>32</b> combines parts of expressions A and B so that a scalar quantity can be obtained from the input data by computing a feature extraction expression after the combination, that is, so that no contradiction in the processing axes occurs when computation processing is sequentially performed from the beginning.</p>
<p id="p-0225" num="0224">In step S<b>126</b>, the feature-extraction-expression list generating section <b>32</b> adds the new feature extraction expression generated in step S<b>125</b> to the next-generation feature extraction expressions.</p>
<p id="p-0226" num="0225">In step S<b>127</b>, the feature-extraction-expression list generating section <b>32</b> determines whether or not the cross loop parameter MX is smaller than the number &#x201c;mx&#x201d; of crosses, the number &#x201c;mx&#x201d; being the maximum value of the cross loop parameter MX. When it is determined in step S<b>127</b> that the cross-loop parameter MX is smaller than the number &#x201c;mx&#x201d; of crosses, the cross loop parameter MX is incremented by 1. The process then returns to step S<b>122</b> and the processing subsequent thereto is repeated. On the other hand, when it is determined in step S<b>127</b> that the cross loop parameter MX is not smaller than the number &#x201c;mx&#x201d; of crosses, i.e., is equal to the number &#x201c;mx&#x201d; of crosses, the cross loop is exited. Thus, the process returns to step S<b>63</b> in <figref idref="DRAWINGS">FIG. 41</figref> and proceeds to step S<b>64</b>.</p>
<p id="p-0227" num="0226">Each time the processing in steps S<b>122</b> to S<b>126</b>, which form the cross loop, is executed once, one feature extraction expression to be included in the feature extraction expressions in the next-generation feature-extraction-expression list is generated. When the cross loop is completed, mx feature extraction expressions of the feature extraction expressions included in the feature-extraction-expression list are generated.</p>
<p id="p-0228" num="0227">In such processing, feature extraction expressions in the current-generation feature-extraction-expression list are selected while a weight is being assigned so as to facilitate selection of feature extraction expressions having higher-evaluation values, and the selected feature extraction expressions are used to perform cross generation processing to generate a feature extraction expression included in the next-generation feature-extraction-expression list.</p>
<p id="p-0229" num="0228">The mutation generation processing executed in step S<b>64</b> shown in <figref idref="DRAWINGS">FIG. 41</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 44</figref>.</p>
<p id="p-0230" num="0229">In step S<b>151</b>, the feature-extraction-expression list generating section <b>32</b> initializes a mutation loop parameter MM to 1 and starts a mutation loop. The mutation loop is repeated a number of times corresponding to the number &#x201c;mm&#x201d; of mutations.</p>
<p id="p-0231" num="0230">In step S<b>152</b>, the feature-extraction-expression list generating section <b>32</b> randomly selects one feature extraction expression (referred to as &#x201c;expression A&#x201d;) from all feature extraction expressions included in the current-generation feature-extraction-expression list while assigning a weight so as to facilitate selection of a feature extraction expression having a higher-evaluation value.</p>
<p id="p-0232" num="0231">In step S<b>153</b>, the feature-extraction-expression list generating section <b>32</b> causes mutation by performing processing, for example, changing or deleting a part of the selected expression A or changing a parameter therein, to thereby create a new feature extraction expression.</p>
<p id="p-0233" num="0232">In this case, the feature-extraction-expression list generating section <b>32</b> changes a part of the expression by using such a method that a scalar quantity can be obtained from the input data by computing a feature extraction expression after changing the part of the expression, that is, by using such a method that no contradiction in the processing axes occurs when computation processing is sequentially performed from the beginning.</p>
<p id="p-0234" num="0233">In step S<b>154</b>, the feature-extraction-expression list generating section <b>32</b> adds the new feature extraction expression generated in step S<b>153</b> to the next-generation feature extraction expressions.</p>
<p id="p-0235" num="0234">In step S<b>155</b>, the feature-extraction-expression list generating section <b>32</b> determines whether or not the mutation loop parameter MM is smaller than the number &#x201c;mm&#x201d; of mutations, the number &#x201c;mm&#x201d; being the maximum value of the mutation loop parameter MM. When it is determined in step S<b>155</b> that the mutation loop parameter MM is smaller than the number &#x201c;mm&#x201d; of mutations, the mutation loop parameter MM is incremented by 1. The process then returns to step S<b>152</b> and the processing subsequent thereto is repeated. On the other hand, when it is determined in step S<b>155</b> that the mutation loop parameter MM is not smaller than the number &#x201c;mm&#x201d; of mutations, i.e., is equal to the number &#x201c;mm&#x201d; of mutations, the mutation loop is exited. Thus, the process returns to step S<b>64</b> in <figref idref="DRAWINGS">FIG. 41</figref> and proceeds to step S<b>65</b>.</p>
<p id="p-0236" num="0235">Each time the processing in steps S<b>152</b> to S<b>154</b>, which form the mutation loop, is executed once, one feature extraction expression to be included in the next-generation feature-extraction-expression list is generated. When the mutation loop is completed, mm feature extraction expressions of the feature extraction expressions included in the feature-extraction-expression list are generated.</p>
<p id="p-0237" num="0236">In such processing, feature extraction expressions in the current-generation feature-extraction-expression list are selected from the current-generation feature-extraction-expression list while a weight is being assigned so as to facilitate selection of a feature extraction expression having a higher-evaluation value, and the selected feature extraction expression is used to perform mutation generation processing to generate a feature extraction expression to be included in the next-generation feature-extraction-expression list.</p>
<p id="p-0238" num="0237">The random generation processing executed in step S<b>65</b> shown in <figref idref="DRAWINGS">FIG. 41</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 45</figref>.</p>
<p id="p-0239" num="0238">In step S<b>181</b>, the feature-extraction-expression list generating section <b>32</b> initializes a random generation loop parameter MR to 1 and starts a random generation loop. The random generation loop is repeated a number of times corresponding to the number &#x201c;mr&#x201d; of feature extraction expressions included in the feature-extraction-expression list.</p>
<p id="p-0240" num="0239">In step S<b>182</b>, the feature-extraction-expression list generating section <b>32</b> determines a type of an input signal of the MRth feature extraction expression (hereinafter, also referred to as &#x201c;feature extraction expression [MR]&#x201d;). In the present embodiment, it is determined that the type of the input signal is &#x201c;wav&#x201d; representing a waveform.</p>
<p id="p-0241" num="0240">In step S<b>183</b>, the feature-extraction-expression list generating section <b>32</b> randomly determines a processing-target axis and one operator for the feature extraction expression [MR] to be generated.</p>
<p id="p-0242" num="0241">In step S<b>184</b>, the feature-extraction-expression list generating section <b>32</b> determines whether or not a computation result of the feature-extraction expression [MR] that has been generated up to the current point in time is a scalar quantity (a first order value). When it is determined in step S<b>184</b> that the computation result is not a scalar quantity, the process returns to step S<b>183</b> and the processing subsequent thereto is repeated, so that one operator is added.</p>
<p id="p-0243" num="0242">When it is determined in step S<b>184</b> that the computation result is a scalar quantity, the process proceeds to step S<b>185</b> in which the feature-extraction-expression list generating section <b>32</b> determines whether or not the random generation loop parameter MR is smaller than the maximum value mr. When it is determined in step S<b>185</b> that the random generation loop parameter MR is smaller than the maximum value mr, the random generation loop parameter MR is incremented by 1. The process then returns to step S<b>182</b> and the processing subsequent thereto is repeated. On the other hand, when it is determined in step S<b>185</b> that the random generation loop parameter MR is not smaller than the maximum value mr (i.e., is equal to the maximum value mr), the random generation loop is exited. The process then returns to step S<b>65</b> in <figref idref="DRAWINGS">FIG. 41</figref>, further returns to step S<b>23</b> in <figref idref="DRAWINGS">FIG. 39</figref>, and proceeds to step S<b>24</b>.</p>
<p id="p-0244" num="0243">Each time the processing in steps S<b>182</b> to S<b>185</b>, which form the random generation loop, is executed once, one feature extraction expression to be included in the next-generation feature-extraction-expression list is generated. When the random generation loop is completed, mr feature extraction expressions of the feature extraction expressions included in the feature-extraction-expression list are generated.</p>
<p id="p-0245" num="0244">In such processing, some of the feature extraction expressions to be included in the next-generation feature-extracting-feature list are generated through the random generation processing.</p>
<p id="p-0246" num="0245">The amount-of-feature calculation processing executed in step S<b>3</b> shown in <figref idref="DRAWINGS">FIG. 38</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 46</figref>.</p>
<p id="p-0247" num="0246">In step S<b>211</b>, the amount-of-feature calculating section <b>33</b> obtains the feature-extraction-expression list from the feature-extraction-expression list generating section <b>32</b>, initializes an expression loop parameter M to 1, and starts an expression loop. The expression loop is repeated a number of times corresponding to the number &#x201c;m&#x201d; of feature extraction expressions included in the feature-extraction-expression list.</p>
<p id="p-0248" num="0247">In step S<b>212</b>, the amount-of-feature calculating section <b>33</b> initializes a data loop parameter N to 1 and starts a data loop. The data loop is repeated a number of times corresponding to the number &#x201c;n&#x201d; of pieces of data D cut out by the learning-data cutout section <b>31</b>.</p>
<p id="p-0249" num="0248">In step S<b>213</b>, with respect to data D<sub>N</sub>, the amount-of-feature calculating section <b>33</b> calculates an amount of features when the Mth feature extraction expression (the feature extraction expression [M]) is used.</p>
<p id="p-0250" num="0249">In step S<b>214</b>, the amount-of-feature calculating section <b>33</b> determines whether or not the data loop parameter N is smaller than a maximum value n. When it is determined in step S<b>214</b> that the data loop parameter N is smaller than the maximum value n, the data loop parameter N is incremented by 1. The process then returns to step S<b>213</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0251" num="0250">On the other hand, when it is determined in step S<b>214</b> that the data loop parameter N is not smaller than the maximum value n, (i.e., is equal to the maximum value n), the data loop is exited and the process proceeds to step S<b>215</b>.</p>
<p id="p-0252" num="0251">In step S<b>215</b>, the amount-of-feature calculating section <b>33</b> determines whether or not the expression loop parameter M is smaller than the maximum value m. When it is determined in step S<b>215</b> that the expression loop parameter M is smaller than the maximum value m, the expression loop parameter M is incremented by 1. The process then returns to step S<b>212</b> and the processing subsequent thereto is repeated. On the other hand, when it is determined in step S<b>215</b> that the expression loop parameter M is not smaller than the maximum value m (i.e., is equal to the maximum value m), the expression loop is exited. The process then returns to step S<b>3</b> in <figref idref="DRAWINGS">FIG. 38</figref> and proceeds to step S<b>4</b>.</p>
<p id="p-0253" num="0252">As a result of the above-described processing, the amount of features extracted using each feature extraction expression is calculated.</p>
<p id="p-0254" num="0253">The machine learning processing executed in step S<b>4</b> shown in <figref idref="DRAWINGS">FIG. 38</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 47</figref>.</p>
<p id="p-0255" num="0254">In step S<b>241</b>, the machine learning section <b>34</b> creates a reference state of the current-generation feature extraction expressions 1 to m. That is, the machine learning section <b>34</b> sets all current-generation feature extraction expressions 1 to m to so that they are not used.</p>
<p id="p-0256" num="0255">In step S<b>242</b>, the machine learning section <b>34</b> initializes an amount-of-feature loop parameter M to 1 and starts an amount-of-feature loop. The amount-of-feature loop is repeated a number of times corresponding to the number &#x201c;m&#x201d; of feature extraction expressions included in the feature-extraction-expression list.</p>
<p id="p-0257" num="0256">In step S<b>243</b>, on the basis of the feature extraction expressions 1 to m in the reference state, the machine learning section <b>34</b> determines a use combination of feature extraction expressions in which use/not-use of the amount of features resulting from the Mth feature extraction expression is inverted (the use combination may also be referred to as the &#x201c;use combination [M] of the feature extraction expressions, hereinafter).</p>
<p id="p-0258" num="0257">In step S<b>244</b>, the machine learning section <b>34</b> generates (estimates) an abnormal-sound discrimination expression U for the use combination [M] of the inverted feature extraction expressions.</p>
<p id="p-0259" num="0258">In step S<b>245</b>, the machine learning section <b>34</b> calculates an evaluation reference value (AIC) of the generated abnormal-sound discrimination expression U.</p>
<p id="p-0260" num="0259">In step S<b>246</b>, the machine learning section <b>34</b> determines whether or not the amount-of-feature loop parameter M is smaller than the maximum value m. When it is determined in step S<b>246</b> that the amount-of-feature loop parameter M is smaller than the maximum value m, the amount-of-feature loop parameter M is incremented by 1. The process then returns to step S<b>243</b> and the processing subsequent thereto is repeated. On the other hand, when it is determined in step S<b>246</b> that the amount-of-feature loop parameter M is not smaller than the maximum value m, (i.e., is equal to the maximum value m), the amount-of-feature loop is exited and the process proceeds to step S<b>247</b>.</p>
<p id="p-0261" num="0260">In step S<b>247</b>, the machine learning section <b>34</b> selects, as a new reference state, a use combination of inverted feature extraction expressions which has a highest evaluation (i.e., which has a smallest evaluation reference value).</p>
<p id="p-0262" num="0261">In step S<b>248</b>, the machine learning section <b>34</b> determines whether or not no more evaluation reference value of the generated abnormal-sound discrimination expression U is updated. When it is determined in step S<b>248</b> that the evaluation reference value is still updated, the process returns to step S<b>242</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0263" num="0262">On the other hand, when it is determined in step S<b>248</b> that the no more evaluation reference value is updated, the process returns step S<b>4</b> shown in <figref idref="DRAWINGS">FIG. 38</figref> and proceeds to step S<b>5</b>.</p>
<p id="p-0264" num="0263">The tradeoff analysis processing executed by the tradeoff analyzing section <b>14</b> will now be described with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 48</figref>.</p>
<p id="p-0265" num="0264">In step S<b>271</b>, the tradeoff analyzing section <b>14</b> performs Pareto optimal solution initialization processing, which is described below with reference to <figref idref="DRAWINGS">FIG. 49</figref>. In the initialization processing, multiple Pareto optimal solutions in an initial state are generated.</p>
<p id="p-0266" num="0265">In step S<b>272</b>, the tradeoff analyzing section <b>14</b> randomly selects one of the Pareto optimal solutions in the initial state and generates a new solution through mutation of the selected solution. For example, with respect to one to three feature extraction expressions in the selected solution, the tradeoff analyzing section <b>14</b> generates a new solution in which use/not-use is randomly changed.</p>
<p id="p-0267" num="0266">In step S<b>273</b>, the tradeoff analyzing section <b>14</b> calculates a total calculation time and an accuracy when the new solution is used.</p>
<p id="p-0268" num="0267">Subsequently, in steps S<b>274</b> to S<b>277</b>, the tradeoff analyzing section <b>14</b> updates the Pareto optimal solutions. That is, in step S<b>274</b>, the tradeoff analyzing section <b>14</b> adds the new solution to the Pareto optimal solutions, initializes a solution loop parameter K to 1, and starts a solution loop. The solution loop is repeated a number of times corresponding to the total number &#x201c;k&#x201d; of currently held Pareto optimal solutions.</p>
<p id="p-0269" num="0268">In step S<b>275</b>, the tradeoff analyzing section <b>14</b> determines whether or not a solution having a higher speed and a higher accuracy than the Kth Pareto optimal solution (hereinafter, also referred to as the &#x201c;Pareto optimal solution [K]&#x201d;) exists. When it is determined in step S<b>275</b> that a solution having a higher speed and a higher accuracy than the Pareto optimal solution [K] exists, the process proceeds to step S<b>276</b>. In step S<b>276</b>, the tradeoff analyzing section <b>14</b> deletes the Pareto optimal solution [K] from the Pareto optimal solutions.</p>
<p id="p-0270" num="0269">When it is determined in step S<b>275</b> that no solution having a higher speed and a higher accuracy than the Pareto optimal solution [K] exists, the process proceeds to step S<b>277</b>. In step S<b>277</b>, the tradeoff analyzing section <b>14</b> determines whether or not the solution loop parameter K is smaller than a maximum value k. When it is determined in step S<b>277</b> that the solution loop parameter K is smaller than the maximum value k, the solution loop parameter K is incremented by 1. The process then returns to step S<b>275</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0271" num="0270">On the other hand, when it is determined that the solution loop parameter K is not smaller than the maximum value k, (i.e., is equal to the maximum value k), the solution loop is exited and the process proceeds to step S<b>278</b>.</p>
<p id="p-0272" num="0271">In step S<b>278</b>, the tradeoff analyzing section <b>14</b> determines whether or not a Pareto optimal solution is searched for by performing the processing in which a new solution is generated through mutation from the Pareto optimal solutions has been repeated a predetermined number of times. When it is determined in step S<b>278</b> that the processing in which a Pareto optimal solution is searched for has not been repeated the predetermined number of times, the process returns to step S<b>272</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0273" num="0272">On the other hand, when it is determined in step S<b>278</b> that the processing in which a Pareto optimal solution is searched for has been repeated the predetermined number of times, the processing ends.</p>
<p id="p-0274" num="0273">The Pareto-optimal-solution initialization processing executed in step S<b>271</b> shown in <figref idref="DRAWINGS">FIG. 48</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 49</figref>.</p>
<p id="p-0275" num="0274">In step S<b>301</b>, the tradeoff analyzing section <b>14</b> initializes an amount-of-feature loop parameter M to 1 and starts an amount-of-feature loop. The amount-of-feature loop is repeated a number of times corresponding to the number &#x201c;m&#x201d; of feature extraction expressions included in the feature-extraction-expression list.</p>
<p id="p-0276" num="0275">In step S<b>302</b>, the tradeoff analyzing section <b>14</b> calculates an average time (average calculation time) taken for calculating the Mth feature extraction expression (hereinafter, may also be referred to as &#x201c;feature extraction expression [M]&#x201d;) and label-data correlation coefficient.</p>
<p id="p-0277" num="0276">In step S<b>303</b>, the tradeoff analyzing section <b>14</b> determines whether or not the amount-of-feature loop parameter M is smaller than the maximum value m. When it is determined in step S<b>303</b> that the amount-of-feature loop parameter M is smaller than the maximum value m, the amount-of-feature loop parameter M is incremented by 1. The process then returns to step S<b>302</b> and the processing subsequent thereto is repeated. On the other hand, when it is determined in step S<b>303</b> that the amount-of-feature loop parameter M is not smaller than the maximum value m, (i.e., is equal to the maximum value m), the amount-of-feature loop is exited and the process proceeds to step S<b>304</b>.</p>
<p id="p-0278" num="0277">In step S<b>304</b>, the tradeoff analyzing section <b>14</b> sets, of the feature extraction expressions in use, a feature extraction expression having a smallest correlation coefficient so that the feature extraction expression is not used.</p>
<p id="p-0279" num="0278">In step S<b>305</b>, the tradeoff analyzing section <b>14</b> performs linear discrimination using only the amounts of features extracted by the remaining feature extraction expressions, except the feature extraction expression set as not being used, and determines a new abnormal-sound discrimination expression U. In step S<b>305</b>, the tradeoff analyzing section <b>14</b> also calculates an accuracy when the determined new abnormal-sound discrimination expression U is used to perform abnormal-sound discrimination.</p>
<p id="p-0280" num="0279">In step S<b>306</b>, the tradeoff analyzing section <b>14</b> calculates a time (a total calculation time) taken when the determined new abnormal-sound discrimination expression U is used to perform abnormal-sound discrimination, and adds the determined total calculation time to a list of Pareto optimal solution candidates.</p>
<p id="p-0281" num="0280">In step S<b>307</b>, the tradeoff analyzing section <b>14</b> determines whether or not the number of remaining feature extraction expressions is one. When it is determined in step S<b>307</b> that the number of remaining feature extraction expressions is not one, the process returns to step S<b>304</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0282" num="0281">On the other hand, when it is determined that the number of remaining feature extraction expressions is one, the process proceeds to step S<b>308</b> in which the tradeoff analyzing section <b>14</b> performs processing for updating the Pareto optimal solutions. This Pareto-optimal-solution update processing is analogous to the processing in steps S<b>274</b> to S<b>277</b> described above and shown in <figref idref="DRAWINGS">FIG. 48</figref>, and thus descriptions thereof are not given hereinafter. After step S<b>308</b>, the process returns to step S<b>271</b> in <figref idref="DRAWINGS">FIG. 48</figref> and proceeds to step S<b>272</b>.</p>
<p id="p-0283" num="0282">As described above, after the Pareto solution solutions are initialized, more appropriate Pareto optimal solutions are searched for, so that update is performed.</p>
<p id="p-0284" num="0283">On the basis of information such as the currently available resource state and requested performance of the information processing apparatus <b>1</b>, the use-algorithm determining section <b>16</b> determines an optimum algorithm out of the algorithms (the abnormal-sound discrimination expressions U) serving as Pareto optimal solutions determined by the above-described tradeoff analysis processing and stored in the Pareto-optimal-solution storage section <b>15</b>. The use-algorithm determining section <b>16</b> then supplies the determined optimum algorithm to the automatic abnormal-sound-discrimination section <b>18</b>. On the basis of the algorithm supplied from the use-algorithm determining section <b>16</b>, the automatic abnormal-sound-discrimination section <b>18</b> discriminates whether the input new signal has an abnormal sound or a normal sound.</p>
<p id="p-0285" num="0284">Thus, the information processing apparatus <b>1</b> can present multiple algorithms having a tradeoff relationship in evaluation indices, such as the available resource status (processing speed) and the requested performance (accuracy). Depending on the current situation, the information processing apparatus <b>1</b> can also determine an optimal one of the algorithms having a tradeoff relationship in the evaluation indices and can perform discrimination using the determined algorithm.</p>
<p id="p-0286" num="0285">Other embodiments using the concept of the above-described Pareto optimal solutions will now be described.</p>
<p id="p-0287" num="0286"><figref idref="DRAWINGS">FIG. 50</figref> is a block diagram showing an automatic algorithm-configuring system <b>101</b> according to another embodiment of the present invention. Sections corresponding to those in the above-described embodiment are denoted by the same reference numerals, and descriptions thereof are not given.</p>
<p id="p-0288" num="0287">The automatic algorithm-configuring system <b>101</b> includes a teacher-data obtaining section <b>131</b>, a feature-extraction-expression list Generating section <b>32</b>, an amount-of-feature calculating section <b>132</b>, an evaluation-value calculating section <b>133</b>, and an algorithm output section <b>134</b>.</p>
<p id="p-0289" num="0288">The teacher-data obtaining section <b>131</b> obtains waveform signals (waveform data) input as teacher data and the values of target variables to be extracted from the data, supplies the waveform signals to the amount-of-feature calculating section <b>132</b>, and also supplies the values of the target variables of the waveform signals to the evaluation-value calculating section <b>133</b>.</p>
<p id="p-0290" num="0289">For example, as shown in <figref idref="DRAWINGS">FIG. 51</figref>, the teacher-data obtaining section <b>131</b> obtains I pieces of waveform data DD<sub>1 </sub>to DD<sub>1 </sub>and the values of the target variables thereof as teacher data, and supplies the waveform data DD<sub>1 </sub>to DD<sub>1 </sub>and the values of the target variables to the amount-of-feature calculating section <b>132</b> and the evaluation-value calculating section <b>133</b>, respectively.</p>
<p id="p-0291" num="0290">The amount-of-feature calculating section <b>132</b> performs calculation of the feature extraction expressions 1 to m, supplied from the feature-extraction-expression list generating section <b>32</b>, with respect to the waveform data DD<sub>1 </sub>to DD<sub>1 </sub>supplied from the teacher-data obtaining section <b>131</b>. The amount-of-feature calculating section <b>132</b> then supplies calculation results, i.e., the amounts of features of the waveform data DD<sub>1 </sub>to DD<sub>1</sub>, to the evaluation-value calculating section <b>133</b>.</p>
<p id="p-0292" num="0291">The amount-of-feature calculating section <b>132</b> calculates an average time taken for calculating each feature extraction expression and supplies the calculated average time to the evaluation-value calculating section <b>133</b>.</p>
<p id="p-0293" num="0292"><figref idref="DRAWINGS">FIG. 52</figref> shows an example of the amounts of features and the average times which are calculated by the amount-of-feature calculating section <b>132</b> and supplied to the evaluation-value calculating section <b>133</b>.</p>
<p id="p-0294" num="0293">The evaluation-value calculating section <b>133</b> generates solution candidates, i.e., candidates of the Pareto optimal solutions.</p>
<p id="p-0295" num="0294">First, the evaluation-value calculating section <b>133</b> randomly determines whether or not to use each of the amounts of features resulting from the feature extraction expressions 1 to m. When one or more Pareto optimal solutions are already held, the evaluation-value calculating section <b>133</b> can determine whether or not to use the amount of features resulting from each of the feature extraction expressions 1 to m by randomly selecting one solution from the held Pareto optimal solutions and causing mutation of the selected solution (by changing use/not-use of an arbitrary feature extraction expression).</p>
<p id="p-0296" num="0295"><figref idref="DRAWINGS">FIG. 53A</figref> shows an example in which the use/not-use of the amount of features of each of feature extraction expressions 1 to m is randomly determined, and <figref idref="DRAWINGS">FIG. 53B</figref> shows an example in which the use/not-use of the amount of features resulting from each of the feature extraction expressions 1 to m is determined by causing mutation of a solution selected from the Pareto optimal solutions.</p>
<p id="p-0297" num="0296">Next, the evaluation-value calculating section <b>133</b> generates (estimates) an information estimation expression (which is a solution candidate) based on a use combination of the determined m feature extraction expressions. That is, as shown in <figref idref="DRAWINGS">FIG. 54</figref>, the evaluation-value calculating section <b>133</b> associates the amounts of the features of the waveform data DD<sub>1 </sub>to DD<sub>1 </sub>with the target variables, the amounts of features and the target variables being supplied from the amount-of-feature calculating section <b>132</b>, and assigns the associated values to an information estimation expression that is similar to expression (1) to determine linear combination coefficients b<sub>1 </sub>to b<sub>m </sub>and intercept b<sub>0 </sub>so that the squared error of estimated errors of each target variable becomes the smallest. The information estimation expressions are created with respect to both target variables of the speed and brightness.</p>
<p id="p-0298" num="0297">Subsequently, using expression (2), the evaluation-value calculating section <b>133</b> calculates, for each target variable, an evaluation reference value (an amount-of-quantity reference) for evaluating the generated information estimation expression.</p>
<p id="p-0299" num="0298">The evaluation-value calculating section <b>133</b> also calculates a total calculation time taken when the generated information estimation expression is used to calculate the target variable. More specifically, as shown in <figref idref="DRAWINGS">FIG. 55</figref>, of the average times taken for computation of the feature extraction expressions, the average times being supplied from the amount-of-feature calculating section <b>132</b>, the sum of times excluding the average time of a feature extraction expression that is not used is the total calculation time of the generated information estimation expression. In the example shown in <figref idref="DRAWINGS">FIG. 55</figref>, since the feature extraction expression 3 is not used, the average time (0.3 ms) of the feature extraction expression 3 is not included in the total calculation time.</p>
<p id="p-0300" num="0299"><figref idref="DRAWINGS">FIG. 56</figref> shows a speed evaluation reference value, a brightness evaluation reference value, and a total calculation time, which are obtained by performing calculation on an information estimation expression that is a solution candidate.</p>
<p id="p-0301" num="0300">Next, the evaluation-value calculating section <b>133</b> compares the speed evaluation reference value, the brightness evaluation reference value, and the total calculation time of the solution candidate with a speed evaluation reference value, a brightness evaluation reference value, and a total calculation time of a currently held Pareto optimal solution, and deletes a non-Pareto optimal solution.</p>
<p id="p-0302" num="0301"><figref idref="DRAWINGS">FIG. 57</figref> shows an example in which the solution candidate is compared with p Pareto optimal solutions that are currently held.</p>
<p id="p-0303" num="0302">In the example shown in <figref idref="DRAWINGS">FIG. 57</figref>, the Pareto optimal solution <b>3</b> is inferior to the solution candidate in all evaluation indices of the speed evaluation reference value, the brightness evaluation reference value, and the total calculation time, and is thus deleted from the Pareto optimal solutions as a non-Pareto optimal solution.</p>
<p id="p-0304" num="0303">For each Pareto optimal solution or solution candidate, the evaluation-value calculating section <b>133</b> holds information indicating use/not-use of each feature extraction expression, parameters (the linear combination coefficients b<sub>1 </sub>to b<sub>m </sub>and the intercept b<sub>0</sub>) of the information estimation expression for each target variable (information type), an evaluation reference value for each target variable (information type), and a total calculation time.</p>
<p id="p-0305" num="0304">Next, by using the remaining Pareto optimal solutions, the evaluation-value calculating section <b>133</b> determines evaluation values of the feature extraction expressions. The basic concept of determining evaluation values of the feature extraction expressions is similar to that in the embodiment described above. In the present embodiment, specifically, since multiple target variables (information types) and multiple Pareto optimal solutions (information estimation expressions) exist, the evaluation-value calculating section <b>133</b> determines a value obtained by summing a contribution rate C<sub>i </sub>of the ith amount of features of the Pareto optimal solutions with respect to all target variables (information types).</p>
<p id="p-0306" num="0305">That is, using a contribution rate C(o,X<sub>i</sub>,T<sub>j</sub>) of the amount &#x201c;X<sub>i</sub>&#x201d; of features of the Pareto optimal solution o (o=1 to p) with respect to the ith target variable (information type) T<sub>j </sub>(j=1 to k), the contribution rate C<sub>i </sub>in expression (3) noted above can be rewritten as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>C</i>(<i>o,X</i><sub>i</sub><i>,T</i><sub>j</sub>)=<i>b</i><sub>oji</sub>&#xf7;StDev(<i>X</i><sub>i</sub>)&#xd7;StDev(<i>T</i><sub>j</sub>)&#xd7;Correl(<i>X</i><sub>i</sub><i>,T</i><sub>j</sub>)&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0307" num="0306">The evaluation values of the feature extraction expressions can be calculated from SUM_C<sub>i </sub>in expression (5) in which the contribution rates C(o,X<sub>i</sub>,T<sub>j</sub>) are summed with respect to all target variables of all Pareto optimal solutions.</p>
<p id="p-0308" num="0307">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>SUM_C</mi>
          <mi>i</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>o</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>p</mi>
          </munderover>
          <mo>&#x2062;</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>j</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>k</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>C</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>o</mi>
                  <mo>,</mo>
                  <msub>
                    <mi>X</mi>
                    <mi>i</mi>
                  </msub>
                  <mo>,</mo>
                  <msub>
                    <mi>T</mi>
                    <mi>j</mi>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0309" num="0308">The evaluation-value calculating section <b>133</b> calculates the evaluation value SUM_C<sub>i </sub>(represented by expression (5)) of the feature extraction expressions and supplies a calculation result to the feature-extraction-expression list generating section <b>32</b>.</p>
<p id="p-0310" num="0309">When a predetermined condition is reached by performing processing for updating the Pareto optimal solution and calculating the evaluation value of each feature extraction expression a predetermined number of times, the evaluation-value calculating section <b>133</b> supplies ultimately remained Pareto optimal solutions to the algorithm output section <b>134</b>.</p>
<p id="p-0311" num="0310">The algorithm output section <b>134</b> selects an optimum algorithm from the supplied Pareto optimal solutions, depending on whether to place importance on accuracies of speed and brightness or on the requested processing speed (the total calculation time). The algorithm output section <b>134</b> then outputs the selected algorithm. Which algorithm is to be selected may be determined in accordance with, for example, a user instruction.</p>
<p id="p-0312" num="0311">The Pareto optimal solution search processing performed by the feature-extraction-expression list generating section <b>32</b>, the amount-of-feature calculating section <b>132</b>, and the evaluation-value calculating section <b>133</b> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 58</figref>.</p>
<p id="p-0313" num="0312">First, in step S<b>401</b>, on the basis of evaluation values of the current-generation feature extraction expressions 1 to m, the evaluation values being supplied from the evaluation-value calculating section <b>133</b>, the feature-extraction-expression list generating section <b>32</b> executes feature-extraction-expression list generation processing for generating next-generation feature extraction expressions 1 to m (a feature-extraction-expression list). This processing is analogous to the processing in step S<b>2</b> described above and shown in <figref idref="DRAWINGS">FIG. 38</figref>, and thus detailed description thereof is not given hereinbelow.</p>
<p id="p-0314" num="0313">In step S<b>402</b>, the amount-of-feature calculating section <b>132</b> executes amount-of-feature calculation processing. That is, with respect to the waveform data DD<sub>1 </sub>to DD<sub>1 </sub>supplied from the teacher-data obtaining section <b>131</b>, the amount-of-feature calculating section <b>132</b> calculates an amount of features resulting from each feature extraction expression and an average time taken for calculating each feature extraction expression. Details of the amount-of-feature calculation processing are described below with reference a flowchart shown in <figref idref="DRAWINGS">FIG. 59</figref>.</p>
<p id="p-0315" num="0314">In step S<b>403</b>, the evaluation-value calculating section <b>133</b> executes evaluation-value calculation processing. That is, the evaluation-value calculating section <b>133</b> generates a solution candidate that is a Pareto solution candidate, compares the solution candidate with currently held Pareto optimal solutions, performs Pareto-optimal-solution update involving deletion of a non-Pareto solution, and calculates evaluation values of the feature extraction expressions on the basis of the remaining Pareto optimal solutions. Details of the evaluation-value calculation processing are described below with reference a flowchart shown in <figref idref="DRAWINGS">FIG. 61</figref>.</p>
<p id="p-0316" num="0315">In step S<b>404</b>, the evaluation-value calculating section <b>133</b> determines whether or not a completion condition is satisfied. More specifically, for example, when the processing in step S<b>401</b> to S<b>403</b> described above is repeatedly executed a preset number of times or when an operation for stopping is detected, the evaluation-value calculating section <b>133</b> determines that the completion condition is satisfied.</p>
<p id="p-0317" num="0316">When it is determined in step S<b>404</b> that the completion condition is not satisfied, the process returns to step S<b>401</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0318" num="0317">On the other hand, when it is determined in step S<b>404</b> that the completion condition is satisfied, the process proceeds to step S<b>405</b>. In step S<b>405</b>, the evaluation-value calculating section <b>133</b> outputs the last-generation feature extraction expressions and the Pareto optima solutions of the information estimation expressions to the algorithm output section <b>134</b>, thereby ending the processing.</p>
<p id="p-0319" num="0318">For example, depending on whether to place importance on accuracies of speed and brightness or on the processing speed (the total calculation time), the algorithm output section <b>134</b> selects an optimum algorithm from the Pareto optimal solutions supplied from the evaluation-value calculating section <b>133</b>, as appropriate. The algorithm output section <b>134</b> then outputs the selected algorithm.</p>
<p id="p-0320" num="0319">The amount-of-feature calculation processing executed in step S<b>402</b> shown in <figref idref="DRAWINGS">FIG. 58</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 59</figref>.</p>
<p id="p-0321" num="0320">In step S<b>411</b>, the amount-of-feature calculating section <b>132</b> obtains the feature-extraction-expression list from the feature-extraction-expression list generating section <b>32</b>, initializes an expression loop parameter M to 1, and starts an expression loop. The expression loop is repeated a number of times corresponding to the number &#x201c;m&#x201d; of feature extraction expressions included in the feature-extraction-expression list.</p>
<p id="p-0322" num="0321">In step S<b>412</b>, the amount-of-feature calculating section <b>132</b> initializes a data loop parameter L to 1 and starts a data loop. The data loop is repeated a number of times corresponding the number &#x201c;1&#x201d; of pieces of the waveform data DD obtained by the teacher-data obtaining section <b>131</b>.</p>
<p id="p-0323" num="0322">In step S<b>413</b>, the amount-of-feature calculating section calculates an amount of features when the Mth feature extraction expression (the feature extraction expression [M]) is used with respect to waveform data DD<sub>L </sub>and holds a time taken for the calculation.</p>
<p id="p-0324" num="0323">In step S<b>414</b>, the amount-of-feature calculating section determines whether or not the data loop parameter L is smaller than a maximum value 1. When it is determined in step S<b>414</b> that the data loop parameter L is smaller than the maximum value 1, the data loop parameter L is incremented by 1. The process then returns to step S<b>413</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0325" num="0324">On the other hand, when it is determined in step S<b>414</b> that the data loop parameter L is not smaller than the maximum value 1, (i.e., is equal to the maximum value 1), the data loop is exited and the process proceeds to step S<b>415</b>.</p>
<p id="p-0326" num="0325">In step S<b>415</b>, the amount-of-feature calculating section calculates an average time taken for calculating the feature extraction expression [M]. That is, the amount-of-feature calculating section <b>132</b> calculates an average time of the calculation times of the feature extraction expressions [M] for the waveform data DD<sub>1 </sub>to DD<sub>1</sub>, the calculation times being calculated in step S<b>413</b> and being held.</p>
<p id="p-0327" num="0326">In step S<b>416</b>, the amount-of-feature calculating section <b>132</b> determines whether or not the expression loop parameter M is smaller than a maximum value m. When it is determined in step S<b>416</b> that the expression loop parameter M is smaller than the maximum value m, the expression loop parameter M is incremented by 1. The process then returns to step S<b>412</b> and the processing subsequent thereto is repeated. On the other hand, when it is determined in step S<b>416</b> that the expression loop parameter M is not smaller than the maximum value m (i.e., is equal to the maximum value m), the expression loop is exited. The process then returns to step S<b>402</b> in <figref idref="DRAWINGS">FIG. 58</figref> and proceeds to step S<b>403</b>.</p>
<p id="p-0328" num="0327">The evaluation-value calculation processing executed in step S<b>403</b> shown in <figref idref="DRAWINGS">FIG. 58</figref> will be described next with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. 60</figref>.</p>
<p id="p-0329" num="0328">In step S<b>431</b>, the evaluation-value calculating section <b>133</b> determines whether or not to use the amount of features resulting from each of the feature extraction expressions 1 to m. The evaluation-value calculating section <b>133</b> can determine whether or not to use the amount of features resulting from each of the feature extraction expressions 1 to m by changing use/not-use of one selection (through mutation of one selection) randomly selected from the held Pareto optimal solutions. However, when the processing in step S<b>431</b> is performed for the first time, the use/not-use of the amount of features is randomly determined since no Pareto optimal solutions are held.</p>
<p id="p-0330" num="0329">In step S<b>432</b>, the evaluation-value calculating section <b>133</b> initializes a target variable loop parameter K to 1 and starts a data loop. The target variable loop is repeated a number of times corresponding to the number &#x201c;k&#x201d; of information types for the target variables.</p>
<p id="p-0331" num="0330">In step S<b>433</b>, the evaluation-value calculating section <b>133</b> generates (estimates) an information estimation expression that serves as a solution candidate and that is used for estimating the amount of information corresponding to the target variable K when the use combination of the m feature extraction expressions determined in step S<b>431</b> is used.</p>
<p id="p-0332" num="0331">In step S<b>434</b>, using expression (2) noted above, the evaluation-value calculating section <b>133</b> calculates an evaluation reference value for evaluating an information estimation expression for estimating the amount of information corresponding to the target variable K.</p>
<p id="p-0333" num="0332">In step S<b>435</b>, the evaluation-value calculating section <b>133</b> determines whether or not the target variable loop parameter K is smaller than a maximum value k. When it is determined in step S<b>435</b> that the target-variable loop parameter K is smaller than the maximum value k, the target variable loop parameter K is incremented by 1. The process then returns to step S<b>433</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0334" num="0333">On the other hand, when it is determined in step S<b>435</b> that the target variable loop parameter K is not smaller than the maximum value k, (i.e., is equal to the maximum value k), the target variable loop is exited and the process proceeds to step S<b>436</b>.</p>
<p id="p-0335" num="0334">In step S<b>436</b>, on the basis of the average time of each feature extraction expression supplied from the amount-of-feature calculating section <b>132</b>, the evaluation-value calculating section <b>133</b> calculates a total calculation time taken when the generated information estimation expression is used to calculate the target variable.</p>
<p id="p-0336" num="0335">In steps S<b>437</b> to S<b>440</b>, the evaluation-value calculating section <b>133</b> updates the Pareto optimal solutions. That is, in step S<b>437</b>, the evaluation-value calculating section <b>133</b> adds the solution candidate to the Pareto optimal solutions, initializes a solution loop parameter o to 1, and starts a solution loop. The solution loop is repeated a number of times corresponding to the total number &#x201c;p&#x201d; of currently held Pareto optimal solutions including the solution candidate(s).</p>
<p id="p-0337" num="0336">In step S<b>438</b>, the evaluation-value calculating section <b>133</b> determines whether or not a solution having a higher speed and a higher accuracy than the oth Pareto optimal solution (hereinafter, also referred to as the &#x201c;Pareto optimal solution [o]&#x201d;) exists. When it is determined in step S<b>438</b> that a solution having a higher speed and a higher accuracy than the Pareto optimal resolution [o] exists, the process proceeds to step S<b>439</b>. In step S<b>439</b>, the evaluation-value calculating section <b>133</b> deletes the Pareto optimal solution [o] from the Pareto optimal solutions.</p>
<p id="p-0338" num="0337">On the other hand, when it is determined in step S<b>438</b> that no solution having a higher speed and a higher accuracy than the Pareto optimal resolution [o] exists, the process proceeds to step S<b>440</b>. In step S<b>440</b>, the evaluation-value calculating section <b>133</b> determines whether or not the solution loop parameter o is smaller than a maximum value p. When it is determined in step S<b>438</b> that the solution loop parameter o is smaller than the maximum value p, the solution loop parameter o is incremented by 1. The process then returns to step S<b>438</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0339" num="0338">On the other hand, when it is determined in step S<b>438</b> that the solution loop parameter o is not smaller than the maximum value p, (i.e., is equal to the maximum value p), the solution loop is exited and the process proceeds to step S<b>441</b>.</p>
<p id="p-0340" num="0339">In step S<b>441</b>, the evaluation-value calculating section <b>133</b> determines whether or not the processing in which the solution candidate is generated and the Pareto optimal solution is searched for is repeated a predetermined number of times. When it is determined in step S<b>441</b> that the processing in which the Pareto optimal solution is searched for has not been repeated the predetermined number of times, the process returns to step S<b>431</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0341" num="0340">On the other hand, when it is determined in step S<b>441</b> that the processing in which the Pareto optimal solution is searched for has been repeated the predetermined number of times, the process proceeds to step S<b>442</b>. In step S<b>442</b>, using the remaining Pareto optimal solutions, the evaluation-value calculating section <b>133</b> executes evaluation-value determination processing for determining evaluation values of the feature extraction expressions.</p>
<p id="p-0342" num="0341"><figref idref="DRAWINGS">FIG. 61</figref> is a flowchart showing details of the evaluation-value determination processing in step S<b>442</b> shown in <figref idref="DRAWINGS">FIG. 60</figref>.</p>
<p id="p-0343" num="0342">First, in step S<b>461</b>, the evaluation-value calculating section <b>133</b> initializes an amount-of-feature loop parameter i to 1 and starts an amount-of-feature loop. The amount-of-feature loop is repeated a number of times corresponding to the number &#x201c;m&#x201d; of feature extraction expressions included in the feature-extraction-expression list.</p>
<p id="p-0344" num="0343">In step S<b>462</b>, the evaluation-value calculating section <b>133</b> resets the evaluation value SUM_C<sub>i </sub>of the ith feature extraction expression, the evaluation value being given by expression (5) noted above.</p>
<p id="p-0345" num="0344">In step S<b>463</b>, the evaluation-value calculating section initializes a solution loop parameter o to 1 and starts a solution loop. The solution loop is repeated a number of times corresponding to the total number &#x201c;p&#x201d; of currently held Pareto optimal solutions.</p>
<p id="p-0346" num="0345">In step S<b>464</b>, the evaluation-value calculating section initializes a target variable loop parameter j to 1 and starts a target-variable loop. The target variable loop is repeated a number of times corresponding to the number &#x201c;k&#x201d; of information types for the target variables.</p>
<p id="p-0347" num="0346">In step S<b>465</b>, the evaluation-value calculating section determines a contribution rate C(o,X<sub>i</sub>,T<sub>j</sub>) of the ith amount &#x201c;X<sub>i</sub>&#x201d; of features of the Pareto optimal solution o with respect to the jth target variable T<sub>j</sub>, and adds the determined contribution rate to the evaluation value SUM_C<sub>i</sub>.</p>
<p id="p-0348" num="0347">In step S<b>466</b>, the evaluation-value calculating section determines whether or not the target variable loop parameter j is smaller than a maximum value k. When it is determined in step S<b>466</b> that the target-variable loop parameter j is smaller than the maximum value k, the target variable loop parameter j is incremented by 1. The process then returns to step S<b>465</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0349" num="0348">On the other hand, when it is determined in step S<b>466</b> that the target variable loop parameter j is not smaller than the maximum value k, (i.e., is equal to the maximum value k), the target variable loop is exited and the process proceeds to step S<b>467</b>.</p>
<p id="p-0350" num="0349">In step S<b>467</b>, the evaluation-value calculating section <b>133</b> determines whether or not the solution loop parameter o is smaller than a maximum value p. When it is determined in step S<b>467</b> that the solution loop parameter o is smaller than the maximum value p, the solution loop parameter o is incremented by 1. The process then returns to step S<b>464</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0351" num="0350">On the other hand, when it is determined in step S<b>467</b> that the solution loop parameter o is not smaller than the maximum value p, (i.e., is equal to the maximum value p), the solution loop is exited and the process proceeds to step S<b>468</b>.</p>
<p id="p-0352" num="0351">In step S<b>468</b>, the evaluation-value calculating section <b>133</b> determines whether or not the amount-of-feature loop parameter i is smaller than the maximum value m. When it is determined in step S<b>468</b> that the amount-of-feature loop parameter i is smaller than the maximum value m, the amount-of-feature loop parameter i is incremented by 1. The process then returns to step S<b>462</b> and the processing subsequent thereto is repeated.</p>
<p id="p-0353" num="0352">On the other hand, when it is determined in step S<b>468</b> that the amount-of-feature loop parameter i is not smaller than the maximum value m (i.e., is equal to the maximum value m), the amount-of-feature loop is exited. The process then returns to step S<b>442</b> in <figref idref="DRAWINGS">FIG. 60</figref>, further returns to S<b>403</b> in step <figref idref="DRAWINGS">FIG. 58</figref>, and proceeds to step S<b>404</b> in <figref idref="DRAWINGS">FIG. 58</figref>.</p>
<p id="p-0354" num="0353">As described above, the automatic algorithm-configuring system <b>101</b> creates, as a Pareto optimal solution in an initial state, a single algorithm by randomly determining use/not-use of the amount of features resulting from each of the extraction expressions 1 to m supplied from the feature-extraction-expression list generating section <b>32</b>. On the basis of the Pareto optimal solution in the initial state, the automatic algorithm-configuring system <b>101</b> creates a solution candidate of a Pareto optimal solution by randomly determining use/not-use of the amount of features resulting from each of the feature extraction expressions 1 to m or by determining use/not-use of the amount of features resulting from each of the feature extraction expressions 1 to m through mutation of one solution randomly selected from held Pareto optimal solutions. The automatic algorithm-configuring system <b>101</b> then compares the evaluation reference value and the total calculation time of the solution candidate with the evaluation reference values and the total calculation times of the Pareto optimal solutions held as solution candidates, to thereby determine multiple Pareto optimal solutions in which the degrees of importance to be placed on the accuracies or the processing speeds of the target variables are varied from each other.</p>
<p id="p-0355" num="0354">With this arrangement, for extraction of the speed and the amount of brightness of an input new signal, the automatic algorithm-configuring system <b>101</b> can present multiple information extraction algorithms serving as Pareto optimal solutions in which the degrees of importance to be placed on the accuracies or the processing speeds of the target variables are different from each other. That is, the automatic algorithm-configuring system <b>101</b> can present multiple algorithms having a tradeoff relationship in evaluation indices.</p>
<p id="p-0356" num="0355">The user then can select an optimum algorithm in accordance with the degree of importance of the accuracy or the processing speed of each target index and can obtain an extraction result of the target variable.</p>
<p id="p-0357" num="0356">In the information processing apparatus <b>1</b> shown in <figref idref="DRAWINGS">FIG. 1</figref>, the automatic algorithm-configuring section <b>13</b> and the tradeoff analyzing section <b>14</b> can be replaced with an information extracting block that includes the teacher-data obtaining section <b>131</b>, the amount-of-feature calculating section <b>132</b>, the evaluation-value calculating section <b>133</b>, and the feature-extraction-expression list generating section <b>32</b> provided in the automatic algorithm-configuring system <b>101</b>. In such a case, the information processing apparatus <b>1</b> can be used as an apparatus for performing information extraction processing for extracting a speed and the amount of brightness from an input new signal.</p>
<p id="p-0358" num="0357">In the automatic algorithm-configuration technology of the related art, only an algorithm for the highest performance (the highest accuracy) is determined. Thus, unlike a case in which an algorithm is manually configured, it is difficult to configure an algorithm that is capable of performing processing with a minimum amount of resource and a desired accuracy while satisfying a request. However, the information processing apparatus <b>1</b> and the automatic algorithm-configuring system <b>101</b> according to the embodiments of the present invention can generate an algorithm that can perform such processing.</p>
<p id="p-0359" num="0358">The above-described series of processing can be executed by hardware or software. In such a case, the above-described processing may be executed by a computer <b>200</b> as shown in <figref idref="DRAWINGS">FIG. 62</figref>.</p>
<p id="p-0360" num="0359">In <figref idref="DRAWINGS">FIG. 62</figref>, a CPU (central processing unit) <b>201</b> executes various types of processing in accordance with a program stored in a ROM (read only memory) <b>202</b> or a program loaded from a storage section <b>208</b> into a RAM (random access memory) <b>203</b>. The RAM <b>203</b> stores data and so on that are used when the CPU <b>201</b> executes various types of processing and so on, as appropriate.</p>
<p id="p-0361" num="0360">The CPU <b>201</b>, the ROM <b>202</b>, and the RAM <b>203</b> are interconnected through an internal bus <b>204</b>. The internal bus <b>204</b> is also connected to an input/output interface <b>205</b>.</p>
<p id="p-0362" num="0361">An input section <b>206</b>, an output section <b>207</b>, the storage section <b>208</b>, and a communication section <b>209</b> are also connected to the input/output interface <b>205</b>. The input section <b>206</b> includes, for example, a keyboard and a mouse. The output section <b>207</b> includes, for example, a speaker and a display, such as a CRT (cathode ray tube) display or an LCD (liquid crystal display). The storage section <b>208</b> includes, for example, a hard disk. The communication section <b>209</b> includes a modem, a terminal adapter, or the like. The communication section <b>209</b> performs communication processing through a network including a telephone line or a CATV (cable television) network.</p>
<p id="p-0363" num="0362">A drive <b>210</b> may also be connected to the input/output interface <b>205</b>, to which a removable medium, such as a magnetic disk, an optical disk, a magneto-optical disk, or a semiconductor memory, is attached as appropriate. A computer program read from the removable medium <b>221</b> is installed to the storage section <b>208</b>, as appropriate.</p>
<p id="p-0364" num="0363">The program executed by the computer <b>200</b> may be a program that time-sequentially performs processing according to the sequence described hereinabove or may be a program that performs processing concurrently or upon call.</p>
<p id="p-0365" num="0364">Herein, the steps shown in the flowcharts not only include processing that is time-sequentially performed according to the described sequence, but also include processing that is concurrently or individually executed without being necessarily time-sequentially processed.</p>
<p id="p-0366" num="0365">The present application contains subject matter related to that disclosed in Japanese Priority Patent Application JP 2008-183019 filed in the Japan Patent Office on Jul. 14, 2008, the entire content of which is hereby incorporated by reference.</p>
<p id="p-0367" num="0366">Embodiments of the present invention are not limited to the above-described embodiments, and various changes can be made thereto without departing from the spirit and scope of the present invention.</p>
<p id="p-0368" num="0367">It should be understood by those skilled in the art that various modifications, combinations, sub-combinations and alterations may occur depending on design requirements and other factors insofar as they are within the scope of the appended claims or the equivalents thereof.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08626685-20140107-M00001.NB">
<img id="EMI-M00001" he="4.23mm" wi="76.20mm" file="US08626685-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08626685-20140107-M00002.NB">
<img id="EMI-M00002" he="9.57mm" wi="76.20mm" file="US08626685-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An information processing apparatus comprising:
<claim-text>an algorithm configuring section that configures an amount-of-feature extraction algorithm that determines whether an input signal has a particular characteristic by using a genetic search technique, the algorithm comprising:
<claim-text>feature extraction expressions that specify:
<claim-text>a type of the input signal, the type indicating a type of data representing the input signal; and</claim-text>
<claim-text>operations to be performed on the input signal; and</claim-text>
</claim-text>
<claim-text>an information estimation expression including a linear combination of the feature extraction expressions, wherein the information estimation expression uses first-order values output from the feature extraction expressions to estimate information indicating features of the input signal;</claim-text>
</claim-text>
<claim-text>a tradeoff analyzing section that generates pareto optimal solutions by selecting information estimation expressions having maximum values of evaluation indices;</claim-text>
<claim-text>an optimum algorithm determining section that selects, from the pareto optimal solutions, an optimum algorithm that matches a requested condition of the evaluation indices; and</claim-text>
<claim-text>a storage for storing the algorithm.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing the operations specified by each feature extraction expression on the input signal produces a scalar value.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the tradeoff analyzing section generates pareto optimal solutions in an initial state by using algorithms having different numbers of feature extraction expressions, the algorithms being created by deleting the feature extraction expressions one by one from the algorithm configured by the algorithm configuring section.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The information processing apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the tradeoff analyzing section updates the pareto optimal solutions by randomly changing use or not-use of each feature extraction expression in the pareto optimal solutions in the initial state.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the tradeoff analyzing section uses, as a pareto optimal solution in an initial state, the algorithm configured by the algorithm configuring section.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the optimum-algorithm determining section determines, of the pareto optimal solutions, the optimum algorithm on a basis of requested processing time and accuracy.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising an evaluation-value calculating section that determines evaluation values of the feature extraction expressions in the information estimation expression, wherein the algorithm configuring section updates the feature extraction expressions in the information estimation expression, on a basis of the determined evaluation values of the feature extraction expressions.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The information processing apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein when the algorithm configuring section configures multiple information estimation expressions as algorithms, a total value of contribution rates of the same feature extraction expressions in the information estimation expressions is used as the evaluation value of the corresponding feature extraction expression.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A computer-implemented information processing method comprising:
<claim-text>configuring, using a processor, an amount-of-feature extraction algorithm that determines whether an input signal has a particular characteristic by using a genetic search technique, the algorithm comprising:
<claim-text>feature extraction expressions that specify:
<claim-text>a type of the input signal, the type indicating a type of data representing the input signal; and</claim-text>
<claim-text>operations to be performed on the input signal; and</claim-text>
</claim-text>
<claim-text>an information estimation expression including a linear combination of the feature extraction expressions, wherein the information estimation expression uses first-order values output from the feature extraction expressions to estimate information indicating features of the input signal; and</claim-text>
</claim-text>
<claim-text>generating pareto optimal solutions by selecting information estimation expressions having maximum values of evaluation indices; and</claim-text>
<claim-text>selecting, from the pareto optimal solutions, an optimum algorithm that matches a requested condition of the evaluation indices.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The computer-implemented method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein optimizing includes generating pareto optimal solutions in an initial state by using algorithms having different numbers of feature extraction expressions, the algorithms being created by deleting the feature extraction expressions one by one from the algorithm.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The computer-implemented method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein in an initial state, the algorithm configured in the configuring step is used as-as a pareto optimal solution.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The computer-implemented method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein determining pareto optimal solutions includes updating the pareto optimal solutions by randomly changing use or not-use of each feature extraction expression in the pareto optimal solutions in the initial state.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The computer-implemented method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the optimum algorithm is determined on a basis of requested processing time and accuracy.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computer-implemented method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:
<claim-text>determining evaluation values of the feature extraction expressions in the information estimation expression, and</claim-text>
<claim-text>updating the feature extraction expressions in the information estimation expression, on a basis of the determined evaluation values of the feature extraction expressions.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computer-implemented method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein in the configuring step, a total value of contribution rates of the same feature extraction expressions in the information estimation expressions is used as the evaluation value of the corresponding feature extraction expression.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A non-transitory computer-readable medium storing instructions for causing a computer to execute operations comprising:
<claim-text>configuring an amount-of-feature extraction algorithm that determines whether an input signal has a particular characteristic by using a genetic search technique, the algorithm comprising:
<claim-text>feature extraction expressions that specify:
<claim-text>a type of the input signal, the type indicating a type of data representing the input signal; and</claim-text>
<claim-text>operations to be performed on the input signal; and</claim-text>
</claim-text>
<claim-text>an information estimation expression including a linear combination of the feature extraction expressions, wherein the information estimation expression uses first-order values output from the feature extraction expressions to estimate information indicating features of the input signal;</claim-text>
</claim-text>
<claim-text>generating pareto optimal solutions by selecting information estimation expressions having maximum values of evaluation indices; and</claim-text>
<claim-text>selecting, from the pareto optimal solutions, an optimum algorithm that matches a requested condition of the evaluation indices.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
