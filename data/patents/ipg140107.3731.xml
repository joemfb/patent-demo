<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624797-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624797</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12895746</doc-number>
<date>20100930</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>KR</country>
<doc-number>10-2009-0096210</doc-number>
<date>20091009</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>582</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>G</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345  4</main-classification>
<further-classification>345  11</further-classification>
</classification-national>
<invention-title id="d2e71">Mobile terminal and controlling method thereof</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6054969</doc-number>
<kind>A</kind>
<name>Haisma</name>
<date>20000400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6466202</doc-number>
<kind>B1</kind>
<name>Suso et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345169</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6512607</doc-number>
<kind>B1</kind>
<name>Windsor et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>359 15</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7333072</doc-number>
<kind>B2</kind>
<name>Yamazaki et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345  5</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7669543</doc-number>
<kind>B2</kind>
<name>Soltendieck et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>116 624</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7724208</doc-number>
<kind>B1</kind>
<name>Engel et al.</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345  4</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2001/0038412</doc-number>
<kind>A1</kind>
<name>McNelley et al.</name>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 141</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2004/0029636</doc-number>
<kind>A1</kind>
<name>Wells</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2005/0040753</doc-number>
<kind>A1</kind>
<name>Osame et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2006/0133047</doc-number>
<kind>A1</kind>
<name>Tomizuka et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2007/0150180</doc-number>
<kind>A1</kind>
<name>Matsuo et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2009/0132130</doc-number>
<kind>A1</kind>
<name>Kumon et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>701 49</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2009/0207124</doc-number>
<kind>A1</kind>
<name>Park et al.</name>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2009/0295976</doc-number>
<kind>A1</kind>
<name>Choi</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833311</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>WO</country>
<doc-number>2008/041313</doc-number>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>European Patent Office Application Serial No. 10186446.0, Search Report dated Oct. 8, 2013, 10 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>23</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345  11</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345  13</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345  4</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345  5</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345  6</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345173</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345175</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>178 1803- 1809</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715863</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715864</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>17</number-of-drawing-sheets>
<number-of-figures>17</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110084893</doc-number>
<kind>A1</kind>
<date>20110414</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Yun Sung</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Seong Joon</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Tho</last-name>
<first-name>Gi Hoon</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Ho Phil</first-name>
<address>
<city>Anyang-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Yun Sung</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Seong Joon</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Tho</last-name>
<first-name>Gi Hoon</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Ho Phil</first-name>
<address>
<city>Anyang-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Lee, Hong, Degerman, Kang &#x26; Waimey</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>LG Electronics Inc.</orgname>
<role>03</role>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Sheng</last-name>
<first-name>Tom</first-name>
<department>2692</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A mobile terminal is presented. The mobile terminal includes a main display unit comprising a first display unit, a transparent display unit comprising a second display unit, the transparent display unit being attached to the main display unit such that the transparent display unit may be viewed at an angle in comparison to the main display unit, and a controller controlling a three-dimensional (3D) and a two-dimensional (2D) image to be output by selectively displaying an image on at least the first display unit or the second display unit while the main display unit and the transparent display unit are positioned at an angle.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="207.09mm" wi="146.47mm" file="US08624797-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="233.93mm" wi="192.28mm" file="US08624797-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="214.04mm" wi="155.11mm" orientation="landscape" file="US08624797-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="142.24mm" wi="180.68mm" file="US08624797-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="235.80mm" wi="183.30mm" file="US08624797-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="182.03mm" wi="171.70mm" file="US08624797-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="234.53mm" wi="151.21mm" file="US08624797-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="183.30mm" wi="124.97mm" file="US08624797-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="200.58mm" wi="161.46mm" file="US08624797-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="226.82mm" wi="138.43mm" file="US08624797-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="237.07mm" wi="143.51mm" file="US08624797-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="192.28mm" wi="156.97mm" file="US08624797-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="191.60mm" wi="165.95mm" file="US08624797-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="197.36mm" wi="172.38mm" file="US08624797-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="242.91mm" wi="150.62mm" file="US08624797-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="250.53mm" wi="142.92mm" file="US08624797-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="239.69mm" wi="117.26mm" file="US08624797-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="236.47mm" wi="145.46mm" file="US08624797-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">Pursuant to 35 U.S.C. &#xa7;119(a), this application claims the benefit of earlier filing date and right of priority to Korean Application No. 10-2009-0096210, filed on Oct. 9, 2009, the contents of which are hereby incorporated by reference herein in their entirety.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to a mobile terminal, and more particularly, to a mobile terminal and controlling method thereof. Although the present invention is suitable for a wide scope of applications, it is particularly suitable for providing at least two display units including a transparent display unit.</p>
<p id="p-0005" num="0004">2. Discussion of the Related Art</p>
<p id="p-0006" num="0005">Generally, terminals can be classified as mobile terminals and stationary terminals according to their mobility. Mobile terminals can be classified into handheld terminals and vehicle mounted terminals according to their handheld portability.</p>
<p id="p-0007" num="0006">As functions of the terminals are diversified, the terminals are implemented into multimedia players provided with complex functions for photography, playback of music or video, game play, and broadcast reception.</p>
<p id="p-0008" num="0007">In order to support and enhance the terminal functions, structural and software improvements for the terminal may be taken into consideration.</p>
<p id="p-0009" num="0008">Recently, since a mobile terminal can be provided with an optical transparent display device, many efforts have been made to provide more convenient and diverse functions.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">Features and advantages of the invention will be set forth in the description which follows, and in part will be apparent from the description, or may be learned by practice of the invention. The objectives and other advantages of the invention will be realized and attained by the structure particularly pointed out in the written description and claims hereof as well as the appended drawings.</p>
<p id="p-0011" num="0010">According to one embodiment, a mobile terminal is presented. The mobile terminal includes a main display unit comprising a first display unit, a transparent display unit comprising a second display unit, the transparent display unit being attached to the main display unit such that the transparent display unit may be viewed at an angle in comparison to the main display unit, and a controller controlling a three-dimensional (3D) and a two-dimensional (2D) image to be output by selectively displaying an image on at least the first display unit or the second display unit while the main display unit and the transparent display unit are positioned at an angle.</p>
<p id="p-0012" num="0011">According to one feature, the transparent display unit further comprises a third display unit, such that the third display unit is adjacent to the first display unit and the second display unit is opposite the third display unit. Additionally, the 3D image is visible when the second display unit faces a user. Moreover, the 2D image is displayed on at least the first display unit or the third display unit when the main display unit and the transparent display unit are positioned at an angle. Furthermore, the 2D image is displayed on at least the second display unit or the third display unit when the mobile terminal is in a closed state. Finally, the 2D image is simultaneously displayed on at least the first display unit, the second display unit, or the third display unit when the 3D image is visible from the second display unit.</p>
<p id="p-0013" num="0012">According to another feature, the transparent display unit is deactivated when the mobile terminal is in a closed state, such that the transparent display unit covers the main display unit.</p>
<p id="p-0014" num="0013">According to yet another feature, the mobile terminal includes a first camera configured to photograph a user's image, wherein the controller detects a view point of the user via the user's image, corrects a position of an image displayed on the second display unit to correspond to the detected view point, and controls the image displayed on the second display unit to be displayed at a predetermined position on the first display unit. Additionally, the mobile terminal includes a memory unit configured to store a lookup table including a correction value for correcting the position of the image displayed on the second display unit according to the detected view point of the user, wherein the controller performs the position correction of the image displayed on the second display unit by referring to the lookup table.</p>
<p id="p-0015" num="0014">According to yet another feature an interactive menu is displayed on at least the second display unit or third display unit when the 3D image is visible from the second display unit.</p>
<p id="p-0016" num="0015">According to still yet another feature, the mobile terminal includes a pointer detecting unit configured to detect a position of a pointer in a space formed between the transparent display unit and the main display unit, wherein the controller controls an operation corresponding to the pointer position detected by the pointer detecting unit to be executed via a 3D user interface.</p>
<p id="p-0017" num="0016">According to another feature, when a 3D map image is visible from the second unit, a 2D map corresponding to the 3D map is displayed on the first display unit and an interactive menu for receiving user input is displayed on at least the third display unit, the second display unit, or the first display unit.</p>
<p id="p-0018" num="0017">According to another embodiment, a mobile terminal is presented. The mobile terminal includes a main display unit configured to display a first display, a transparent display unit configured to display a second display, the transparent display unit being attached to the main display unit such that the transparent display unit may be viewed at an angle in comparison to the main display unit, and a controller controlling a three-dimensional (3D) image to be output by selectively displaying an image on at least one of the first display and the second display while the main display unit and the transparent display unit are spaced apart from each other.</p>
<p id="p-0019" num="0018">According to yet another embodiment, a method for displaying an image is presented. The method includes displaying an image on a main display unit comprising a first display unit, displaying a second image on a transparent display unit comprising a second display unit, the transparent display unit being attached to the main display unit such that the transparent display unit may be viewed at an angle in comparison to the main display unit, and displaying a three-dimensional (3D) or a two-dimensional (2D) image by selectively displaying at least the first or second image while the main display unit and the transparent display unit are positioned at an angle.</p>
<p id="p-0020" num="0019">According to still yet another embodiment, a method for displaying an image is presented. The method includes displaying an image on a main display unit comprising a first display unit, displaying a second image on a transparent display unit comprising a second display unit, the transparent display unit being attached to the main display unit such that the transparent display unit may be viewed at an angle in comparison to the main display unit, and displaying a three-dimensional (3D) image by selectively displaying at least the first or second image while the main display unit and the transparent display unit are positioned at an angle.</p>
<p id="p-0021" num="0020">These and other embodiments will also become readily apparent to those skilled in the art from the following detailed description of the embodiments having reference to the attached figures, the invention not being limited to any particular embodiment disclosed.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0022" num="0021">The accompanying drawings, which are included to provide a further understanding of the invention and are incorporated in and constitute a part of this application, illustrate embodiment(s) of the invention and together with the description serve to explain the principle of the invention. In the drawings:</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 2</figref> is a front view diagram of a mobile terminal for describing one operational state of the mobile terminal according to an embodiment the present invention.</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 3</figref> is a diagram for describing proximity depth of a proximity sensor.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram illustrating a method of controlling a touch action when a pair of display units are overlapped with each other.</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram illustrating a 3D effect using a distance difference between two display units according to one embodiment of the present invention.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram illustrating a mobile terminal according to one embodiment of the present invention using the basic principle described with reference to <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart for a method of operating a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram of one example of correcting a picture displayed on a transparent display unit through recognition of user's position relative to a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram of another example of correcting a picture displayed on a transparent display unit through recognition of user's position relative to a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 10</figref> is a diagram for a method of implementing a 3D image on a parallex barrier type display unit applicable to embodiments of the present invention.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 11</figref> is a diagram illustrating a method of inputting a command to a mobile terminal and a method of recognizing the command according to one embodiment of the present invention.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 12</figref> is a diagram of one example of games playable in a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 13</figref> is a diagram of examples of providing a 3D map and geographical information using the 3D map in a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 14</figref> is a diagram of another example of executing a navigation function in a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 15</figref> is a diagram illustrating displaying a 3D message in a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 16</figref> is a diagram of one example of displaying a 3D gift interactive with a user in a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 17</figref> is a diagram of one example of generating and displaying a 3D photo in a mobile terminal according to another embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0040" num="0039">The suffixes &#x2018;module&#x2019;, &#x2018;unit&#x2019; and &#x2018;part&#x2019; may be used for elements in order to facilitate the disclosure. Significant meanings or roles may not be given to the suffixes themselves and it is understood that the &#x2018;module&#x2019;, &#x2018;unit&#x2019; and &#x2018;part&#x2019; may be used together or interchangeably.</p>
<p id="p-0041" num="0040">Embodiments of the present disclosure may be applicable to various types of terminals. Examples of such terminals may include mobile terminals as well as stationary terminals, such as mobile phones, user equipment, smart phones, digital televisions (DTVs), computers, digital broadcast terminals, personal digital assistants, portable multimedia players (PMPs) and/or navigators.</p>
<p id="p-0042" num="0041">A further description may be provided with regard to a mobile terminal, although such teachings may apply equally to other types of terminals.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram of a mobile terminal in accordance with an example embodiment. Other embodiments and arrangements may also be provided. <figref idref="DRAWINGS">FIG. 1</figref> shows a mobile terminal <b>100</b> having various components, although other components may also be used. More or less components may alternatively be implemented.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 1</figref> shows that the mobile terminal <b>100</b> includes a wireless communication unit <b>110</b>, an audio/video (A/V) input unit <b>120</b>, a user input unit <b>130</b>, a sensing unit <b>140</b>, an output unit <b>150</b>, a memory <b>160</b>, an interface unit <b>170</b>, a controller <b>180</b> and a power supply <b>190</b>.</p>
<p id="p-0045" num="0044">The wireless communication unit <b>110</b> may be configured with several components and/or modules. The wireless communication unit <b>110</b> may include a broadcast receiving module <b>111</b>, a mobile communication module <b>112</b>, a wireless Internet module <b>113</b>, a short-range communication module <b>114</b> and a position-location module <b>115</b>. The wireless communication unit <b>110</b> may include one or more components that permit wireless communication between the mobile terminal <b>100</b> and a wireless communication system or a network within which the mobile terminal <b>100</b> is located. In case of non-mobile terminals, the wireless communication unit <b>110</b> may be replaced with a wired communication unit. The wireless communication unit <b>110</b> and the wired communication unit may be commonly referred to as a communication unit.</p>
<p id="p-0046" num="0045">Examples of broadcast associated information may include information associated with a broadcast channel, a broadcast program, a broadcast service provider, etc. For example, broadcast associated information may include an electronic program guide (EPG) of a digital multimedia broadcasting (DMB) system and an electronic service guide (ESG) of a digital video broadcast-handheld (DVB-H) system.</p>
<p id="p-0047" num="0046">The broadcast signal may be a TV broadcast signal, a radio broadcast signal, and/or a data broadcast signal. The broadcast signal may further include a broadcast signal combined with a TV or radio broadcast signal.</p>
<p id="p-0048" num="0047">The broadcast receiving module <b>111</b> may receive broadcast signals transmitted from various types of broadcast systems. As a non-limiting example, the broadcasting systems may include a digital multimedia broadcasting-terrestrial (DMB-T) system, a digital multimedia broadcasting-satellite (DMB-S) system, a digital video broadcast-handheld (DVB-H) system, a data broadcasting system known as media forward link only (MediaFLO&#xae;) and an integrated services digital broadcast-terrestrial (ISDB-T) system. The reception of multicast signals may also be provided. Data received by the broadcast receiving module <b>111</b> may be stored in the memory <b>160</b>, for example.</p>
<p id="p-0049" num="0048">The mobile communication module <b>112</b> may communicate wireless signals with one or more network entities (e.g. a base station or Node-B). The signals may represent audio, video, multimedia, control signaling, and data, etc.</p>
<p id="p-0050" num="0049">The wireless Internet module <b>113</b> may support Internet access for the mobile terminal <b>100</b>. This wireless Internet module <b>113</b> may be internally or externally coupled to the mobile terminal <b>100</b>. Suitable technologies for wireless Internet may include, but are not limited to, WLAN (Wireless LAN)(Wi-Fi), Wibro (Wireless broadband), Wimax (World Interoperability for Microwave Access), and/or HSDPA (High Speed Downlink Packet Access). The wireless Internet module <b>113</b> may be replaced with a wired Internet module in non-mobile terminals. The wireless Internet module <b>113</b> and the wired Internet module may be referred to as an Internet module.</p>
<p id="p-0051" num="0050">The short-range communication module <b>114</b> may facilitate short-range communications. Suitable technologies for short-range communication may include, but are not limited to, radio frequency identification (RFID), infrared data association (IrDA), ultra-wideband (UWB), as well as networking technologies such as Bluetooth and ZigBee.</p>
<p id="p-0052" num="0051">The position-location module <b>115</b> may identify or otherwise obtain a location of the mobile terminal <b>100</b>. The position-location module <b>115</b> may be provided using global positioning system (GPS) components that cooperate with associated satellites, network components, and/or combinations thereof.</p>
<p id="p-0053" num="0052">The position-location module <b>115</b> may precisely calculate current three-dimensional position information based on longitude, latitude and altitude by calculating distance information and precise time information from at least three satellites and then by applying triangulation to the calculated information. Location and time information may be calculated using three satellites, and errors of the calculated location position and time information may then be amended or changed using another satellite. The position-location module <b>115</b> may calculate speed information by continuously calculating a real-time current location.</p>
<p id="p-0054" num="0053">The audio/video (A/V) input unit <b>120</b> may provide audio or video signal input to the mobile terminal <b>100</b>. The A/V input unit <b>120</b> may include a camera <b>121</b> and a microphone <b>122</b>. The camera <b>121</b> may receive and process image frames of still pictures and/or video.</p>
<p id="p-0055" num="0054">The microphone <b>122</b> may receive an external audio signal while the mobile terminal is in a particular mode, such as a phone call mode, a recording mode and/or a voice recognition mode. The received audio signal may then be processed and converted into digital data.</p>
<p id="p-0056" num="0055">The mobile terminal <b>100</b>, and in particular, the A/V input unit <b>120</b>, may include a noise removing algorithm (or noise canceling algorithm) to remove noise generated in the course of receiving the external audio signal. Data generated by the A/V input unit <b>120</b> may be stored in the memory <b>160</b>, utilized by the output unit <b>150</b>, and/or transmitted via one or more modules of the wireless communication unit <b>110</b>. Two or more microphones and/or cameras may also be provided.</p>
<p id="p-0057" num="0056">The user input unit <b>130</b> may generate input data responsive to user manipulation of an associated input device or devices. Examples of such devices may include a keypad, a dome switch, a touchpad (e.g., static pressure/capacitance), a jog wheel and/or a jog switch. A specific example is one in which the user input unit <b>130</b> is configured as a touchpad in cooperation with a display, as will be described below.</p>
<p id="p-0058" num="0057">The broadcast receiving module <b>111</b> may receive a broadcast signal and/or broadcast associated information from an external broadcast managing entity via a broadcast channel. The broadcast channel may include a satellite channel and a terrestrial channel. The broadcast managing entity may refer to a system that transmits a broadcast signal and/or broadcast associated information.</p>
<p id="p-0059" num="0058">At least two broadcast receiving modules <b>111</b> may be provided in the mobile terminal <b>100</b> to pursue simultaneous reception of at least two broadcast channels or facilitation of broadcast channel switching.</p>
<p id="p-0060" num="0059">The sensing unit <b>140</b> may provide status measurements of various aspects of the mobile terminal <b>100</b>. For example, the sensing unit <b>140</b> may detect an open/close status (or state) of the mobile terminal <b>100</b>, a relative positioning of components (e.g., a display and a keypad) of the mobile terminal <b>100</b>, a change of position of the mobile terminal <b>100</b> or a component of the mobile terminal <b>100</b>, a presence or absence of user contact with the mobile terminal <b>100</b>, and/or an orientation or acceleration/deceleration of the mobile terminal <b>100</b>. The sensing unit <b>140</b> may also comprise a proximity sensor <b>141</b>.</p>
<p id="p-0061" num="0060">The mobile terminal <b>100</b> may be configured as a slide-type mobile terminal. In such a configuration, the sensing unit <b>140</b> may sense whether a sliding portion of the mobile terminal <b>100</b> is opened or closed. The sensing unit <b>140</b> may also sense presence or absence of power provided by the power supply <b>190</b>, presence or absence of a coupling or other connection between the interface unit <b>170</b> and an external device, etc.</p>
<p id="p-0062" num="0061">The output unit <b>150</b> may generate an output relevant to a sight sense, an auditory sense, a tactile sense and/or the like. The output unit <b>150</b> may include a display <b>151</b>, an audio output module <b>152</b>, an alarm <b>153</b>, a haptic module <b>154</b>, a projector module <b>155</b>, and/or the like.</p>
<p id="p-0063" num="0062">The display <b>151</b> may display (output) information processed by the terminal <b>100</b>. For example, in case that the terminal is in a call mode, the display <b>151</b> may display a user interface (UI) or a graphic user interface (GUI) associated with the call. If the mobile terminal <b>100</b> is in a video communication mode or a photograph mode, the display <b>151</b> may display a photographed and/or received picture, a UI or a GUI.</p>
<p id="p-0064" num="0063">The display <b>151</b> may include at least one of a liquid crystal display (LCD), a thin film transistor liquid crystal display (TFT LCD), an organic light-emitting diode (OLED), a flexible display, and a three-dimensional (3D) display.</p>
<p id="p-0065" num="0064">The display <b>151</b> may have a transparent or light-transmissive type configuration to enable an external environment to be seen through. This may be called a transparent display. A transparent OLED (TOLED) may be an example of a transparent display. A backside structure of the display <b>151</b> may also have the light-transmissive type configuration. In this configuration, a user may see an object located behind the terminal body through the area occupied by the display <b>151</b> of the terminal body.</p>
<p id="p-0066" num="0065">At least two displays <b>151</b> may also be provided. For example, a plurality of displays may be provided on a single face of the terminal <b>100</b> by being built in one body or spaced apart from the single face. Alternatively, each of a plurality of displays may be provided on different faces of the terminal <b>100</b>.</p>
<p id="p-0067" num="0066">If the display <b>151</b> and a sensor for detecting a touch action (hereafter a touch sensor) are constructed in a mutual-layered structure (hereafter a touchscreen), the display <b>151</b> may be used as an input device as well as an output device. For example, the touch sensor may include a touch film, a touch sheet, a touchpad and/or the like.</p>
<p id="p-0068" num="0067">The touch sensor may convert a pressure applied to a specific portion of the display <b>151</b> or a variation of electrostatic capacity generated from a specific portion of the display <b>151</b> to an electric input signal. The touch sensor may detect a pressure of a touch as well as a position and size of the touch.</p>
<p id="p-0069" num="0068">If a touch input is provided to the touch sensor, signal(s) corresponding to the touch input may be transferred to a touch controller. The touch controller may process the signal(s) and then transfer corresponding data to the controller <b>180</b>. The controller <b>180</b> may therefore know which portion of the display <b>151</b> is touched.</p>
<p id="p-0070" num="0069">The audio output module <b>152</b> may output audio data that is received from the wireless communication unit <b>110</b> in a call signal reception mode, a call mode, a recording mode, a voice recognition mode, a broadcast receiving mode and/or the like. The audio output module <b>152</b> may output audio data stored in the memory <b>160</b>. The audio output module <b>152</b> may output an audio signal relevant to a function (e.g., a call signal receiving sound, a message receiving sound, etc.) performed by the mobile terminal <b>100</b>. The audio output module <b>152</b> may include a receiver, a speaker, a buzzer and/or the like.</p>
<p id="p-0071" num="0070">The alarm <b>153</b> may output a signal for announcing an event occurrence of the mobile terminal <b>100</b>. An event occurring in the mobile terminal <b>100</b> may include one of a call signal reception, a message reception, a key signal input, a touch input and/or the like. The alarm <b>153</b> may output a signal for announcing an event occurrence by way of vibration or the like as well as a video signal or an audio signal. The video signal may be outputted via the display <b>151</b>. The audio signal may be outputted via the audio output module <b>152</b>. The display <b>151</b> or the audio output module <b>152</b> may be classified as part of the alarm <b>153</b>.</p>
<p id="p-0072" num="0071">The haptic module <b>154</b> may bring about various haptic effects that can be sensed by a user. Vibration is a representative example for the haptic effect brought about by the haptic module <b>154</b>. Strength and pattern of the vibration generated from the haptic module <b>154</b> may be controllable. For example, vibrations differing from each other may be outputted in a manner of being synthesized together or may be sequentially outputted.</p>
<p id="p-0073" num="0072">The haptic module <b>154</b> may generate various haptic effects including a vibration, an effect caused by such a stimulus as a pin array vertically moving against a contact skin surface, a jet power of air via outlet, a suction power of air via inlet, a skim on a skin surface, a contact of an electrode, an electrostatic power and the like, and/or an effect by hot/cold sense reproduction using an endothermic or exothermic device as well as the vibration.</p>
<p id="p-0074" num="0073">The haptic module <b>154</b> may provide the haptic effect via direct contact. The haptic module <b>154</b> may enable a user to experience the haptic effect via muscular sense of a finger, an arm and/or the like. Two or more haptic modules <b>154</b> may be provided according to a configuration of the mobile terminal <b>100</b>.</p>
<p id="p-0075" num="0074">The memory <b>160</b> may store a program for operations of the controller <b>180</b>. The memory <b>160</b> may temporarily store input/output data (e.g., phonebook, message, still picture, moving picture, etc.). The memory <b>160</b> may store data of vibration and sound in various patterns outputted in case of a touch input to the touchscreen.</p>
<p id="p-0076" num="0075">The memory <b>160</b> may include at least one of a flash memory, a hard disk, a multimedia card micro type memory, a card type memory (e.g., SD memory, XD memory, etc.), a random access memory (RAM), a static random access memory (SRAM), a read-only memory (ROM), an electrically erasable programmable read-only memory, a programmable read-only memory, a magnetic memory, a magnetic disk, an optical disk, and/or the like. The mobile terminal <b>100</b> may operate in association with a web storage that performs a storage function of the memory <b>160</b> in the Internet.</p>
<p id="p-0077" num="0076">The interface unit <b>170</b> may play a role as a passage to external devices connected to the mobile terminal <b>100</b>. The interface unit <b>170</b> may receive data from an external device. The interface unit <b>170</b> may be supplied with a power and then the power may be delivered to elements within the mobile terminal <b>100</b>. The interface unit <b>170</b> may enable data to be transferred to an external device from an inside of the mobile terminal <b>100</b>. The interface unit <b>170</b> may include a wired/wireless headset port, an external charger port, a wired/wireless data port, a memory card port, a port for coupling to a device having an identity module, an audio input/output (I/O) port, a video input/output (I/O) port, an earphone port and/or the like.</p>
<p id="p-0078" num="0077">The controller <b>180</b> may control overall operations of the mobile terminal <b>100</b>. For example, the controller <b>180</b> may perform control and processing relevant to a voice call, a data communication, a video conference and/or the like. The controller <b>180</b> may have a multimedia module <b>181</b> for multimedia playback. The multimedia module <b>181</b> may be implemented within the controller <b>180</b> or may be configured separate from the controller <b>180</b>.</p>
<p id="p-0079" num="0078">Moreover, the controller <b>180</b> is able to perform a pattern recognizing process for recognizing a writing input and a picture drawing input carried out on the touchscreen as characters or images, respectively.</p>
<p id="p-0080" num="0079">The controller <b>180</b> may perform pattern recognizing processing for recognizing a handwriting input performed on the touchscreen as a character and/or recognizing a picture drawing input performed on the touchscreen as an image.</p>
<p id="p-0081" num="0080">The power supply <b>190</b> may receive an external or internal power and then supply the power required for operations of the respective elements under control of the controller <b>180</b>.</p>
<p id="p-0082" num="0081">Embodiments of the present disclosure explained in the following description may be implemented within a recording medium that can be read by a computer or a computer-like device using software, hardware or combination thereof.</p>
<p id="p-0083" num="0082">According to the hardware implementation, arrangements and embodiments may be implemented using at least one of application specific integrated circuits (ASICs), digital signal processors (DSPs), digital signal processing devices DSPDs), programmable logic devices (PLDs), field programmable gate arrays (FPGAs), processors, controllers, microcontrollers, microprocessors and electrical units for performing other functions. In some cases, embodiments may be implemented by the controller <b>180</b>.</p>
<p id="p-0084" num="0083">For a software implementation, arrangements and embodiments described herein may be implemented with separate software modules, such as procedures and functions, each of which may perform one or more of the functions and operations described herein. Software codes may be implemented with a software application written in any suitable programming language and may be stored in memory such as the memory <b>160</b>, and may be executed by a controller or processor, such as the controller <b>180</b>.</p>
<p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. 2</figref> is a front-view diagram of a terminal according to one embodiment of the present invention for explaining an operational state thereof.</p>
<p id="p-0086" num="0085">Visual information can be displayed on the display <b>151</b>. For example, the information may comprise characters, numerals, symbols, graphics, icons and the like.</p>
<p id="p-0087" num="0086">In order to input the information, at least one of the characters, numerals, symbols, graphics and icons are represented as a single predetermined array to be implemented in a keypad formation. The keypad formation may be referred to as &#x201c;soft keys.&#x201d;</p>
<p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. 2</figref> illustrates that a touch applied to a soft key is input via a rear face of a terminal body. <figref idref="DRAWINGS">FIG. 2</figref> illustrates an example wherein the terminal body is horizontally arranged (landscape). The display <b>151</b> may be configured to adjust an output picture according to the direction of the terminal body.</p>
<p id="p-0089" num="0088">In <figref idref="DRAWINGS">FIG. 2</figref>, it is shown that a text input mode is activated in the terminal.</p>
<p id="p-0090" num="0089">An output window <b>201</b> and an input window <b>202</b> are displayed on the display <b>151</b>. A plurality of soft keys <b>203</b> representing at least one of characters, symbols and digits can be arranged in the input window <b>202</b>. The soft keys <b>203</b> can be arranged in the QWERTY key formation.</p>
<p id="p-0091" num="0090">If the soft keys <b>203</b> are touched via the touchpad, the characters, symbols and digits corresponding to the touched soft keys are outputted to the output window <b>201</b>. Thus, the touch input via the touchpad <b>135</b> is advantageous in that the soft keys <b>203</b> can be prevented from being blocked by a finger in case of touch, which is compared to the touch input via the display <b>151</b>. In case that the display <b>151</b> and the touchpad <b>135</b> are configured transparent, a user may view an input device, such as fingers, located at the backside of the terminal body.</p>
<p id="p-0092" num="0091">The display <b>151</b> or the touchpad <b>135</b> can be configured to receive a touch input by scroll. A user scrolls the display <b>151</b> or the touchpad <b>135</b> to shift a cursor or pointer displayed on the display <b>151</b>. Furthermore, a path of a shifted input device may also be visually displayed on the display <b>151</b>. This may be useful in editing an image displayed on the display <b>151</b>.</p>
<p id="p-0093" num="0092">Additionally, both of the display <b>151</b> and the touchpad <b>135</b> may be touched together within a predetermined time in order to execute a function of the terminal. The above example of the simultaneous touch may correspond to a case that the terminal body is held by a user using a thumb and a first finger (clamping). The executed function may include activation or deactivation for the display <b>151</b> or the touchpad <b>135</b>.</p>
<p id="p-0094" num="0093">The proximity sensor <b>141</b> described with reference to <figref idref="DRAWINGS">FIG. 1</figref> is explained in detail with reference to <figref idref="DRAWINGS">FIG. 3</figref> as follows.</p>
<p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. 3</figref> illustrates a proximity depth of the proximity sensor.</p>
<p id="p-0096" num="0095">As shown in <figref idref="DRAWINGS">FIG. 3</figref>, when a pointer such as a user's finger approaches the touch screen, the proximity sensor located inside or near the touch screen senses the approach and outputs a proximity signal.</p>
<p id="p-0097" num="0096">The proximity sensor can be constructed such that it outputs a proximity signal according to the distance between the pointer approaching the touch screen and the touch screen, referred to as &#x201c;proximity depth.&#x201d;</p>
<p id="p-0098" num="0097">The distance in which the proximity signal is output when the pointer approaches the touch screen is referred to as a &#x201c;detection distance.&#x201d; The proximity depth may be determined via a plurality of proximity sensors having different detection distances and comparing proximity signals respectively output from the proximity sensors.</p>
<p id="p-0099" num="0098">The proximity sensor is capable of sensing three proximity depths. Alternatively, proximity sensors capable of sensing less than three or more than three proximity depths may be arranged in the touch screen.</p>
<p id="p-0100" num="0099">Specifically, when the pointer completely comes into contact with the touch screen (D<b>0</b>), it is recognized as a contact touch. When the pointer is located within a distance D<b>1</b> from the touch screen, it is recognized as a proximity touch of a first proximity depth. When the pointer is located in a range between the distance D<b>1</b> and a distance D<b>2</b> from the touch screen, it is recognized as a proximity touch of a second proximity depth. When the pointer is located in a range between the distance D<b>2</b> and a distance D<b>3</b> from the touch screen, it is recognized as a proximity touch of a third proximity depth. When the pointer is located beyond the distance D<b>3</b> from the touch screen, it is recognized as a cancellation of a proximity touch.</p>
<p id="p-0101" num="0100">Accordingly, the controller <b>180</b> can recognize the proximity touch as various input signals according to the proximity distance and proximity position of the pointer with respect to the touch screen and perform various operation controls according to the input signals.</p>
<p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. 4</figref> is a diagram illustrating a method of controlling a touch action for a pair of displays <b>156</b> and <b>157</b> overlapped with each other.</p>
<p id="p-0103" num="0102">Referring to <figref idref="DRAWINGS">FIG. 4</figref>, a terminal shown in the drawing is a folder type terminal in which a folder part is connected to a main body.</p>
<p id="p-0104" num="0103">A first display <b>156</b> provided to the folder part <b>400</b> is a light-transmissive or transparent type display such as a TOLED, while a second display <b>157</b> provided to the main body <b>410</b> may be a non-transmissive type display such as an LCD. Each of the first display <b>156</b> and second display <b>157</b> may include a touchscreen.</p>
<p id="p-0105" num="0104">According to an embodiment, if a touch, such as a contact touch or proximity touch, to the first display or TOLED <b>156</b> is detected, the controller <b>180</b> selects or runs at least one image from an image list displayed on the TOLED <b>156</b> according to a touch type and a touch duration.</p>
<p id="p-0106" num="0105">In the following description, a method of controlling information displayed on a second display <b>157</b> in response to a touch to the first display <b>156</b> exposed in an overlapped configuration is explained.</p>
<p id="p-0107" num="0106">In the overlapped state, such as a state that the mobile terminal is closed or folded, the first display <b>156</b> is configured to be overlapped with the second display <b>157</b>. In this state, if a touch which is different from a touch for controlling an image displayed on the first display <b>155</b>, such as a touch having a duration greater than a predetermined period of time, is detected, the controller <b>180</b> enables at least one image to be selected from an image list displayed on the second display <b>157</b> according to the detected touch input. The result from selecting the image is displayed on the first display <b>156</b>.</p>
<p id="p-0108" num="0107">The touch which is detected as having a duration greater than a predetermined period of time is usable in selectively shifting an item displayed on the second display <b>157</b> to the first display <b>156</b>. A touch which has a duration greater than a predetermined period of time may be referred to as a long touch.</p>
<p id="p-0109" num="0108">Additionally, an item displayed on the first display <b>156</b> may be displayed on the second display <b>157</b> according to a touch input, such as flicking or swirling, to the first display <b>156</b>. As illustrated in <figref idref="DRAWINGS">FIG. 4</figref>, a menu displayed on the second display <b>157</b> is displayed by being shifted to the first display <b>156</b>.</p>
<p id="p-0110" num="0109">Moreover, the controller <b>180</b> may execute a function associated with an image selected by the long touch and display a preview picture for the image on the first display <b>156</b> if another input, such as a drag, is detected with a long touch. As illustrated in <figref idref="DRAWINGS">FIG. 4</figref>, a preview image <b>420</b> for a second menu item is displayed.</p>
<p id="p-0111" num="0110">While the preview image is outputted, If a drag toward a different image is performed on the first display <b>156</b> while maintaining the long touch when the preview image is displayed, the controller <b>180</b> shifts a selection cursor of the second display <b>157</b> and then displays the second image <b>430</b> selected by the selection cursor. After completion of the touch (long touch and drag), the controller <b>180</b> displays the preview image <b>410</b> which was originally selected by the long touch.</p>
<p id="p-0112" num="0111">The touch action described above, long touch and drag, is similar to an example of when a proximity drag, a proximity touch corresponding to the drag, is detected together with a long proximity touch, such as when a proximity touch is maintained for a predetermined period of time on the first display <b>156</b>.</p>
<p id="p-0113" num="0112">The method of controlling the terminal in response to the touch action while in the overlapped state is also applicable to a terminal having a single display and non-folder type terminals with a dual display.</p>
<p id="p-0114" num="0113">For clarity and convenience of the following description, a mobile terminal is assumed to comprise at least one of the components illustrated in <figref idref="DRAWINGS">FIG. 1</figref>. Specifically, a mobile terminal, to which the present invention is applicable, may include at least two display units. At least one display unit may include the transparent display unit <b>156</b> described in <figref idref="DRAWINGS">FIG. 4</figref>. One of the display units may be referred to as a main display unit <b>157</b> if it is able to perform an interaction with the transparent display unit while in an overlapped state. The main display unit <b>157</b> is distinct from the transparent display unit and may be hinged to the transparent display unit. Generally, the main display unit <b>157</b> is a non-transparent display, yet, if necessary, the main display unit <b>157</b> may have a configuration capable of transparent display functionality.</p>
<p id="p-0115" num="0114">According to one embodiment of the present invention, a mobile terminal and controlling method thereof can implement a three-dimensional (3D) user interface using at least two display units including a transparent display unit.</p>
<p id="p-0116" num="0115">In the following description, a 3D user interface implemented by the present invention is schematically explained with reference to <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. 5</figref> is a diagram for generating a 3D effect using a distance measurement between two display units according to one embodiment of the present invention.</p>
<p id="p-0118" num="0117">Referring to <figref idref="DRAWINGS">FIG. 5(</figref><i>a</i>), a main display unit <b>157</b> and a transparent display unit <b>156</b> are arranged by being spaced apart at a prescribed angle from each other. As illustrated in <figref idref="DRAWINGS">FIG. 5(</figref><i>a</i>) it is assumed that a user views the transparent display unit <b>156</b> from a front view. A front view refers to a view in which the main display unit <b>157</b> is positioned away from the user.</p>
<p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. 5(</figref><i>b</i>) shows the transparent display unit <b>156</b> from a lateral view.</p>
<p id="p-0120" num="0119">In the above-described embodiments, the letters &#x201c;CD&#x201d; <b>500</b> displayed on the transparent display unit <b>156</b> and the letters &#x201c;AB&#x201d; <b>510</b> displayed on the main display unit <b>157</b> are located at distances which are different from a view point of a user, respectively, whereby the user views a 3D effect of the letters &#x201c;CD&#x201d; <b>500</b> and &#x201c;AB&#x201d; <b>510</b> according to a relative distance. In this example, an extent of the distance may vary according to a space between the transparent display unit <b>156</b> and the main display unit <b>157</b> or a relative size of the letters &#x201c;CD&#x201d; <b>500</b> and &#x201c;AB&#x201d; <b>510</b> displayed on the corresponding display unit. <figref idref="DRAWINGS">FIG. 5(</figref><i>c</i>) illustrates an example of the display from a user's viewpoint of <figref idref="DRAWINGS">FIGS. 5(</figref><i>a</i>) and <b>5</b>(<i>b</i>).</p>
<p id="p-0121" num="0120"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram illustrating a mobile terminal according to one embodiment of the present invention with reference to <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0122" num="0121">Referring to <figref idref="DRAWINGS">FIG. 6(</figref><i>a</i>), the main display unit <b>157</b> is provided to a top-side of the mobile terminal <b>100</b>, while the transparent display unit <b>156</b> can be rotatably connected via a hinge <b>108</b> to one lateral side of the plane having the main display unit <b>157</b>. In particular, the transparent display unit <b>156</b> is folded onto the main display unit <b>157</b> and is then turned at a prescribed angle to provide a space <b>600</b> in-between. Preferably, the angle between the transparent display unit <b>156</b> and the main display unit <b>157</b> is set not to exceed 90 degrees.</p>
<p id="p-0123" num="0122">The main display unit <b>157</b> can be implemented with a touchscreen. Both sides of the transparent display unit <b>156</b> may also be implemented with touchscreens, respectively. Alternatively, only one side of the transparent display unit <b>156</b> may include a touchscreen.</p>
<p id="p-0124" num="0123">In the following description, a fold state will refer to a scenario when the transparent display unit <b>156</b> is closed over the main display unit <b>157</b>. Additionally, a tilt state will refer to a scenario when the transparent display unit <b>156</b> is turned at a prescribed angle against the main display unit <b>157</b>. In the fold state, a backside will refer to a face of the transparent display unit <b>156</b> opposite to the main display unit <b>157</b>. Moreover, a front side will refer to a face opposite to the backside of the transparent display unit <b>156</b>.</p>
<p id="p-0125" num="0124">A camera may be provided to one lateral side of the mobile terminal <b>100</b>, and preferably on the front side of the transparent display unit <b>156</b>. The controller <b>180</b> may recognize a relative position of a user with reference to the mobile terminal <b>100</b> via face recognition from a picture photographed via the camera <b>121</b>.</p>
<p id="p-0126" num="0125">A sensing module <b>140</b> may be provided on the main display unit <b>157</b>. The sensing module <b>140</b> may detect a motion from an input device, such as a finger or a stylus pen, in the space <b>600</b> provided between the main display unit <b>157</b> and the transparent display unit <b>156</b>. The controller <b>180</b> may execute a function corresponding to input detected by the sensing module <b>140</b>. Alternatively, a camera may recognize a position of the pointer by photographing the input device.</p>
<p id="p-0127" num="0126"><figref idref="DRAWINGS">FIG. 6(</figref><i>b</i>) illustrates one example of using the mobile terminal shown in <figref idref="DRAWINGS">FIG. 6(</figref><i>a</i>).</p>
<p id="p-0128" num="0127">Referring to <figref idref="DRAWINGS">FIG. 6(</figref><i>b</i>), the mobile terminal <b>100</b> according to one embodiment of the present invention is preferably tilted and placed in a manner that the main display unit <b>157</b> may be viewed via the transparent display unit <b>156</b>. A picture <b>620</b> displayed on the main display unit <b>157</b> can become a background image of the picture <b>610</b> displayed on the transparent display unit <b>156</b>. Moreover, a user can be provided with a 3D effect attributed to distance and angle differences between the two pictures. Specifically, a 3D picture is displayed as the picture <b>610</b> displayed on the transparent display unit <b>156</b>.</p>
<p id="p-0129" num="0128">If the mobile terminal <b>100</b> is located to oppose a user in the above manner, the camera <b>121</b> is set to face the user. Therefore, the camera <b>121</b> can take a picture of the user and may provide the picture to the controller <b>180</b>.</p>
<p id="p-0130" num="0129">Moreover, the user may input a command via a touch or a proximity touch to the front side or backside of the transparent display unit <b>156</b> or the main display unit <b>157</b>. Alternatively, a user may input a command via the position recognition provided by the sensing module <b>140</b> or the camera.</p>
<p id="p-0131" num="0130">A method of operating the above-configured mobile terminal is explained with reference to <figref idref="DRAWINGS">FIG. 7</figref>. <figref idref="DRAWINGS">FIG. 7</figref> is a flowchart for a method of operating a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0132" num="0131">Referring to <figref idref="DRAWINGS">FIG. 7</figref>, a transparent display can be activated when entering a tilt state [S<b>10</b>]. The transparent display in the tilt state may be activated if a specific application is executed or a user manipulates a prescribed menu.</p>
<p id="p-0133" num="0132">Subsequently, a 3D user interface corresponding to an executed function can be output via the transparent display unit <b>156</b> or the main display unit <b>157</b> [S<b>20</b>]. In doing so, as mentioned in the foregoing description, a two-dimensional (2D) picture can be output to the transparent display unit <b>156</b>. Alternatively, the 3D image shown in <figref idref="DRAWINGS">FIG. 6(</figref><i>b</i>) can be output to the transparent display unit <b>156</b>.</p>
<p id="p-0134" num="0133">When the camera <b>121</b> is provided to one lateral side of the mobile terminal <b>100</b> the controller <b>180</b> may detect if a user's view point is changed using a picture taken via the camera <b>121</b> [S<b>30</b>].</p>
<p id="p-0135" num="0134">After detecting that the user's view point is changed via the picture taken via the camera <b>121</b>, the controller <b>180</b> may correct the picture displayed on the transparent display unit <b>156</b> or the main display unit <b>157</b> to provide the user with a 3D image suitable for the changed view point [S<b>40</b>].</p>
<p id="p-0136" num="0135">In the following description, detailed examples for applying the above described mobile terminal operating method are explained. To help the understanding of the following description, details of an exterior, such as a housing shape or position of user input unit, of the mobile terminal are not depicted but the relative arrangement of the components related to the 3D user interface are schematically explained.</p>
<p id="p-0137" num="0136">The steps S<b>30</b> and S<b>40</b> mentioned in the above description are explained in detail with reference to <figref idref="DRAWINGS">FIG. 8</figref> as follows.</p>
<p id="p-0138" num="0137"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram of one example of correcting a picture displayed on a transparent display unit via recognition of a user's position relative to a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0139" num="0138">Referring to <figref idref="DRAWINGS">FIG. 8(</figref><i>a</i>), when a 3D user interface is activated in the mobile terminal in a tilt state, it is able to display a hexahedron <b>800</b> on the transparent display unit <b>156</b>. In this example, if a quadrangle is displayed on a facet of the hexahedron <b>800</b>, this facet is assumed as a front facet. If a triangle is displayed on a facet of the hexahedron <b>800</b>, this facet is assumed as a right lateral facet.</p>
<p id="p-0140" num="0139">The controller <b>180</b> may determine a user's position relative to that of the mobile terminal <b>100</b> using a picture taken via the camera <b>121</b>. The following description explains one example of a user located opposite to a front side <b>810</b> of the mobile terminal and another example of a user located opposite to a lateral side <b>820</b> of the mobile terminal.</p>
<p id="p-0141" num="0140">In the example of determining that the user is located opposite to the front side <b>810</b> of the mobile terminal, the controller <b>180</b> may display a picture of a front facet of the hexahedron <b>800</b> on the transparent display unit <b>156</b> (<figref idref="DRAWINGS">FIG. 8(</figref><i>b</i>)). Additionally, in the example of determining that the user is located opposite to the lateral side <b>820</b> of the mobile terminal, the controller <b>180</b> may display a perspective picture, such as a picture of front and lateral sides, of the hexahedron <b>800</b> on the transparent display unit <b>156</b> (<figref idref="DRAWINGS">FIG. 8(</figref><i>c</i>)).</p>
<p id="p-0142" num="0141">The mobile terminal <b>100</b> according to the present invention may output a picture of a 3D object corresponding to a view point of a user via the transparent display unit <b>156</b>, thereby providing a 3D image effect to the user.</p>
<p id="p-0143" num="0142">A mobile terminal according to the present invention may change a position of a picture displayed via a transparent display unit in consideration of a user's view point as well as a configuration of the displayed picture. Such a position change is effective to provide a 3D user interface to a user when a picture displayed on the transparent display unit is interoperating with a picture displayed on a main display unit. This is explained with reference to <figref idref="DRAWINGS">FIG. 9</figref> as follows.</p>
<p id="p-0144" num="0143"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram of another example of correcting a picture displayed on a transparent display unit via recognition of a user's position relative to a mobile terminal according to an embodiment of the present invention.</p>
<p id="p-0145" num="0144">The transparent display unit <b>156</b> and the main display unit <b>157</b> provided to the mobile terminal according to the present embodiment are rotatably hinged to each other. To better understand the present embodiment, it is assumed that the two displays units are positioned in parallel with each other. Additionally it is assumed that the controller <b>180</b> detects or monitors a user's position via a picture taken with the camera <b>121</b>.</p>
<p id="p-0146" num="0145">In the example illustrated in <figref idref="DRAWINGS">FIG. 9(</figref><i>a</i>), it is assumed that a view point <b>900</b> of a user faces a front side of the transparent display unit <b>156</b>. Accordingly, the user may view a picture <b>910</b> displayed on the transparent display unit <b>156</b> as if the picture <b>910</b> is located at a center <b>920</b> of cross hairs displayed on the main display unit <b>157</b>.</p>
<p id="p-0147" num="0146">After setting the configuration illustrated in <figref idref="DRAWINGS">FIG. 9(</figref><i>a</i>), the view point <b>900</b> of the user is changed into the configuration illustrated in <figref idref="DRAWINGS">FIG. 9(</figref><i>b</i>) when the user moves in a right direction. In this example, the controller <b>180</b> shifts the picture <b>910</b> displayed on the transparent display unit to a right side from a previous position <b>940</b>. Thus by the user's viewpoint, the user sees the picture <b>910</b> displayed on the transparent display unit <b>156</b> as if the picture <b>910</b> is located at the center <b>920</b> of the cross hairs displayed on the main display unit <b>157</b>.</p>
<p id="p-0148" num="0147">Accordingly, using the position changing method described above, the picture displayed on the transparent display unit <b>156</b> is viewed as always being located at the fixed point in the picture displayed on the main display unit <b>157</b>.</p>
<p id="p-0149" num="0148">In order to reduce a load of the controller <b>180</b>, which is generated from calculating a rotation or position shift of a 3D object according to the view point change, the controller <b>180</b> may use a lookup table comprising pre-calculated correction values according to a user's view point and a distance or angle of the arrangements of the transparent display unit <b>156</b> and the main display unit <b>157</b>. The lookup table may be stored in the memory <b>160</b>.</p>
<p id="p-0150" num="0149">In the following description, a method of representing a 3D image on a transparent display unit is explained.</p>
<p id="p-0151" num="0150">A method of representing a 3D image on a 2D display unit comprises configuring a 3D object into a polyhedron using a plurality of polygons and representing a shape seen at a specific view point as a 2D image. In this example, each of the polygons indicates a polygon of a smallest basic unit for configuring a 3D graphic. Each facet of the polyhedron, including a plurality of the polygons, is covered with a previously prepared texture to represent one of various colors, patterns, or texts. For example, in order to configure the hexahedron <b>800</b> shown in <figref idref="DRAWINGS">FIG. 8</figref>, the controller <b>180</b> first forms six facets by connecting twelve lines to eight vertexes and then covers three of the six facets with textures of a circle, a triangle, and a quadrangle, respectively. Afterwards, if the hexahedron formed by the above described method is rotated to correspond to a view point of a user, the controller <b>180</b> is able to output a picture seen in one direction, as shown in <figref idref="DRAWINGS">FIG. 8(</figref><i>b</i>) or <figref idref="DRAWINGS">FIG. 8(</figref><i>c</i>), to the transparent display unit <b>156</b>.</p>
<p id="p-0152" num="0151">A second method for displaying a 3D image may comprise enabling a user to feel a depth and reality of a 3D image in a manner of providing two different 2D images to a pair of user eyes to be merged together in the user's brain. This method requires additional equipment.</p>
<p id="p-0153" num="0152">For example the additional equipment may comprise polarized film glasses. This method is referred to as a stereoscopic system. In this method, one image may be generated from mixing two different images provided to a pair of eyes via the polarized glasses.</p>
<p id="p-0154" num="0153">Alternatively, a parallex barrier type display unit may be utilized to provide a 3D image. This is explained with reference to <figref idref="DRAWINGS">FIG. 10</figref> as follows.</p>
<p id="p-0155" num="0154"><figref idref="DRAWINGS">FIG. 10</figref> illustrates a method of implementing a 3D image on a parallex barrier type display unit applicable to embodiments of the present invention.</p>
<p id="p-0156" num="0155">Referring to <figref idref="DRAWINGS">FIG. 10</figref>, a structure of a parallex barrier type display unit <b>151</b> for displaying a 3D image may be configured in a manner that a general display device <b>151</b><i>a </i>is combined with switch LC (liquid crystals) <b>151</b><i>b</i>. A propagating direction of light is controlled by activating an optical parallex barrier <b>1000</b>, as shown in <figref idref="DRAWINGS">FIG. 10(</figref><i>a</i>), using the switch LC <b>151</b><i>b</i>, whereby the light is separated into two different lights to arrive at left and right eyes, respectively. Thus, when an image generated from combining an image for the right eye and an image for the left eye together is displayed on the display device <b>151</b><i>a</i>, a user sees the images corresponding to the eyes, respectively, thereby feeling the 3D or stereoscopic effect.</p>
<p id="p-0157" num="0156">Alternatively, referring to <figref idref="DRAWINGS">FIG. 10(</figref><i>b</i>), the parallex barrier <b>1000</b> attributed to the switch LC may be electrically controlled to enable light to be fully transmitted therethrough, whereby the light separation due to the parallex barrier is avoided. Therefore, the same image can be seen through left and right eyes. The parallex barrier type display may provide for both a 2D or 3D image.</p>
<p id="p-0158" num="0157">The above mentioned 3D image displaying method is applicable to the transparent display unit <b>156</b> according to one embodiment of the present invention. If a method using the additional equipments is adopted, either the transparent display unit <b>156</b> or the main display unit <b>157</b> can be provided to the mobile terminal. Yet, according to the present invention, the picture displayed on the transparent display unit <b>156</b> can interoperate with the other image displayed on the main display unit <b>157</b>. Preferably, both types of display units are provided to the mobile terminal.</p>
<p id="p-0159" num="0158">In the following description, a command inputting method in a mobile terminal according to the present invention is explained with reference to <figref idref="DRAWINGS">FIG. 11</figref>.</p>
<p id="p-0160" num="0159"><figref idref="DRAWINGS">FIG. 11</figref> is a diagram for a method of inputting a command to a mobile terminal and a method of recognizing the command according to one embodiment of the present invention.</p>
<p id="p-0161" num="0160">Referring to <figref idref="DRAWINGS">FIG. 11(</figref><i>a</i>), the mobile terminal <b>100</b> may utilize the proximity sensor <b>141</b> at a position of the sensing module <b>140</b> described with reference to <figref idref="DRAWINGS">FIG. 6(</figref><i>a</i>). The controller <b>180</b> recognizes the position of the input device sensed by the proximity sensor <b>141</b> as a user's command input and then enables a function corresponding to the command to be executed.</p>
<p id="p-0162" num="0161">As mentioned in the foregoing description with reference to <figref idref="DRAWINGS">FIG. 4</figref>, the proximity sensor <b>141</b> may recognize a distance between the pointer and the sensor in a manner of dividing the distance into at least two steps, such as d<b>1</b>, d<b>2</b> and d<b>3</b>. If the haptic module <b>154</b> is provided to the mobile terminal, the controller <b>180</b> generates a different kind of haptic effect each time the pointer crosses each distance step.</p>
<p id="p-0163" num="0162">In another example, referring to <figref idref="DRAWINGS">FIG. 11(</figref><i>b</i>), a camera <b>123</b> may be provided for photographing a space generated between the transparent display unit <b>156</b> and the main display unit <b>157</b> when in a tilt state. In particular, the controller <b>180</b> recognizes a size and position of an input device appearing in a picture taken via the camera <b>123</b> and is thereby able to recognize the size and position as a user's command input. Moreover, in case of using the camera <b>123</b>, the controller <b>180</b> is able to further recognize a gesture or pattern as well as the size and position of the pointer.</p>
<p id="p-0164" num="0163">In another example, referring to <figref idref="DRAWINGS">FIG. 11(</figref><i>c</i>), the mobile terminal <b>100</b> may recognize a touch input or a proximity touch input to a backside <b>1110</b> of the transparent display unit <b>156</b> or a specific point <b>1120</b> on the main display unit <b>157</b>.</p>
<p id="p-0165" num="0164">In another example, the mobile terminal <b>100</b> may utilize a method of using augmented reality (AR). In this example, the augmented reality refers to a virtual reality in which a real environment seen via user's eyes and a virtual environment merge together to provide one picture. Although the augmented reality uses the virtual environment including a computer graphic, the real environment plays a main role and the computer graphic plays a role in providing information necessary for the real environment. Namely, a real image is overlapped with a 3D virtual image, whereby a distinction between the real environment and the virtual picture becomes vague.</p>
<p id="p-0166" num="0165">For example, a headset type computer screen device which may be a wearable computer device enables a user to view a real environment overlapped with computer graphics or characters.</p>
<p id="p-0167" num="0166">In another example, an input unit for inputting a specific pattern on a surface of a real object is photographed via a camera. A type and position of the input unit are recognized from the photographed picture and the photographed picture is then output as being overlapped with a digital image corresponding to the input unit. An augmented reality implementing the aforementioned method is generally referred to as a &#x201c;marker system.&#x201d;</p>
<p id="p-0168" num="0167">Additionally, a controller may determine an object via an image obtaining means, such as a camera, for obtaining a real image without an input unit. Relevant information is obtained according to the determined object, the obtained relevant information is then added to the obtained real image. This method is applicable to the implementation of the augmented reality. The aforementioned process is referred to as a &#x201c;markerless system.&#x201d; In the following description, some embodiments of the present invention are explained through the implementation of the marker system, by which the present invention is non-limited. These embodiments are also applicable to the markerless system.</p>
<p id="p-0169" num="0168">In the example of using the marker system, a user may input a command in a manner of moving a pointer having a specific pattern in a space generated between the transparent display unit <b>156</b> and the main display unit <b>157</b>. Alternatively, the pointer marker can be replaced by a pointer marker displayed on the main display unit. For example, referring to <figref idref="DRAWINGS">FIG. 11(</figref><i>d</i>), a user may apply a touch and drag input to a marker <b>1160</b> displayed on the main display unit <b>156</b> using a pointer <b>1100</b>. The controller <b>180</b> may then move a 3D object <b>1150</b> displayed on the transparent display unit <b>156</b> to correspond to a motion of the marker <b>1160</b>. In this example, in order to photograph the marker, the mobile terminal may be provided with the camera <b>123</b> shown in <figref idref="DRAWINGS">FIG. 11(</figref><i>b</i>).</p>
<p id="p-0170" num="0169">Thus, a mobile terminal according to the present invention receives an input of a command from a user using one of the above described methods or can recognize a command input from a user by combining at least two of the above described methods.</p>
<p id="p-0171" num="0170">Examples for the detailed functions executable in the mobile terminal according to one embodiment of the present invention through the above mentioned structure and command inputting method of the mobile terminal are explained with reference to <figref idref="DRAWINGS">FIGS. 12 to 16</figref> as follows.</p>
<p id="p-0172" num="0171">First, a game function is described with reference to <figref idref="DRAWINGS">FIG. 12</figref>.</p>
<p id="p-0173" num="0172"><figref idref="DRAWINGS">FIG. 12</figref> is a diagram of one example for a game in a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0174" num="0173">Referring to <figref idref="DRAWINGS">FIG. 12(</figref><i>a</i>), a game may be played when two mobile terminals <b>100</b><i>a </i>and <b>100</b><i>b </i>are placed within a distance from each other. In particular, the two mobile terminals <b>100</b><i>a </i>and <b>100</b><i>b </i>are set to interoperate with each other via the wireless communication unit <b>110</b>. A character <b>1210</b> displayed on the transparent display unit <b>156</b>, while in a tilt state, may be manipulated by a backside touch to implement a fighting game.</p>
<p id="p-0175" num="0174">In another example of a game executable in a mobile terminal according to one embodiment of the present invention, referring to <figref idref="DRAWINGS">FIG. 12(</figref><i>b</i>), a game may be played in a manner of pushing away one of cubic blocks <b>1220</b> laid in a prescribed shape without breaking the prescribed shape.</p>
<p id="p-0176" num="0175">In playing this game, a user may change a view point of looking at the transparent display unit <b>156</b> to select a block to push away or check a configuration of the laid blocks. In this case, the controller <b>180</b> determines the user's view point using a picture photographed via the camera <b>121</b> and then rotates the shape of the cubic blocks <b>1220</b> to correspond to the view point.</p>
<p id="p-0177" num="0176">When a user pushes away one of the blocks, a position of a pointer, such as a user's finger, may be recognized by the controller <b>180</b> via at least one of the methods described with reference to <figref idref="DRAWINGS">FIG. 11</figref>. In this example, since the controller <b>180</b> has to recognize the position of the finger in the space generated between the transparent display unit <b>156</b> and the main display unit <b>157</b>, it is preferable to use at least one of a camera <b>123</b> or the proximity sensor <b>141</b>. For example, if a user uses a marker having a prescribed pattern printed on its end portion, the controller <b>180</b> recognizes or acquires a position of the marker using a picture photographed via the camera <b>123</b> and is then able to move a block corresponding to the position of the marker.</p>
<p id="p-0178" num="0177">Furthermore, a mobile terminal according to one embodiment of the present invention may provide a user with a navigation function using a 3D user interface. This is explained with reference to <figref idref="DRAWINGS">FIGS. 13 and 14</figref>. Assume that a mobile terminal shown in <figref idref="DRAWINGS">FIG. 13</figref> or <figref idref="DRAWINGS">FIG. 14</figref> is provided with a position location module <b>115</b> capable of receiving position information from a satellite.</p>
<p id="p-0179" num="0178"><figref idref="DRAWINGS">FIG. 13</figref> is a diagram of examples for providing a 3D map and geographical information using the 3D map in a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0180" num="0179">Referring to <figref idref="DRAWINGS">FIG. 13</figref>, in an initial stage, a mobile terminal is able to display a 2D map via a main display unit <b>157</b> while a transparent display unit <b>156</b>, as shown in <figref idref="DRAWINGS">FIG. 13(</figref><i>a</i>), is in a fold state that it is folded over the main display unit <b>157</b>.</p>
<p id="p-0181" num="0180">When a user intends to view a 3D map, the user may enable the transparent display unit <b>156</b> to enter a tilt state by turning the transparent display unit <b>156</b> by about 45 degrees. Accordingly, 3D building objects <b>1301</b>, <b>1302</b>, and <b>1303</b> can be displayed via the transparent display unit <b>156</b>. The 3D buildings correspond to at least some of major buildings displayed in the 2D map displayed on the main display unit <b>157</b>. In this example, when a user focuses on the main display unit <b>157</b>, each of the 3D building objects is preferably displayed in a manner of interoperating with the respective buildings of the 2D map. In other words, a bottom of each of the 3D building objects is preferably displayed in a manner of matching a size and position of each of the buildings marked on the 2D map displayed on the main display unit <b>157</b>.</p>
<p id="p-0182" num="0181">When attempting to scroll a map to search another adjacent point from a currently displayed point, a user scrolls the map by performing a touch and drag on the main display unit <b>157</b> in a specific scroll direction. Moreover, when incorporating a multi-input touchscreen, the main display unit <b>157</b> may simultaneously recognize multiple touch inputs, and thereby allow a user to rotate a map in a manner of performing a second touch for drawing an arc centering on a first point while maintaining a first touch to the first point on the main display unit <b>157</b>. As the map on the main display unit <b>157</b> is rotated, the 3D building objects displayed on the transparent display unit <b>156</b> can be interoperably rotated. The aforementioned method of inputting the command for the scroll or rotation of the map is merely exemplary, by which the present invention is non-limited. Alternatively, various command inputting methods are applicable to the present invention.</p>
<p id="p-0183" num="0182">As shown in <figref idref="DRAWINGS">FIG. 13(</figref><i>c</i>), the 3D building objects <b>1301</b>, <b>1302</b> and <b>1303</b> displayed on the transparent display unit <b>156</b> can be displayed larger as compared to the building objects shown in <figref idref="DRAWINGS">FIG. 13(</figref><i>b</i>) if the user further rotates the transparent display unit <b>156</b> approximately 90 degrees.</p>
<p id="p-0184" num="0183">In order to acquire information on a specific building (<figref idref="DRAWINGS">FIG. 13(</figref><i>d</i>)), the user may point to a specific 3D building object <b>1303</b> via an input unit such as a finger <b>1310</b>. The controller <b>180</b> determines which 3D building object is pointed to by the finger <b>1310</b> via at least one of the former methods described with reference to <figref idref="DRAWINGS">FIG. 11</figref>, and may display information on the corresponding building <b>1303</b>, such as a name <b>1320</b> of the building, on the transparent display unit <b>156</b>. Moreover, menus <b>1330</b> of additional functions executable in association with the corresponding building can be further displayed on the transparent display unit <b>156</b> together with the information on the corresponding building. For example, the corresponding building may be added to a bookmark or navigation to the corresponding building can be executed. The menus <b>1330</b> displayed on the transparent display unit <b>156</b> may be selected from either the front or the back of the transparent display unit <b>156</b>.</p>
<p id="p-0185" num="0184">Additionally, the user may increase or decrease the size of the 3D building objects <b>1301</b>, <b>1302</b> and <b>1303</b> displayed on the transparent display unit <b>156</b> via an input. For example, when viewing the 3D building objects <b>1301</b>, <b>1302</b> and <b>1303</b> displayed on the transparent display unit <b>156</b> as illustrated in <figref idref="DRAWINGS">FIG. 13(</figref><i>b</i>), the user may perform a multi touch input, such as a double tap to enlarge the size of the 3D map (not shown), furthermore, the user may perform a multi touch input, such as a triple tap to decrease the size of the 3D map (not shown). Increasing and decreasing the size of the 3D map is not limited to the size of the buildings and is similar to a &#x201c;zoom in&#x201d; or &#x201c;zoom out&#x201d; function. Additionally, the increasing and decreasing of the size of the 3D map is not limited to a multi touch input and may be performed by a predetermined input or an input set by the user.</p>
<p id="p-0186" num="0185">The navigation function is executable in a manner that 3D building objects are interoperably displayed on the 2D map shown in <figref idref="DRAWINGS">FIG. 13(</figref><i>b</i>) or <figref idref="DRAWINGS">FIG. 13(</figref><i>c</i>) or can be performed in a manner of matching real geographical features. This is explained with reference to <figref idref="DRAWINGS">FIG. 14</figref> as follows.</p>
<p id="p-0187" num="0186"><figref idref="DRAWINGS">FIG. 14</figref> is a diagram of another example of executing a navigation function in a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0188" num="0187">Referring to <figref idref="DRAWINGS">FIG. 14(</figref><i>a</i>), it is assumed that the navigation has been selected from the menu <b>1330</b> (<figref idref="DRAWINGS">FIG. 13(</figref><i>d</i>)). The present embodiment is applicable to navigation in response to a search via an input of address or destination name.</p>
<p id="p-0189" num="0188">In this example, the menu <b>1410</b> indicating that the navigation function for the specific building <b>1303</b> can be maintained on the transparent display unit <b>156</b>. A message <b>1420</b> for instructing the user to rotate the transparent display unit <b>156</b> may be further displayed.</p>
<p id="p-0190" num="0189">Once the transparent display unit <b>156</b> is rotated according to the rotation instruction message <b>1420</b> (<figref idref="DRAWINGS">FIG. 14(</figref><i>b</i>)), a path instruction image <b>1440</b> may be displayed on the transparent display unit <b>156</b> to display real geographical features, such as roads and streets. Moreover, a destination <b>1450</b> can be displayed on the transparent display unit <b>156</b> to match the real geographical features. In this example, information on major adjacent geographical features, such as buildings, can be displayed in addition to the destination <b>1450</b>. If another terminal user is performing the same function, information on the other terminal user can be additionally displayed on the transparent display unit <b>156</b>. In this example, if another user is detected via the short range communication module or position information on another user is provided via the wireless communication module, the information on another user can be displayed. Optionally, a function, such as sending a message or making a phone call, can be performed according to information of another user displayed on the transparent display unit.</p>
<p id="p-0191" num="0190">In the following description, a message and gift delivering function is explained with reference to <figref idref="DRAWINGS">FIG. 15</figref> and <figref idref="DRAWINGS">FIG. 16</figref>.</p>
<p id="p-0192" num="0191"><figref idref="DRAWINGS">FIG. 15</figref> is a diagram of one example for displaying a 3D message in a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0193" num="0192">Referring to <figref idref="DRAWINGS">FIG. 15(</figref><i>a</i>), a 3D message is delivered to a mobile terminal in a fold state. Text of the message and a message for instructing to change into a tilt state can be displayed on a transparent display unit <b>156</b>. In this example, the 3D message can include a multimedia message (MMS).</p>
<p id="p-0194" num="0193">When a user enables the mobile terminal to enter the tilt state, as shown in <figref idref="DRAWINGS">FIG. 15(</figref><i>b</i>), a cake <b>1520</b>, which is a 3D gift object, can be displayed on the transparent display unit <b>156</b>. Moreover, additional messages <b>1530</b>, <b>1531</b>, and <b>1532</b> relevant to a general message can also be displayed on the transparent display unit <b>156</b>.</p>
<p id="p-0195" num="0194">Although the 3D gift object included in the 3D message shown in <figref idref="DRAWINGS">FIG. 15</figref> is simply displayed, one embodiment of the present invention provides a 3D interactive gift object. This is explained with reference to <figref idref="DRAWINGS">FIG. 16</figref> as follows.</p>
<p id="p-0196" num="0195"><figref idref="DRAWINGS">FIG. 16</figref> is a diagram of one example for displaying a 3D interactive gift with a user in a mobile terminal according to one embodiment of the present invention.</p>
<p id="p-0197" num="0196"><figref idref="DRAWINGS">FIG. 16(</figref><i>a</i>) assumes a situation after the mobile terminal shown in <figref idref="DRAWINGS">FIG. 15(</figref><i>a</i>) has been rotated to enter the tilt state.</p>
<p id="p-0198" num="0197">Referring to <figref idref="DRAWINGS">FIG. 16(</figref><i>a</i>), a gift box <b>1610</b> tied up with a wrap string is displayed as a 3D gift object on a transparent display unit <b>156</b>. If a user takes an action, such as pulling the wrap string <b>1620</b> via an input device, such as a user's hand <b>1600</b>, an image for showing that the string <b>1620</b> is loosened can be displayed via the transparent unit <b>156</b>.</p>
<p id="p-0199" num="0198">Referring to <figref idref="DRAWINGS">FIG. 16(</figref><i>b</i>), if the user takes an action of lifting a box lid <b>1611</b> using the user's hand <b>1600</b>, the lid <b>1611</b> can be removed.</p>
<p id="p-0200" num="0199">After the lid <b>1611</b> has been removed, referring to <figref idref="DRAWINGS">FIG. 16(</figref><i>c</i>), a stuffed animal <b>1640</b> can be displayed as a 3D gift object on the transparent display unit <b>156</b>. The user takes an action of grabbing the stuffed animal <b>1640</b> to interact with the image, such as a rotation, a size adjustment, or a shift to a specific point. A menu applicable to the 3D gift object such as the stuffed animal <b>1640</b> can be displayed on a main display unit <b>157</b> as well as additional menu icons <b>1630</b>. In this example, the user may store the 3D gift object as a picture file by dragging and dropping the stuffed animal <b>1640</b> to a photo menu region <b>1650</b> on the main display unit <b>157</b>.</p>
<p id="p-0201" num="0200">According to a second embodiment of the present invention, a mobile terminal is provided with at least two cameras arranged in a same direction by being spaced apart from each other with a prescribed distance in-between. Therefore, the mobile terminal is able to generate and play back a 3D picture or image.</p>
<p id="p-0202" num="0201">In order to generate a 3D picture, a 3D polyhedron is formed using the polygons, the polygons are covered with a prepared 2D texture. Another method for generating a 3D picture comprises at least two images generated from photographing a same object in different viewpoints and combined together or different images can be seen intact according to the user's viewpoint. Yet another method for generating a 3D picture comprises, generating one image for a right eye and the other image for a left eye by combining at least two images generated from photographing a same object at different viewpoints together, both of the right and left eye images are simultaneously displayed, and the respective images are made to arrive at both eyes of a user, via polarized glasses or a parallex barrier. In this example, a method of generating and playing back a 3D image is explained with reference to <figref idref="DRAWINGS">FIG. 17</figref> as follows.</p>
<p id="p-0203" num="0202"><figref idref="DRAWINGS">FIG. 17</figref> is a diagram of one example for generating and displaying a 3D photo in a mobile terminal according to another embodiment of the present invention.</p>
<p id="p-0204" num="0203">Referring to <figref idref="DRAWINGS">FIG. 17</figref>, a mobile terminal according to an embodiment of the present invention has a configuration similar to that of the mobile terminal according to other embodiments of the present invention. Yet, referring to <figref idref="DRAWINGS">FIG. 17(</figref><i>a</i>), two camera modules <b>121</b><i>a </i>and <b>121</b><i>b </i>for recognizing a motion of a pointer in a space provided between a transparent display unit <b>156</b> and a main display unit <b>157</b> are provided in a manner of being spaced apart from each other with a prescribed distance in-between.</p>
<p id="p-0205" num="0204">Once two camera modules are arranged by being spaced apart from each other at a prescribed distance, referring to <figref idref="DRAWINGS">FIG. 17(</figref><i>b</i>), when an object <b>1700</b> is photographed, the cameras <b>121</b><i>a </i>and <b>121</b><i>b </i>have different viewpoints due to the arranged interval, respectively.</p>
<p id="p-0206" num="0205">A result of photographing is explained with reference to <figref idref="DRAWINGS">FIG. 17(</figref><i>c</i>) as follows. First, a picture of an object <b>1700</b> photographed via a left camera <b>121</b><i>a </i>is shown in <b>1710</b>. A picture of the object <b>1700</b> is photographed via a right camera <b>121</b><i>b </i>is shown in <b>1730</b>. The controller <b>180</b> synthesizes the left picture <b>1710</b> and the right picture <b>1730</b> together to generate the object <b>1700</b> shown in <b>1720</b>. Therefore, the controller <b>180</b> is able to play back a 3D picture or image in a manner of detecting a user's eyes via the above mentioned method according to one embodiment of the present invention and then display a picture of an object corresponding to a direction of the user's eyes via the transparent display unit <b>156</b>.</p>
<p id="p-0207" num="0206">According to another method of generating and outputting a 3D image, a specific focal length is set for each camera under the control of the controller <b>180</b> and an image deviating from a focus is then blurred. Subsequently, the controller <b>180</b> separates an image photographed via each of the cameras into an in-focus part, an out-of-focus, and blurred part. An image corresponding to the in-focus part is displayed via the transparent display unit <b>156</b>, while the out-of-focus part is displayed via the main display unit <b>157</b>. Thus, a 3D image may be provided to a user.</p>
<p id="p-0208" num="0207">In another example of a mobile terminal having two camera modules according to another embodiment of the present invention, the mobile terminal may simultaneously photograph one object using cameras by differentiating focal distances of the cameras from each other. For this, at least one of two cameras is preferably provided with a zoom function of adjusting a focal distance. In particular, for example, the controller <b>180</b> controls one camera to photograph a same object with a wide angle and controls the other camera to photograph the same object with telephoto. The mobile terminal may then generate a photograph from one photographic event with two different angles of view.</p>
<p id="p-0209" num="0208">In a further example, the mobile terminal may apply a different image filter to an image photographed via each of the cameras. In this example, the image filter refers to an additional effect given to an image photographed by a preset method. For instance, an image photographed via one camera is set to be automatically converted to black and white, while an image photographed via the other camera is set to be automatically converted to a sephia tone. Through this, a visual effect may be applied to the photograph.</p>
<p id="p-0210" num="0209">In the above described embodiments, the transparent display unit and the main display unit are explained assuming that the mobile terminal includes the transparent and main display units of which are respectively hinged to each other by contacting each other. This is just one example for one type of mobile terminal according to the present invention. The transparent display unit and the main display unit can be arranged in various manners for generating a 3D effect attributed to a spaced distance or angle in-between. For instance, the main display unit and the transparent display unit can be connected together using at least two hinges. Alternatively, the main display unit and the transparent display unit can be connected together to enable a pivot using at least one ball joint to freely adjust arrangement. In this case, a stopper or latch for fixing a position by a plurality of steps can be provided to such a connecting means as a hinge, a pivot, a ball point and the like. The controller can further include a means for electronically providing information on an interval or angle between the main display unit and the transparent display unit.</p>
<p id="p-0211" num="0210">Accordingly, a fold state or a tilt state can be changed into another terminology corresponding to a system of spacing two kinds of display units apart from each other.</p>
<p id="p-0212" num="0211">Meanwhile, according to a further embodiment of the present invention, the above-described methods can be implemented in a programmable recorded medium as computer-readable codes. The computer-readable media include all kinds of recording devices in which data readable by a computer system are stored. The computer-readable media include ROM, RAM, CD-ROM, magnetic tapes, floppy discs, optical data storage devices, and the like for example and also include carrier-wave type implementations (e.g., transmission via Internet).</p>
<p id="p-0213" num="0212">It will be apparent to those skilled in the art that various modifications and variations can be made in the present invention without departing from the spirit or scope of the inventions. Thus, it is intended that the present invention covers the modifications and variations of this invention provided they come within the scope of the appended claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A mobile terminal comprising:
<claim-text>a main display unit comprising a first display unit;</claim-text>
<claim-text>a transparent display unit comprising a second display unit, the transparent display unit being attached to the main display unit such that the transparent display unit may be viewed at an angle in comparison to the main display unit; and</claim-text>
<claim-text>a controller controlling a three-dimensional (3D) and a two-dimensional (2D) image to be output by selectively displaying an image on at least the first display unit or the second display unit while the main display unit and the transparent display unit are positioned at an angle,</claim-text>
<claim-text>wherein at least one 2D object is displayed on the first display unit when the mobile terminal is in a closed state, and when the main display unit and the transparent display unit are positioned at an angle from the closed state, at least one 3D object is displayed on the second display while displaying the at least one 2D object, and</claim-text>
<claim-text>wherein each of the at least one 3D object corresponds to each of the at least one 2D object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The mobile terminal of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 3D image is visible when the second display unit faces a user.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The mobile terminal of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the 2D image is simultaneously displayed on the first display unit when the 3D image is visible from the second display unit.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The mobile terminal of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising a memory unit configured to store a lookup table including a correction value for correcting the position of the image displayed on the second display unit according to the detected view point of the user, wherein the controller performs the position correction of the image displayed on the second display unit by referring to the lookup table.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The mobile terminal of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 2D image is displayed on the first display unit when the main display unit and the transparent display unit are positioned at an angle.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The mobile terminal of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the 2D image is displayed on the second display unit.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The mobile terminal of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the transparent display unit is deactivated when the mobile terminal is in the closed state, such that the transparent display unit covers the main display unit.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The mobile terminal of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a first camera configured to photograph a user's image, wherein the controller detects a viewpoint of the user via the user's image, corrects a position of an image displayed on the second display unit to correspond to the detected viewpoint, and controls the image displayed on the first display unit to be displayed at a predetermined position on the first display unit.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The mobile terminal of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein an interactive menu is displayed on the second display unit when the 3D image is visible from the second display unit.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The mobile terminal of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a pointer detecting unit configured to detect a position of a pointer in a space formed between the transparent display unit and the main display unit, wherein the controller controls an operation corresponding to the pointer position detected by the pointer detecting unit to be executed via a 3D user interface.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The mobile terminal of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein when a 3D map image is visible from the second unit, a 2D map corresponding to the 3D map is displayed on the first display unit and an interactive menu for receiving user input is displayed on at least the second display unit, or the first display unit.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The mobile terminal of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein at least a size or shape of the at least one 3D object is changed according to the angle.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A mobile terminal comprising:
<claim-text>a main display unit configured to display a first display;</claim-text>
<claim-text>a transparent display unit configured to display a second display, the transparent display unit being attached to the main display unit such that the transparent display unit may be viewed at an angle in comparison to the main display unit;</claim-text>
<claim-text>a controller controlling a three-dimensional (3D) image to be output by selectively displaying an image on at least one of the first display and the second display while the main display unit and the transparent display unit are spaced apart from each other; and</claim-text>
<claim-text>a pointer detecting unit configured to detect a position of a pointer in a space formed between the transparent display unit and the main display unit,</claim-text>
<claim-text>wherein the controller controls an operation corresponding to the pointer position detected by the pointer detecting unit to be executed via a 3D user interface.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The mobile terminal of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the 3D image is visible when the second display faces a user.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A method for displaying an image, the method comprising:
<claim-text>displaying an image on a main display unit comprising a first display unit;</claim-text>
<claim-text>displaying a second image on a transparent display unit comprising a second display unit, the transparent display unit being attached to the main display unit such that the transparent display unit may be viewed at an angle in comparison to the main display unit; and</claim-text>
<claim-text>displaying a three-dimensional (3D) or a two-dimensional (2D) image by selectively displaying at least the first or second image while the main display unit and the transparent display unit are positioned at an angle,</claim-text>
<claim-text>wherein at least one 2D object is displayed on the first display unit when the mobile terminal is in a closed state, and when the main display unit and the transparent display unit are positioned at an angle from the closed state, at least one 3D object is displayed on the second display while displaying the at least one 2D object, and</claim-text>
<claim-text>wherein each of the at least one 3D object corresponds to each of the at least one 2D object.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the 3D image is visible when the second display unit faces a user.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the 2D image is simultaneously displayed on at least the first display unit or the second display unit when the 3D image is visible from the second display unit.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein an interactive menu is displayed on the second display unit when the 3D image is visible from the second display unit.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein when a 3D map image is visible from the second unit, a 2D map corresponding to the 3D map is displayed on the first display unit and an interactive menu for receiving user input is displayed on at least the second display unit the first display unit.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the 2D image is displayed on the first display unit when the main display unit and the transparent display unit are positioned at an angle.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the 2D image is displayed on the second display unit when the mobile terminal is in a closed state.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. A method for displaying an image, the method comprising:
<claim-text>displaying an image on a main display unit comprising a first display unit;</claim-text>
<claim-text>displaying a second image on a transparent display unit comprising a second display unit, the transparent display unit being attached to the main display unit such that the transparent display unit may be viewed at an angle in comparison to the main display unit;</claim-text>
<claim-text>displaying a three-dimensional (3D) image by selectively displaying at least the first or second image while the main display unit and the transparent display unit are positioned at an angle;</claim-text>
<claim-text>detecting, via a pointer detecting unit, a position of a pointer in a space formed between the transparent display unit and the main display unit; and</claim-text>
<claim-text>controlling an operation corresponding to the pointer position detected by the pointer detecting unit to be executed via a 3D user interface.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the 3D image is visible when the second display unit faces a user. </claim-text>
</claim>
</claims>
</us-patent-grant>
