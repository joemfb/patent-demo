<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626507-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626507</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13690037</doc-number>
<date>20121130</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>17</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704249</main-classification>
<further-classification>704243</further-classification>
<further-classification>704251</further-classification>
<further-classification>704255</further-classification>
<further-classification>704256</further-classification>
<further-classification>704270</further-classification>
<further-classification>704  9</further-classification>
<further-classification>706 11</further-classification>
<further-classification>715809</further-classification>
<further-classification>715863</further-classification>
</classification-national>
<invention-title id="d2e51">Systems and methods for extracting meaning from multimodal inputs using finite-state devices</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5502774</doc-number>
<kind>A</kind>
<name>Bellegarda et al.</name>
<date>19960300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5600765</doc-number>
<kind>A</kind>
<name>Ando et al.</name>
<date>19970200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5677993</doc-number>
<kind>A</kind>
<name>Ohga et al.</name>
<date>19971000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5748974</doc-number>
<kind>A</kind>
<name>Johnson</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5781663</doc-number>
<kind>A</kind>
<name>Sakaguchi et al.</name>
<date>19980700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5855000</doc-number>
<kind>A</kind>
<name>Waibel et al.</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5884249</doc-number>
<kind>A</kind>
<name>Namba et al.</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6167376</doc-number>
<kind>A</kind>
<name>Ditzik</name>
<date>20001200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6438523</doc-number>
<kind>B1</kind>
<name>Oberteuffer et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6484136</doc-number>
<kind>B1</kind>
<name>Kanevsky et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  9</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6529863</doc-number>
<kind>B1</kind>
<name>Ball et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6665640</doc-number>
<kind>B1</kind>
<name>Bennett et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6735566</doc-number>
<kind>B1</kind>
<name>Brand</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6823308</doc-number>
<kind>B2</kind>
<name>Keiller et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704256</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>7054812</doc-number>
<kind>B2</kind>
<name>Charlesworth et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>7069215</doc-number>
<kind>B1</kind>
<name>Baglore et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7240004</doc-number>
<kind>B1</kind>
<name>Allauzen et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704255</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7321854</doc-number>
<kind>B2</kind>
<name>Sharma et al.</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704243</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7783485</doc-number>
<kind>B2</kind>
<name>Allauzen et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704255</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2004/0119754</doc-number>
<kind>A1</kind>
<name>Bangalore et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345809</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2008/0052080</doc-number>
<kind>A1</kind>
<name>Narayanan</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704270</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2010/0100509</doc-number>
<kind>A1</kind>
<name>Johnston et al.</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 11</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2010/0281435</doc-number>
<kind>A1</kind>
<name>Bangalore et al.</name>
<date>20101100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715863</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Sharma, et al Toward Multimodal Human-Computer Interface Proc. of the IEEE, vol. 86, Issue 5, May 1998, pp. 853-869.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Chen, et al., &#x201c;Gesture-Speech Based HMI for a Rehabilitation Robot,&#x201d; Proc. of the Southeastern '96,&#x201c;Bringing Together Edu., Science &#x26; Tech.&#x201d;, Apr. 11-14, 1996, pp. 29-36.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Roy, et al., Word Learning in a Multimodal Environment, Proc. of the '98 IEEE Confr. on Acoustics Speech &#x26; Signal Processing, ICASSP'98, May 12-15, 1998, vol. 6, pp. 3761-3764.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Salem, et al., Current Trends in Multimodal INput Recognition, IEE Colliquium on Virtual Reality Personal Mobile &#x26; Practical Application&#x2014;98/454 Oct. 28, 1998, pp. 3/1-3/6.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Kettebekov, et al., &#x201c;Toward Multimodal Interpretation in a Natural Speech/ Gesture Interface&#x201d;, Proc. 1999 Int'l Conf. on Information and Intelligence Systems, pp. 328-335.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Bangalore, et al.; &#x201c;Finite State Multimodal Parsing and Understanding&#x201d;; Jul. 31, 2000-Aug. 4, 2000; International Conference of Computer Linguistics.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Bangalore, et al.; &#x201c;Integrating Multimodal Language Processing with Speech Recognition&#x201d;; Oct. 2000; Proceeding of International Conference on Spoken Language Processing.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Johnston, et al.; &#x201c;Unification-based Multimodal Integration&#x201d;, 1997, Proceeding of the 35th Anual Meeting of the Association for Computational Linguistics.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Johnston, M.; &#x201c;Unification-based Multimodal Parsing&#x201d;; 1998; In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics, pp. 624-630.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Johnston, M.; &#x201c;Deixis and Conjunction in Multimodal Systems&#x201d;; 2000, proceedings of Coling&#x2014;2000.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>18</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704  9</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704251</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704256</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704243</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704255</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704270</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>706 11</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715809</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715863</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>11</number-of-drawing-sheets>
<number-of-figures>17</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>13485574</doc-number>
<date>20120531</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8355916</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13690037</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>13291427</doc-number>
<date>20111108</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8214212</doc-number>
<date>20120703</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13485574</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11904085</doc-number>
<date>20070926</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8103502</doc-number>
<date>20120124</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13291427</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10970215</doc-number>
<date>20041021</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7295975</doc-number>
<date>20071113</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11904085</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>09904252</doc-number>
<date>20010712</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6868383</doc-number>
<date>20050315</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10970215</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130158998</doc-number>
<kind>A1</kind>
<date>20130620</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>AT&#x26;T Intellectual Property II, L.P.</orgname>
<address>
<city>Atlanta</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Bangalore</last-name>
<first-name>Srinivas</first-name>
<address>
<city>Hackettstown</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Johnston</last-name>
<first-name>Michael J.</first-name>
<address>
<city>Hoboken</city>
<state>NJ</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Wolff &#x26; Samson, PC</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>AT&#x26;T Intellectual Property II, L.P.</orgname>
<role>02</role>
<address>
<city>Atlanta</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Colucci</last-name>
<first-name>Michael</first-name>
<department>2658</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Multimodal utterances contain a number of different modes. These modes can include speech, gestures, and pen, haptic, and gaze inputs, and the like. This invention use recognition results from one or more of these modes to provide compensation to the recognition process of one or more other ones of these modes. In various exemplary embodiments, a multimodal recognition system inputs one or more recognition lattices from one or more of these modes, and generates one or more models to be used by one or more mode recognizers to recognize the one or more other modes. In one exemplary embodiment, a gesture recognizer inputs a gesture input and outputs a gesture recognition lattice to a multimodal parser. The multimodal parser generates a language model and outputs it to an automatic speech recognition system, which uses the received language model to recognize the speech input that corresponds to the recognized gesture input.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="230.29mm" wi="135.55mm" file="US08626507-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="119.04mm" wi="189.40mm" file="US08626507-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="240.62mm" wi="179.15mm" file="US08626507-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="249.51mm" wi="172.13mm" orientation="landscape" file="US08626507-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="216.58mm" wi="165.44mm" file="US08626507-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="138.43mm" wi="126.58mm" file="US08626507-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="248.92mm" wi="151.64mm" orientation="landscape" file="US08626507-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="245.03mm" wi="151.64mm" orientation="landscape" file="US08626507-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="234.78mm" wi="150.45mm" file="US08626507-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="248.92mm" wi="164.42mm" orientation="landscape" file="US08626507-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="252.14mm" wi="114.55mm" orientation="landscape" file="US08626507-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="238.68mm" wi="101.77mm" orientation="landscape" file="US08626507-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">This application is a continuation of prior U.S. patent application Ser. No. 13/485,574 filed May 31, 2012, which is a continuation of prior U.S. patent application Ser. No. 13/291,427 filed Nov. 8, 2011 which issued as U.S. Pat. No. 8,214,212 on Jul. 3, 2012, which is a continuation of prior U.S. patent application Ser. No. 11/904,085 filed Sep. 26, 2007 which issued as U.S. Pat. No. 8,103,502 on Jan. 24, 2012, which is a continuation of prior U.S. patent application Ser. No. 10/970,215 filed Oct. 21, 2004 which issued as U.S. Pat. No. 7,295,975 on Nov. 13, 2007, which is a continuation of prior U.S. patent application Ser. No. 09/904,252 filed Jul. 12, 2001 which issued as U.S. Pat. No. 6,868,383 on Mar. 15, 2005, each of which is incorporated herein by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of Invention</p>
<p id="p-0004" num="0003">This invention is directed to parsing and understanding of utterances whose content is distributed across multiple input modes.</p>
<p id="p-0005" num="0004">2. Description of Related Art</p>
<p id="p-0006" num="0005">Multimodal interfaces allow input and/or output to be conveyed over multiple different channels, such as speech, graphics, gesture and the like. Multimodal interfaces enable more natural and effective interaction, because particular modes are best-suited for particular kinds of content. Multimodal interfaces are likely to play a critical role in the ongoing migration of interaction from desktop computing to wireless portable computing devices, such as personal digital assistants, like the Palm Pilot&#xae;, digital cellular telephones, public information kiosks that are wirelessly connected to the Internet or other distributed networks, and the like. One barrier to adopting such wireless portable computing devices is that they offer limited screen real estate, and often have limited keyboard interfaces, if any keyboard interface at all.</p>
<p id="p-0007" num="0006">To realize the full potential of such wireless portable computing devices, multimodal interfaces need to support not just input from multiple modes. Rather, multimodal interfaces also need to support synergistic multimodal utterances that are optimally distributed over the various available modes. In order to achieve this, the content from different modes needs to be effectively integrated.</p>
<p id="p-0008" num="0007">One previous attempt at integrating the content from the different modes is disclosed in &#x201c;Unification-Based Multimodal Integration&#x201d;, M. Johnston et al., <i>Proceedings of the </i>35<i>th ACL</i>, Madrid Spain, p. 281-288, 1997 (Johnston 1), incorporated herein by reference in its entirety. Johnston 1 disclosed a pen-based device that allows a variety of gesture utterances to be input through a gesture mode, while a variety of speech utterances can be input through a speech mode.</p>
<p id="p-0009" num="0008">In Johnston 1, a unification operation over typed feature structures was used to model the integration between the gesture mode and the speech mode. Unification operations determine the consistency of two pieces of partial information. If the two pieces of partial information are determined to be consistent, the unification operation combines the two pieces of partial information into a single result. Unification operations were used to determine whether a given piece of gestural input received over the gesture mode was compatible with a given piece of spoken input received over the speech mode. If the gestural input was determined to be compatible with the spoken input, the two inputs were combined into a single result that could be further interpreted.</p>
<p id="p-0010" num="0009">In Johnston 1, typed feature structures were used as a common meaning representation for both the gestural inputs and the spoken inputs. In Johnston 1, the multimodal integration was modeled as a cross-product unification of feature structures assigned to the speech and gestural inputs. While the technique disclosed in Johnston 1 overcomes many of the limitations of earlier multimodal systems, this technique does not scale well to support multi-gesture utterances, complex unimodal gestures, or other modes and combinations of modes. To address these limitations, the unification-based multimodal integration technique disclosed in Johnston 1 was extended in &#x201c;Unification-Based Multimodal Parsing&#x201d;, M. Johnston, <i>Proceedings of COLING</i>-<i>ACL </i>98, p. 624-630, 1998 (Johnston 2), herein incorporated by reference in its entirety. The multimodal integration technique disclosed in Johnston 2 uses a multi-dimensional chart parser. In Johnston 2, elements of the multimodal input are treated as terminal edges by the parser. The multimodal input elements are combined together in accordance with a unification-based multimodal grammar. The unification-based multimodal parsing technique disclosed in Johnston 2 was further extended in &#x201c;Multimodal Language Processing&#x201d;, M. Johnston, <i>Proceedings of ICSLP </i>1998, 1998 (published on CD-ROM only) (Johnston 3), incorporated herein by reference in its entirety.</p>
<p id="p-0011" num="0010">Johnston 2 and 3 disclosed how techniques from natural language processing can be adapted to support parsing and interpretation of utterances distributed over multiple modes. In the approach disclosed by Johnston 2 and 3, speech and gesture recognition produce n-best lists of recognition results. The n-best recognition results are assigned typed feature structure representations by speech interpretation and gesture interpretation components. The n-best lists of feature structures from the spoken inputs and the gestural inputs are passed to a multi-dimensional chart parser that uses a multimodal unification-based grammar to combine the representations assigned to the input elements. Possible multimodal interpretations are then ranked. The optimal interpretation is then passed on for execution.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0012" num="0011">However, the unification-based approach disclosed in Johnston 1-Johnston 3 does not allow for tight coupling of multimodal parsing with speech and gesture recognition. Compensation effects are dependent on the correct answer appearing in each of the n-best list of interpretations obtained from the recognitions obtained from the inputs of each mode. Moreover, multimodal parsing cannot directly influence the progress of either speech recognition or gesture recognition. The multi-dimensional parsing approach is also subject to significant concerns in terms of computational complexity. In the worst case, for the multi-dimensional parsing technique disclosed in Johnston 2, the number of parses to be considered is exponential relative to the number of input elements and the number of interpretations the input elements have. This complexity is manageable when the inputs yield only n-best results for small n. However, the complexity quickly gets out of hand if the inputs are sizable lattices with associated probabilities.</p>
<p id="p-0013" num="0012">The unification-based approach also runs into significant problems when choosing between multiple competing parses and interpretations. Probabilities associated with composing speech events and multiple gestures need to be combined. Uni-modal interpretations need to be compared to multimodal interpretations and so on. While this can all be achieved using the unification-based approach disclosed in Johnston 1-Johnston 3, significant post-processing of sets of competing multimodal interpretations generated by the multimodal parser will be involved.</p>
<p id="p-0014" num="0013">This invention provides systems and methods that allow parsing understanding and/or integration of the gestural inputs and the spoken inputs using one or more finite-state devices.</p>
<p id="p-0015" num="0014">This invention separately provides systems and methods that allow multi-dimensional parsing and understanding using weighted finite-state automata.</p>
<p id="p-0016" num="0015">This invention further provides systems and methods that allow multi-dimensional parsing and understanding using a three-tape weighted finite-state automaton.</p>
<p id="p-0017" num="0016">This invention separately provides systems and methods that use combinations of finite-state transducers to integrate the various modes of the multimodal interface.</p>
<p id="p-0018" num="0017">This invention separately provides systems and methods that use the recognition results of one mode of the multimodal input received from the multimodal interface as a language model or other model in the recognition process of other modes of the multimodal inputs received from the multimodal interface.</p>
<p id="p-0019" num="0018">This invention separately provides systems and methods that use the recognition results of one mode of the multimodal input received from the multimodal interface to constrain the recognition process of one or more of the other modes of the multimodal input received from the multimodal interface.</p>
<p id="p-0020" num="0019">This invention further provides systems and methods that integrate the recognition results from the second multimodal input, which are based on the recognition results of the first multimodal input, with the recognition results of the first multimodal input and then extract meaning from the combined recognition results.</p>
<p id="p-0021" num="0020">This invention further provides systems and methods that base the speech recognition on the results of the gesture recognition.</p>
<p id="p-0022" num="0021">The various exemplary embodiments of the systems and methods according to this invention allow spoken language and gesture input streams to be parsed and integrated by a single weighted finite-state device. This single weighted finite-state device provides language models for speech and gesture recognition and composes the meaning content from the speech and gesture input streams into a single semantic representation. Thus, the systems and methods according to this invention not only address multimodal language recognition, but also encode the semantics as well as the syntax into a single weighted finite-state device. Compared to the previous approaches for integrating multimodal input streams, such as those described in Johnston 1-3, which compose elements from n-best lists of recognition results, the systems and methods according to this invention provide the potential for direct compensation among the various multimodal input modes.</p>
<p id="p-0023" num="0022">Various exemplary embodiments of the systems and methods according to this invention allow the gestural input to dynamically alter the language model used for speech recognition. Various exemplary embodiments of the systems and methods according to this invention reduce the computational complexity of multi-dimensional multimodal parsing. In particular, the weighted finite-state devices used in various exemplary embodiments of the systems and methods according to this invention provide a well-understood probabilistic framework for combining the probability distributions associated with the speech and gesture or other input modes and for selecting among multiple competing multimodal interpretations.</p>
<p id="p-0024" num="0023">These and other features and advantages of this invention are described in, or are apparent from, the following detailed description of various exemplary embodiments of the systems and methods according to this invention.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0025" num="0024">Various exemplary embodiments of this invention will be described in detail, with reference to the following figures, wherein:</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating one exemplary embodiment of a conventional automatic speech recognition system usable with a multimodal meaning recognition system according to this invention;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram illustrating one exemplary embodiment of a multimodal user input device and one exemplary embodiment of a multimodal meaning recognition system according to this invention;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 3</figref> is a block diagram illustrating in greater detail one exemplary embodiment of the gesture recognition system of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 4</figref> is a block diagram illustrating in greater detail one exemplary embodiment of the multimodal parser and meaning recognition system of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 5</figref> is a block diagram illustrating in greater detail one exemplary embodiment of the multimodal user input device of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 6</figref> is one exemplary embodiment of a multimodal grammar fragment usable by the multimodal meaning recognition system according to this invention;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 7</figref> is one exemplary embodiment of a three-tape multimodal finite-state automaton usable to recognize the multimodal inputs received from the exemplary embodiment of the multimodal user input device shown in <figref idref="DRAWINGS">FIG. 5</figref>;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 8</figref> is one exemplary embodiment of a gesture finite-state machine generated by recognizing the gesture inputs shown in the exemplary embodiment of the multimodal user input device shown in <figref idref="DRAWINGS">FIG. 5</figref>;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 9</figref> is one exemplary embodiment of a gesture-to-speech finite-state transducer that represents the relationship between speech and gesture for the exemplary embodiment of the multimodal user input device shown in <figref idref="DRAWINGS">FIG. 5</figref>;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 10</figref> is one exemplary embodiment of a speech/gesture/meaning finite-state transducer that represents the relationship between the combined speech and gesture symbols and the semantic meaning of the multimodal input for the exemplary embodiment of the multimodal input device shown in <figref idref="DRAWINGS">FIG. 5</figref>;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 11</figref> is a flowchart outlining one exemplary embodiment of a method for extracting meaning from a plurality of multimodal inputs;</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 12</figref> is one exemplary embodiment of a gesture/language finite-state transducer illustrating the composition of the gesture finite-state machine shown in <figref idref="DRAWINGS">FIG. 8</figref> with the gesture-to-speech finite-state transducer shown in <figref idref="DRAWINGS">FIG. 9</figref>;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 13</figref> is one exemplary embodiment of a finite-state machine generated by taking a projection on the output tape of the gesture/language finite-state transducer shown in <figref idref="DRAWINGS">FIG. 12</figref>;</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 14</figref> is one exemplary embodiment of a lattice of possible word sequences generated by the automatic speech recognition system shown in <figref idref="DRAWINGS">FIG. 1</figref> when using the finite-state machine shown in <figref idref="DRAWINGS">FIG. 13</figref> as a language model in view of the speech input received from the exemplary embodiment of the multimodal user input device shown in <figref idref="DRAWINGS">FIG. 5</figref>;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 15</figref> illustrates one exemplary embodiment of a gesture/speech finite-state transducer generated by composing the gesture/language finite-state transducer shown in <figref idref="DRAWINGS">FIG. 12</figref> with the word sequence lattice shown in <figref idref="DRAWINGS">FIG. 14</figref>;</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 16</figref> is one exemplary embodiment of a gesture/speech finite-state machine obtained from the gesture/speech finite-state transducer shown in <figref idref="DRAWINGS">FIG. 14</figref>; and</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 17</figref> is one exemplary embodiment of a finite-state transducer, obtained from composing the gesture/speech finite-state machine shown in <figref idref="DRAWINGS">FIG. 16</figref> with the speech/gesture/meaning finite-state transducer shown in <figref idref="DRAWINGS">FIG. 10</figref>, which extracts the meaning from the multimodal gestural and spoken inputs received when using the exemplary embodiment of the multimodal user input device shown in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS</heading>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 1</figref> illustrates one exemplary embodiment of an automatic speech recognition system <b>100</b> usable with the multimodal recognition and/or meaning system <b>1000</b> according to this invention that is shown in <figref idref="DRAWINGS">FIG. 2</figref>. As shown in <figref idref="DRAWINGS">FIG. 1</figref>, automatic speech recognition can be viewed as a processing pipeline or cascade.</p>
<p id="p-0044" num="0043">In each step of the processing cascade, one or two lattices are input and composed to produce an output lattice. In automatic speech recognition and in the following description of the exemplary embodiments of the systems and methods of this invention, the term &#x201c;lattice&#x201d; denotes a directed and labeled graph, which is possibly weighted. In each lattice, there is typically a designated start node &#x201c;s&#x201d; and a designated final node &#x201c;t&#x201d;. Each possible pathway through the lattice from the start node s to the final node t induces a hypothesis based on the arc labels between each pair of nodes in the path. For example, in a word lattice, the arc labels are words and the various paths between the start node s and the final node t form sentences. The weights on the arcs on each path between the start node s and the final node t are combined to represent the likelihood that that path will represent a particular portion of the utterance.</p>
<p id="p-0045" num="0044">As shown in <figref idref="DRAWINGS">FIG. 1</figref>, one exemplary embodiment of a known automatic speech recognition system <b>100</b> includes a signal processing subsystem <b>110</b>, an acoustic model lattice <b>120</b>, a phonetic recognition subsystem <b>130</b>, a lexicon lattice <b>140</b>, a word recognition subsystem <b>150</b>, a grammar or language model lattice <b>160</b>, and a task recognition subsystem <b>170</b>. In operation, uttered speech is input via a microphone, which converts the sound waves of the uttered speech into an electronic speech signal. The electronic speech signal is input to the signal processing subsystem <b>110</b> on a speech signal input line <b>105</b>. The signal processing subsystem <b>110</b> digitizes the electronic speech signal to generate a feature vector lattice <b>115</b>. The feature vector lattice <b>115</b> is a lattice of acoustic feature vectors. The feature vector lattice <b>115</b> is input along with the acoustic model lattice <b>120</b> to the phonetic recognition subsystem <b>130</b>. The acoustic model lattice <b>120</b> represents a set of acoustic models and is applied to transform the feature vector lattice <b>115</b> into a phone lattice. Each node of the phone lattice represents a spoken sound, such as, for example, the vowel /e/ in &#x201c;bed&#x201d;.</p>
<p id="p-0046" num="0045">The phone lattice <b>135</b> is input along with the lexicon lattice <b>140</b> into the word recognition subsystem <b>150</b>. The lexicon lattice <b>140</b> describes different pronunciations of various words and transforms the phone lattice <b>135</b> into a word lattice <b>155</b>. The word lattice <b>155</b> is then input, along with the grammar or language model lattice <b>160</b>, into the utterance recognition subsystem <b>170</b>. The grammar or language model lattice <b>160</b> represents task-specific information and is used to extract the most likely sequence of uttered words from the word lattice <b>155</b>. Thus, the utterance recognition subsystem <b>170</b> uses the grammar or language model lattice <b>160</b> to extract the most likely sentence or other type of utterance from the word lattice <b>155</b>. In general, the grammar or language model lattice <b>160</b> will be selected based on the task associated with the uttered speech. The most likely sequence of words, or the lattice of n most-likely sequences of words, is output as the recognized utterance <b>175</b>.</p>
<p id="p-0047" num="0046">In particular, one conventional method of implementing automatic speech recognition forms each of the acoustic model lattice <b>120</b>, the lexicon lattice <b>140</b> and the grammar or language model lattice <b>160</b> as a finite-state transducer. Thus, each of the phonetic recognition subsystem <b>130</b>, the word recognition subsystem <b>150</b>, and the utterance recognition <b>170</b> performs a generalized composition operation between its input finite-state transducers. In addition, the signal processing subsystem <b>110</b> outputs the features vector lattice <b>115</b> as a finite-state transducer.</p>
<p id="p-0048" num="0047">Conventionally, the grammar or language model lattice <b>160</b> is predetermined and incorporated into the automatic speech recognition system <b>100</b> based on the particular recognition task that the automatic speech recognition system <b>100</b> is to perform. In various exemplary embodiments, any of the acoustic model lattice <b>120</b>, the lexicon lattice <b>140</b> and/or the grammar or language model <b>160</b> can be non-deterministic finite-state transducers. In this case, these non-deterministic finite-state transducers can be determinized using the various techniques disclosed in &#x201c;Finite-state transducers in Language and Speech Processing&#x201d;, M. Mohri, <i>Computational Linguistics, </i>23:2, p. 269-312, 1997, U.S. patent application Ser. No. 09/165,423, filed Oct. 2, 1998, and/or U.S. Pat. No. 6,073,098 to Buchsbaum et al., each incorporated herein by reference in its entirety.</p>
<p id="p-0049" num="0048">In contrast, in various exemplary embodiments of the systems and methods according to this invention, in the multimodal recognition or meaning system <b>1000</b> shown in <figref idref="DRAWINGS">FIG. 2</figref>, the automatic speech recognition system <b>100</b> uses a grammar or language model lattice <b>160</b> that is obtained from the recognized gestural input received in parallel with the speech signal <b>105</b>. This is shown in greater detail in <figref idref="DRAWINGS">FIG. 2</figref>. In this way, the output of the gesture recognition system <b>200</b> can be used to compensate for uncertainties in the automatic speech recognition system.</p>
<p id="p-0050" num="0049">Alternatively, in various exemplary embodiments of the systems and methods according this invention, the output of the automatic speech recognition system <b>100</b> and output of the gesture recognition system <b>200</b> can be combined only after each output is independently obtained. In this way, it becomes possible to extract meaning from the composition of two or more different input modes, such as the two different input modes of speech and gesture.</p>
<p id="p-0051" num="0050">Furthermore, it should be appreciated that, in various exemplary embodiments of the systems and methods according to this invention, the output of the gesture recognition system <b>200</b> can be used to provide compensation to the automatic speech recognition system <b>100</b>. Additionally, their combined output can be further processed to extract meaning from the combination of the two different input modes. In general, when there are two or more different input modes, any of one or more of the input modes can be used to provide compensation to one or more other ones of the input modes.</p>
<p id="p-0052" num="0051">Thus, it should further be appreciated that, while the following detailed description focuses on speech and gesture as the two input modes, any two or more input modes that can provide compensation between the modes, which can be combined to allow meaning to be extracted from the two or more recognized outputs, or both, can be used in place of, or in addition to, the speech and gesture input modes discussed herein.</p>
<p id="p-0053" num="0052">In particular, as shown in <figref idref="DRAWINGS">FIG. 2</figref>, when speech and gesture are the implemented input modes, a multimodal user input device <b>400</b> includes a gesture input portion <b>410</b> and a speech input portion <b>420</b>. The gesture input portion <b>410</b> outputs a gesture signal <b>205</b> to a gesture recognition system <b>200</b> of the multimodal recognition and/or meaning system <b>1000</b>. At the same time, the speech input portion <b>420</b> outputs the speech signal <b>105</b> to the automatic speech recognition system <b>100</b>. The gesture recognition system <b>200</b> generates a gesture recognition lattice <b>255</b> based on the input gesture signal <b>205</b> and outputs the gesture recognition lattice <b>255</b> to a multimodal parser and meaning recognition system <b>300</b> of the multimodal recognition and/or meaning system <b>1000</b>.</p>
<p id="p-0054" num="0053">In those various exemplary embodiments that provide compensation between the gesture and speech recognition systems <b>200</b> and <b>100</b>, the multimodal parser/meaning recognition system <b>300</b> generates a new grammar or language model lattice <b>160</b> for the utterance recognition subsystem <b>170</b> of the automatic speech recognition system <b>100</b> from the gesture recognition lattice <b>255</b>. In particular, this new grammar or language model lattice <b>160</b> generated by the multimodal parser/meaning recognition system <b>300</b> is specific to the particular sets of gestural inputs generated by a user through the gesture input portion <b>410</b> of the multimodal user input device <b>400</b>. Thus, this new grammar or language model lattice <b>160</b> represents all of the possible spoken strings that can successfully combine with the particular sequence of gestures input by the user through the gesture input portion <b>410</b>. That is, the recognition performed by the automatic speech recognition system <b>100</b> can be improved because the particular grammar or language model lattice <b>160</b> being used to recognize that spoken utterance is highly specific to the particular sequence of gestures made by the user.</p>
<p id="p-0055" num="0054">The automatic speech recognition system <b>100</b> then outputs the recognized possible word sequence lattice <b>175</b> back to the multimodal parser/meaning recognition system <b>300</b>. In those various exemplary embodiments that do not extract meaning from the combination of the recognized gesture and the recognized speech, the recognized possible word sequences lattice <b>175</b> is then output to a downstream processing task. The multimodal recognition and/or meaning system <b>1000</b> then waits for the next set of inputs from the multimodal user input device <b>400</b>.</p>
<p id="p-0056" num="0055">In contrast, in those exemplary embodiments that additionally extract meaning from the combination of the recognized gesture and the recognized speech, the multimodal parser/meaning recognition system <b>300</b> extracts meaning from the combination of the gesture recognition lattice <b>255</b> and the recognized possible word sequences lattice <b>175</b>. Because the spoken utterances input by the user through the speech input portion <b>420</b> are presumably closely related to the gestures input at the same time by the user through the gesture input portion <b>410</b>, the meaning of those gestures can be tightly integrated with the meaning of the spoken input generated by the user through the speech input portion <b>420</b>.</p>
<p id="p-0057" num="0056">The multimodal parser/meaning recognition system <b>300</b> outputs a recognized possible meaning lattice <b>375</b> in addition to, or in place of, one or both of the gesture recognition lattice <b>255</b> and/or the recognized possible word sequences lattice <b>175</b>. In various exemplary embodiments, the multimodal parser and meaning recognition system <b>300</b> combines the recognized lattice of possible word sequences <b>175</b> generated by the automatic speech recognition system <b>100</b> with the gesture recognition lattice <b>255</b> output by the gesture recognition system <b>200</b> to generate the lattice of possible meaning sequences <b>375</b> corresponding to the multimodal gesture and speech inputs received from the user through the multimodal user input device <b>400</b>.</p>
<p id="p-0058" num="0057">Moreover, in contrast to both of the embodiments outlined above, in those exemplary embodiments that only extract meaning from the combination of the recognized multimodal inputs, the multimodal parser/meaning recognition system <b>300</b> does not generate the new grammar or language model lattice <b>160</b>. Thus, the gesture recognition lattice <b>255</b> does not provide compensation to the automatic speech recognition system <b>100</b>. Rather, the multimodal parser/meaning recognition system <b>300</b> only combines the gesture recognition lattice <b>255</b> and the recognized possible word sequences lattice <b>175</b> to generate the recognition meaning lattice <b>375</b>.</p>
<p id="p-0059" num="0058">When the gesture recognition system <b>200</b> generates only a single recognized possible sequence of gestures as the gesture recognition lattice <b>255</b>, that means there is essentially no uncertainty in the gesture recognition. In this case, the gesture recognition lattice <b>255</b> provides compensation to the automatic speech recognition system <b>100</b> for any uncertainty in the speech recognition process. However, the gesture recognition system <b>200</b> can generate a lattice of n possible recognized gesture sequences as the gesture recognition lattice <b>255</b>. This recognizes that there may also be uncertainty in the gesture recognition process.</p>
<p id="p-0060" num="0059">In this case, the gesture recognition lattice <b>255</b> and the word lattice <b>155</b> provide mutual compensation for the uncertainties in both the speech recognition process and the gesture recognition process. That is, in the face of this uncertainty, the best, i.e., most-probable, combination of one of the n-best word sequences in the word lattice <b>155</b> with one of the n-best gesture sequences in the gesture recognition lattice may not include the best recognition possible sequence from either the word lattice <b>155</b> or the gesture recognition lattice <b>255</b>. For example, the most-probable sequence of gestures in the gesture recognition lattice may combine only with a rather low-probability word sequence through the word lattice, while the most-probable word sequence may combine well only with a rather low-probability gesture sequence. In contrast, a medium-probability word sequence may match very well with a medium-probability gesture sequence. Thus, the net probability of this latter combination of word and gesture sequences may be higher than the probability of the combination of the best word sequence with any of the gesture sequences through the gesture recognition lattice <b>255</b> and may be higher than the probability of the combination of the best gesture sequence with any of the word sequences through the lattice of possible word sequences <b>155</b>. In this way, mutual compensation is provided between the gesture recognition system <b>200</b> and the automatic speech recognition system <b>100</b>.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIGS. 3-5</figref> illustrate in greater detail various exemplary embodiments of the gesture recognition system <b>200</b>, the multimodal parser/meaning recognition system <b>300</b>, and the multimodal user input device <b>400</b>. In particular, as shown in <figref idref="DRAWINGS">FIG. 3</figref>, one exemplary embodiment of the gesture recognition system <b>200</b> includes a gesture feature extraction subsystem <b>210</b> and a gesture recognition subsystem <b>230</b>. Various other exemplary embodiments may include a gesture language model lattice and a gesture meaning subsystem. In operation, gesture utterances are input through the gesture input portion <b>410</b> of the multimodal user input device <b>400</b>, which converts the movements of an input device, such as a mouse, a pen, a trackball, a track pad or any other known or later-developed gestural input device, into an electronic gesture signal <b>205</b>. At the same time, the multimodal user input device <b>400</b> converts the gestural input into digital ink that can be viewed and understood by the user. This is shown in greater detail in <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0062" num="0061">The gesture feature extraction subsystem <b>210</b> converts the motions of the gesture input device represented by the gesture signal <b>205</b> into a gesture feature lattice <b>220</b>. As disclosed in Johnston 1-3, the various gestures that can be made can be as simple as pointing gestures to a particular information element at a particular location within the gesture input portion <b>410</b> of the multimodal user input device <b>400</b>, or can be as complex as a specialized symbol that represents a type of military unit on a military map displayed in the gesture input portion <b>410</b> of the multimodal user input portion <b>400</b> and includes an indication of how the unit is to move, and which unit is to move and how far that unit is to move, as described in detail in Johnston 1.</p>
<p id="p-0063" num="0062">The gesture feature lattice <b>220</b> is input to the gesture recognition subsystem <b>230</b>. The gesture recognition subsystem <b>230</b> may be implemented as a neural network, as a Hidden-Markov Model (HMM) or as a simpler template-based gesture classification algorithm. The gesture recognition subsystem <b>230</b> converts the gesture feature lattice <b>220</b> into the gesture recognition lattice <b>255</b>. The gesture recognition lattice <b>255</b> includes the identities of graphical elements against which diectic and other simple &#x201c;identification&#x201d; gestures are made, possible recognition of more complex gestures that the user may have made and possibly the locations on the displayed graphics where the more complex gesture was made, such as in Johnston 1, and the like. As shown in <figref idref="DRAWINGS">FIG. 2</figref>, the gesture recognition system <b>200</b> outputs the gesture recognition lattice <b>255</b> to the multimodal parser/meaning recognition system <b>300</b>.</p>
<p id="p-0064" num="0063">It should be appreciated that the gesture feature recognition subsystem <b>210</b> and the gesture recognition subsystem <b>230</b> can each be implemented using any known or later-developed system, circuit or technique that is appropriate. In general, the entire gesture recognition system <b>200</b> can be implemented using any known or later-developed system that generates a directed graph from a gesture input.</p>
<p id="p-0065" num="0064">For example, one known system captures the time and location or locations of the gesture. Optionally, these inputs are then normalized and/or rotated. The gestures are then provided to a pattern classification device that is implemented as part of the gesture feature recognition subsystem <b>210</b>. In various exemplary embodiments, this pattern classification device is a template matching system, which transforms the gesture into a feature vector. In various other exemplary embodiments, this pattern classification device is a neural network or a Hidden Markov Model that has been trained to recognize certain patterns of one or more temporally and/or spatially related gesture components as a specific set of features.</p>
<p id="p-0066" num="0065">When a single gesture is formed by two or more temporally and/or spatially related gesture components, those gesture components can be combined into a single gesture either during the recognition process or by the multimodal parser/meaning recognition system <b>300</b>. Once the gesture features are extracted, the gesture recognition subsystem <b>230</b> combines the temporally adjacent gestures into a lattice of one or more recognized possible gesture sequences that represent how the recognized gestures follow each other in time.</p>
<p id="p-0067" num="0066">In various exemplary embodiments, the multimodal parser and meaning recognition system <b>300</b> can be implemented using a single three-tape finite-state device that inputs the output lattices from the speech recognition system <b>100</b> and the gesture recognition system <b>200</b> and directly obtains and outputs a meaning result. In various exemplary embodiments, the three-tape finite-state device is a three-tape grammar model that relates the gestures and the words to a meaning of the combination of a gesture and a word. <figref idref="DRAWINGS">FIG. 7</figref> shows a portion of such a three-tape grammar model usable in the multimodal parser and meaning recognition system <b>300</b> to generate a meaning output from gesture and speech recognition inputs. In general, the multimodal parser and meaning recognition system <b>300</b> can be implemented using an n-tape finite-state device that inputs n&#x2212;1 lattices from a plurality of recognition systems usable to recognize an utterance having a plurality of different modes.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 4</figref> shows the multimodal parser/meaning recognition system <b>300</b> in greater detail. As shown in <figref idref="DRAWINGS">FIG. 4</figref>, the multimodal parser/meaning recognition system <b>300</b> may include one or more of a gesture-to-speech composing subsystem <b>310</b>, a gesture-to-speech finite-state transducer <b>320</b>, a lattice projection subsystem <b>330</b>, a gesture and speech composing subsystem <b>340</b>, a speech/gesture combining subsystem <b>350</b>, a speech/gesture/meaning lattice <b>360</b> and/or a meaning recognition subsystem <b>370</b>. In particular, the gesture-to-speech composing subsystem <b>310</b> inputs the gesture recognition lattice <b>255</b> output by the gesture recognition system <b>200</b> and composes it with the gesture-to-speech finite-state transducer <b>320</b> to generate a gesture/language finite-state transducer <b>325</b>. The gesture/language finite-state transducer <b>325</b> is output to both the lattice projection subsystem <b>330</b> and the gesture and speech composing subsystem <b>340</b>.</p>
<p id="p-0069" num="0068">The lattice projection subsystem <b>330</b> generates a projection of the gesture/language finite-state transducer <b>325</b> and outputs the projection of the gesture/language finite-state transducer <b>325</b> as the grammar or language model lattice <b>160</b> to the automatic speech recognition system <b>100</b>. Thus, if the multimodal parser/meaning recognition system <b>300</b> does not also extract meaning, the gesture and speech composing subsystem <b>340</b>, the speech/gesture combining subsystem <b>350</b>, the speech/gesture/meaning lattice <b>360</b> and the meaning recognition subsystem <b>370</b> can be omitted. Similarly, if the multimodal parser/meaning recognition system <b>300</b> does not generate a new grammar or language model lattice <b>160</b> for the automatic speech recognition system <b>100</b>, at least the lattice projection subsystem <b>330</b> can be omitted.</p>
<p id="p-0070" num="0069">In those various embodiments that combine the gesture recognition lattice <b>255</b> and the recognized possible lattice of word sequences <b>175</b>, whether or not the automatic speech recognition <b>100</b> has generated the lattice of possible word sequences <b>175</b> based on using the projection of the gesture/language finite-state transducer <b>325</b> as the grammar or language model or lattice <b>160</b>, the lattice of possible word sequences <b>175</b> is input by the multimodal parser/meaning recognition system <b>300</b>. In particular, the gesture and speech composing subsystem <b>340</b> inputs both the lattice of possible word sequences <b>175</b> and the gesture/language finite-state transducer <b>325</b>. In those various exemplary embodiments that do not use the output of the gesture recognition system <b>200</b> to provide compensation between the speech and gesture recognition systems <b>100</b> and <b>200</b>, the gesture/language finite-state transducer <b>325</b> can be generated using any known or later-developed technique for relating the gesture recognition lattice <b>255</b> to the recognized possible lattice of word sequences <b>175</b> in place of the gesture-to-speech composing subsystem <b>310</b> and the gesture-to-speech finite-state transducer <b>320</b>.</p>
<p id="p-0071" num="0070">In those various exemplary embodiments that extract meaning from the multimodal inputs, the gesture and speech composing subsystem <b>340</b> composes these lattices to generate a gesture/speech finite-state transducer <b>345</b>. The gesture and speech composing subsystem <b>340</b> outputs the gesture/speech finite-state transducer <b>345</b> to the speech/gesture combining subsystem <b>350</b>. The speech/gesture combining subsystem <b>350</b> converts the gesture/speech finite-state transducer <b>345</b> to a gesture/speech finite-state machine <b>355</b>. The gesture/speech finite-state machine <b>355</b> is output by the speech/gesture combining subsystem <b>350</b> to the meaning recognition subsystem <b>370</b>. The meaning recognition subsystem <b>370</b> composes the gesture/speech finite-state machine <b>355</b> with the speech/gesture/meaning finite-state transducer <b>360</b> to generate a meaning lattice <b>375</b>. The meaning lattice <b>375</b> combines the recognition of the speech utterance input through the speech input portion <b>420</b> and the recognition of the gestures input through the gesture input portion <b>410</b> of the multimodal user input device <b>400</b>. The most probable meaning is then selected from the meaning lattice <b>375</b> and output to a downstream task.</p>
<p id="p-0072" num="0071">It should be appreciated that the systems and methods disclosed herein use certain simplifying assumptions with respect to temporal constraints. In multi-gesture utterances, the primary function of temporal constraints is to force an order on the gestures. For example, if a user generates the spoken utterance &#x201c;move this here&#x201d; and simultaneously makes two gestures, then the first gesture corresponds to the spoken utterance &#x201c;this&#x201d;, while the second gesture corresponds to the spoken utterance &#x201c;here&#x201d;. In the various exemplary embodiments of the systems and methods according to this invention described herein, the multimodal grammars encode order, but do not impose explicit temporal constraints. However, it should be appreciated that there are multimodal applications in which more specific temporal constraints are relevant. For example, specific temporal constraints can be relevant in selecting among unimodal and multimodal interpretations. That is, if a gesture is temporally distant from the speech, then the unimodal interpretation should be preferred.</p>
<p id="p-0073" num="0072">To illustrate the operation of the multimodal recognition and/or meaning system <b>1000</b>, assume the multimodal user input device <b>400</b> includes the gesture input portions <b>410</b> and speed input portion <b>420</b> as shown in <figref idref="DRAWINGS">FIG. 5</figref>. In particular, the gesture input portion <b>410</b> displays a graphical user interface that allows the user to direct either e-mail messages or pager messages to the various persons, departments, and/or organizations represented by the objects <b>412</b> displayed in the gesture input portion <b>410</b>. The multimodal user input device <b>400</b> also allows the user to input spoken commands to the speech input portion, or microphone, <b>420</b>. For simple illustration, further assume that the user has generated the two gestures <b>414</b> shown in <figref idref="DRAWINGS">FIG. 5</figref> and has spoken the utterance &#x201c;e-mail this person and that organization&#x201d; in association with generating the gestures <b>414</b> against the graphical user interface object <b>412</b> labeled &#x201c;Robert DeNiro&#x201d; and the graphical user interface object <b>412</b> labeled &#x201c;Monumental Pictures&#x201d;, respectively.</p>
<p id="p-0074" num="0073">The structure interpretation of multimodal commands of this kind can be captured declaratively in a multimodal context-free grammar. A multimodal context-free grammar can be defined formally as the quadruple MCFG as follows:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0074">MCFG=&#x3c;N, T, P, S&#x3e; where</li>
        <li id="ul0002-0002" num="0075">N is the set of non-terminals;</li>
        <li id="ul0002-0003" num="0076">P is the set of projections of the form:
        <ul id="ul0003" list-style="none">
            <li id="ul0003-0001" num="0077">A&#x2192;&#x3b1; where A&#x3b5;N and &#x3b1;&#x3b5;(N&#x222a;T)*;</li>
        </ul>
        </li>
        <li id="ul0002-0004" num="0078">S is the start symbol for the grammar;</li>
        <li id="ul0002-0005" num="0079">T is the set of terminals:</li>
        <li id="ul0002-0006" num="0080">((W&#x222a;&#x3b5;)&#xd7;(G&#x222a;&#x3b5;)&#xd7;(M&#x222a;&#x3b5;)<sup>+</sup>),</li>
        <li id="ul0002-0007" num="0081">where W is the vocabulary of the speech;</li>
        <li id="ul0002-0008" num="0082">G is the vocabulary of gesture:</li>
        <li id="ul0002-0009" num="0083">G=(GestureSymbols &#x222a; EventSymbols);</li>
        <li id="ul0002-0010" num="0084">GestureSymbols={Gp,Go,Gpf,Gpm . . . };</li>
        <li id="ul0002-0011" num="0085">Finite collections of EventSymbols={e<sub>1</sub>,e<sub>2 </sub>. . . }; and</li>
        <li id="ul0002-0012" num="0086">M is the vocabulary that represents meaning and includes EventSymbols&#x2282;M.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0075" num="0087">In general, a context-free grammar can be approximated by a finite-state automaton. The transition symbols of the finite-state automaton are the terminals of the context-free grammar. In the case of the multimodal context-free grammar defined above, these terminals contain three components, W, G and M. With respect to the discussion outlined above regarding temporal constraints, more specific temporal constraints than order can be encoded in the finite-state approach by writing symbols representing the passage of time onto the gesture tape and referring to such symbols in the multimodal grammar.</p>
<p id="p-0076" num="0088"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a fragment of such a multimodal context-free grammar that is capable of handling the gesture and spoken utterances illustrated in <figref idref="DRAWINGS">FIG. 5</figref>. <figref idref="DRAWINGS">FIG. 7</figref> illustrates a three-tape finite-state automaton corresponding to the multimodal context-free grammar fragment shown in <figref idref="DRAWINGS">FIG. 6</figref> that is capable of handling the gesture and spoken utterances illustrated in <figref idref="DRAWINGS">FIG. 5</figref>. The non-terminals in the multimodal context-free grammar shown in <figref idref="DRAWINGS">FIG. 6</figref> are atomic symbols. The multimodal aspects of the context-free grammar become apparent in the terminals. Each terminal contains three components &#x201c;W:G:M&#x201d;, corresponding to the n+1 tapes, where:
<ul id="ul0004" list-style="none">
    <li id="ul0004-0001" num="0000">
    <ul id="ul0005" list-style="none">
        <li id="ul0005-0001" num="0089">W represents the spoken language component;</li>
        <li id="ul0005-0002" num="0090">G represents the gesture component; and</li>
        <li id="ul0005-0003" num="0091">M represents the combined meaning of the spoken language and gesture components.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0077" num="0092">The &#x3b5; symbol is used to indicate when one of these components is empty in a given terminal. The symbols in the spoken language component W are words from the speech recognition lattice, i.e., the lattice of possible word sequences <b>175</b>. The symbols in the gesture component G include the gesture symbols discussed above, as well as the identifier variables e.</p>
<p id="p-0078" num="0093">In the exemplary embodiment of the gesture input portion <b>410</b> shown in <figref idref="DRAWINGS">FIG. 5</figref>, the gestures <b>414</b> are simple deictic circling gestures. The gesture meaning subsystem <b>250</b> assigns semantic types to each gesture <b>414</b> based on the underlining portion of the gesture input portion <b>410</b> against which the gestures <b>414</b> are made. In the exemplary embodiment shown in <figref idref="DRAWINGS">FIG. 5</figref>, the gestures <b>414</b> are made relative to the objects <b>412</b> that can represent people, organizations or departments to which an e-mail message or a pager message can be directed. If the gesture input portion <b>410</b> were instead a map, the gestures would be referenced against particular map coordinates, where the gesture indicates the action to be taken at particular map coordinates or the location of people or things at the indicated map location.</p>
<p id="p-0079" num="0094">Compared with a feature-based multimodal grammar, these semantic types constitute a set of atomic categories which make the relevant distinctions for gesture events to predict speech events and vice versa. For example, if the gesture is a deictic, i.e., pointing, gesture to an object in the gesture input portion <b>410</b> that represents a particular person, then spoken utterances like &#x201c;this person&#x201d;, &#x201c;him&#x201d;, &#x201c;her&#x201d;, and the like, are the preferred or predicted speech events and vice versa. These categories also play a role in constraining the semantic representation when the speech is underspecified with respect to the semantic type, such as, for example, spoken utterances like &#x201c;this one&#x201d;.</p>
<p id="p-0080" num="0095">In some exemplary embodiments, the gesture symbols G can be organized into a type hierarchy reflecting the ontology of the entities in the application domain. For example, in the exemplary embodiment of the gesture input portion <b>410</b> shown in <figref idref="DRAWINGS">FIG. 5</figref>, a pointing gesture may be assigned the general semantic type &#x201c;G&#x201d;. This general semantic gesture &#x201c;G&#x201d; may have various subtypes, such as &#x201c;Go&#x201d; and &#x201c;Gp&#x201d;, where &#x201c;Go&#x201d; represents a gesture made against an organization object, while the &#x201c;Gp&#x201d; gesture is made against a person object. Furthermore, the &#x201c;Gp&#x201d; type gesture may itself have subtypes, such as, for example, &#x201c;Gpm&#x201d; and &#x201c;Gpf&#x201d; for objects that respectively represent male and female persons.</p>
<p id="p-0081" num="0096">In the unification-based multimodal grammar disclosed in Johnston 1-3, spoken phrases and gestures are assigned typed feature structures by the natural language and gesture interpretation systems, respectively. In particular, each gesture feature structure includes a content portion that allows the specific location of the gesture on the gesture input portion of the multimodal user input device <b>400</b> to be specified.</p>
<p id="p-0082" num="0097">In contrast, when using finite-state automata, a unique identifier is needed for each object or location in the gesture input portion <b>410</b> that a user can gesture on. For example, in the exemplary embodiment shown in <figref idref="DRAWINGS">FIG. 5</figref>, the finite-state automata would need to include a unique identifier for each object <b>412</b>. In particular, as part of the composition process performed by the gesture recognition system <b>200</b>, as well as the various composition processes described below, these identifiers would need to be copied from the gesture feature lattice <b>220</b> into the semantic representation represented by the gesture recognition lattice <b>255</b> generated by the gesture meaning subsystem <b>250</b>.</p>
<p id="p-0083" num="0098">In the unification-based approach to multimodal integration disclosed in Johnston 1-3, this is achieved by feature sharing. In the finite-state approach used in the systems and methods according to this invention, one possible, but ultimately unworkable solution, would be to incorporate all of the different possible identifiers for all of the different possible elements of the gesture input device <b>410</b>, against which a gesture could be made, into the finite-state automata. For example, for an object having an identifier &#x201c;object identifier <b>345</b>&#x201d;, an arc in the lattices would need to be labeled with that identifier to transfer that piece of information from the gesture tape to the meaning tape of the finite-state automaton. Moreover, the arc for each different identifier would have to be repeated numerous times in the network wherever this transfer of information would be needed. Furthermore, the various arcs would have to be updated as the underlying objects within the gesture input portion <b>410</b> were updated or changed.</p>
<p id="p-0084" num="0099">In various exemplary embodiments, the systems and methods according to this invention overcome this problem by storing the specific identifiers of the elements within the gesture input portion <b>410</b> associated with incoming gestures into a finite set of variables labeled &#x201c;e<sub>1</sub>&#x201d;, &#x201c;e<sub>2</sub>&#x201d;, &#x201c;e<sub>3</sub>&#x201d;, . . . . Then, in place of the specific object identifier, the labels of the variables storing that specific object identifier are incorporated into the various finite-state automata. Thus, instead of having arcs labeled with specific values in the finite-state automata, the finite-state automata include arcs labeled with the labels of the variables.</p>
<p id="p-0085" num="0100">Therefore, instead of having the specific values for each possible object identifier in a finite-state automaton, that finite-state automaton instead incorporates the transitions &#x201c;&#x3b5;:e<sub>1</sub>:e<sub>1</sub>&#x201d;, &#x201c;&#x3b5;:e<sub>2</sub>:e<sub>2</sub>&#x201d;, &#x201c;&#x3b5;:e<sub>3</sub>:e<sub>3</sub>&#x201d;, . . . in each location in the finite-state automaton where specific content needs to be transferred from the gesture tape to the meaning tape. These transitions labeled with the variable labels are generated from the &#x201c;ENTRY&#x201d; productions in the multimodal context-free grammar shown in <figref idref="DRAWINGS">FIG. 6</figref>.</p>
<p id="p-0086" num="0101">In operation, the gesture recognition system <b>200</b> empties the variables e<sub>1</sub>, e<sub>2</sub>, e<sub>3 </sub>. . . after each multimodal command, so that all of the variables can be reused after each multimodal command. This allows the finite-state automaton to be built using a finite set of variables. However, this limits the number of distinct gesture events in a single utterance to no more than the available number of variables.</p>
<p id="p-0087" num="0102">Accordingly, assuming a user using the gesture input portion <b>410</b> shown in <figref idref="DRAWINGS">FIG. 5</figref> made the gestures <b>414</b> shown in <figref idref="DRAWINGS">FIG. 5</figref>, the gesture recognition system <b>200</b> would output, as the gesture recognition lattice <b>255</b>, the finite-state machine shown in <figref idref="DRAWINGS">FIG. 8</figref>. In this case, as shown in <figref idref="DRAWINGS">FIG. 8</figref>, the arc labeled &#x201c;Gp&#x201d; corresponds to the gesture made against a person object while the arc labeled &#x201c;Go&#x201d; represents a gesture made against an organization object. The are labeled &#x201c;e<sub>1</sub>&#x201d; stores the identifier of the person object <b>412</b>, in this case, the person object <b>412</b> labeled &#x201c;Robert DeNiro&#x201d;, against which the person-object gesture &#x201c;Gp&#x201d; <b>414</b> was made. Similarly, the arc labeled &#x201c;e<sub>2</sub>&#x201d; represents the variable storing the identifier of the organization object <b>412</b>, in this case &#x201c;Monumental Pictures&#x201d;, against which the organization gesture <b>414</b> was made.</p>
<p id="p-0088" num="0103">In the finite-state automata approach used in the systems and methods according to this invention, in addition to capturing the structure of language with the finite-state device, meaning is also captured. This is significant in multimodal language processing, because the central goal is to capture how the multiple modes contribute to the combined interpretation. In the finite-state automata technique used in the systems and methods according to this invention, symbols are written onto the third tape of the three-tape finite-state automaton, which, when concatenated together, yield the semantic representation for the multimodal utterance.</p>
<p id="p-0089" num="0104">In the following discussion, based on the exemplary utterance outlined above with respect to <figref idref="DRAWINGS">FIG. 5</figref>, a simple logical representation can be used. This simple representation includes predicates &#x201c;pred ( . . . )&#x201d; and lists &#x201c;[a,b, . . . ]&#x201d;. However, it should be appreciated that many other kinds of semantic representations could be generated, such as code in a programming or scripting language that could be executed directly.</p>
<p id="p-0090" num="0105">In the simple logical representation outlined above, referring to the exemplary multimodal utterance outlined above with respect to <figref idref="DRAWINGS">FIG. 5</figref>, the recognized word &#x201c;e-mail&#x201d; causes the predicate &#x201c;e-mail ([&#x201d; to be added to the semantics tape. Similarly, the recognized word &#x201c;person&#x201d; causes the predicate &#x201c;person (&#x201d; to be written on the semantics tape. The e-mail predicate and the list internal to the e-mail predicate are closed when the rule &#x201c;S&#x2192;V MP &#x3b5;:&#x3b5;:])&#x201d;, as shown in <figref idref="DRAWINGS">FIG. 6</figref>, applies.</p>
<p id="p-0091" num="0106">Returning to the exemplary utterance &#x201c;e-mail this person and that organization&#x201d; and the associated gestures outlined above with respect to <figref idref="DRAWINGS">FIG. 5</figref>, assume that the objects against which the gestures <b>414</b> have been made have the identifiers &#x201c;objid367&#x201d; and &#x201c;objid893&#x201d;. Then, the elements on the meaning tape of the three-tape finite-state automaton are concatenated and the variable references are replaced to yield the meaning &#x201c;e-mail([person(objid367), organization (objid893)])&#x201d;.</p>
<p id="p-0092" num="0107">As more recursive semantic phenomena, such as possessives and other complex noun phrases, are added to the grammar, the resulting finite-state automata become ever larger. The computational consequences of this can be lessened by the lazy evaluation techniques in Mohri.</p>
<p id="p-0093" num="0108">While a three-tape finite-state automaton is feasible in principle, currently available tools for finite-state language processing generally only support two-tape finite-state automata, i.e., finite-state transducers. Furthermore, speech recognizers typically do not support the use of a three-tape finite-state automaton as a language model. Accordingly, the multimodal recognition and/or meaning system <b>1000</b> implements this three-tape finite-state automaton approach by using a series of finite-state transducers in place of the single three-tape finite-state automaton shown in <figref idref="DRAWINGS">FIG. 7</figref>, as described below. In particular, the three-tape finite-state automaton shown in <figref idref="DRAWINGS">FIG. 7</figref> and illustrated by the grammar fragment shown in <figref idref="DRAWINGS">FIG. 6</figref> can be decomposed into an input component relating the gesture symbols G and the word symbols W and an output component that relates the input component to the meaning symbols M.</p>
<p id="p-0094" num="0109">As indicated above, <figref idref="DRAWINGS">FIG. 7</figref> shows a three-tape finite-state automaton that corresponds to the grammar fragment shown in <figref idref="DRAWINGS">FIG. 6</figref> and that is usable to recognize the meaning of the various spoken and gestural inputs that can be generated using the graphical user interface displayed in the gesture input portion <b>410</b> of the multimodal user input device <b>400</b> shown in <figref idref="DRAWINGS">FIG. 5</figref>. The three-tape finite-state automaton shown in <figref idref="DRAWINGS">FIG. 7</figref> is decomposed into the gesture-to-speech finite-state transducer shown in <figref idref="DRAWINGS">FIG. 9</figref> and the speech/gesture/meaning finite-state transducer shown in <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0095" num="0110">The gesture-to-speech finite-state transducer shown in <figref idref="DRAWINGS">FIG. 9</figref> maps the gesture symbols G to the word symbols W that are expected to coincide with each other. Thus, in the exemplary embodiment of the multimodal user input device <b>400</b> shown in <figref idref="DRAWINGS">FIG. 4</figref>, the verbal pointers &#x201c;that&#x201d; and &#x201c;this&#x201d; are expected to be accompanied by the deictic gestures <b>414</b> made against either a department object, an organization object or a person object <b>412</b>, where the object identifier for the object <b>412</b> against which the deictic gesture <b>414</b> was made is stored in one of the variables e<sub>1</sub>, e<sub>2</sub>, or e<sub>3</sub>. The gesture-to-speech transducer shown in <figref idref="DRAWINGS">FIG. 9</figref> captures the constraints that the gestures made by the user through the gesture input portion <b>410</b> of the multimodal user input device <b>400</b> place on the speech utterance that accompanies those gestures. Accordingly, a projection of the output tape of the gesture-to-speech finite-state transducer shown in <figref idref="DRAWINGS">FIG. 9</figref> can be used, in conjunction with the recognized gesture string, such as the recognized gesture string shown in <figref idref="DRAWINGS">FIG. 8</figref> that represents the gestures illustrated in the exemplary embodiment of the multimodal user input device <b>400</b> shown in <figref idref="DRAWINGS">FIG. 5</figref>, as a language model usable to constrain the possible sequences of words to be recognized by the utterance recognition subsystem <b>170</b> of the automatic speech recognition system <b>100</b>.</p>
<p id="p-0096" num="0111">It should be appreciated that, in those exemplary embodiments that do not also extract meaning, the further processing outlined below with respect to <figref idref="DRAWINGS">FIGS. 10-17</figref> can be omitted. Similarly, in those exemplary embodiments that do not use one or more of the multimodal inputs to provide compensation to one or more of the other multimodal inputs, the processing outlined above with respect to <figref idref="DRAWINGS">FIGS. 7-9</figref> can be omitted.</p>
<p id="p-0097" num="0112">The speech/gesture/meaning finite-state transducer shown in <figref idref="DRAWINGS">FIG. 10</figref> uses the cross-product of the gesture symbols G and the word symbols Was an input component or first tape. Thus, the gesture-to-speech finite-state transducer shown in <figref idref="DRAWINGS">FIG. 9</figref> implements the function <img id="CUSTOM-CHARACTER-00001" he="3.13mm" wi="2.46mm" file="US08626507-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/>: G&#x2192;W. The output or second tape of the speech/gesture/meaning finite-state transducer shown in <figref idref="DRAWINGS">FIG. 10</figref> contains the meaning symbols M that capture the semantic representation of the multimodal utterance, as shown in <figref idref="DRAWINGS">FIG. 7</figref> and outlined above. Thus, the speech/gesture/meaning finite-state transducer shown in <figref idref="DRAWINGS">FIG. 10</figref> implements the function &#x2111;: (G&#xd7;W)&#x2192;M. That is, the speech/gesture/meaning finite-state transducer shown in <figref idref="DRAWINGS">FIG. 10</figref> is a finite-state transducer in which gesture symbols and words are on the input tape and the meaning is on the output tape.</p>
<p id="p-0098" num="0113">Thus, the gesture-to-speech finite-state transducer and the speech/gesture/meaning finite-state transducers shown in <figref idref="DRAWINGS">FIGS. 9 and 10</figref> are used with the speech recognition system <b>100</b> and the multimodal parser/meaning recognition system <b>300</b> to recognize, parse, and/or extract the meaning from the multimodal inputs received from the gesture and speech input portions <b>410</b> and <b>420</b> of the multimodal user input device <b>400</b>.</p>
<p id="p-0099" num="0114">It should be appreciated that there are any variety of ways in which the multimodal finite-state transducers can be integrated with the automatic speech recognition system <b>100</b>, the gesture recognition system <b>200</b> and the multimodal parser/meaning recognition system <b>300</b>. Clearly, for any particular recognition task, the more appropriate approach will depend on the properties of the particular multimodal user input interface <b>400</b> through which the multimodal inputs are generated and/or received.</p>
<p id="p-0100" num="0115">The approach outlined in the following description of <figref idref="DRAWINGS">FIGS. 8-17</figref> involves recognizing the gesture string first. The recognized gesture string is then used to modify the language model used by the automatic speech recognition system <b>100</b>. In general, this will be appropriate when there is limited ambiguity in the recognized gesture string. For example, there will be limited ambiguity in the recognized gesture string when the majority of gestures are unambiguous deictic pointing gestures. Obviously, if more complex gestures are used, such as the multi-element gestures described in Johnston 1-3, other ways of combining the gesture and speech recognition systems may be more appropriate.</p>
<p id="p-0101" num="0116">Accordingly, for the specific exemplary embodiment of the multimodal user input device <b>400</b> shown in <figref idref="DRAWINGS">FIG. 5</figref>, the gesture recognition system <b>200</b> first processes the incoming gestures to construct a gesture finite-state machine, such as that shown in <figref idref="DRAWINGS">FIG. 8</figref>, corresponding to the range of gesture interpretations. In the exemplary embodiments described above with respect to <figref idref="DRAWINGS">FIGS. 5</figref>, <b>6</b> and <b>7</b>, the gesture input is unambiguous. Thus, as shown in <figref idref="DRAWINGS">FIG. 8</figref>, a simple linearly-connected set of states forms the gesture finite-state machine shown in <figref idref="DRAWINGS">FIG. 8</figref>. It should be appreciated that, if the received gestures involved more complex gesture recognition or were otherwise ambiguous, the recognized string of gestures would be represented as a lattice indicating all of the possible gesture recognitions and interpretations for the received gesture stream. Moreover, a weighted finite-state transducer could be used to incorporate the likelihoods of the various paths in such a lattice.</p>
<p id="p-0102" num="0117"><figref idref="DRAWINGS">FIG. 11</figref> is a flowchart outlining one exemplary embodiment of a method for combining and converting the various multimodal input streams into a combined finite-state transducer representing the semantic meaning of the combined multimodal input streams. Beginning in step <b>500</b>, control continues to step <b>510</b>, where gesture and speech utterances are input through one or more input devices that together combine to form a multimodal user input device. Then, in step <b>520</b>, a gesture lattice or finite-state machine is generated from the input gesture utterance.</p>
<p id="p-0103" num="0118">Next, in step <b>530</b>, the gesture lattice is composed with the gesture-to-speech transducer to generate a gesture/language finite-state transducer. For example, in the exemplary embodiment described above, the gesture finite-state machine shown in <figref idref="DRAWINGS">FIG. 8</figref> is composed with the gesture-to-speech finite-state transducer shown in <figref idref="DRAWINGS">FIG. 9</figref> to form the gesture/language finite-state transducer shown in <figref idref="DRAWINGS">FIG. 12</figref>. The gesture/language finite-state transducer represents the relationship between the recognized stream of gestures and all of the possible word sequences that could occur with those gestures of the recognized stream of gestures.</p>
<p id="p-0104" num="0119">Then, in step <b>540</b>, in order to use this information to guide the speech recognition system <b>100</b>, a projection of the gesture/language finite-state transducer is generated. In particular, a projection on the output tape or speech portion of the gesture/language finite-state transducer shown in <figref idref="DRAWINGS">FIG. 12</figref> is taken to yield a finite-state machine. In particular, in the exemplary embodiment outlined above, a projection of the gesture/language finite-state transducer shown in <figref idref="DRAWINGS">FIG. 12</figref> is illustrated in <figref idref="DRAWINGS">FIG. 13</figref>.</p>
<p id="p-0105" num="0120">Next, in step <b>550</b>, the speech utterance is recognized using the projection of the gesture/language finite-state transducer as the language model. Using the projection of the gesture/language finite-state transducer as the language model enables the gestural information to directly influence the recognition process performed by the automatic speech recognition system <b>100</b>. In particular, as shown in step <b>560</b>, the automatic speech recognition system generates a word sequence lattice based on the projection of the gesture/language finite-state transducer in view of the word lattice <b>155</b>. In the exemplary embodiment outlined above, using the projection of the gesture/language finite-state transducer shown in <figref idref="DRAWINGS">FIG. 13</figref> as the language model for the speech recognition process results in the recognized word sequence lattice &#x201c;e-mail this person and that organization&#x201d;, as shown in <figref idref="DRAWINGS">FIG. 14</figref>.</p>
<p id="p-0106" num="0121">Then, in step <b>570</b>, the gesture/language finite-state transducer is composed with the recognized word sequences lattice to generate a gesture/speech finite-state transducer. This reintegrates the gesture information that was removed when the projection of the gesture/language finite-state transducer was generated in step <b>540</b>. The generated gesture/speech finite-state transducer contains the information both from the speech utterance and the gesture utterance received from the various portions of the multimodal user input device <b>400</b>. For the example outlined above, composing the gesture/language finite-state transducer shown in <figref idref="DRAWINGS">FIG. 12</figref> with the word sequences lattice shown in <figref idref="DRAWINGS">FIG. 14</figref> generates the gesture/speech finite-state transducer shown in <figref idref="DRAWINGS">FIG. 15</figref>.</p>
<p id="p-0107" num="0122">Then, in step <b>580</b>, the gesture/speech finite-state transducer is converted to a gesture/speech finite-state machine. In particular, the gesture/speech finite-state machine combines the input and output tapes of the gesture/speech finite-state transducer onto a single tape. In the exemplary embodiment outlined above, converting the gesture/speech finite-state transducer shown in <figref idref="DRAWINGS">FIG. 15</figref> results in the gesture/speech finite-state machine shown in <figref idref="DRAWINGS">FIG. 16</figref>.</p>
<p id="p-0108" num="0123">Next, in step <b>590</b>, the gesture/speech finite-state machine is composed with the speech/gesture/meaning finite-state transducer shown in <figref idref="DRAWINGS">FIG. 10</figref> to generate the meaning finite-state transducer shown in <figref idref="DRAWINGS">FIG. 17</figref>. Because the speech/gesture/meaning finite-state transducer relates the speech and gesture symbols to meaning, composing the gesture/speech finite-state machine results in the meaning finite-state transducer which captures the combined semantic meaning or representation contained in the independent modes input using the multimodal user input device. Thus, the meaning of the multimodal input received from the multimodal user input device can be read from the output tape of the meaning finite-state transducer. In the exemplary embodiment outlined above, composing the gesture/speech finite-state machine shown in <figref idref="DRAWINGS">FIG. 16</figref> with the speech/gesture/meaning finite-state transducer shown in <figref idref="DRAWINGS">FIG. 10</figref> results in the meaning finite-state transducer shown in <figref idref="DRAWINGS">FIG. 17</figref>. In particular, it should be appreciated that the meaning finite-state transducer shown in <figref idref="DRAWINGS">FIG. 17</figref> is a linear finite-state transducer that unambiguously yields the meaning &#x201c;e-mail ([person (e<sub>1</sub>), org (e<sub>2</sub>)])&#x201d;.</p>
<p id="p-0109" num="0124">It should be appreciated that, in embodiments that use much more complex multimodal interfaces, such as those illustrated in Johnston 1-3, the meaning finite-state transducer may very well be a weighted finite-state transducer having multiple paths between the start and end nodes representing the various possible meanings for the multimodal input and the probability corresponding to each path. In this case, in step <b>595</b>, the most likely meaning would be selected from the meaning finite-state transducer based on the path through the meaning finite-state transducer having the highest probability. However, it should be appreciated that step <b>595</b> is optional and can be omitted. Then, in step <b>600</b>, the process ends.</p>
<p id="p-0110" num="0125">As outlined above, the various exemplary embodiments described herein allow spoken language and gesture input streams to be parsed and integrated by a single weighted finite-state device. This single weighted finite-state device provides language models for speech and gesture recognition and composes the meaning content from the speech and gesture input streams into a single semantic representation. Thus, the various systems and methods according to this invention not only address multimodal language recognition, but also encode the semantics as well as the syntax into a single weighted finite-state device. Compared to the previous approaches for integrating multimodal input streams, such as those described in Johnston 1-3, which compose elements from n-best lists of recognition results, the systems and methods according to this invention provide the potential for mutual compensation among the various multimodal input modes.</p>
<p id="p-0111" num="0126">The systems and methods according to this invention allow the gestural input to dynamically alter the language model used for speech recognition. Additionally, the systems and methods according to this invention reduce the computational complexity of multi-dimensional multimodal parsing. In particular, the weighted finite-state devices used in the systems and methods according to this invention provide a well-understood probabilistic framework for combining the probability distributions associated with the speech and gesture input streams and for selecting among multiple competing multimodal interpretations.</p>
<p id="p-0112" num="0127">It should be appreciated that the multimodal recognition and/or meaning system <b>1000</b> shown in <figref idref="DRAWINGS">FIG. 2</figref>, and/or each of the gesture recognition system <b>200</b>, the multimodal parser/meaning recognition system <b>300</b> and/or the automatic speech recognition system <b>100</b> can each be implemented on a programmed general purpose computer. However, any or all of these systems can also be implemented on a special purpose computer, a programmed microprocessor or microcontroller and peripheral integrated circuit elements, an ASIC or other integrated circuit, a digital signal processor, a hardwired electronic or a logic circuit such as a discrete element circuit, a programmable logic device such as a PLD, a PLA, a FPGA or a PAL, or the like. In general, any device capable of implementing a finite-state machine that is in turn capable of implementing the flowchart shown in <figref idref="DRAWINGS">FIG. 10</figref> and/or the various finite-state machines and transducers shown in <figref idref="DRAWINGS">FIGS. 7-9</figref> and <b>11</b>-<b>17</b> can be used to implement one or more of the various systems shown in <figref idref="DRAWINGS">FIGS. 1-4</figref>.</p>
<p id="p-0113" num="0128">Thus, it should be understood that each of the various systems and subsystems shown in <figref idref="DRAWINGS">FIGS. 1-4</figref> can be implemented as portions of a suitably programmed general purpose computer. Alternatively, each of the systems or subsystems shown in <figref idref="DRAWINGS">FIGS. 1-4</figref> can be implemented as physically distinct hardware circuits within an ASIC, or using a FPGA, a PLD, a PLA, or a PAL, or using discrete logic elements or discrete circuit elements. The particular form each of the systems and/or subsystems shown in <figref idref="DRAWINGS">FIGS. 1-4</figref> will take is a design choice and will be obvious and predictable to those skilled in the art.</p>
<p id="p-0114" num="0129">It should also be appreciated that, while the above-outlined description of the various systems and methods according to this invention and the figures focus on speech and gesture as the multimodal inputs, any known or later-developed set of two or more input streams representing different modes of information or communication, such as speech, electronic-ink-based gestures or haptic modes, keyboard input, inputs generated by observing or sensing human body motions, including hand motions, gaze motions, facial expressions, or other human body motions, or any other known or later-developed method for communicating information, can be combined and used as one of the input streams in the multimodal utterance.</p>
<p id="p-0115" num="0130">Thus, while this invention has been described in conjunction with the exemplary embodiments outlined above, it is evident that many alternatives, modifications and variations will be apparent to those skilled in the art. Accordingly, the exemplary embodiments of these systems and methods according to this invention, as set forth above, are intended to be illustrative, not limiting. Various changes may be made without departing from the spirit and scope of this invention.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for recognizing an utterance comprising:
<claim-text>receiving an utterance comprising a first portion having a first mode and a second portion having a second mode;</claim-text>
<claim-text>generating a first mode recognition lattice and a first finite-state transducer, based on the first mode;</claim-text>
<claim-text>relating the first portion of the utterance to the second portion of the utterance based on the first finite-state transducer;</claim-text>
<claim-text>generating a second finite-state transducer, comprising a gesture and speech recognition model finite-state transducer, based on the first mode recognition lattice and the first finite-state transducer; and</claim-text>
<claim-text>outputting a recognition result based on the utterance and the second finite-state transducer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>recognizing the second mode based on the second finite-state transducer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the first mode is a gesture mode.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the second mode is a speech mode.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the generating the first mode recognition lattice is further based on a first mode feature lattice.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A method for recognizing an utterance comprising:
<claim-text>receiving an utterance comprising a plurality of modes;</claim-text>
<claim-text>relating a first portion of the utterance comprising a first mode of the plurality of modes to a second portion of the utterance comprising a second mode of the plurality of modes, based on a first finite-state transducer;</claim-text>
<claim-text>generating a second finite-state transducer, comprising a gesture and speech recognition model finite-state transducer, based on a first mode recognition lattice and the first finite-state transducer; and</claim-text>
<claim-text>outputting a recognition result based on the utterance and the second finite-state transducer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref> further comprising:
<claim-text>generating the first mode recognition lattice and the first finite-state transducer, based on the first mode.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref> further comprising:
<claim-text>recognizing the second mode based on the second finite-state transducer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein the first mode is a gesture mode.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein the second mode is a speech mode.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein the generating the first mode recognition lattice is further based on a first mode feature lattice.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref> further comprising:
<claim-text>generating the first mode feature lattice based on the utterance.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A method for extracting meaning from multimodal inputs comprising:
<claim-text>receiving a multimodal input;</claim-text>
<claim-text>relating a first portion of the multimodal input comprising a first mode to a second portion of the multimodal input comprising a second mode, based on a first finite-state transducer;</claim-text>
<claim-text>generating a second finite-state transducer, comprising a gesture and speech recognition model finite-state transducer, based on a first mode recognition lattice and the first finite-state transducer; and</claim-text>
<claim-text>outputting a recognition result based on the multimodal input and the second finite-state transducer.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref> further comprising:
<claim-text>generating the first mode recognition lattice and the first finite-state transducer, based on the first mode.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the first mode is a gesture mode.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the second mode is a speech mode.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the generating the first mode recognition lattice is further based on a first mode feature lattice.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref> further comprising:
<claim-text>generating the first mode feature lattice based on the multimodal input. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
