<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625902-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625902</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13193294</doc-number>
<date>20110728</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>235</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>66</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>60</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382190</main-classification>
<further-classification>382305</further-classification>
</classification-national>
<invention-title id="d2e53">Object recognition using incremental feature extraction</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5850490</doc-number>
<kind>A</kind>
<name>Johnson</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382306</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5852823</doc-number>
<kind>A</kind>
<name>De Bonet</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382279</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6236768</doc-number>
<kind>B1</kind>
<name>Rhodes et al.</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382306</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6304197</doc-number>
<kind>B1</kind>
<name>Freking et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6711293</doc-number>
<kind>B1</kind>
<name>Lowe</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7787711</doc-number>
<kind>B2</kind>
<name>Agam et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382305</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>8396331</doc-number>
<kind>B2</kind>
<name>Jia et al.</name>
<date>20130300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382305</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2007/0092143</doc-number>
<kind>A1</kind>
<name>Higgins</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382228</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2008/0013836</doc-number>
<kind>A1</kind>
<name>Nakamura et al.</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2008/0111721</doc-number>
<kind>A1</kind>
<name>Reznik</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2008/0192998</doc-number>
<kind>A1</kind>
<name>Takeguchi et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2008/0205770</doc-number>
<kind>A1</kind>
<name>Jia et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382217</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2010/0303354</doc-number>
<kind>A1</kind>
<name>Reznik</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2010/0310174</doc-number>
<kind>A1</kind>
<name>Reznik</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2011/0314059</doc-number>
<kind>A1</kind>
<name>Hu</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707771</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2012/0263388</doc-number>
<kind>A1</kind>
<name>Vaddadi et al.</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382225</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>WO</country>
<doc-number>2008087466</doc-number>
<kind>A1</kind>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>WO</country>
<doc-number>2010141474</doc-number>
<kind>A1</kind>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>WO</country>
<doc-number>2010141926</doc-number>
<kind>A1</kind>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Lowe, D.G., &#x201c;Distinctive image features from scale invariant keypoints,&#x201d; IJCV 60(2), 91-110 (2004).</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Bhatia, S.: Hierarchical clustering for image databases. In: Intl. Conference on Electro Information Technology, pp. 6-12 (2005).</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Shyu et al, A unified framework for image database clustering and content-based retrieval, in Proc. of the Second ACM International Workshop on Multimedia Databases (ACM MMDB), 2004.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Krishnamachari et al, Santhana Krishnamachari and Mohamed Abdel-Mottaleb &#x201c;Hierarchical clustering algorithm for fast image retrieval&#x201d;, Proc. SPIE 3656, Storage and Retrieval for Image and Video Databases VII, 427 (Dec. 17, 1998).</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Van Gemert et al., &#x201c;Kernel Codebooks for Scene Categorization,&#x201d; Oct. 12, 2008, ECCV 2008, pp. 696-709.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>Nowak et al., &#x201c;Sampling Strategies for Bag-of-Features Image Classification,&#x201d; Jan. 1, 2006, ECCV 2006, pp. 490-503.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Zhu et al., &#x201c;Keyblock: An approach for content-based image retrieval,&#x201d; Proceedings of the eighth ACM international conference on Multimedia 2000, pp. 157-166.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00027">
<othercit>Chen et al, &#x201c;Tree Histogram Coding for Mobile Image Matching,&#x201d; Data Compression Conference, Mar. 16, 2009, pp. 143-152.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00028">
<othercit>Chandrasekhar et al., &#x201c;CHoG: Compressed Histogram of Gradients: A Low Bit-Rate Feature Descriptor,&#x201d; CVPR 2009 papers on the web May 4, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00029">
<othercit>Yeo et al., &#x201c;Rate-efficient visual correspondences using random projections,&#x201d; Image Processing ICIP 2008, Oct. 12, 2008, pp. 217-220.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00030">
<othercit>Sakaguchi et al., &#x201c;A context tree weighting algorithm with a finite window,&#x201d; Electronics and Communications in Japan (Part III: Fundamental Electronic Science), vol. 83, No. 1, 2000, pp. 21-30.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00031">
<othercit>Makar et al., &#x201c;Compression of image patches for local feature extraction,&#x201d; ICASSP 2009, Apr. 19, 2009, pp. 821-824.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Varshney et al., &#x201c;Benefiting from disorder: Source coding for Unordered Data,&#x201d; Feb. 1, 2008, http://arxiv.org/PS<sub>&#x2014;</sub>cache/arxiv/pdf/0708/0708.2310vl.pdf.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00033">
<othercit>Willems et al., &#x201c;Complexity Reduction of Context-Tree Weighting Algorithm: A Study for KPN Research,&#x201d; Oct. 17, 1995, www.ele.tue.n1/ctw/download/eidma.pdf.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00034">
<othercit>International Search Report and Written Opinion of international application No. PCT/US2011/045942, dated Apr. 26, 2012, 12 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00035">
<othercit>Lee, S. et al., &#x201c;Familiarity based unified visual attention model for fast and robust object recognition,&#x201d; Pattern Recognition, Elsevier, Jul. 2009, 13 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00036">
<othercit>Fritz et al., &#x201c;A Mobile System for Urban Detection with Informative Local Descriptors,&#x201d; Proceedings of the Fourth IEEE International Conference on Computer Vision Systems, 2006, 9 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00037">
<othercit>Wagner et al., &#x201c;Real-Time Detection and Tracking for Augmented Reality on Mobile Phones,&#x201d; IEEE Transactions on Visualization and Computer Graphics, vol. 16, No. 3, May/Jun. 2010, 14 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00038">
<othercit>Hoffmann et al., &#x201c;Identification of Paintings in Camera-Phone Images,&#x201d; Identification of Paintings in Camera-Phone Images, 2007, http://www.stanford.edu/class/ee368/Project<sub>&#x2014;</sub>07/reports/ee368group04.pdf, 6 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>Vaddadi et al., &#x201c;Keypoint Clustering for Robust Image Matching,&#x201d; Proc. SPIE vol. 7798, Applications of Digital Image Processing, San Diego, CA, Aug. 2010, 12 pp.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00040">
<othercit>U.S. Appl. No. 13/158,013, by Yuriy Reznik, filed Jun. 10, 2011.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>50</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382162</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382165</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382190</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382195</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382225</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382275</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382279</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382305</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382306</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707  6</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707104</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707736</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707771</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707E17014</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>6</number-of-drawing-sheets>
<number-of-figures>6</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61369228</doc-number>
<date>20100730</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120027290</doc-number>
<kind>A1</kind>
<date>20120202</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Baheti</last-name>
<first-name>Pawan Kumar</first-name>
<address>
<city>Bangalore</city>
<country>IN</country>
</address>
</addressbook>
<residence>
<country>IN</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Vaddadi</last-name>
<first-name>Sundeep</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Swaminathan</last-name>
<first-name>Ashwin</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Reznik</last-name>
<first-name>Yuriy</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hamsici</last-name>
<first-name>Onur C.</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="006" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chari</last-name>
<first-name>Murali Ramaswamy</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="007" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hong</last-name>
<first-name>John H.</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="008" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Chong Uk</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Baheti</last-name>
<first-name>Pawan Kumar</first-name>
<address>
<city>Bangalore</city>
<country>IN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Vaddadi</last-name>
<first-name>Sundeep</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Swaminathan</last-name>
<first-name>Ashwin</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Reznik</last-name>
<first-name>Yuriy</first-name>
<address>
<city>Seattle</city>
<state>WA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Hamsici</last-name>
<first-name>Onur C.</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="006" designation="us-only">
<addressbook>
<last-name>Chari</last-name>
<first-name>Murali Ramaswamy</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="007" designation="us-only">
<addressbook>
<last-name>Hong</last-name>
<first-name>John H.</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="008" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Chong Uk</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Shumaker &#x26; Sieffert, P.A.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>QUALCOMM Incorporated</orgname>
<role>02</role>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Allison</last-name>
<first-name>Andrae S</first-name>
<department>2668</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">In one example, an apparatus includes a processor configured to extract a first set of one or more keypoints from a first set of blurred images of a first octave of a received image, calculate a first set of one or more descriptors for the first set of keypoints, receive a confidence value for a result produced by querying a feature descriptor database with the first set of descriptors, wherein the result comprises information describing an identity of an object in the received image, and extract a second set of one or more keypoints from a second set of blurred images of a second octave of the received image when the confidence value does not exceed a confidence threshold. In this manner, the processor may perform incremental feature descriptor extraction, which may improve computational efficiency of object recognition in digital images.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="229.62mm" wi="181.61mm" file="US08625902-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="241.55mm" wi="182.03mm" orientation="landscape" file="US08625902-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="254.42mm" wi="186.52mm" file="US08625902-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="218.19mm" wi="164.68mm" file="US08625902-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="257.64mm" wi="186.52mm" file="US08625902-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="232.16mm" wi="178.31mm" file="US08625902-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="236.81mm" wi="174.84mm" file="US08625902-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CLAIM OF PRIORITY UNDER 35 U.S.C. &#xa7;119</heading>
<p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Application No. 61/369,228, filed Jul. 30, 2010, which is incorporated by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">This disclosure relates to image processing systems and, more particularly, object recognition in images.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Visual search in the context of computing devices or computers refers to techniques that enable a computer or other device to provide identifying information for objects in one or more images. Recent interest in visual search has resulted in algorithms that enable computers to identify partially occluded objects and/or features in a wide variety of changing image conditions, including changes in image scale, noise, illumination, and local geometric distortion. During this same time, mobile devices that include cameras have emerged, but such mobile devices may have limited user interfaces for entering text or otherwise interfacing with the mobile devices. Developers of mobile devices and mobile device applications have sought to utilize the camera of the mobile device to enhance user interactions with the mobile device.</p>
<p id="p-0005" num="0004">To illustrate one enhancement, a user of a mobile device may use a camera of the mobile device to capture an image of any given product while shopping at a store. The mobile device may then initiate a visual search algorithm within a set of archived feature descriptors for various images to identify the product based on matching imagery. After identifying the product, the mobile device may then initiate an Internet-based search and present a webpage that contains information about the identified product, which may include, for example, a lowest cost for which the product is available from nearby merchants and/or online merchants. In another example, object recognition may be used to generate supplemental information, which can be overlayed in the display of the mobile device to achieve so-called augmented reality.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0006" num="0005">In general, this disclosure describes techniques for object identification in digital images. These techniques may reduce the computational complexity of identifying objects in an image. Rather than extracting feature descriptors from a predetermined number of octaves, the techniques of this disclosure provide for incremental feature descriptor extraction, progressing through the octaves. Moreover, this disclosure provides techniques for selecting a starting octave and techniques for selecting subsequent octaves from which to extract feature descriptors.</p>
<p id="p-0007" num="0006">In one example, a method includes extracting a first set of one or more keypoints from a first set of blurred images of a first octave of a received image, calculating a first set of one or more descriptors for the first set of keypoints, receiving a confidence value for a result produced by querying a feature descriptor database with the first set of descriptors, wherein the result comprises information describing an identity of an object in the received image, and extracting a second set of one or more keypoints from a second set of blurred images of a second octave of the received image when the confidence value does not exceed a confidence threshold.</p>
<p id="p-0008" num="0007">In another example, an apparatus includes a processor configured to extract a first set of one or more keypoints from a first set of blurred images of a first octave of a received image, calculate a first set of one or more descriptors for the first set of keypoints, receive a confidence value for a result produced by querying a feature descriptor database with the first set of descriptors, wherein the result comprises information describing an identity of an object in the received image, and extract a second set of one or more keypoints from a second set of blurred images of a second octave of the received image when the confidence value does not exceed a confidence threshold.</p>
<p id="p-0009" num="0008">In another example, an apparatus includes means for extracting a first set of one or more keypoints from a first set of blurred images of a first octave of a received image, means for calculating a first set of one or more descriptors for the first set of keypoints, means for receiving a confidence value for a result produced by querying a feature descriptor database with the first set of descriptors, wherein the result comprises information describing an identity of an object in the received image, and means for extracting a second set of one or more keypoints from a second set of blurred images of a second octave of the received image when the confidence value does not exceed a confidence threshold.</p>
<p id="p-0010" num="0009">In another example, a computer program product includes a computer-readable medium having stored thereon instructions that, when executed, cause a processor to extract a first set of one or more keypoints from a first set of blurred images of a first octave of a received image, calculate a first set of one or more descriptors for the first set of keypoints, receive a confidence value for a result produced by querying a feature descriptor database with the first set of descriptors, wherein the result comprises information describing an identity of an object in the received image, and extract a second set of one or more keypoints from a second set of blurred images of a second octave of the received image when the confidence value does not exceed a confidence threshold.</p>
<p id="p-0011" num="0010">The details of one or more examples are set forth in the accompanying drawings and the description below. Other features, objects, and advantages will be apparent from the description and drawings, and from the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an example system in which techniques for incremental feature descriptor extraction may be applied.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2</figref> is a conceptual diagram illustrating a difference of Gaussian (DoG) pyramid that has been determined for use in keypoint extraction.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> is a conceptual diagram illustrating detection of a keypoint in more detail.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> is a conceptual diagram illustrating example techniques by which a feature extraction unit may obtain gradient distributions and orientation histograms.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5</figref> is a line drawing of a chart illustrating an example keypoint distribution across various Gaussian scale spaces.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart illustrating an example method for performing incremental feature descriptor extraction.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0018" num="0017">In general, this disclosure describes techniques for object identification in digital images. These techniques may reduce the computational complexity of identifying objects in an image. Rather than extracting feature descriptors from a predetermined number of octaves, the techniques of this disclosure provide for incremental feature descriptor extraction, progressing through the octaves. Feature descriptors may generally be extracted from one or more octaves, where each octave corresponds to a particular size (e.g., pixel resolution) of the image. For a given octave, the input image may be scaled to a size associated with the resolution. A plurality of increasingly more Gaussian-blurred images may then be formed, each having the resolution associated with the octave. These increasingly more blurry images for the octave may be analyzed to extract feature descriptors.</p>
<p id="p-0019" num="0018">Rather than extracting feature descriptors for a predetermined number of octaves, the techniques of this disclosure are directed to extracting feature descriptors for a first octave, then attempting to detect an object in the image based on this first set of feature descriptors. If the object can be detected using these feature descriptors, then no further octaves need to be analyzed. However, if the object cannot be determined from the first octave, feature descriptors may be extracted from a subsequent octave.</p>
<p id="p-0020" num="0019">Moreover, this disclosure provides techniques for selecting a starting octave and techniques for selecting subsequent octaves from which to extract feature descriptors. Various factors may be used to select the starting octave. For example, a scaling factor for the image may first be estimated based on, for example, whether the image was captured indoors or outdoors, and/or a depth of objects in the image relative to the camera used to capture the image (that is, the distance from the camera to the objects in the image).</p>
<p id="p-0021" num="0020">A device including the camera, e.g., a mobile device such as a smartphone, tablet computing device, laptop computer, or other mobile device, may also include sensors that can be used to estimate whether the device was indoors or outdoors when the image was captured. For example, a global positioning system (GPS) unit may provide information indicating the location of the camera when the image was captured, which may be used to determine whether the camera was indoors or outdoors. As another example, if the device included a wireless network interface that was connected to a wireless access point when the image was captured, this may be used as a factor in favor of estimating that the device was indoors when the camera was captured. As still another example, depth information for objects in the image may be determined, e.g., when the device has a stereo camera arrangement.</p>
<p id="p-0022" num="0021">In some examples, the device may include depth estimation units configured to estimate depths for objects in the image, in addition to or in the alternative to the depth estimation performed using a stereo camera arrangement. For example, the device may include a depth estimation unit that collects measurement data from one or more infrared sensors to estimate relative depths of objects in the scene. As another example, the device may include active probing sensors, such as a light detection and ranging (LIDAR) unit, for estimating depths of objects in an image. The LIDAR unit may use ultraviolet, visible, or infrared light to image the objects and process returned signals to estimate depths for the objects. These depth estimation units may be used alone or in any combination with each other to estimate depths of objects in an image. The depth estimation units may operate substantially simultaneously with a camera capturing the image, such that the units may determine depths for objects in the image captured by the camera without actually processing image data from the camera.</p>
<p id="p-0023" num="0022">The device may include configuration data that associates various factors, e.g., based on the image and additional sensors of the device, with an estimated object scale. For example, the configuration data may map particular starting octaves and scales to various location cells (or &#x201c;loxels&#x201d;) for the object (that is, loxels in which the object may occur). As an example, usage statistics may provide an estimate for the most likely scales and/or octaves when an object occurs in a given loxel, to determine a starting scale and/or octave, as well as subsequent scales and/or octaves. In other words, in some examples, the techniques of this disclosure may select a starting octave and/or scale for an object (as well as subsequent octaves) based on the loxel in which the object occurs.</p>
<p id="p-0024" num="0023">In some examples, keypoint distribution may be used to estimate a scale for an object. Keypoints are generally areas of an image that can be used to generate feature descriptors. In general, keypoints detected at relatively higher scales correspond to relatively larger features of an object. Likewise, as image resolution decreases, finer details are more difficult to recognize. Therefore, details detected at higher scales for larger resolutions are typically detected, for smaller resolutions, at lower scales (and thus become finer details). Statistics can be collected indicative of distribution of keypoints across a scale space, which may be used to estimate the scaling factor of an image. Using keypoints extracted for a previous octave, a control unit may estimate a scale for an object of a query image, and use the estimated scale to select a minimum octave of a database of feature descriptors to search. That is, the control unit may cause the database to query only feature descriptors at octaves equal to and greater, but not less than, the minimum octave.</p>
<p id="p-0025" num="0024">In some cases, a camera of a device may continually capture images, and a processor or other unit for object recognition may attempt to detect objects in all, or a subset, of the captured images. In some cases, the processor may arrive at an approximation of an object scale in one of the earlier images. The techniques of this disclosure may use previously determined object scale information for a previous picture to initialize an estimation of an object scale for a current image, e.g., after a user moves and causes an image tracker to lose a previously acquired target.</p>
<p id="p-0026" num="0025">In general, for object recognition, a database of feature descriptor sets and other discriminating information is derived from training images. Feature descriptors are then extracted from a target image and used to query the database, to assess the contents of a given query image. For augmented reality or visual search applications, the client (for example, a cell phone) captures an image of an object of interest and compares it against the database of images, features, and meta-data information. This database can be stored on a server on the network, and can either be retrieved by the client for local processing or alternatively, the query can be transmitted to the server to be processed using network resources. The techniques of this disclosure are generally described with respect to scale invariant feature transform (SIFT) algorithm to perform the localization of keypoints and the extraction of feature descriptors. Each SIFT feature may have the following attributes: 2D (x, y) feature location, scale at which the feature is detected, the rotational orientation of the feature as given by that of the strongest image gradient in the feature neighborhood, and a vector that describes the local pixel variations in a discriminating way, essentially a histogram of local image gradients.</p>
<p id="p-0027" num="0026">Keypoint identification and descriptor extraction can be computationally demanding. For instance, a typical half-size video graphics array (HVGA) image can produce thousands of SIFT features. The implementation of the SIFT algorithm, including the iterative matching process, can easily outstrip the computational resources available on some mobile platforms. However, in most practical cases, the number of actual feature descriptors that lead to matches with those in the database tend to be much lower than the number of feature descriptors actually calculated. That is, of the set of calculated feature descriptors, a small subset may yield an object identification result. This may be caused, in part, by different imaging conditions in the query image (e.g., illumination, perspective, etc.) affecting feature descriptors such that only a few end up matching with features in the database. The techniques of this disclosure may exploit the interrelationships between keypoints in the scale space to help reduce the computational load imposed by the descriptor extraction and matching process. Also, when recognizing/tracking multiple objects within a given scene, false positives can pose problems, especially if some objects are similar. In some examples, these techniques may include computation of a rough, relative depth-map of the scene to constrain the matching, so as to reduce false positives.</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an example system <b>2</b> in which techniques for incremental feature descriptor extraction may be applied. In this example, system <b>2</b> includes client device <b>10</b> and server device <b>50</b>, which communicate via network <b>40</b>. Client device <b>10</b>, in this example, represents a mobile device, such as a laptop, a so-called netbook, a tablet computer, a personal digital assistant (PDA), a cellular or mobile phone or handset (including so-called &#x201c;smartphones&#x201d;), a global positioning system (GPS) device, a digital camera, a digital media player, a game device, or any other mobile device capable of communicating with server device <b>50</b>. While described in this disclosure with respect to a mobile client device <b>10</b>, the techniques of described in this disclosure should not be limited in this respect to mobile client devices. Instead, the techniques may be implemented by any device capable of storing a local object database and/or capable of communicating with server device <b>50</b> via network <b>40</b> or any other communication medium.</p>
<p id="p-0029" num="0028">Server device <b>50</b> represents a server device that accepts connections, e.g., transmission control protocol (TCP) connections, and responds with its own TCP connection to form a TCP session by which to receive query data and provide identification data. Server device <b>50</b> may represent a visual search server device in that server device <b>50</b> performs or otherwise implements a visual search algorithm to identify one or more features or objects within an image.</p>
<p id="p-0030" num="0029">Network <b>40</b> represents a public network, such as the Internet, that interconnects client device <b>10</b> and server device <b>50</b>. Commonly, network <b>40</b> implements various layers of the open system interconnection (OSI) model to facilitate transfer of communications or data between client device <b>10</b> and server device <b>50</b>. Network <b>40</b> typically includes any number of network devices, such as switches, hubs, routers, servers, to enable the transfer of the data between client device <b>10</b> and server device <b>50</b>. While shown as a single network, network <b>40</b> may comprise one or more sub-networks that are interconnected to form network <b>40</b>. These sub-networks may comprise service provider networks, access networks, backend networks or any other type of network commonly employed in a public network to provide for the transfer of data throughout network <b>40</b>. While described in this example as a public network, network <b>40</b> may comprise a private network that is not accessible generally by the public.</p>
<p id="p-0031" num="0030">As shown in the example of <figref idref="DRAWINGS">FIG. 1</figref>, client device <b>10</b> includes camera <b>12</b>, sensors <b>14</b>, control unit <b>20</b>, local object database <b>30</b>, and network interface <b>32</b>. Control unit <b>20</b>, in this example, includes feature extraction unit <b>22</b>, scale estimation unit <b>24</b>, and depth information unit <b>26</b>. In some examples, control unit <b>20</b> may also include units similar to feature matching unit <b>62</b> and confidence valuation unit <b>64</b> of server device <b>50</b>. For purposes of example, these units will be discussed with respect to server device <b>50</b>, though it should be understood that similar units may be included within control unit <b>20</b> of client device <b>10</b>.</p>
<p id="p-0032" num="0031">Functionality attributed to control unit <b>20</b> and control unit <b>60</b> and sub-units thereof may be implemented by any combination of hardware, software, and/or firmware. When one or more functions attributed to control unit <b>20</b> are implemented in software or firmware, control unit <b>20</b> may include one or more computer-readable storage media for storing instructions for the software, as well as one or more hardware-based processing units for executing the software. Such processing units may comprise one or more general purpose processors. Control unit <b>20</b> and control unit <b>60</b> may additionally or alternatively include one or more hardware units, such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Any or all of the functionality attributed to control unit <b>20</b> and control unit <b>60</b> may be performed by any respective hardware, software, or firmware thereof. Thus, although separate units are illustrated within control units <b>20</b> and <b>60</b> in the example of <figref idref="DRAWINGS">FIG. 1</figref>, it should be understood that in practice, such separate units may be combined in any reasonable fashion.</p>
<p id="p-0033" num="0032">Camera <b>12</b> may include a two-dimensional array of individual image sensor elements, e.g., arranged in rows and columns. Camera <b>12</b> may comprise, for example, an array of solid state sensor elements such as complementary metal-oxide semiconductor (CMOS) sensors, or other sensor elements. The image sensor elements within camera <b>12</b> are exposed to a scene to obtain light information from the scene and to capture a digital image of the scene. The digital image may include a two-dimensional matrix of pixel values, which may be defined in a particular color space, e.g., having red-green-blue values, or pixel luminance (intensity or brightness) values and pixel chrominance (color) values.</p>
<p id="p-0034" num="0033">Sensors <b>14</b> include one or more environmental sensors that provide information to control unit <b>20</b>. Control unit <b>20</b> may use this information to approximate a scale for an object in an image captured by camera <b>12</b>. In some examples, sensors <b>14</b> may include a global positioning system (GPS) unit that interacts with GPS satellites to determine a geographic location of client device <b>10</b>. The GPS unit may also provide an indication of a direction in which camera <b>12</b> was pointed when the image was captured. Control unit <b>20</b> may use this location and direction information to limit the search to potential objects that could be in the line of sight of camera <b>12</b>.</p>
<p id="p-0035" num="0034">For example, remote object database <b>52</b> may store geographic locations for objects of remote object database <b>52</b>. When the GPS unit of sensors <b>14</b> determines a geographic location and direction of camera <b>12</b>, control unit <b>20</b> may retrieve a subset of data from remote object database <b>52</b> having geographic locations in the line of sight of camera <b>12</b> and store the retrieved data as local object database <b>30</b>. Feature extraction unit <b>22</b> may then extract feature descriptors from the image and query local object database <b>30</b> with these extracted feature descriptors.</p>
<p id="p-0036" num="0035">As another example, feature extraction unit <b>22</b> may use the geographic location direction to approximate an initial scale of objects in the line of sight of camera <b>12</b>. For example, remote object database <b>52</b> may store a most likely scale for objects in the geographic location near client device <b>10</b> and in the line of sight of camera <b>12</b>, based on the location of client device <b>10</b> determined by the GPS unit of sensors <b>14</b>. Feature extraction unit <b>22</b> may then initially extract feature descriptors using this initial scale and send a query to server device <b>50</b> via network <b>40</b>, to query remote object database <b>52</b>. The query may include the location information, in addition to the feature descriptors, in some examples.</p>
<p id="p-0037" num="0036">Other examples of sensor may include, for example, temperature sensors, pressure sensors (e.g., barometric sensors), light sensors, infrared sensors, ultraviolet sensors, humidity sensors, carbon dioxide (CO<sub>2</sub>) sensors, or other such environmental sensors, any or all of which may provide values for respective sensed values. Control unit <b>20</b> may include data ranges for any or all of these sensors indicative of indoor and outdoor environments. Control unit <b>20</b> may compare current values for any or all of the sensors to the corresponding data ranges to determine whether client device <b>10</b> was indoors or outdoors when camera <b>12</b> captured a query image.</p>
<p id="p-0038" num="0037">Feature extraction unit <b>22</b> represents a unit that performs feature extraction in accordance with a feature extraction algorithm, such as a scale invariant feature transform (SIFT) algorithm, a compressed histogram of gradients (CHoG) algorithm, or other keypoint or feature description extraction algorithms. Generally, feature extraction unit <b>22</b> operates on image data, which may be captured locally using camera <b>12</b> or other image capture device included within client device <b>10</b>. Alternatively, client device <b>10</b> may store image data without capturing this image data itself, e.g., by downloading the image data via network <b>40</b>, locally via a wired connection with another computing device or via any other wired or wireless form of communication.</p>
<p id="p-0039" num="0038">Scale estimation unit <b>24</b> represents a unit that estimates a scale for an object in a query image captured by, e.g., camera <b>12</b>. For example, scale estimation unit <b>24</b> may estimate a scale for an object in a query image based on an expected distribution of keypoints from various scales (e.g., derived from a set of training images) compared to an actual keypoint distribution from the various scales for the image. Feature extraction unit <b>22</b> may extract keypoints for a previous octave for the image and provide these keypoints to scale estimation unit <b>24</b>, and scale estimation unit <b>24</b> may use these keypoints to approximate a scale for an object in the query image, using the configuration data. Using the estimated scale, scale estimation unit <b>24</b> may calculate a minimum octave of a database (e.g., either or both of local object database <b>30</b> and/or remote object database <b>52</b>) to search.</p>
<p id="p-0040" num="0039">Scale estimation unit <b>24</b> may store configuration data describing keypoint distribution for various scales. Scale estimation unit <b>24</b> may then compare distributions of keypoints for the query image to the distributions of the configuration data. Scale estimation unit <b>24</b> may then determine an approximate scale for the object in the query image as the scale having the best-matching keypoint distribution. That is, scale estimation unit <b>24</b> may determine values representative of keypoint distribution for the query image at various scales, and compare these values to the expected values for keypoint distribution of various scales of the configuration data. Scale estimation unit <b>24</b> may approximate the scale of an object as the scale for which keypoint distribution values from the query image most closely match the estimated keypoint distribution values from the configuration data for the corresponding scale.</p>
<p id="p-0041" num="0040">In accordance with the techniques of this disclosure, feature extraction unit <b>22</b> may start at an octave corresponding to the estimated scale for the query image. That is, feature extraction unit <b>22</b> may extract feature descriptors for the octave corresponding to the estimated scale, using keypoints extracted from that octave. Then, control unit <b>20</b> may determine whether an identity of an object in the query image can be determined using only these feature descriptors, and if not, proceed to extract feature descriptors for a subsequent octave.</p>
<p id="p-0042" num="0041">Depth estimation unit <b>26</b> is generally configured to estimate depth for one or more objects in an image captured by camera <b>12</b>. Depth estimation unit <b>26</b> may comprise a depth sensor, such as a time-of-flight sensor, structured light sensor, or Kinects. In some examples, client device <b>10</b> may include a camera array having two or more cameras, e.g., for capturing three-dimensional image and/or video data. In such examples, depth estimation unit <b>26</b> may be configured to calculate depth information for objects in an image captured by the camera array. For example, depth estimation unit <b>26</b> may receive two or more images of a scene that were captured by the camera array. By comparing horizontal locations of similar pixels of the images, depth estimation unit <b>26</b> may calculate horizontal disparity (or parallax) of similar pixels between the images, and based on this disparity, determine a depth of an object corresponding to the pixels. Depth estimation unit <b>26</b> may send one or more depth values (e.g., a depth map) to feature extraction unit <b>22</b>, which may use the depth values when determining a starting octave from which to extract feature descriptors.</p>
<p id="p-0043" num="0042">In some examples, a device need not include a depth estimation unit, and certain techniques of this disclosure may still be operable. However, when the device includes a depth estimation unit, such as depth estimation unit <b>26</b>, the device may be configured to use depth information to select the octave in which features are computed and the order in which octaves are processed. For example, if camera <b>12</b> faces a relatively large object directly (that is, head-on), pixels corresponding to the object may generally occur at the same distance from client device <b>10</b>. Such information may be used to determine the scale at which the most valuable keypoints are to be found, in some examples. In addition, or in the alternative, the depth information can be used along with the confidence values to determine the scales where the most valuable keypoints are found. Similarly, in some examples, if the object is placed at an angle relative to the camera, then the depth differences between the closest point to the farthest point in the object could be used to identify the range of scales where the most valuable keypoints are found. In this manner, the depth information may be used, at least in part, to estimate a scale value for an object in an image.</p>
<p id="p-0044" num="0043">As explained below, network interface <b>32</b> may comprise a wireless network interface. When network interface <b>32</b> includes a wireless network interface, the wireless network interface may implement one or more wireless network protocols, such as one or more of the IEEE 802.11 standards (e.g., 802.11a/b/g/n), the Third Generation Mobile Telecommunications (3G) standards, the Fourth Generation telecommunications standards (which may include mobile Worldwide Interoperability for Microwave Access (mobile WiMAX), 3G Long Term Evolution (LTE), LTE Advanced, and WirelessMAN-Advanced (IEEE 802.16)), or other wireless network protocols. In general, if a wireless network is available, control unit <b>20</b> may determine that it is highly likely that client device <b>10</b> is indoors. When client device <b>10</b> is indoors, control unit <b>20</b> may determine that sizes of objects in an image captured by camera <b>12</b> are likely to be relatively small. Accordingly, feature extraction unit <b>22</b> may be configured to treat whether a wireless network is available as a factor in determining a starting octave from which to extract feature descriptors for a query image.</p>
<p id="p-0045" num="0044">In the example of <figref idref="DRAWINGS">FIG. 1</figref>, server device <b>50</b> stores remote object database <b>52</b>, while client device <b>10</b> stores local object database <b>30</b>. Client device <b>10</b> may retrieve data for local object database <b>30</b> from server device <b>50</b> via network <b>40</b>. Accordingly, local object database <b>30</b> may represent all or a portion of remote object database <b>52</b>. In some examples, local object database <b>30</b> may include data from remote object database <b>52</b> as well as data from other remote object databases of other server devices (not shown in <figref idref="DRAWINGS">FIG. 1</figref>). In general, the techniques of this disclosure may query either or both of local object database <b>30</b> and/or remote object database <b>52</b> to determine an identity of an object in an image.</p>
<p id="p-0046" num="0045">As described in greater detail below, feature extraction unit <b>22</b> may generally extract keypoints from various sets of filtered images based on the original image. In some examples, the original image may be filtered using Gaussian filters to produce a set of Gaussian-blurred images. While it should be understood that various filtering techniques may be applied, this disclosure will primarily focus on Gaussian filters as an example. However, other similar filtering techniques may also be applied without departing from the techniques of this disclosure. In general, each of the filtered images in one set may have a similar size (e.g., in terms of pixel resolution), where each image in the set may be progressively more filtered. Each of the sets may be referred to as an &#x201c;octave.&#x201d; Gaussian blurring generally involves convolving the image data for a particular octave with a Gaussian blur function at a defined scale. Feature extraction unit <b>22</b> may incrementally convolve the image data, where a fixed multiplicative factor, referred to by the variable sigma (&#x3c3;), increments consequent Gaussian filters within the octave. Feature extraction unit <b>22</b> may form what may be referred to as a &#x201c;Gaussian pyramid&#x201d; having each of the Gaussian-blurred images for a particular octave. Feature extraction unit <b>22</b> may then compare two successively stacked Gaussian-blurred images in the pyramid to generate difference of Gaussian (DoG) images. The DoG images may form what is referred to as a &#x201c;DoG space.&#x201d;</p>
<p id="p-0047" num="0046">Based on this DoG space, feature extraction unit <b>22</b> may detect keypoints, where a keypoint refers to a region or patch of pixels around a particular sample point or pixel in the image data that is potentially interesting from a geometrical perspective. Generally, feature extraction unit <b>22</b> identifies keypoints as local maxima and/or local minima in the constructed DoG space. Feature extraction unit <b>22</b> may then assign these keypoints one or more orientations, or directions, based on directions of a local image gradient for the patch in which the keypoint was detected. To characterize these orientations, feature extraction unit <b>22</b> may define the orientation in terms of a gradient orientation histogram. Feature extraction unit <b>22</b> may then define a feature descriptor as a location and an orientation (e.g., by way of the gradient orientation histogram). After defining the feature descriptor, feature extraction unit <b>22</b> may output the feature descriptor, e.g., by querying local object database <b>30</b> or remote object database <b>52</b> with the feature descriptor. Feature extraction unit <b>22</b> may output a set of feature descriptors using this process.</p>
<p id="p-0048" num="0047">Network interface <b>32</b> represents any type of interface that is capable of communicating with server device <b>50</b> via network <b>40</b>, including wireless interfaces and wired interfaces. Network interface <b>32</b> may represent a wireless cellular interface and include the necessary hardware or other components, such as antennas, modulators and the like, to communicate via a wireless cellular network with network <b>40</b> and via network <b>40</b> with server device <b>50</b>. In this instance, although not shown in the example of <figref idref="DRAWINGS">FIG. 1</figref>, network <b>40</b> includes the wireless cellular access network by which wireless cellular network interface <b>32</b> communicates with network <b>40</b>. Although not illustrated in <figref idref="DRAWINGS">FIG. 1</figref>, client device <b>10</b> may further include a display, e.g., any type of display unit capable of displaying images, such as the image data in which object identities are determined, or any other types of data. The a display may, for example, comprise a light emitting diode (LED) display device, an organic LED (OLED) display device, a liquid crystal display (LCD) device, a plasma display device, or any other type of display device.</p>
<p id="p-0049" num="0048">Server device <b>50</b> includes network interface <b>54</b>, remote object database <b>52</b>, and control unit <b>60</b>. Control unit <b>60</b>, in this example, includes feature matching unit <b>62</b> and confidence valuation unit <b>64</b>. Network interface <b>54</b> may be similar to network interface <b>32</b> of client device <b>10</b>, in that network interface <b>54</b> may represent any type of interface capable of communicating with a network, such a network <b>40</b>. Feature matching unit <b>62</b> represents a unit that performs feature matching to identify one or more features or objects in the image data based on feature descriptors received from client device <b>10</b>.</p>
<p id="p-0050" num="0049">Feature matching unit <b>62</b> may access remote object database <b>52</b> to perform this feature identification, where remote object database <b>52</b> stores data defining feature descriptors and associates at least some of the received feature descriptors with identification data identifying the corresponding feature or object extracted from the image data. Confidence valuation unit <b>64</b> determines a confidence value, representative of a confidence that the object identified as corresponding to the received feature identifiers is the actual identity of the object in the image data.</p>
<p id="p-0051" num="0050">Objects stored in remote object database <b>52</b> may include a plurality of feature descriptors, and the feature descriptors received from client device <b>10</b> may match only a subset of the feature descriptors of an object in remote object database <b>52</b>. In general, the confidence value represents a correspondence between matches between the received feature descriptors and the feature descriptors associated with a corresponding object. Thus, a higher confidence value may reflect that the received feature descriptors match a relatively large number of feature descriptors of the object stored by remote object database <b>52</b>, while a lower confidence value may reflect that the received feature descriptors match a relatively small number of feature descriptors of the object stored by remote object database <b>52</b>.</p>
<p id="p-0052" num="0051">After determining an identity of an object for feature descriptors received from client device <b>10</b>, feature matching unit <b>62</b> provides identification data representative of the determined identity. Likewise, confidence valuation unit <b>64</b> assesses the confidence that the determined identity properly matches the received feature descriptors, generates a corresponding confidence value, and provides the confidence value to client device <b>10</b>. Client device <b>10</b> may determine whether the confidence value exceeds a threshold, and if not, extract additional feature descriptors from further octaves for the image data and send these additional feature descriptors to server device <b>50</b> for further analysis.</p>
<p id="p-0053" num="0052">Initially, a user of client device <b>10</b> may interface with client device <b>10</b> to initiate a visual search. The user may interface with a user interface or other type of interface presented by a display of client device <b>10</b> to select the image data and then initiate the visual search to identify one or more features or objects that are the focus of the image stored as the image data. For example, the image data may correspond to an image of a piece of famous artwork. The user may have captured this image using camera <b>12</b> of client device <b>10</b>, downloaded this image from network <b>40</b>, or locally retrieved the image via a wired or wireless connection with another computing device. In any event, after selecting the image data, the user may initiate the visual search, in this example, to identify the piece of famous artwork by, for example, name, artist and date of completion.</p>
<p id="p-0054" num="0053">In response to initiating the visual search, client device <b>10</b> invokes feature extraction unit <b>22</b> to extract at least one the feature descriptor describing one of the so-called &#x201c;keypoints&#x201d; found through analysis of the image data. Control unit <b>20</b> may query local object database <b>30</b> using the feature descriptor and/or send the feature descriptor to server device <b>50</b> via network <b>40</b> to query remote object database <b>52</b> using the feature descriptor. In some examples, feature extraction unit <b>22</b> forwards the feature descriptor to a feature compression unit (not shown in this example), which may compress the feature descriptor prior to transmission of the feature descriptor via network <b>40</b>. When sent to server device <b>50</b>, control unit <b>20</b> may encapsulate the feature descriptor (which may be compressed prior to encapsulation) as a network packet, e.g., a TCP/IP packet.</p>
<p id="p-0055" num="0054">While various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, these units do not necessarily require realization by different hardware units. Rather, various units may be combined in a hardware unit or provided by a collection of inter-operative hardware units, including one or more processors as described above, in conjunction with suitable software and/or firmware stored to computer-readable mediums. In this respect, reference to units in this disclosure is intended to suggest different functional units that may or may not be implemented as separate hardware units and/or hardware and software units.</p>
<p id="p-0056" num="0055">As discussed above, feature extraction unit <b>22</b>, in accordance with the techniques of this disclosure, may be configured to sequentially extract feature descriptors from an image captured by camera <b>12</b>. That is, feature extraction unit <b>22</b> may extract feature descriptors from a first octave, determine whether those feature descriptors can be used to accurately determine the identity of an object in the image, and if not, extract feature descriptors from one or more subsequent octaves. Moreover, elements of client device <b>10</b>, such as sensors <b>14</b>, network interface <b>32</b>, data stored in local object database <b>30</b>, scale estimation unit <b>24</b>, and/or depth estimation unit <b>26</b>, may provide data to feature extraction unit <b>22</b> may use to select a first and/or subsequent octave. In general, upper octaves may be relatively smoother and relatively more stable than lower octaves. Thus, in some examples, feature extraction unit <b>22</b> may try a last octave, if the first octave did not yield a result, followed by higher octaves if the last octave does not yield a result. In some examples, if there are no matches when trying octave zero (0), the reference image in the database may exist at a higher scale, and thus, feature extraction unit <b>22</b> may move to a higher octave for feature extraction.</p>
<p id="p-0057" num="0056">Feature attributes from training images or test images may be used to reduce complexity in feature generation and matching. These statistics may be used to initialize parameters involved in the feature extraction process. Once initialized, feature extraction unit <b>22</b> may incrementally extract features, perform recognition and/or inference, and if the recognition is not sufficient for reliable decisions as to the identity of the object in the image, update the feature extraction parameters for a next set of feature extractions. Using training data (which may be stored in local object database <b>30</b> and/or retrieved from remote object database <b>52</b>) and past query statistics, recognition of objects in a query image may be achieved in fewer steps, which may result in a computational savings.</p>
<p id="p-0058" num="0057">Scale estimation unit <b>24</b> may attempt to identify a scale of an object based on feature attributes. The scale of an object may be used to select a minimum octave of the database to query. That is, when submitting feature descriptors to the database, the database may search octaves equal to or greater, but not less than, the minimum octave, when a minimum octave is specified. Thus, after each set of keypoints are extracted, scale estimation unit <b>24</b> may use the distribution of the keypoints to approximate a scale for an object in the image, and use the scale to estimate the minimum octave. <figref idref="DRAWINGS">FIG. 5</figref> below illustrates an example relationship between keypoint distribution and corresponding minimum octaves. More details regarding the use of keypoint distribution to estimate the minimum octave of the database are discussed below with respect to <figref idref="DRAWINGS">FIG. 5</figref>. In any case, using incremental extraction of feature descriptors, as well as a minimum octave, may provide a relatively faster search both by extracting fewer feature descriptors initially, as well as reducing the amount of the database that is actually searched.</p>
<p id="p-0059" num="0058">Depth estimation unit <b>26</b> may determine depth values for a query image. For example, depth estimation unit <b>26</b> may generate a depth map for the query image, where the depth map includes depth values for pixels or objects in the query image. Using the depth map, feature extraction unit <b>22</b> may constrain matching or tracking, which may result in less false positives. Moreover, feature extraction unit <b>22</b> may limit matching or tracking to a particular depth level, which may result in lower complexity.</p>
<p id="p-0060" num="0059">In this manner, the techniques of this disclosure may take advantage of certain observations on feature descriptor extraction. These observations include, first, that the scale of an object in a query image may generally determine how its keypoints are distributed in the scale space. Smaller objects tend to have a distribution that is squished towards lower scales in the scale space. Also, if certain regions in the image are more textured than others, resulting keypoints tend to vary in their stability. Likewise, the qualities for feature descriptors anchored at the keypoints also vary in their stability. Furthermore, the image quality generally improves with better imaging conditions (e.g., good illumination), which may result in more keypoints from the image. However, when the resolution is relatively high, the number of actual feature descriptors tends to be lower, due to larger variations, that is, greater detail, relative to training images for which data is stored in, e.g., local object database <b>30</b> and/or remote object database <b>52</b>. These differences may result from different illumination, perspective, or other photography parameters.</p>
<p id="p-0061" num="0060">Local object database <b>30</b> and/or remote object database <b>52</b>, as discussed above, generally store various data for a set of training images. This data may include feature descriptors for various objects, as well as data that assist in scale estimation, depth mapping, and/or other techniques for reducing the complexity of object identification in images. For example, the databases may store data indicative of what octaves and scales from which to extract feature descriptors in a given location cell (or &#x201c;loxel&#x201d;) of an image. An image may be divided into a two-dimensional matrix of regions, referred to as loxels. The database data may include side information about the ordering of the octaves and scales, and/or the octaves and scales to which matching features typically belong. The databases may also store usage statistics that describe the most likely scales or octaves that match well within a given loxel.</p>
<p id="p-0062" num="0061">In some examples, the databases may store keypoint distribution based image scale estimation. The databases may also store, in some examples, data corresponding to sensor-based location information, which may be used to initialize scale and octave parameters to feed in feature extraction. For example, control unit <b>20</b> may determine that a query image is of an outdoor environment using GPS information from sensors <b>14</b>, and the databases may store information about the locations of objects with respect to the geographic location of client device <b>10</b> when the query image is captured. Control unit <b>20</b> may determine that a query image is of an indoor environment based on indoor positioning techniques, such as determining whether a wireless network is available. A stereo camera may be used to estimate the depth of an object in a query image, which may help in initializing octave selection. Camera intrinsic parameters (e.g., focal length , , , principal point, image format) may be used to estimate depth for an object. In some instances, intrinsic parameters need not be configured a priori for a sparse depth map. In some examples, control unit <b>20</b> may calculate intrinsic and extrinsic parameters from feature correspondences, e.g., using the Eight-Point Algorithm. In general, the Eight-Point Algorithm includes using eight corresponding points in a stereo image pair to compute the essential matrix or fundamental matrix for the stereo image pair.</p>
<p id="p-0063" num="0062">In some examples, control unit <b>20</b> may be configured to use historical data to determine an initial scale or octave in which to perform feature extraction. For example, a tracker (not shown) executed by control unit <b>20</b> may determine depth or scale for objects in images captured by camera <b>12</b>. If the tracker loses a target, control unit <b>20</b> may use the most recent &#x201c;good&#x201d; information about tracked targets' scale when determining scale for objects in a more recent image.</p>
<p id="p-0064" num="0063">Using any or all of the data described above, control unit <b>20</b> may be configured to determine an identity of an object in a query image. For example, feature extraction unit <b>22</b> may initially select a preferred octave and set of scales and extract feature descriptors for the preferred octave and set of scales. Feature extraction unit <b>22</b> may progressively add next octaves in the scale-space as needed, which may allow early termination of the object recognition process, without the risk of terminating the process prematurely. The preference for octaves and set of scales can be derived from data of local object database <b>30</b> and/or remote object database <b>52</b> and/or query statistics from the tracker. Scale estimation unit <b>24</b> may estimate the scale of the object using cumulative threshold statistics after each iteration of keypoint extraction, which may also be refined using data of the databases. Moreover, the estimated scale may be refined based on additional inputs, such as data from sensors <b>14</b>, depth information determined by depth estimation unit <b>26</b>, or other data.</p>
<p id="p-0065" num="0064">The techniques of this disclosure may provide one or more advantages. For example, these techniques may provide faster, localized extraction of feature descriptors, e.g., for SIFT, than SIFT would otherwise achieve. These techniques may allow for estimation of a most relevant subset of scale-space levels (e.g., octave levels) for processing, which may improve speed and complexity of object recognition. These techniques may further allow faster searching in local object database <b>30</b> of client device <b>10</b> (e.g., a mobile phone). That is, these techniques allow feature descriptor extraction from upper levels first, and stopping the algorithm when sufficient accuracy is reached. In this manner, there may be a relatively short response in a distributed visual search system, such as system <b>2</b> of <figref idref="DRAWINGS">FIG. 1</figref>. Feature descriptors may be transmitted via network <b>40</b> to server device <b>50</b> progressively, starting with upper levels first, and server device <b>50</b> may perform several iterations of searches on received data and send results or a termination signal back once sufficient accuracy match is found. Client device <b>10</b> may stop sending feature descriptors to server device <b>50</b> after results of the search or the termination signal is received.</p>
<p id="p-0066" num="0065">The techniques of this disclosure may also reduce complexity and improve speed of tracking. Once an object in an image is recognized, its relative scale may become known as well. If the tracker loses the object, the next search operation may be simplified using the scale of the object that was previously being tracked. These techniques may further initialize a kernel size for clustering of keypoints using an estimated scale, and then perform segmentation based on a relative depth map estimate.</p>
<p id="p-0067" num="0066">In this manner, client device <b>10</b> represents an example of a device including a processor configured to extract a first set of one or more keypoints from a first set of blurred images of a first octave of a received image, calculate a first set of one or more descriptors for the first set of keypoints, receive a confidence value for a result produced by querying a feature descriptor database with the first set of descriptors, wherein the result comprises information describing an identity of an object in the received image, and extract a second set of one or more keypoints from a second set of blurred images of a second octave of the received image when the confidence value does not exceed a confidence threshold.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 2</figref> is a conceptual diagram illustrating a difference of Gaussian (DoG) pyramid <b>104</b> that has been determined for use in keypoint extraction. The example of <figref idref="DRAWINGS">FIG. 2</figref> illustrates a set <b>100</b> of images in Gaussian pyramid <b>102</b> and corresponding DoG pyramid <b>104</b>, where set <b>100</b> corresponds to a first selected octave, and a second set <b>120</b> of images in Gaussian pyramid <b>122</b> and corresponding DoG pyramid <b>124</b>, where set <b>120</b> corresponds to a second octave. In accordance with the techniques of this disclosure, control unit <b>20</b> may be configured to generate data for set <b>120</b> when an object identified using data for set <b>100</b> yields a confidence value below a threshold value.</p>
<p id="p-0069" num="0068">Feature extraction unit <b>22</b> of <figref idref="DRAWINGS">FIG. 1</figref> may construct DoG pyramid <b>104</b> by computing the difference of any two consecutive Gaussian-blurred images in Gaussian pyramid <b>102</b>. The input image I(x, y), which may be received by feature extraction unit <b>22</b> from camera <b>12</b> in the example of <figref idref="DRAWINGS">FIG. 1</figref>, is gradually Gaussian blurred to construct Gaussian pyramid <b>102</b>. Gaussian blurring generally involves a process of convolving the original image I(x, y) with the Gaussian blur function G(x, y, c) at scale c such that the Gaussian blurred function L(x, y, c) is defined as L(x, y, c)=G(x, y, c)*I(x, y). Here, G is a Gaussian kernel, c denotes the standard deviation of the Gaussian function that is used for blurring the image I(x, y). As c, is varied (c<sub>0</sub>&#x3c;c<sub>1</sub>&#x3c;c<sub>2</sub>&#x3c;c<sub>3</sub>&#x3c;c<sub>4</sub>), the standard deviation c varies and a gradual blurring is obtained. Sigma is the base scale variable (essentially the width of the Gaussian kernel). When the initial image I(x, y) is incrementally convolved with Gaussians G to produce the blurred images L, the blurred images L are separated by the constant factor c in the scale space. In the example of SIFT, Gaussian kernels may be used to generate the scale-space. In some examples, alternate, low-pass kernels may be used to generate a scale space, e.g., a box function, a triangular function, or other such functions.</p>
<p id="p-0070" num="0069">In DoG space or pyramid <b>104</b>, D(x, y, a)=L(x, y, c<sub>n</sub>)&#x2212;L(x, y, c<sub>n-1</sub>). A DoG image D(x, y,) is the difference between two adjacent Gaussian blurred images L at scales c<sub>n </sub>and c<sub>n-1</sub>. The scale of the D(x, y,) lies somewhere between c<sub>n </sub>and c<sub>n-1</sub>. As the number of Gaussian-blurred images L increase and the approximation provided for Gaussian pyramid <b>102</b> approaches a continuous space, the two scales also approach into one scale. The convolved images L may be grouped by octave, where an octave corresponds to a doubling of the value of the standard deviation. Moreover, the values of the multipliers k (e.g., c<sub>0</sub>&#x3c;c<sub>1</sub>&#x3c;c<sub>z</sub>&#x3c;c<sub>3</sub>&#x3c;c<sub>4</sub>), are selected such that a fixed number of convolved images L are obtained per octave. Then, the DoG images D may be obtained from adjacent Gaussian-blurred images L per octave.</p>
<p id="p-0071" num="0070">In accordance with the techniques of this disclosure, after images D are obtained for a given octave, feature extraction unit <b>22</b> may extract keypoints for the octave and determine feature descriptors for these extracted keypoints. The feature descriptors for the current octave (and any previous octaves) may be used to attempt to determine the identity of an object in the image. After an identity of the object is determined, control unit <b>20</b> may obtain a confidence value for the determined identity. If the confidence value is less than a threshold value, feature extraction unit <b>22</b> may proceed to a subsequent octave, determining additional feature descriptors for the subsequent octave and again attempting to determine the identity of the object in the image. On the other hand, when the confidence value exceeds the threshold, feature extraction unit <b>22</b> need not proceed to a subsequent octave. As described in greater detail below, control unit <b>20</b> may select the first octave and subsequent octaves based on various factors, such as, for example, an approximated scale for objects in the image, sensor data indicating whether the query image was captured in an indoor or outdoor environment, and/or a depth map for the query image indicative of depth values for an object in the query image.</p>
<p id="p-0072" num="0071">Feature extraction unit <b>22</b> may then use DoG pyramid <b>104</b> to identify keypoints for the image I(x, y). In performing keypoint extraction, feature extraction unit <b>22</b> determines whether the local region or patch around a particular sample point or pixel in the image is a potentially interesting patch (geometrically speaking). Generally, feature extraction unit <b>22</b> identifies local maxima and/or local minima in the DoG space <b>104</b> and uses the locations of these maxima and minima as keypoint locations in DoG space <b>104</b>. In the example illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, feature extraction unit <b>22</b> identifies a keypoint <b>108</b> within a patch <b>106</b>. Finding the local maxima and minima (also known as local extreme detection) may be achieved by comparing each pixel (e.g., the pixel for keypoint <b>108</b>) in DoG space <b>104</b> to its eight neighboring pixels at the same scale and to the nine neighboring pixels (in adjacent patches <b>110</b> and <b>112</b>) in each of the neighboring scales on the two sides, for a total of 26 pixels (9&#xd7;2+8=26). If the pixel value for the keypoint <b>106</b> is a maximum or a minimum among all 26 compared pixels in the patches <b>106</b>, <b>110</b>, and <b>108</b>, then feature extraction unit <b>22</b> selects this as a keypoint. Feature extraction unit <b>22</b> may further process the keypoints such that their location is identified more accurately. Feature extraction unit <b>22</b> may, in some instances, discard some of the keypoints, such as the low contrast key points and edge keypoints.</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 3</figref> is a conceptual diagram illustrating detection of a keypoint in more detail. In the example of <figref idref="DRAWINGS">FIG. 3</figref>, each of the patches <b>106</b>, <b>110</b>, and <b>112</b> include a 3&#xd7;3 pixel region. Feature extraction unit <b>22</b> first compares a pixel of interest (e.g., keypoint <b>108</b>) to its eight neighboring pixels <b>132</b> at the same scale (e.g., patch <b>106</b>) and to the nine neighboring pixels <b>134</b> and <b>136</b> in adjacent patches <b>110</b> and <b>112</b> in each of the neighboring scales on the two sides of the keypoint <b>108</b>.</p>
<p id="p-0074" num="0073">Feature extraction unit <b>22</b> may assign each keypoint one or more orientations, or directions, based on the directions of the local image gradient. By assigning a consistent orientation to each keypoint based on local image properties, feature extraction unit <b>22</b> may represent the keypoint descriptor relative to this orientation and therefore achieve invariance to image rotation. Feature extraction unit <b>22</b> then calculates magnitude and direction for every pixel in the neighboring region around the keypoint <b>108</b> in the Gaussian-blurred image L and/or at the keypoint scale. The magnitude of the gradient for the keypoint <b>108</b> located at (x, y) may be represented as m(x, y) and the orientation or direction of the gradient for the keypoint at (x, y) may be represented as &#x393;(x, y).</p>
<p id="p-0075" num="0074">Feature extraction unit <b>22</b> then uses the scale of the keypoint to select the Gaussian smoothed image, L, with the closest scale to the scale of the keypoint <b>108</b>, so that all computations are performed in a scale-invariant manner. For each image sample, L(x, y), at this scale, feature extraction unit <b>22</b> computes the gradient magnitude, m(x, y), and orientation, &#x393;(x, y), using pixel differences. For example the magnitude m(x,y) may be computed in accordance with the following equation (1):</p>
<p id="p-0076" num="0075">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>m</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>x</mi>
              <mo>,</mo>
              <mi>y</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <msqrt>
            <mrow>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mrow>
                      <mi>L</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mrow>
                            <mi>x</mi>
                            <mo>+</mo>
                            <mn>1</mn>
                          </mrow>
                          <mo>,</mo>
                          <mi>y</mi>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>-</mo>
                    <mrow>
                      <mi>L</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mrow>
                            <mi>x</mi>
                            <mo>-</mo>
                            <mn>1</mn>
                          </mrow>
                          <mo>,</mo>
                          <mi>y</mi>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mrow>
                      <mi>L</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>x</mi>
                          <mo>,</mo>
                          <mrow>
                            <mi>y</mi>
                            <mo>+</mo>
                            <mn>1</mn>
                          </mrow>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>-</mo>
                    <mrow>
                      <mi>L</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>x</mi>
                          <mo>,</mo>
                          <mrow>
                            <mi>y</mi>
                            <mo>-</mo>
                            <mn>1</mn>
                          </mrow>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </msqrt>
          <mo>.</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
Feature extraction unit <b>22</b> may calculate the direction or orientation &#x393;(x, y) in accordance with the following equation (2):
</p>
<p id="p-0077" num="0076">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>&#x393;</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>x</mi>
              <mo>,</mo>
              <mi>y</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mi>arctan</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>[</mo>
            <mfrac>
              <mrow>
                <mrow>
                  <mi>L</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>x</mi>
                      <mo>,</mo>
                      <mrow>
                        <mi>y</mi>
                        <mo>+</mo>
                        <mn>1</mn>
                      </mrow>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <mi>L</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>x</mi>
                      <mo>,</mo>
                      <mrow>
                        <mi>y</mi>
                        <mo>-</mo>
                        <mn>1</mn>
                      </mrow>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mrow>
                <mrow>
                  <mi>L</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mrow>
                        <mi>x</mi>
                        <mo>+</mo>
                        <mn>1</mn>
                      </mrow>
                      <mo>,</mo>
                      <mi>y</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>-</mo>
                <mrow>
                  <mi>L</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mrow>
                        <mi>x</mi>
                        <mo>-</mo>
                        <mn>1</mn>
                      </mrow>
                      <mo>,</mo>
                      <mi>y</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mfrac>
            <mo>]</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
In equation (2), L(x, y) represents a sample of the Gaussian-blurred image L(x, y), at scale which is also the scale of the keypoint.
</p>
<p id="p-0078" num="0077">Feature extraction unit <b>22</b> may consistently calculate the gradients for the keypoint either for the plane in the Gaussian pyramid that lies above, at a higher scale, than the plane of the keypoint in the DoG space or in a plane of the Gaussian pyramid that lies below, at a lower scale, than the keypoint. Either way, for each keypoint, feature extraction unit <b>22</b> calculates the gradients at the same scale in a rectangular area (e.g., patch) surrounding the keypoint. Moreover, the frequency of an image signal is reflected in the scale of the Gaussian-blurred image. Yet, SIFT and other algorithm, such as a compressed histogram of gradients (CHoG) algorithm, simply use gradient values at all pixels in the patch (e.g., rectangular area). A patch is defined around the keypoint; sub-blocks are defined within the block; samples are defined within the sub-blocks and this structure remains the same for all keypoints even when the scales of the keypoints are different. Therefore, while the frequency of an image signal changes with successive application of Gaussian smoothing filters in the same octave, the keypoints identified at different scales may be sampled with the same number of samples irrespective of the change in the frequency of the image signal, which is represented by the scale.</p>
<p id="p-0079" num="0078">To characterize a keypoint orientation, feature extraction unit <b>22</b> may generate a gradient orientation histogram (see <figref idref="DRAWINGS">FIG. 4</figref>) by using, for example, Compressed Histogram of Gradients (CHoG). The contribution of each neighboring pixel may be weighted by the gradient magnitude and a Gaussian window. Peaks in the histogram correspond to dominant orientations. Feature extraction unit <b>22</b> may measure all the properties of the keypoint relative to the keypoint orientation, and this may provide invariance to rotation.</p>
<p id="p-0080" num="0079">In one example, feature extraction unit <b>22</b> computes the distribution of the Gaussian-weighted gradients for each block, where each block is 2 sub-blocks by 2 sub-blocks for a total of 4 sub-blocks. To compute the distribution of the Gaussian-weighted gradients, feature extraction unit <b>22</b> forms an orientation histogram with several bins with each bin covering a part of the area around the keypoint. For example, the orientation histogram may have 36 bins, each bin covering 10 degrees of the 360 degree range of orientations. Alternatively, the histogram may have 8 bins, each covering 45 degrees of the 360 degree range. It should be clear that the histogram coding techniques described herein may be applicable to histograms of any number of bins.</p>
<p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. 4</figref> is a conceptual diagram illustrating example techniques by which feature extraction unit <b>22</b> may obtain gradient distributions and orientation histograms. Here, a two-dimensional gradient distribution (dx, dy) (e.g., block <b>156</b>) is converted to a one-dimensional distribution (e.g., histogram <b>164</b>). The keypoint <b>108</b> is located at a center of the patch <b>156</b> (also called a cell or region) that surrounds the keypoint <b>108</b>. The gradients that are pre-computed for each level of the pyramid are shown as small arrows at each sample location <b>158</b>. As shown, 4&#xd7;4 groups of samples <b>158</b> form a sub-block <b>160</b>, and 2&#xd7;2 groups of sub-blocks form the block <b>156</b>. Block <b>156</b> may also be referred to as a descriptor window.</p>
<p id="p-0082" num="0081">The Gaussian weighting function is shown with the circle <b>152</b> and may be used to assign a weight to the magnitude of each of sample points <b>158</b>. The weight in the circular window <b>152</b> falls off smoothly. The purpose of the Gaussian window <b>152</b> is to avoid sudden changes in the descriptor with small changes in position of the window and to give less emphasis to gradients that are far from the center of the descriptor. A 2&#xd7;2=4 array of orientation histograms <b>162</b> is obtained from the 2&#xd7;2 sub-blocks with 8 orientations in each bin of the histogram resulting in a (2&#xd7;2)&#xd7;8=32 dimensional feature descriptor vector. For example, orientation histograms <b>163</b> and <b>165</b> may correspond to the gradient distribution for sub-block <b>160</b>. However, using a 4&#xd7;4 array of histograms with 8 orientations in each histogram (8-bin histograms), resulting in a (4&#xd7;4)&#xd7;8=128 dimensional feature descriptor vector for each keypoint may yield a better result. Note that other types of quantization bin constellations (e.g., with different Voronoi cell structures) may also be used to obtain gradient distributions.</p>
<p id="p-0083" num="0082">As used herein, a histogram is a mapping k<sub>i </sub>that counts the number of observations, sample, or occurrences (e.g., gradients) that fall into various disjoint categories known as bins. The graph of a histogram is merely one way to represent a histogram. Thus, if k is the total number of observations, samples, or occurrences and m is the total number of bins, the frequencies in histogram k<sub>i </sub>satisfy the following condition:</p>
<p id="p-0084" num="0083">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>n</mi>
          <mo>=</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>m</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <msub>
              <mi>k</mi>
              <mi>i</mi>
            </msub>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where &#x3a3; is the summation operator.
</p>
<p id="p-0085" num="0084">Each sample added to the histograms <b>162</b> may be weighted by its gradient magnitude within a Gaussian-weighted circular window <b>152</b> with a standard deviation that is 1.5 times the scale of the keypoint. Peaks in the resulting orientation histogram <b>164</b> correspond to dominant directions of local gradients. The highest peak in the histogram is detected and then any other local peak that is within a certain percentage, such as 80%, of the highest peak is used to also create a keypoint with that orientation. Therefore, for locations with multiple peaks of similar magnitude, there can be multiple keypoints created at the same location and scale but different orientations.</p>
<p id="p-0086" num="0085">The histograms from the sub-blocks may be concatenated to obtain a feature descriptor vector for the keypoint. If the gradients in 8-bin histograms from 16 sub-blocks are used, a 128 dimensional feature descriptor vector may result.</p>
<p id="p-0087" num="0086">In this manner, a descriptor may be obtained for each keypoint, where such descriptor may be characterized by a location (x, y), an orientation, and a descriptor of the distributions of the Gaussian-weighted gradients. Note that an image may be characterized by one or more keypoint descriptors (also referred to as image descriptors).</p>
<p id="p-0088" num="0087">In some exemplary applications, an image may be obtained and/or captured by a mobile device and object recognition may be performed on the captured image or part of the captured image. According to a first option, the captured image may be sent by the mobile device to a server where it may be processed (e.g., to obtain one or more descriptors) and/or compared to a plurality of images (e.g., one or more descriptors for the plurality of images) to obtain a match (e.g., identification of the captured image or object therein). However, in this option, the whole captured image is sent, which may be undesirable due to its size. In a second option, the mobile device processes the image (e.g., perform feature extraction on the image) to obtain one or more image descriptors and sends the descriptors to a server for image and/or object identification. Because the keypoint descriptors for the image are sent, rather than the image, this may take less transmission time so long as the keypoint descriptors for the image are smaller than the image itself. Thus, compressing the size of the keypoint descriptors may be highly desirable.</p>
<p id="p-0089" num="0088">In order to minimize the size of a keypoint descriptor, it may beneficial to compress the descriptor of the distribution of gradients. Since the descriptor of the distribution of gradients is represented by histogram, efficient coding techniques for histograms are described herein.</p>
<p id="p-0090" num="0089">In order to efficiently represent and/or compress feature descriptors, the descriptor of the distributions (e.g., orientation histograms) may be more efficiently represented. Thus, one or more methods or techniques for efficiently coding of histograms are described herein. Note that these methods or techniques may be implemented with any type of histogram implementation to efficiently (or even optimally) code a histogram in a compressed form. Efficiently coding of a histogram is a distinct problem not addressed by traditional encoding techniques. Traditional encoding techniques have focused on efficiently encoding a sequence of values. Because sequence information is not used in a histogram, efficiently encoding a histogram is a different problem than those addressed by conventional techniques.</p>
<p id="p-0091" num="0090">As a first step, consideration is given to the optimal (smallest size or length) coding of a histogram. Information theoretic principles may be applied to obtain a maximum length for lossless and/or lossy encoding of a histogram.</p>
<p id="p-0092" num="0091">As noted above, for a particular patch (e.g., often referred to as a cell or region), the distribution of gradients in the patch may be represented as a histogram. A histogram may be represented as an alphabet A having a length of m symbols (2&#x2266;m&#x2266;&#x221e;), where each symbol is associated with a bin in the histogram. Therefore, the histogram has a total number of m bins. For example, each symbol (bin) in the alphabet A may correspond to a gradient/orientation from a set of defined gradients/orientations. Here, n may represent the total number of observations, samples, or occurrences (gradient samples in a cell, patch, or region) and k represents the number of observations, samples, or occurrences in a particular bin (e.g., k<sub>1 </sub>is number of gradient samples in first bin . . . k<sub>m </sub>is the number of gradient samples in mth bin), such that</p>
<p id="p-0093" num="0092">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mrow>
  <mi>n</mi>
  <mo>=</mo>
  <mrow>
    <munder>
      <mo>&#x2211;</mo>
      <mrow>
        <mi>i</mi>
        <mo>=</mo>
        <mrow>
          <mn>1</mn>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>&#x2026;</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>m</mi>
        </mrow>
      </mrow>
    </munder>
    <mo>&#x2062;</mo>
    <mrow>
      <msub>
        <mi>k</mi>
        <mi>i</mi>
      </msub>
      <mo>.</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
That is, the sum of all gradient samples in the histogram bins is equal to the total number of gradient samples in the patch. Because a histogram may represent a probability distribution for a first distribution of gradient samples within a cell, patch, or region, it is possible that different cells, patches, or regions having a second distribution (different from the first distribution) of gradient samples may nonetheless have the same histogram.
</p>
<p id="p-0094" num="0093">If P denotes an m-ary probability distribution [p<sub>1</sub>, . . . , p<sub>m</sub>], the entropy H(P) of this distribution can be defined as:</p>
<p id="p-0095" num="0094">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>H</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mi>P</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mo>-</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>m</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <msub>
                <mi>p</mi>
                <mi>i</mi>
              </msub>
              <mo>&#x2062;</mo>
              <mi>log</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <msub>
                  <mi>p</mi>
                  <mi>i</mi>
                </msub>
                <mo>.</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>4</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
In this case, the relative entropy D(P&#x2225;Q) between two known distributions P and Q is given by
</p>
<p id="p-0096" num="0095">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>D</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>P</mi>
              <mo>||</mo>
              <mi>Q</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>m</mi>
          </munderover>
          <mo>&#x2062;</mo>
          <mrow>
            <msub>
              <mi>p</mi>
              <mi>i</mi>
            </msub>
            <mo>&#x2062;</mo>
            <mi>log</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <mfrac>
                <msub>
                  <mi>p</mi>
                  <mi>i</mi>
                </msub>
                <msub>
                  <mi>q</mi>
                  <mi>i</mi>
                </msub>
              </mfrac>
              <mo>.</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
For a given sample w of gradient distributions, assume that the number of times each gradient value appears is given by k<sub>i </sub>(for i=1, . . . m). The probability P(w) of the sample w is thus given by:
</p>
<p id="p-0097" num="0096">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>P</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mi>w</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x220f;</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>m</mi>
          </munderover>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mrow>
            <msub>
              <mi>p</mi>
              <mi>i</mi>
            </msub>
            <mo>&#x2062;</mo>
            <msup>
              <mi>k</mi>
              <mi>i</mi>
            </msup>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>6</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where &#x3a0; is the product operator.
<br/>
For example, in the case of a cell or patch, the probability P(w) is a probability of a particular cell or patch.
</p>
<p id="p-0098" num="0097">However, Equation 6 assumes that the distribution P is known. In the case where the source distribution is unknown, as may be the case with typical gradients in a patch, the probability of a sample w may be given by the Krichecvsky-Trofimov (KT) estimate:</p>
<p id="p-0099" num="0098">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <msub>
              <mi>P</mi>
              <mi>KT</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>w</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mi>&#x393;</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mfrac>
                  <mi>m</mi>
                  <mn>2</mn>
                </mfrac>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>&#x2062;</mo>
            <mfrac>
              <mrow>
                <munderover>
                  <mo>&#x220f;</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>m</mi>
                </munderover>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>&#x393;</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msub>
                        <mi>k</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>+</mo>
                      <mfrac>
                        <mn>1</mn>
                        <mn>2</mn>
                      </mfrac>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mrow>
                <msup>
                  <mi>&#x3c0;</mi>
                  <mfrac>
                    <mi>m</mi>
                    <mn>2</mn>
                  </mfrac>
                </msup>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>&#x393;</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>n</mi>
                      <mo>+</mo>
                      <mfrac>
                        <mi>m</mi>
                        <mn>2</mn>
                      </mfrac>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mfrac>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>7</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where &#x393; is the Gamma function such that &#x393;(n)=(n&#x2212;1)!.
</p>
<p id="p-0100" num="0099">If the sample w is to be encoded using the KT-estimate of its probability, the length L of such encoding (under actual distribution P) satisfies:</p>
<p id="p-0101" num="0100">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>L</mi>
            <mi>KT</mi>
          </msub>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>w</mi>
              <mo>,</mo>
              <mi>P</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mrow>
              <munder>
                <mo>&#x2211;</mo>
                <mi>w</mi>
              </munder>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>P</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>m</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mi>log</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mrow>
                    <msub>
                      <mi>P</mi>
                      <mi>KT</mi>
                    </msub>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>w</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>~</mo>
                  <mrow>
                    <mi>nH</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>P</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>+</mo>
          <mrow>
            <mfrac>
              <mrow>
                <mi>m</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
              <mn>2</mn>
            </mfrac>
            <mo>&#x2062;</mo>
            <mi>log</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>n</mi>
              <mo>.</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>8</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
Equation 8 provides the maximum code length for lossless encoding of a histogram. The redundancy of KT-estimator-based code may be given by:
</p>
<p id="p-0102" num="0101">
<maths id="MATH-US-00010" num="00010">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mrow>
              <msub>
                <mi>R</mi>
                <mi>KT</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>n</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>~</mo>
            <mfrac>
              <mrow>
                <mi>m</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
              <mn>2</mn>
            </mfrac>
          </mrow>
          <mo>&#x2062;</mo>
          <mi>log</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mi>n</mi>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>9</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
which does not depend on the actual source distribution. This implies that such code is universal. Thus, the KT-estimator provides a close approximation of actual probability P so long as the sample w used is sufficiently long.
</p>
<p id="p-0103" num="0102">Note that the KT-estimator is only one way to compute probabilities for distributions. For example, a maximum likelihood (ML) estimator may also be used.</p>
<p id="p-0104" num="0103">Also, when coding a histogram, it may be assumed that both the encoder and decoder can determine or are configured with the total number of samples n in the histogram and the number of bins m for the histogram. Thus, in this case, this information need not be encoded. Therefore, the encoding is focused on the number of samples for each of the m bins.</p>
<p id="p-0105" num="0104">Rather than transmitting the histogram itself as part of the keypoint (or image) descriptor, a compressed form of the histogram may be used. To accomplish this, histograms may be represented by types. Generally, a type is a compressed representation of a histogram (e.g., where the type represents the shape of the histogram rather than full histogram). The type t of a sample w may be defined as:</p>
<p id="p-0106" num="0105">
<maths id="MATH-US-00011" num="00011">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>t</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mi>w</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mo>[</mo>
          <mrow>
            <mfrac>
              <msub>
                <mi>k</mi>
                <mn>1</mn>
              </msub>
              <mi>n</mi>
            </mfrac>
            <mo>,</mo>
            <mi>&#x2026;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>,</mo>
            <mfrac>
              <msub>
                <mi>k</mi>
                <mi>n</mi>
              </msub>
              <mi>n</mi>
            </mfrac>
          </mrow>
          <mo>]</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>10</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
such that the type t(w) represents a set of frequencies of its symbols (e.g., the frequencies of gradient distributions k<sub>i</sub>). A type can also be understood as an estimate of the true distribution of the source that produced the sample. Thus, encoding and transmission of type t(w) is equivalent to encoding and transmission of the shape of the distribution as it can be estimated based on a particular sample w.
</p>
<p id="p-0107" num="0106">However, traditional encoding techniques have focused on efficiently encoding a sequence of values. Because sequence information is not used in a histogram, efficiently encoding a histogram is a different problem than those addressed by traditional encoding techniques. Assuming the number of bins is known to the encoder and decoder, encoding of histograms involves encoding the total number of points (e.g., gradients) and the points per bin.</p>
<p id="p-0108" num="0107">Hereafter, one goal is to figure out how to encode type t(w) efficiently. Notice that any given type t may be defined as:</p>
<p id="p-0109" num="0108">
<maths id="MATH-US-00012" num="00012">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>t</mi>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mo>[</mo>
            <mrow>
              <mfrac>
                <msub>
                  <mi>k</mi>
                  <mn>1</mn>
                </msub>
                <mi>n</mi>
              </mfrac>
              <mo>,</mo>
              <mi>&#x2026;</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>,</mo>
              <mrow>
                <mrow>
                  <mfrac>
                    <msub>
                      <mi>k</mi>
                      <mi>n</mi>
                    </msub>
                    <mi>n</mi>
                  </mfrac>
                  <mo>&#x2062;</mo>
                  <munderover>
                    <mrow>
                      <mo>:</mo>
                      <mo>&#x2211;</mo>
                    </mrow>
                    <mrow>
                      <mi>i</mi>
                      <mo>=</mo>
                      <mn>1</mn>
                    </mrow>
                    <mi>m</mi>
                  </munderover>
                  <mo>&#x2062;</mo>
                  <msub>
                    <mi>k</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
                <mo>=</mo>
                <mi>n</mi>
              </mrow>
            </mrow>
            <mo>]</mo>
          </mrow>
          <mo>.</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>11</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where k<sub>1 </sub>to k<sub>m </sub>denote the number of possible types t given the total number of samples n. Therefore, the total number of possible sequences with type t can be given by:
</p>
<p id="p-0110" num="0109">
<maths id="MATH-US-00013" num="00013">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>&#x3be;</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mi>t</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mo>(</mo>
          <mtable>
            <mtr>
              <mtd>
                <mi>n</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <msub>
                    <mi>k</mi>
                    <mn>1</mn>
                  </msub>
                  <mo>,</mo>
                  <mi>&#x2026;</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>,</mo>
                  <msub>
                    <mi>k</mi>
                    <mi>m</mi>
                  </msub>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>12</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where &#x3be;(t) is total number of possible arrangements of symbols with a population t.
</p>
<p id="p-0111" num="0110">The total number of possible types is essentially the number of all integers k<sub>1</sub>, . . . , k<sub>m </sub>such that k<sub>1</sub>+ . . . +k<sub>m</sub>=n, and it is given by the multiset coefficient:</p>
<p id="p-0112" num="0111">
<maths id="MATH-US-00014" num="00014">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>M</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>m</mi>
              <mo>,</mo>
              <mi>n</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mo>(</mo>
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mi>n</mi>
                  <mo>+</mo>
                  <mi>m</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <mi>m</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>13</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0113" num="0112">The probability of occurrence of any sample w of type t may be denoted by P(t). Since there are &#x3be;(t) such possible samples, and they all have the same probabilities, then:</p>
<p id="p-0114" num="0113">
<maths id="MATH-US-00015" num="00015">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mi>P</mi>
                <mo>&#x2062;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>t</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>=</mo>
              <mi/>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>&#x3be;</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>t</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>P</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mrow>
                        <mi>w</mi>
                        <mo>:</mo>
                        <mrow>
                          <mi>t</mi>
                          <mo>&#x2061;</mo>
                          <mrow>
                            <mo>(</mo>
                            <mi>w</mi>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </mrow>
                      <mo>=</mo>
                      <mi>t</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mo>=</mo>
              <mi/>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mo>(</mo>
                  <mtable>
                    <mtr>
                      <mtd>
                        <mi>n</mi>
                      </mtd>
                    </mtr>
                    <mtr>
                      <mtd>
                        <mrow>
                          <msub>
                            <mi>k</mi>
                            <mn>1</mn>
                          </msub>
                          <mo>,</mo>
                          <mi>&#x2026;</mi>
                          <mo>&#x2062;</mo>
                          <mstyle>
                            <mspace width="0.8em" height="0.8ex"/>
                          </mstyle>
                          <mo>,</mo>
                          <msub>
                            <mi>k</mi>
                            <mi>m</mi>
                          </msub>
                        </mrow>
                      </mtd>
                    </mtr>
                  </mtable>
                  <mo>)</mo>
                </mrow>
                <mo>&#x2062;</mo>
                <msubsup>
                  <mi>p</mi>
                  <mn>1</mn>
                  <msub>
                    <mi>k</mi>
                    <mn>1</mn>
                  </msub>
                </msubsup>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mi>&#x2026;</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <msubsup>
                  <mi>p</mi>
                  <mi>m</mi>
                  <msub>
                    <mi>k</mi>
                    <mi>m</mi>
                  </msub>
                </msubsup>
              </mrow>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>14</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
This density P(t) may be referred to as a distribution of types. It is clearly a multinomial distribution, with maximum (mode) at:
</p>
<p id="p-0115" num="0114">
<maths id="MATH-US-00016" num="00016">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>P</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <msup>
              <mi>t</mi>
              <mo>*</mo>
            </msup>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mi>P</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>t</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>:</mo>
                  <msub>
                    <mi>k</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
                <mo>=</mo>
                <msub>
                  <mi>np</mi>
                  <mi>i</mi>
                </msub>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mo>(</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mi>n</mi>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mrow>
                      <msub>
                        <mi>np</mi>
                        <mn>1</mn>
                      </msub>
                      <mo>,</mo>
                      <mi>&#x2026;</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.8em" height="0.8ex"/>
                      </mstyle>
                      <mo>,</mo>
                      <msub>
                        <mi>np</mi>
                        <mi>m</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>)</mo>
            </mrow>
            <mo>&#x2062;</mo>
            <msubsup>
              <mi>p</mi>
              <mn>1</mn>
              <msub>
                <mi>np</mi>
                <mn>1</mn>
              </msub>
            </msubsup>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x2026;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.8em" height="0.8ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mrow>
              <msubsup>
                <mi>p</mi>
                <mi>m</mi>
                <msub>
                  <mi>np</mi>
                  <mi>m</mi>
                </msub>
              </msubsup>
              <mo>.</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>15</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
The entropy of distribution of types is subsequently (by concentration property):
</p>
<p id="p-0116" num="0115">
<maths id="MATH-US-00017" num="00017">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>H</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>P</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mo>-</mo>
            <mrow>
              <munder>
                <mo>&#x2211;</mo>
                <mi>t</mi>
              </munder>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>P</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>t</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mi>log</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mrow>
                    <mi>P</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>t</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>~</mo>
                  <mi>log</mi>
                </mrow>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>P</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <msup>
                        <mi>t</mi>
                        <mo>*</mo>
                      </msup>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mfrac>
                <mrow>
                  <mi>m</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
                <mn>2</mn>
              </mfrac>
              <mo>&#x2062;</mo>
              <mi>log</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>n</mi>
            </mrow>
            <mo>+</mo>
            <mrow>
              <mrow>
                <mi>O</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mn>1</mn>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>.</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>16</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0117" num="0116">Given a sample w of length n, the task of universal encoder is to design a code f(w) (or equivalently, its induced distribution P<sub>f</sub>(w)), such that its worst-case average redundancy:</p>
<p id="p-0118" num="0117">
<maths id="MATH-US-00018" num="00018">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>R</mi>
          <mo>*</mo>
          <mrow>
            <mo>(</mo>
            <mi>n</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <munder>
            <mi>sup</mi>
            <mi>P</mi>
          </munder>
          <mo>[</mo>
          <mrow>
            <mrow>
              <munder>
                <mo>&#x2211;</mo>
                <mrow>
                  <mrow>
                    <mo>&#xf603;</mo>
                    <mi>w</mi>
                    <mo>&#xf604;</mo>
                  </mrow>
                  <mo>=</mo>
                  <mi>n</mi>
                </mrow>
              </munder>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>P</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>w</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mrow>
                  <mo>&#xf603;</mo>
                  <mrow>
                    <mi>f</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>w</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>&#xf604;</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mrow>
              <mi>nH</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>P</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mo>]</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;</mi>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mn>17</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>&#x2265;</mo>
          <mi/>
          <mo>&#x2062;</mo>
          <mrow>
            <munder>
              <mi>sup</mi>
              <mi>P</mi>
            </munder>
            <mo>&#x2062;</mo>
            <mrow>
              <munder>
                <mo>&#x2211;</mo>
                <mrow>
                  <mrow>
                    <mo>&#xf603;</mo>
                    <mi>w</mi>
                    <mo>&#xf604;</mo>
                  </mrow>
                  <mo>=</mo>
                  <mi>n</mi>
                </mrow>
              </munder>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>P</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>w</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mi>log</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mfrac>
                  <mrow>
                    <mi>P</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>w</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mrow>
                    <msub>
                      <mi>P</mi>
                      <mi>f</mi>
                    </msub>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>w</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mfrac>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mi>n</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.3em" height="0.3ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <munder>
            <mi>sup</mi>
            <mi>P</mi>
          </munder>
          <mo>&#x2062;</mo>
          <mrow>
            <mi>D</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>P</mi>
                <mo>||</mo>
                <msub>
                  <mi>P</mi>
                  <mi>f</mi>
                </msub>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mn>18</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
is minimal. Equations 17 and 18 describe the problem being addressed by universal coding, which given a sequence, a code length is sought where the difference between an average code length and n*H(P) is minimal for all possible input distributions. That is, the minimum worst-case code length is sought without knowing the distribution beforehand.
</p>
<p id="p-0119" num="0118">Since probabilities of samples of the same type are the same, and code induced distribution P<sub>f</sub>(w) is expected to retain this property, P<sub>f</sub>(w) can be defined as:</p>
<p id="p-0120" num="0119">
<maths id="MATH-US-00019" num="00019">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <msub>
              <mi>P</mi>
              <mi>f</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>w</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mfrac>
            <mrow>
              <msub>
                <mi>P</mi>
                <mi>f</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>w</mi>
                    <mo>:</mo>
                    <mrow>
                      <mi>t</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mi>w</mi>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>=</mo>
                  <mi>t</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mrow>
              <mi>&#x3be;</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>t</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mfrac>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>19</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where P<sub>f</sub>(t) is the probability of a type t(w) and &#x3be;(t) is the total number of sequences within the same type t(w). The probability P<sub>f </sub>of a code assigned to a type t(w) can thus be defined as:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>f</sub>(<i>t</i>)=&#x3be;(<i>t</i>)<i>P</i><sub>f</sub>(<i>w:t</i>(<i>w</i>)=<i>t</i>)&#x2003;&#x2003;(20)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
is code-induced distribution of types.
</p>
<p id="p-0121" num="0120">By plugging such decomposition in Equation 18 and changing the summation to go over types (instead of individual samples), the average redundancy R*(n) may be defined as:</p>
<p id="p-0122" num="0121">
<maths id="MATH-US-00020" num="00020">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>R</mi>
          <mo>*</mo>
          <mrow>
            <mo>(</mo>
            <mi>n</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>&#x2265;</mo>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <munder>
            <mi>sup</mi>
            <mi>P</mi>
          </munder>
          <mo>&#x2062;</mo>
          <mrow>
            <munder>
              <mo>&#x2211;</mo>
              <mrow>
                <mo>|</mo>
                <mrow>
                  <mi>w</mi>
                  <mo>&#x2208;</mo>
                  <msup>
                    <mi>A</mi>
                    <mi>n</mi>
                  </msup>
                </mrow>
              </mrow>
            </munder>
            <mo>&#x2062;</mo>
            <mrow>
              <mrow>
                <mi>P</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>w</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>&#x2062;</mo>
              <mi>log</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mfrac>
                <mrow>
                  <mi>P</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>w</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mrow>
                  <msub>
                    <mi>P</mi>
                    <mi>f</mi>
                  </msub>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>w</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mfrac>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi>&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;</mi>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mn>21.1</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mo>=</mo>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <munder>
            <mi>sup</mi>
            <mi>P</mi>
          </munder>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>[</mo>
            <mrow>
              <munder>
                <mo>&#x2211;</mo>
                <mi>t</mi>
              </munder>
              <mo>&#x2062;</mo>
              <mrow>
                <munder>
                  <mo>&#x2211;</mo>
                  <mrow>
                    <mrow>
                      <mi>w</mi>
                      <mo>:</mo>
                      <mrow>
                        <mi>t</mi>
                        <mo>&#x2061;</mo>
                        <mrow>
                          <mo>(</mo>
                          <mi>w</mi>
                          <mo>)</mo>
                        </mrow>
                      </mrow>
                    </mrow>
                    <mo>=</mo>
                    <mi>t</mi>
                  </mrow>
                </munder>
                <mo>&#x2062;</mo>
                <mrow>
                  <mrow>
                    <mi>P</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>w</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>&#x2062;</mo>
                  <mi>log</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.6em" height="0.6ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mfrac>
                    <mrow>
                      <mi>P</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mi>t</mi>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mrow>
                      <msub>
                        <mi>P</mi>
                        <mi>f</mi>
                      </msub>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mi>t</mi>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mfrac>
                </mrow>
              </mrow>
            </mrow>
            <mo>]</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mn>21.2</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mo>=</mo>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <munder>
            <mi>sup</mi>
            <mi>p</mi>
          </munder>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>[</mo>
            <mrow>
              <munder>
                <mo>&#x2211;</mo>
                <mi>t</mi>
              </munder>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <mi>P</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mi>t</mi>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>&#x2062;</mo>
                <mi>log</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.3em" height="0.3ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mfrac>
                  <mrow>
                    <mi>P</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>t</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mrow>
                    <msub>
                      <mi>P</mi>
                      <mi>f</mi>
                    </msub>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>t</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mfrac>
              </mrow>
            </mrow>
            <mo>]</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mn>21.3</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>=</mo>
          <mi/>
          <mo>&#x2062;</mo>
          <mrow>
            <munder>
              <mi>sup</mi>
              <mi>P</mi>
            </munder>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>D</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>P</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>t</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>||</mo>
                  <mrow>
                    <msub>
                      <mi>P</mi>
                      <mi>f</mi>
                    </msub>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>t</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mi/>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>(</mo>
          <mn>21.4</mn>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
where &#x201c;sup&#x201d; is the supreme operator, where a value is a supreme with respect to a set if it is at least as large as any element of that set. These equations mean that the problem of coding of types is equivalent to the problem of minimum redundancy universal coding.
</p>
<p id="p-0123" num="0122">Consequently, the problem of lossless coding of types can be asymptotically optimally solved by using KT-estimated distribution of types:</p>
<p id="p-0124" num="0123">
<maths id="MATH-US-00021" num="00021">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>P</mi>
            <mi>KT</mi>
          </msub>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mi>t</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mi>&#x3be;</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>&#x2062;</mo>
          <mrow>
            <msub>
              <mi>P</mi>
              <mi>KT</mi>
            </msub>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <mi>w</mi>
                  <mo>:</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mi>w</mi>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>=</mo>
                <mi>t</mi>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>22.1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mstyle>
          <mspace width="3.9em" height="3.9ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mrow>
          <mo>=</mo>
          <mrow>
            <mrow>
              <mo>(</mo>
              <mtable>
                <mtr>
                  <mtd>
                    <mi>n</mi>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mrow>
                      <msub>
                        <mi>k</mi>
                        <mn>1</mn>
                      </msub>
                      <mo>,</mo>
                      <mi>&#x2026;</mi>
                      <mo>&#x2062;</mo>
                      <mstyle>
                        <mspace width="0.8em" height="0.8ex"/>
                      </mstyle>
                      <mo>,</mo>
                      <msub>
                        <mi>k</mi>
                        <mi>m</mi>
                      </msub>
                    </mrow>
                  </mtd>
                </mtr>
              </mtable>
              <mo>)</mo>
            </mrow>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>&#x393;</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mfrac>
                  <mi>m</mi>
                  <mn>2</mn>
                </mfrac>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>&#x2062;</mo>
            <mfrac>
              <mrow>
                <munderover>
                  <mo>&#x220f;</mo>
                  <mrow>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>m</mi>
                </munderover>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>&#x393;</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msub>
                        <mi>k</mi>
                        <mn>1</mn>
                      </msub>
                      <mo>+</mo>
                      <mfrac>
                        <mn>1</mn>
                        <mn>2</mn>
                      </mfrac>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mrow>
                <msup>
                  <mi>&#x3c0;</mi>
                  <mfrac>
                    <mi>m</mi>
                    <mn>2</mn>
                  </mfrac>
                </msup>
                <mo>&#x2062;</mo>
                <mrow>
                  <mi>&#x393;</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>n</mi>
                      <mo>+</mo>
                      <mfrac>
                        <mi>m</mi>
                        <mn>2</mn>
                      </mfrac>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mfrac>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>22.2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
Based on this Equation 22.2, it becomes clear that types with near uniform populations fall in the valleys of the estimated density, while types with singular populations (ones with zero counts) become its peaks.
</p>
<p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. 5</figref> is a line drawing of a chart <b>180</b> illustrating an example keypoint distribution across various Gaussian scale spaces. The data for graph <b>180</b> was generated from a set of training images, namely, 105 images with 700 by 1000 pixel resolution. For each image, data was collected for scales of 1.0 (graph <b>186</b>), 0.75 (graph <b>188</b>), 0.5 (graph <b>190</b>), and 0.25 (graph <b>192</b>). Chart <b>180</b> shows a typical distribution of keypoints across the scale space. Based on this observation, control unit <b>20</b> (<figref idref="DRAWINGS">FIG. 1</figref>) may calculate a statistic that allows estimation of the scaling factor of an image.</p>
<p id="p-0126" num="0125">X-axis <b>184</b> of graph <b>180</b> represents the possible scale indexes and corresponding scales in the Gaussian scale space (GSS) for this particular example set of data. Y-axis <b>182</b> represents the number of keypoints for this example.</p>
<p id="p-0127" num="0126">For a natural image, there may be a distribution of the detected key points with respect to the scale parameter along the Gaussian scale space, as shown. Keypoints detected at higher scales generally correspond to larger features. As the image resolution decreases (as the user goes farther away from target, for example), finer details are reduced and details which were detected at higher scales (corresponding to coarse details) move to lower scales (corresponding to finer details). Most of the keypoints, therefore, will fall in the lower scales. A histogram of keypoints built with the scale space levels as bin centers may have the characteristic that as the image size decreases, most of the area of the histogram may be concentrated in the lower bin centers. In effect, this means that the distribution of keypoints in scale space is texture-dependent, which in turns changes according to the distance of the camera from the object.</p>
<p id="p-0128" num="0127">A number of objects can appear in a given scene, each at its own distance from the camera and therefore at a unique scale. In order to properly assess the scale associated with an object with which the user wants to interact, some rough segmentation may be performed to isolate the object from others in the image, and to make localized inferences about the scale of the object. This can be accomplished, for example, by using a dynamic window, or other algorithms that approximately delineate boundaries between object regions.</p>
<p id="p-0129" num="0128">In order to calculate a cutoff scale that can be used to determine a minimum octave of a database to search, scale estimation unit <b>24</b> may estimate the distribution of keypoints over scale space levels s, with a histogram of keypoints P(s). Next, scale estimation unit <b>24</b> may estimate the cutoff scale s<sub>c </sub>such that approximately 90% of the keypoints are preserved within the scale levels up to s<sub>c</sub>. Formally, scale estimation unit <b>24</b> may calculate formula (23) below:</p>
<p id="p-0130" num="0129">
<maths id="MATH-US-00022" num="00022">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>s</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <msub>
              <mi>S</mi>
              <mi>c</mi>
            </msub>
          </munderover>
          <mo>&#x2062;</mo>
          <mrow>
            <mi>P</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>s</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mn>0.9</mn>
          <mo>*</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>s</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>S</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>P</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>s</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>23</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
Then, scale estimation unit <b>24</b> may estimate a function that maps the tracked scale space cutoff level to an image scaling factor, Y. That is, scale estimation unit <b>24</b> may calculate:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Y=f</i>(<i>s</i><sub>c</sub>)&#x2003;&#x2003;(24)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where Y is the image scaling factor and s<sub>c </sub>is the cutoff scale space level. To learn the function, scale estimation unit <b>24</b> may pick Y and s<sub>c </sub>data points from the most repeated path, e.g., one of graphs <b>186</b>, <b>188</b>, <b>190</b>, or <b>192</b>.
</p>
<p id="p-0131" num="0130">In general, as the image resolution increases, s<sub>c </sub>moves further up in the scale space, and vice versa. Graph <b>180</b> also illustrates example cutoff scales (corresponding to minimum octaves) for scales of 1.0 (point <b>198</b>), 0.75 (point <b>196</b>), and 0.5 (point <b>194</b>) in this example data set. It can be seen from this example that as the resolution decreases, the cutoff scale moves towards the left. That is, as the resolution decreases, the cutoff scale decreases.</p>
<p id="p-0132" num="0131"><figref idref="DRAWINGS">FIG. 6</figref> is a flowchart illustrating an example method for performing incremental feature descriptor extraction. For purposes of explanation, the method of <figref idref="DRAWINGS">FIG. 6</figref> is described with respect to the components of client device <b>10</b>. However, it should be understood that the method of <figref idref="DRAWINGS">FIG. 6</figref> may be performed by other devices, or performed jointly by various devices. For example, control unit <b>60</b> of server device <b>50</b> (<figref idref="DRAWINGS">FIG. 1</figref>) may be configured to perform the method of <figref idref="DRAWINGS">FIG. 6</figref>. Likewise, additional steps may be performed, or certain steps omitted, and steps of the method may be performed in a different order (or in parallel) without departing from the techniques of this disclosure.</p>
<p id="p-0133" num="0132">In the example of <figref idref="DRAWINGS">FIG. 6</figref>, initially, control unit <b>20</b> receives an image (also referred to as a query image) of an object (<b>200</b>). For example, control unit <b>20</b> may receive the image from camera <b>12</b>. Alternatively, control unit <b>20</b> may retrieve a stored image from a memory of client device <b>10</b>. Feature extraction unit <b>22</b> of control unit <b>20</b> may then select a first octave for the image from which to extract feature descriptors (<b>202</b>).</p>
<p id="p-0134" num="0133">Feature extraction unit <b>22</b> may select the first octave using any of a variety of techniques, including combinations of techniques, as discussed in this disclosure. For example, feature extraction unit <b>22</b> may receive data from sensors <b>14</b>, which feature extraction unit <b>22</b> may use to select the first octave. Feature extraction unit <b>22</b> may, for example, receive location data from a GPS unit of sensors <b>14</b>, indicative of a location of client device <b>10</b>. Feature extraction unit <b>22</b> may use the location data to determine locations of objects near client device <b>10</b>. Feature extraction unit <b>22</b> may use objects near client device <b>10</b> to approximate a scale for an object in the image.</p>
<p id="p-0135" num="0134">For example, if the GPS data indicates that client device <b>10</b> is in Paris, France and near the Eiffel tower, then feature extraction unit <b>22</b> may determine that the likely scale of the object in the image is relatively large. This determination may be based on an assumption (through configuration data) that the image is of the Eiffel tower, which may be confirmed through feature descriptor extraction and comparison. On the other hand, if the GPS data indicates that client device <b>10</b> is in Paris, France and near or inside the Louvre, then feature extraction unit <b>22</b> may determine that the likely scale of the object in the image is relatively small. This determination may be based on an assumption (through configuration data) that the image is of a work kept in the Louvre, in this example.</p>
<p id="p-0136" num="0135">In addition to, or in the alternative to, location data from a GPS unit or other types of sensed data, control unit <b>20</b> may select the first octave based on a depth map calculated by depth estimation unit <b>26</b>. Depth estimation unit <b>26</b> may use any of a variety of techniques for calculating the depth map. For example, depth estimation unit <b>26</b> may analyze horizontal disparity (or parallax) between pixels of two or more images captured by stereo cameras. One of these images may correspond to the query image. Based on depth for an object in the query image, control unit <b>20</b> may determine a scale for the object, which may correspond to the first octave. Other factors that may influence the first octave include, for example, a loxel in which the object is present in the image, whether a wireless network is available (e.g., whether a wireless access point is in range), whether a scale had been determined for a previous, recent query image that the tracker since lost, or other factors.</p>
<p id="p-0137" num="0136">After determining the first octave, feature extraction unit <b>22</b> may extract keypoints for the first octave (<b>204</b>). As discussed above, feature extraction unit <b>22</b> may calculate an image for the first octave from the query image by scaling the resolution of the query image to the size corresponding to the first octave, as selected above. Feature extraction unit <b>22</b> may then apply Gaussian blur filters to the first octave image of varying degrees, and calculate a difference of Gaussian pyramid from which to extract the keypoints. Using the keypoints, feature extraction unit <b>22</b> may extract one or more feature descriptors for the first octave of the query image (<b>206</b>).</p>
<p id="p-0138" num="0137">Feature extraction unit <b>22</b> may then attempt to determine the identity of the object in the query image using the feature descriptors (<b>208</b>). In some examples, control unit <b>20</b> may retrieve all or a portion of remote object database <b>52</b> and store the data locally as local object database <b>30</b>. In other examples, control unit <b>20</b> may send one or more of the extracted feature descriptors to server device <b>50</b> via network <b>40</b>. Feature matching unit <b>62</b> may determine whether any or all of the feature descriptors received from client device <b>10</b> match an object of remote object database <b>52</b>. Feature matching unit <b>62</b> may determine one or more matches to the feature descriptors. If feature matching unit <b>62</b> has received an indication of a minimum octave to search, feature matching unit <b>62</b> may search only feature descriptors for octaves at or above the minimum octave, but not below the specified minimum octave. Confidence valuation unit <b>64</b> may then determine a confidence value for each of the matches, representative of how well the matches fit the feature descriptors. Control unit <b>60</b> may then send information including identifiers for each of the determined matches and the respective confidence values to client device <b>10</b> via network <b>40</b>.</p>
<p id="p-0139" num="0138">After receiving the information from server device <b>50</b>, control unit <b>20</b> of client device <b>10</b> may determine whether any of the confidence values for the various determined identities of objects exceeds a threshold value (<b>210</b>). For example, control unit <b>20</b> may determine whether a largest confidence value exceeds the threshold. If the largest confidence value does not exceed the threshold (&#x201c;NO&#x201d; branch of <b>210</b>), control unit <b>20</b> may determine a next octave from which to extract feature descriptors, as described below, as well as a cutoff scale (also referred to as a minimum octave) of the database being queried in some examples.</p>
<p id="p-0140" num="0139">Scale estimation unit <b>24</b> may determine which scale fits the keypoint distribution most closely, then determine that the best fitting scale is most likely the scale of the object (<b>212</b>). Based on this approximation of the scale, feature extraction unit <b>22</b> may select a minimum octave in the database to which to compare extracted feature descriptors (<b>214</b>). Feature extraction unit <b>22</b> may also select a next octave from which to extract keypoints (<b>216</b>). Feature extraction unit <b>22</b> may then extract keypoints for this next determined octave (<b>218</b>) and use the extracted keypoints to extract feature descriptors for the current octave (<b>206</b>). Feature extraction unit <b>22</b> may then again determine an object identity using the extracted feature descriptors (<b>208</b>). In particular, feature extraction unit <b>22</b> may cause the database to search objects in octaves of the database at or above the selected minimum octave using the extracted feature descriptors, and receive a new confidence value in a determined identity of the object.</p>
<p id="p-0141" num="0140">When the confidence value in the determined identity of the object exceeds the threshold (&#x201c;YES&#x201d; branch of <b>210</b>), feature extraction unit <b>22</b> may output identity information for the object in the image (<b>218</b>). In some examples, feature extraction unit <b>22</b> may interact with a web browser executed by control unit <b>20</b>, to cause the web browser to submit a search query to an Internet-based search engine, using the determined identity as a search term or keyword. The search may cause the web browser to display various information for the object in the image, such as, for example, another image of the object, location information for the object (e.g., if the object is a landmark), price information (e.g., if the object or a replica thereof can be purchased), a link to a web address (e.g., uniform resource locator (URL)) with more information for the object, or other such information.</p>
<p id="p-0142" num="0141">In this manner, <figref idref="DRAWINGS">FIG. 6</figref> represents an example of a method including extracting a first set of one or more keypoints from a first set of blurred images of a first octave of a received image, calculating a first set of one or more descriptors for the first set of keypoints, receiving a confidence value for a result produced by querying a feature descriptor database with the first set of descriptors, wherein the result comprises information describing an identity of an object in the received image, and extracting a second set of one or more keypoints from a second set of blurred images of a second octave of the received image when the confidence value does not exceed a confidence threshold.</p>
<p id="p-0143" num="0142">In one or more examples, the functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored on or transmitted over as one or more instructions or code on a computer-readable medium and executed by a hardware-based processing unit. Computer-readable media may include computer-readable storage media, which corresponds to a tangible medium such as data storage media, or communication media including any medium that facilitates transfer of a computer program from one place to another, e.g., according to a communication protocol. In this manner, computer-readable media generally may correspond to (1) tangible computer-readable storage media which is non-transitory or (2) a communication medium such as a signal or carrier wave. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and/or data structures for implementation of the techniques described in this disclosure. A computer program product may include a computer-readable medium.</p>
<p id="p-0144" num="0143">By way of example, and not limitation, tangible computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also, any connection is properly termed a computer-readable medium. For example, if instructions are transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. It should be understood, however, that computer-readable storage media and data storage media do not include connections, carrier waves, signals, or other transitory media, but are instead directed to non-transitory, tangible storage media. Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.</p>
<p id="p-0145" num="0144">Instructions may be executed by one or more processors, such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Accordingly, the term &#x201c;processor,&#x201d; as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition, in some aspects, the functionality described herein may be provided within dedicated hardware and/or software modules configured for encoding and decoding, or incorporated in a combined codec. Also, the techniques could be fully implemented in one or more circuits or logic elements.</p>
<p id="p-0146" num="0145">The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a wireless handset, an integrated circuit (IC) or a set of ICs (e.g., a chip set). Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units. Rather, as described above, various units may be combined in a codec hardware unit or provided by a collection of interoperative hardware units, including one or more processors as described above, in conjunction with suitable software and/or firmware.</p>
<p id="p-0147" num="0146">Various examples have been described. These and other examples are within the scope of the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625902-20140107-M00001.NB">
<img id="EMI-M00001" he="5.33mm" wi="76.20mm" file="US08625902-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625902-20140107-M00002.NB">
<img id="EMI-M00002" he="7.03mm" wi="76.20mm" file="US08625902-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08625902-20140107-M00003.NB">
<img id="EMI-M00003" he="8.47mm" wi="76.20mm" file="US08625902-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08625902-20140107-M00004.NB">
<img id="EMI-M00004" he="7.03mm" wi="76.20mm" file="US08625902-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08625902-20140107-M00005.NB">
<img id="EMI-M00005" he="8.47mm" wi="76.20mm" file="US08625902-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08625902-20140107-M00006.NB">
<img id="EMI-M00006" he="8.47mm" wi="76.20mm" file="US08625902-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08625902-20140107-M00007.NB">
<img id="EMI-M00007" he="8.47mm" wi="76.20mm" file="US08625902-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US08625902-20140107-M00008.NB">
<img id="EMI-M00008" he="14.14mm" wi="76.20mm" file="US08625902-20140107-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US08625902-20140107-M00009.NB">
<img id="EMI-M00009" he="7.79mm" wi="76.20mm" file="US08625902-20140107-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00010" nb-file="US08625902-20140107-M00010.NB">
<img id="EMI-M00010" he="6.35mm" wi="76.20mm" file="US08625902-20140107-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00011" nb-file="US08625902-20140107-M00011.NB">
<img id="EMI-M00011" he="6.69mm" wi="76.20mm" file="US08625902-20140107-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00012" nb-file="US08625902-20140107-M00012.NB">
<img id="EMI-M00012" he="8.47mm" wi="76.20mm" file="US08625902-20140107-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00013" nb-file="US08625902-20140107-M00013.NB">
<img id="EMI-M00013" he="7.45mm" wi="76.20mm" file="US08625902-20140107-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00014" nb-file="US08625902-20140107-M00014.NB">
<img id="EMI-M00014" he="7.45mm" wi="76.20mm" file="US08625902-20140107-M00014.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00015" nb-file="US08625902-20140107-M00015.NB">
<img id="EMI-M00015" he="11.26mm" wi="76.20mm" file="US08625902-20140107-M00015.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00016" nb-file="US08625902-20140107-M00016.NB">
<img id="EMI-M00016" he="7.45mm" wi="76.20mm" file="US08625902-20140107-M00016.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00017" nb-file="US08625902-20140107-M00017.NB">
<img id="EMI-M00017" he="7.79mm" wi="76.20mm" file="US08625902-20140107-M00017.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00018" nb-file="US08625902-20140107-M00018.NB">
<img id="EMI-M00018" he="15.49mm" wi="76.20mm" file="US08625902-20140107-M00018.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00019" nb-file="US08625902-20140107-M00019.NB">
<img id="EMI-M00019" he="7.03mm" wi="76.20mm" file="US08625902-20140107-M00019.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00020" nb-file="US08625902-20140107-M00020.NB">
<img id="EMI-M00020" he="32.43mm" wi="76.20mm" file="US08625902-20140107-M00020.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00021" nb-file="US08625902-20140107-M00021.NB">
<img id="EMI-M00021" he="19.05mm" wi="76.20mm" file="US08625902-20140107-M00021.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00022" nb-file="US08625902-20140107-M00022.NB">
<img id="EMI-M00022" he="9.14mm" wi="76.20mm" file="US08625902-20140107-M00022.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of determining identities for objects in an image, the method comprising:
<claim-text>extracting a first set of one or more keypoints from a first set of blurred images of a first octave of a received image;</claim-text>
<claim-text>calculating a first set of one or more descriptors for the first set of keypoints;</claim-text>
<claim-text>receiving a confidence value for a result produced by querying a feature descriptor database with the first set of descriptors, wherein the result comprises information describing an identity of an object in the received image; and</claim-text>
<claim-text>extracting a second set of one or more keypoints from a second set of blurred images of a second octave of the received image when the confidence value does not exceed a confidence threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>determining that the confidence value does not exceed the confidence threshold;</claim-text>
<claim-text>extracting the second set of keypoints from a second set of blurred images of the second octave based on the determination;</claim-text>
<claim-text>calculating a second set of one or more descriptors from the second set of keypoints; and</claim-text>
<claim-text>querying the feature descriptor database with a set of descriptors comprising the first set of descriptors and the second set of descriptors.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>estimating a scale value for the object in the received image;</claim-text>
<claim-text>selecting a minimum octave of the feature descriptor database based on the estimated scale; and</claim-text>
<claim-text>providing an indication of a minimum octave in the feature descriptor database to cause a search of the feature descriptor database to search feature descriptors at or above the selected minimum octave in the feature descriptor database.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein estimating the scale value comprises:
<claim-text>analyzing a distribution of the first set of keypoints across a scale space;</claim-text>
<claim-text>determining a cutoff point corresponding to a scale level in the distribution such that approximately 90% of the keypoints fall below the scale level; and</claim-text>
<claim-text>estimating the scale value as a function of the determined cutoff point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising obtaining depth information for the object, wherein estimating the scale value comprises estimating the scale value based at least in part on the depth information for the object.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>analyzing sensor information from one or more sensors associated with a camera that captured the received image; and</claim-text>
<claim-text>determining an octave level for at least one of the first octave and the second octave based on the analysis of the sensor information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein analyzing the sensor information comprises analyzing global positioning system (GPS) information to determine whether the camera was located in an outdoor environment when the received image was captured.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein analyzing the sensor information comprises analyzing GPS information to determine locations of objects relatively near the camera when the received image was captured, and determining sizes for the objects from descriptive data for the objects.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein analyzing the sensor information comprises determining whether the camera was located in an indoor environment based on network data indicative of whether a device comprising the camera was communicatively coupled to a wireless network when the received image was captured.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein analyzing the sensor information comprises calculating depth information indicative of distances between one or more objects in the received image and the camera when the received image was captured.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein analyzing sensor information comprises estimating a depth value for the object using data provided by an active probing sensor.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising sending the one or more descriptors to a server to cause the server to query the feature descriptor database using the one or more descriptors, wherein receiving the confidence value comprises receiving the confidence value from the server in response to the query.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. An apparatus for determining identities for objects in an image, the apparatus comprising a processor configured to extract a first set of one or more keypoints from a first set of blurred images of a first octave of a received image, calculate a first set of one or more descriptors for the first set of keypoints, receive a confidence value for a result produced by querying a feature descriptor database with the first set of descriptors, wherein the result comprises information describing an identity of an object in the received image, and extract a second set of one or more keypoints from a second set of blurred images of a second octave of the received image when the confidence value does not exceed a confidence threshold.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising a camera configured to capture the image and to provide the image to the processor.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor is further configured, when the processer determines that the confidence value does not exceed the confidence threshold, to extract the second set of keypoints from a second set of blurred images of the second octave based on the determination, calculate a second set of one or more descriptors from the second set of keypoints, and query the feature descriptor database with a set of descriptors comprising the first set of descriptors and the second set of descriptors.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the processor is further configured to estimate a scale value for the object in the received image, select a minimum octave of the feature descriptor database based on the estimated scale, and search descriptors in the feature descriptor database, wherein the searched descriptors correspond to octaves in the feature descriptor database at or above the selected minimum octave.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein to estimate the scale value, the processor is configured to analyze a distribution of the first set of keypoints across a scale space, determine a cutoff point corresponding to a scale level in the distribution such that approximately 90% of the keypoints fall below the scale level, and calculate the scale value as a function of the determined cutoff point.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The apparatus of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising a depth estimation unit configured to obtain depth information for the object, wherein the processor is configured to estimate the scale value based at least in part on the depth information for the object.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor is further configured to analyze sensor information from one or more sensors associated with a camera that captured the received image, and determine an octave level for at least one of the first octave and the second octave based on the analysis of the sensor information.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The apparatus of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising a global positioning system (GPS) unit configured to determine location information for the apparatus, wherein to analyze the sensor information, the processor is configured to determine whether the camera was located in an outdoor environment when the received image was captured based on the location information determined by the GPS unit.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The apparatus of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising a GPS unit configured to determine location information relative to the apparatus, wherein to analyze the sensor information, the processor is configured to analyze the location information determined by the GPS unit to determine locations of objects relatively near the camera when the received image was captured, and to determine sizes for the objects from descriptive data for the objects.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The apparatus of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising a wireless network interface, wherein to analyze the sensor information, the processor is configured to determine whether the camera was located in an indoor environment based on network data indicative of whether the wireless network interface was communicatively coupled to a wireless network when the received image was captured.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The apparatus of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising a camera array comprising at least two cameras including the camera that captured the received image, wherein to analyze the sensor information, the processor is configured to calculate depth information from images captured by the camera array indicative of distances between one or more objects in the received image and the camera when the received image was captured.</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The apparatus of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the one or more sensors comprise an active probing sensor configured to estimate a depth value for the object.</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text>25. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor is configured to send the one or more descriptors to a server to cause the server to query the feature descriptor database using the one or more descriptors, and to receive the confidence value from the server in response to the query.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text>26. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the apparatus comprises at least one of:
<claim-text>an integrated circuit;</claim-text>
<claim-text>a microprocessor; and</claim-text>
<claim-text>a wireless communication device that includes the processor.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text>27. An apparatus for determining identities for objects in an image, the apparatus comprising:
<claim-text>means for extracting a first set of one or more keypoints from a first set of blurred images of a first octave of a received image;</claim-text>
<claim-text>means for calculating a first set of one or more descriptors for the first set of keypoints;</claim-text>
<claim-text>means for receiving a confidence value for a result produced by querying a feature descriptor database with the first set of descriptors, wherein the result comprises information describing an identity of an object in the received image; and</claim-text>
<claim-text>means for extracting a second set of one or more keypoints from a second set of blurred images of a second octave of the received image when the confidence value does not exceed a confidence threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00028" num="00028">
<claim-text>28. The apparatus of <claim-ref idref="CLM-00027">claim 27</claim-ref>, further comprising:
<claim-text>means for determining that the confidence value does not exceed the confidence threshold;</claim-text>
<claim-text>means for extracting the second set of keypoints from a second set of blurred images of the second octave based on the determination;</claim-text>
<claim-text>means for calculating a second set of one or more descriptors from the second set of keypoints; and</claim-text>
<claim-text>means for querying the feature descriptor database with a set of descriptors comprising the first set of descriptors and the second set of descriptors.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00029" num="00029">
<claim-text>29. The apparatus of <claim-ref idref="CLM-00028">claim 28</claim-ref>, further comprising:
<claim-text>means for estimating a scale value for the object in the received image;</claim-text>
<claim-text>means for selecting a minimum octave of the feature descriptor database based on the estimated scale; and</claim-text>
<claim-text>means for providing an indication of a minimum octave in the feature descriptor database to cause a search of the feature descriptor database to search feature descriptors at or above the selected minimum octave in the feature descriptor database.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00030" num="00030">
<claim-text>30. The apparatus of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the means for estimating the scale value comprises:
<claim-text>means for analyzing a distribution of the first set of keypoints across a scale space;</claim-text>
<claim-text>means for determining a cutoff point corresponding to a scale level in the distribution such that approximately 90% of the keypoints fall below the scale level; and</claim-text>
<claim-text>means for estimating the scale value as a function of the determined cutoff point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00031" num="00031">
<claim-text>31. The apparatus of <claim-ref idref="CLM-00029">claim 29</claim-ref>, further comprising means for obtaining depth information for the object, wherein the means for estimating the scale value comprises means for estimating the scale value based at least in part on the depth information for the object.</claim-text>
</claim>
<claim id="CLM-00032" num="00032">
<claim-text>32. The apparatus of <claim-ref idref="CLM-00027">claim 27</claim-ref>, further comprising:
<claim-text>means for analyzing sensor information from one or more sensors associated with a camera that captured the received image; and</claim-text>
<claim-text>means for determining an octave level for at least one of the first octave and the second octave based on the analysis of the sensor information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00033" num="00033">
<claim-text>33. The apparatus of <claim-ref idref="CLM-00032">claim 32</claim-ref>, further comprising means for receiving global positioning system (GPS) information, wherein the means for analyzing the sensor information comprises means for analyzing the GPS information to determine whether the camera was located in an outdoor environment when the received image was captured.</claim-text>
</claim>
<claim id="CLM-00034" num="00034">
<claim-text>34. The apparatus of <claim-ref idref="CLM-00032">claim 32</claim-ref>, further comprising means for receiving GPS information, wherein the means for analyzing the sensor information comprises:
<claim-text>means for analyzing the GPS information to determine locations of objects relatively near the camera when the received image was captured; and</claim-text>
<claim-text>means for determining sizes for the objects from descriptive data for the objects.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00035" num="00035">
<claim-text>35. The apparatus of <claim-ref idref="CLM-00032">claim 32</claim-ref>, further comprising means for communicating via at least one wireless network protocol, wherein the means for analyzing the sensor information comprises means for determining whether the camera was located in an indoor environment based on network data indicative of whether the means for communicating via the wireless network protocol was communicatively coupled to a wireless network when the received image was captured.</claim-text>
</claim>
<claim id="CLM-00036" num="00036">
<claim-text>36. The apparatus of <claim-ref idref="CLM-00032">claim 32</claim-ref>, further comprising means for capturing two or more images of a scene, wherein one of the two or more images comprises the received image, and wherein the means for analyzing the sensor information comprises means for calculating depth information from the two or more images of the scene indicative of distances between one or more objects in the received image and the camera when the received image was captured.</claim-text>
</claim>
<claim id="CLM-00037" num="00037">
<claim-text>37. The apparatus of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the means for analyzing the sensor information comprises means for estimating a depth value for the object using data provided by an active probing sensor.</claim-text>
</claim>
<claim id="CLM-00038" num="00038">
<claim-text>38. The apparatus of <claim-ref idref="CLM-00027">claim 27</claim-ref>, further comprising means for sending the one or more descriptors to a server to cause the server to query the feature descriptor database using the one or more descriptors, wherein the means for receiving the confidence value comprises means for receiving the confidence value from the server in response to the query.</claim-text>
</claim>
<claim id="CLM-00039" num="00039">
<claim-text>39. A computer program product comprising a non-transitory computer-readable medium having stored thereon instructions that, when executed, cause a processor to:
<claim-text>extract a first set of one or more keypoints from a first set of blurred images of a first octave of a received image;</claim-text>
<claim-text>calculate a first set of one or more descriptors for the first set of keypoints;</claim-text>
<claim-text>receive a confidence value for a result produced by querying a feature descriptor database with the first set of descriptors, wherein the result comprises information describing an identity of an object in the received image; and</claim-text>
<claim-text>extract a second set of one or more keypoints from a second set of blurred images of a second octave of the received image when the confidence value does not exceed a confidence threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00040" num="00040">
<claim-text>40. The computer program product of <claim-ref idref="CLM-00039">claim 39</claim-ref>, further comprising instructions that cause the processor to:
<claim-text>determine that the confidence value does not exceed the confidence threshold;</claim-text>
<claim-text>extract the second set of keypoints from a second set of blurred images of the second octave based on the determination;</claim-text>
<claim-text>calculate a second set of one or more descriptors from the second set of keypoints; and</claim-text>
<claim-text>query the feature descriptor database with a set of descriptors comprising the first set of descriptors and the second set of descriptors.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00041" num="00041">
<claim-text>41. The computer program product of <claim-ref idref="CLM-00040">claim 40</claim-ref>, further comprising instructions that cause the processor to:
<claim-text>estimate a scale value for the object in the received image;</claim-text>
<claim-text>select a minimum octave of the feature descriptor database based on the estimated scale; and</claim-text>
<claim-text>provide an indication of a minimum octave in the feature descriptor database to cause a search of the feature descriptor database to search feature descriptors at or above the selected minimum octave in the feature descriptor database.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00042" num="00042">
<claim-text>42. The computer program product of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein the instructions that cause the processor to estimate the scale value comprise instructions that cause the processor to:
<claim-text>analyze a distribution of the first set of keypoints across a scale space;</claim-text>
<claim-text>determine a cutoff point corresponding to a scale level in the distribution such that approximately 90% of the keypoints fall below the scale level; and</claim-text>
<claim-text>estimate the scale value as a function of the determined cutoff point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00043" num="00043">
<claim-text>43. The computer program product of <claim-ref idref="CLM-00041">claim 41</claim-ref>, further comprising instructions that cause the processor to obtain depth information for the object, wherein the instructions that cause the processor to estimate the scale value comprise instructions that cause the processor to estimate the scale value based at least in part on the depth information for the object.</claim-text>
</claim>
<claim id="CLM-00044" num="00044">
<claim-text>44. The computer program product of <claim-ref idref="CLM-00039">claim 39</claim-ref>, further comprising instructions that cause the processor to:
<claim-text>analyze sensor information from one or more sensors associated with a camera that captured the received image; and</claim-text>
<claim-text>determine an octave level for at least one of the first octave and the second octave based on the analysis of the sensor information.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00045" num="00045">
<claim-text>45. The computer program product of <claim-ref idref="CLM-00044">claim 44</claim-ref>, wherein the instructions that cause the processor to analyze the sensor information comprises instructions that cause the processor to analyze global positioning system (GPS) information to determine whether the camera was located in an outdoor environment when the received image was captured.</claim-text>
</claim>
<claim id="CLM-00046" num="00046">
<claim-text>46. The computer program product of <claim-ref idref="CLM-00044">claim 44</claim-ref>, wherein the instructions that cause the processor to analyze the sensor information comprises instructions that cause the processor to analyze GPS information to determine locations of objects relatively near the camera when the received image was captured, and determine sizes for the objects from descriptive data for the objects.</claim-text>
</claim>
<claim id="CLM-00047" num="00047">
<claim-text>47. The computer program product of <claim-ref idref="CLM-00044">claim 44</claim-ref>, wherein the instructions that cause the processor to analyze the sensor information comprises instructions that cause the processor to determine whether the camera was located in an indoor environment based on network data indicative of whether a device comprising the camera was communicatively coupled to a wireless network when the received image was captured.</claim-text>
</claim>
<claim id="CLM-00048" num="00048">
<claim-text>48. The computer program product of <claim-ref idref="CLM-00044">claim 44</claim-ref>, wherein the instructions that cause the processor to analyze the sensor information comprises instructions that cause the processor to calculate depth information indicative of distances between one or more objects in the received image and the camera when the received image was captured.</claim-text>
</claim>
<claim id="CLM-00049" num="00049">
<claim-text>49. The computer program product of <claim-ref idref="CLM-00044">claim 44</claim-ref>, wherein the instructions that cause the processor to analyze the sensor information comprise instructions that cause the processor to estimate a depth value for the object using data provided by an active probing sensor.</claim-text>
</claim>
<claim id="CLM-00050" num="00050">
<claim-text>50. The computer program product of <claim-ref idref="CLM-00039">claim 39</claim-ref>, further comprising instructions that cause the processor to send the one or more descriptors to a server to cause the server to query the feature descriptor database using the one or more descriptors, wherein the instructions that cause the processor to receive the confidence value comprise instructions that cause the processor to receive the confidence value from the server in response to the query.</claim-text>
</claim>
</claims>
</us-patent-grant>
