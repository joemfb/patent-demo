<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627005-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627005</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12946424</doc-number>
<date>20101115</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>324</us-term-extension>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>08</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>711114</main-classification>
<further-classification>711E21016</further-classification>
</classification-national>
<invention-title id="d2e55">System and method for virtualization of networked storage resources</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6622163</doc-number>
<kind>B1</kind>
<name>Tawill et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709211</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6959373</doc-number>
<kind>B2</kind>
<name>Testardi</name>
<date>20051000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6973549</doc-number>
<kind>B1</kind>
<name>Testardi</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6985956</doc-number>
<kind>B2</kind>
<name>Luke et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6986015</doc-number>
<kind>B2</kind>
<name>Testardi</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7013379</doc-number>
<kind>B1</kind>
<name>Testardi</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7206863</doc-number>
<kind>B1</kind>
<name>Oliveira et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7216264</doc-number>
<kind>B1</kind>
<name>Glade et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7225317</doc-number>
<kind>B1</kind>
<name>Glade et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>7315914</doc-number>
<kind>B1</kind>
<name>Venkatanarayanan et al.</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7379990</doc-number>
<kind>B2</kind>
<name>Tsao</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7409495</doc-number>
<kind>B1</kind>
<name>Kekre et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>7620774</doc-number>
<kind>B1</kind>
<name>Waxman</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>7620775</doc-number>
<kind>B1</kind>
<name>Waxman et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>7739448</doc-number>
<kind>B1</kind>
<name>Oliveira et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>7770059</doc-number>
<kind>B1</kind>
<name>Glade et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7818515</doc-number>
<kind>B1</kind>
<name>Umbehocker et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7818517</doc-number>
<kind>B1</kind>
<name>Glade et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7849262</doc-number>
<kind>B1</kind>
<name>Glade et al.</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711114</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7958305</doc-number>
<kind>B1</kind>
<name>Oliveira et al.</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7984253</doc-number>
<kind>B1</kind>
<name>Glade et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7992038</doc-number>
<kind>B1</kind>
<name>Glade et al.</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>8032701</doc-number>
<kind>B1</kind>
<name>Glade et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>8219681</doc-number>
<kind>B1</kind>
<name>Glade et al.</name>
<date>20120700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>8281022</doc-number>
<kind>B1</kind>
<name>Bast et al.</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>8291094</doc-number>
<kind>B2</kind>
<name>Bast et al.</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2003/0126132</doc-number>
<kind>A1</kind>
<name>Kavuri et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2003/0217129</doc-number>
<kind>A1</kind>
<name>Knittel et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2004/0133634</doc-number>
<kind>A1</kind>
<name>Luke et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2005/0025075</doc-number>
<kind>A1</kind>
<name>Dutt et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2005/0114464</doc-number>
<kind>A1</kind>
<name>Amir et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709213</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2005/0138184</doc-number>
<kind>A1</kind>
<name>Amir</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709228</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2006/0161642</doc-number>
<kind>A1</kind>
<name>Bopardikar et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2006/0224799</doc-number>
<kind>A1</kind>
<name>Maki et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>710 62</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2006/0224844</doc-number>
<kind>A1</kind>
<name>Kano et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2006/0242378</doc-number>
<kind>A1</kind>
<name>Kano et al.</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2008/0201391</doc-number>
<kind>A1</kind>
<name>Arakawa et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2010/0274963</doc-number>
<kind>A1</kind>
<name>Innan et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00039">
<othercit>The Cisco MDS 9500 Series Supervisor Module, CISCO.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00040">
<othercit>U.S. Appl. No. 12/946,424, Bradford B. Glade et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00041">
<othercit>U.S. Appl. No. 13/155,734, Bradford B. Glade et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00042">
<othercit>U.S. Appl. No. 13/166,096, Bradford B. Glade et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>U.S. Appl. No. 13/217,400, Bradford B. Glade et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>U.S. Appl. No. 13/443,002, Wentworth, III, James A. et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>U.S. Appl. No. 13/591,579, filed Dec. 20, 2012, Raju C. Bopardikar et al.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00046">
<othercit>Cisco MDS 9000 Family Multilayer Fibre Channel Switches, CISCO.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>Internet, CISCO.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>41</number-of-drawing-sheets>
<number-of-figures>49</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11541262</doc-number>
<date>20060929</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7849262</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12946424</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11479649</doc-number>
<date>20060630</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7620775</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11541262</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
<continuation-in-part>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10810988</doc-number>
<date>20040326</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7206863</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11479649</doc-number>
</document-id>
</child-doc>
</relation>
</continuation-in-part>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Glade</last-name>
<first-name>Bradford B.</first-name>
<address>
<city>Harvard</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Harvey</last-name>
<first-name>David W.</first-name>
<address>
<city>Newton</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kemeny</last-name>
<first-name>John</first-name>
<address>
<city>Westford</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>VanTine</last-name>
<first-name>Lee W.</first-name>
<address>
<city>Oxford</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Waxman</last-name>
<first-name>Matthew D.</first-name>
<address>
<city>Waltham</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Glade</last-name>
<first-name>Bradford B.</first-name>
<address>
<city>Harvard</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Harvey</last-name>
<first-name>David W.</first-name>
<address>
<city>Newton</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Kemeny</last-name>
<first-name>John</first-name>
<address>
<city>Westford</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>VanTine</last-name>
<first-name>Lee W.</first-name>
<address>
<city>Oxford</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Waxman</last-name>
<first-name>Matthew D.</first-name>
<address>
<city>Waltham</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Gupta</last-name>
<first-name>Krishnendu</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>D'Angelo</last-name>
<first-name>Joseph</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>EMC Corporation</orgname>
<role>02</role>
<address>
<city>Hopkinton</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Tran</last-name>
<first-name>Denise</first-name>
<department>2188</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method, system, and program product for managing a plurality of storage area networks including a plurality of data storage volumes and one or more hosts, wherein the volumes are in a storage network, wherein the network includes one or more processors in the network, the program product comprising a computer-readable storage medium encoded with computer-executable program code enabling controlling virtualization of volumes in the storage network and managing a SCSI virtualization service; wherein the volume virtualization controller service supports virtualization enabling virtual Storage Area Networks (SAN)s by parsing between front-end SANs and back-end SANs regardless of physical configuration of the front and back-end SANs, to enable mapping from virtual volumes to back-end storage elements across virtual SANs.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="201.68mm" wi="178.39mm" file="US08627005-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="204.89mm" wi="181.53mm" file="US08627005-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="220.13mm" wi="161.97mm" orientation="landscape" file="US08627005-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="145.97mm" wi="179.32mm" file="US08627005-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="126.92mm" wi="171.96mm" file="US08627005-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="226.99mm" wi="184.15mm" file="US08627005-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="266.87mm" wi="180.26mm" file="US08627005-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="253.41mm" wi="185.00mm" file="US08627005-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="270.51mm" wi="149.35mm" orientation="landscape" file="US08627005-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="252.56mm" wi="197.10mm" file="US08627005-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="233.51mm" wi="177.63mm" file="US08627005-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="225.72mm" wi="151.64mm" file="US08627005-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="206.67mm" wi="134.28mm" file="US08627005-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="181.95mm" wi="148.17mm" file="US08627005-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="228.43mm" wi="151.13mm" orientation="landscape" file="US08627005-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="110.66mm" wi="101.18mm" file="US08627005-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="233.09mm" wi="154.69mm" file="US08627005-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="168.06mm" wi="153.84mm" file="US08627005-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="211.41mm" wi="161.21mm" file="US08627005-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="139.53mm" wi="162.48mm" file="US08627005-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="161.63mm" wi="152.91mm" file="US08627005-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="123.87mm" wi="95.33mm" file="US08627005-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="194.56mm" wi="121.33mm" file="US08627005-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="125.22mm" wi="103.12mm" file="US08627005-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="230.89mm" wi="181.10mm" orientation="landscape" file="US08627005-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="243.08mm" wi="178.90mm" orientation="landscape" file="US08627005-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="260.77mm" wi="185.84mm" file="US08627005-20140107-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="233.51mm" wi="184.57mm" file="US08627005-20140107-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="241.72mm" wi="172.89mm" orientation="landscape" file="US08627005-20140107-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="240.88mm" wi="183.73mm" orientation="landscape" file="US08627005-20140107-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="261.70mm" wi="187.62mm" file="US08627005-20140107-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00031" num="00031">
<img id="EMI-D00031" he="193.63mm" wi="168.49mm" file="US08627005-20140107-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00032" num="00032">
<img id="EMI-D00032" he="219.20mm" wi="176.36mm" file="US08627005-20140107-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00033" num="00033">
<img id="EMI-D00033" he="222.67mm" wi="181.95mm" file="US08627005-20140107-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00034" num="00034">
<img id="EMI-D00034" he="172.89mm" wi="173.74mm" file="US08627005-20140107-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00035" num="00035">
<img id="EMI-D00035" he="217.93mm" wi="178.48mm" file="US08627005-20140107-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00036" num="00036">
<img id="EMI-D00036" he="226.99mm" wi="184.57mm" file="US08627005-20140107-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00037" num="00037">
<img id="EMI-D00037" he="215.31mm" wi="185.00mm" file="US08627005-20140107-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00038" num="00038">
<img id="EMI-D00038" he="223.52mm" wi="184.15mm" file="US08627005-20140107-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00039" num="00039">
<img id="EMI-D00039" he="216.58mm" wi="177.21mm" file="US08627005-20140107-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00040" num="00040">
<img id="EMI-D00040" he="224.87mm" wi="175.85mm" file="US08627005-20140107-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00041" num="00041">
<img id="EMI-D00041" he="212.26mm" wi="178.48mm" file="US08627005-20140107-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation of U.S. application Ser. No. 11/541,262, filed Sep. 29, 2006, which has granted as U.S. Pat. No. 7,849,262. This application is a continuation-in-part of U.S. patent application Ser. No. 10/810,988, filed on Mar. 26, 2004, and entitled &#x201c;System and Method for Managing Storage Networks and for Managing Scalability of Volumes in such a Network&#x201d; and which is assigned to the same assignee as this application, and which has granted as U.S. Pat. No. 7,206,863. It is also a continuation-in-part of application Ser. No. 11/479,649, filed on Jun. 30, 2006, and entitled &#x201c;System And Method For Managing Storage Networks And Providing Virtualization Of Resources In Such A Network Using One Or More Asics&#x201d; and which is assigned to the same assignee as this application, and which has granted as U.S. Pat. No. 7,620,775. Each of these above-referenced applications, for which the present application is a continuation-in-part of, is each incorporated in their entirety by this reference.</p>
<p id="p-0003" num="0002">It is related to U.S. patent application Ser. No. 11/540,416 entitled &#x201c;Architecture for Virtualization of Networked Storage Resources,&#x201d; which is filed on Sep. 29, 2006 which is assigned to the same assignee as this application, and which has granted as U.S. Pat. No. 7,818,517. It is also related to U.S. patent application Ser. No. 11/540,336, entitled &#x201c;Failure Protection in an Environment including Virtualization of Networked Storage Resources,&#x201d; which is filed on Sep. 29, 2006, which is assigned to the same assignee as this application, and which has granted as U.S. Pat. No. 7,770,059.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<p id="p-0004" num="0003">A portion of the disclosure of this patent document may contain command formats and other computer language listings, all of which are subject to copyright protection. The copyright owner, EMC Corporation, has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure, as it appears in the Patent and Trademark Office patent file or records, but otherwise reserves all copyright rights whatsoever.</p>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0005" num="0004">This invention relates generally to managing and analyzing data in a data storage environment, and more particularly to a system and method for managing physical and logical components of storage area networks.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0006" num="0005">Computer systems are constantly improving in terms of speed, reliability, and processing capability. As is known in the art, computer systems which process and store large amounts of data typically include a one or more processors in communication with a shared data storage system in which the data is stored. The data storage system may include one or more storage devices, usually of a fairly robust nature and useful for storage spanning various temporal requirements, e.g., disk drives. The one or more processors perform their respective operations using the storage system. Mass storage systems (MSS) typically include an array of a plurality of disks with on-board intelligent and communications electronics and software for making the data on the disks available.</p>
<p id="p-0007" num="0006">To leverage the value of MSS, these are typically networked in some fashion, Popular implementations of networks for MSS include network attached storage (NAS) and storage area networks (SAN). In NAS, MSS is typically accessed using the TCP Protocol over communication lines such as Ethernet using industry standard file sharing protocols like NFS, and Windows Networking. In SAN, the MSS is typically directly accessed over Fibre Channel switching fabric using encapsulated SCSI protocols.</p>
<p id="p-0008" num="0007">Each network type has its advantages and disadvantages, but SAN's are particularly noted for providing the advantage of being reliable, and being a scalable infrastructure but their complexity and disparate nature makes them difficult to centrally manage. Thus, a problem encountered in the implementation of SAN's is that the dispersion of resources tends to create an unwieldy and complicated data storage environment. Reducing the complexity by allowing unified management of the environment instead of treating as a disparate entity would be advancement in the data storage computer-related arts. While it is an advantage to distribute intelligence over various networks, it should be balanced against the need for unified and centralized management that can grow or scale proportionally with the growth of what is being managed. This is becoming increasingly important as the amount of information being handled and stored grows geometrically over short time periods and such environments add new applications, servers, and networks also at a rapid pace.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0009" num="0008">To overcome the problems described above and to provide the advantages also described above, the present invention in one embodiment includes a system for managing a plurality of storage area networks including a plurality of data storage volumes and one or more hosts, wherein the volumes are in a switched storage network. The system includes one or more processors in communication with switching capability for the switched storage network, wherein the one or more processors include program logic for carrying out a volume virtualization controller service for controlling virtualization of volumes in the switch storage network and a SCSI virtualization service that manages virtual SCSI-related targets and virtual logical units (LU's).</p>
<p id="p-0010" num="0009">In another embodiment, the invention is embodied as a method for carrying out steps carried out by the system described above.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010">The above and further advantages of the present invention may be better under stood by referring to the following description taken into conjunction with the accompanying drawings in which:</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram showing a Data Storage environment including a new architecture embodying the present invention and which is useful in such an environment;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2</figref> is another block diagram showing hardware components of the architecture shown in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> is another block diagram showing hardware components of a processor included in the architecture and components of respective <figref idref="DRAWINGS">FIGS. 1 and 2</figref>;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> is another block diagram showing hardware components of a disk array included in the architecture and components of respective <figref idref="DRAWINGS">FIGS. 1 and 2</figref>;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5</figref> is a schematic illustration of the architecture and environment of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 6</figref> is a functional block diagram showing software components of the processor shown in <figref idref="DRAWINGS">FIG. 3</figref>;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7</figref> is a functional block diagram showing software components of intelligent switches which are included in the architecture of <figref idref="DRAWINGS">FIG. 1</figref> and which are also shown in the hardware components of <figref idref="DRAWINGS">FIG. 2</figref>;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 8</figref> shows an example of implementation of clones in the environment of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 9</figref> shows an example of SNAP Processing at a time in the environment of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 10</figref> shows another example of SNAP Processing at another time in the environment of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 11</figref> shows a schematic block diagram of software components of the architecture of <figref idref="DRAWINGS">FIG. 1</figref> showing location and relationships of such components to each other;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 12</figref> shows an example of Virtualization Mapping from Logical Volume to Physical Storage employed in the Data Storage Environment of <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 13</figref> shows an example of SNAP Processing employing another example of the Virtualization Mapping and showing before a SNAP occurs;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 14</figref> shows another example of SNAP Processing employing the Virtualization Mapping of <figref idref="DRAWINGS">FIG. 12</figref> and showing after a SNAPSHOT occurs but BEFORE a WRITE has taken place;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 15</figref> shows another example of SNAP Processing employing the Virtualization Mapping of <figref idref="DRAWINGS">FIG. 12</figref> and showing after a SNAPSHOT occurs and AFTER a WRITE has taken place;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 16</figref> is a flow logic diagram illustrating a method of managing the resources involved in the SNAP Processing shown in <figref idref="DRAWINGS">FIGS. 14-15</figref>;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 17</figref> is another flow logic diagram illustrating a method of managing the resources involved in the SNAP Processing shown in <figref idref="DRAWINGS">FIGS. 14-15</figref>;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 18</figref> is an example of a data structure involved in a process of identifying and handling volumes that have extent maps that have become fragmented during SNAP Processing and a process to reduce such fragmentation;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 19</figref> is another example of a data structure involved in a process of identifying and handling volumes that have extent maps that have become fragmented during SNAP Processing and a process to reduce such fragmentation;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 20</figref> is another example of a data structure involved in a process of identifying and handling volumes that have extent maps that have become fragmented during SNAP Processing and a process to reduce such fragmentation;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 21</figref> is a schematic showing a hierarchical structure employed in the Data Storage Environment of <figref idref="DRAWINGS">FIG. 1</figref> within the storage processor of <figref idref="DRAWINGS">FIG. 3</figref> for allowing storage applications to be managed for consistent error presentation or handling;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 22</figref> is a schematic of associations present when handling errors for consistent error presentation or handling with the hierarchical structure of <figref idref="DRAWINGS">FIG. 21</figref>;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 23</figref> is an example of a structure implementing error handling for consistent error presentation or handling, and which is a simplified version of the type of structure shown in <figref idref="DRAWINGS">FIG. 21</figref>;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 24</figref> show method steps for consistent error presentation or handling and using the example structure of <figref idref="DRAWINGS">FIG. 23</figref>;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 25</figref> show additional method steps for consistent error presentation or handling and using the example structure of <figref idref="DRAWINGS">FIG. 23</figref>;</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 26</figref> shows a software application for carrying out the methodology described herein and a computer medium including software described herein;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 27</figref> shows an architecture including a plurality of control path processors and paired virtualization ASIC including intelligence for virtualization of storage units and switch management related to switches in a data storage environment;</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 28</figref> shows another embodiment of an architecture including a plurality of control path processors and paired virtualization ASIC including intelligence for virtualization of storage units and onboard switch management and switches in a data storage environment including at least one data storage system;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 29</figref> shows an embodiment of a control path processor like the control path processor shown in <figref idref="DRAWINGS">FIG. 27</figref> and the embodiment of the control path processor shown in <figref idref="DRAWINGS">FIG. 28</figref> including paired storage processors and virtualization ASIC's;</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 30</figref> shows more detail regarding functional elements of an embodiment of a storage processor on the control path processor of <figref idref="DRAWINGS">FIG. 29</figref> and also shows the pairing of the storage processor with the virtualization ASIC's;</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 31</figref> shows an embodiment of a complete ASIC's including an integrated storage processor with virtualization circuitry built in to the complete ASIC's;</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 32</figref> shows an alternative embodiment of an architecture useful for virtualizing networked storage resources and including a virtualization manager that manages logical layers used for virtualizing and presenting resources;</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 33</figref> shows a schematic of a functional block diagram of a virtualization manager that is part of the architecture of <figref idref="DRAWINGS">FIG. 32</figref>;</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 34</figref> shows how volume maps get distributed among the Storage Presentation Layer and Volume Virtualization Layers of the architecture of <figref idref="DRAWINGS">FIG. 32</figref>;</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 35</figref> shows preferred hardware deployments including ASIC's as shown in <figref idref="DRAWINGS">FIGS. 27-31</figref> for the execution environments of the components of the alternative embodiment architecture of <figref idref="DRAWINGS">FIG. 32</figref>;</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 36</figref> shows an example case of a data flow for a volume configured for high availability in the presence of the loss of a Volume Virtualizer;</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 37</figref> shows more of the example case of <figref idref="DRAWINGS">FIG. 36</figref>;</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 38</figref> shows an example hardware and software deployment of the alternative embodiment architecture of <figref idref="DRAWINGS">FIG. 32</figref>;</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 39</figref> shows an example case wherein a host accesses a volume, LU1, that is being virtualized;</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 40</figref> shows another example case wherein the same virtual volume, LU1, as discussed in reference to <figref idref="DRAWINGS">FIG. 39</figref> is being presented to the host while its data is being relocated;</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 41</figref> shows more related to the example case of <figref idref="DRAWINGS">FIGS. 39-40</figref>;</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 42</figref> shows more related to the example case of <figref idref="DRAWINGS">FIGS. 39-41</figref>;</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 43A</figref> shows an example case of the architecture of <figref idref="DRAWINGS">FIG. 32</figref> being used to maintain high availability of storage resources in the event of a failure or problem;</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 43B</figref> shows another example case of the architecture of <figref idref="DRAWINGS">FIG. 32</figref> being used to maintain high availability of storage resources in the event of a failure or problem;</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 43C</figref> shows another example case of the architecture of <figref idref="DRAWINGS">FIG. 32</figref> being used to maintain high availability of storage resources in the event of a failure or problem;</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 43D</figref> shows another example case of the architecture of <figref idref="DRAWINGS">FIG. 32</figref> being used to maintain high availability of storage resources in the event of a failure or problem;</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 43E</figref> shows another example case of the architecture of <figref idref="DRAWINGS">FIG. 32</figref> being used to maintain high availability of storage resources in the event of a failure or problem;</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 43F</figref> shows another example case of the architecture of <figref idref="DRAWINGS">FIG. 32</figref> being used to maintain high availability of storage resources in the event of a failure or problem; and</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 43G</figref> shows another example case of the architecture of <figref idref="DRAWINGS">FIG. 32</figref> being used to maintain high availability of storage resources in the event of a failure or problem.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT</heading>
<p id="p-0061" num="0060">The methods and apparatus of the present invention are intended for use in Storage Area Networks (SAN's) that include data storage systems, such as the Symmetrix Integrated Cache Disk Array system or the Clariion Disk Array system available from EMC Corporation of Hopkinton, Mass. and those provided by vendors other than EMC.</p>
<p id="p-0062" num="0061">The methods and apparatus of this invention may take the form, at least partially, of program code (i.e., instructions) embodied in tangible media, such as floppy diskettes, CD-ROMs, hard drives, random access or read only-memory, or any other machine-readable storage medium, including transmission medium. When the program code is loaded into and executed by a machine, such as a computer, the machine becomes an apparatus for practicing the invention. The methods and apparatus of the present invention may be embodied in the form of program code that is transmitted over some transmission medium, such as over electrical wiring or cabling, through fiber optics, or via any other form of transmission. And may be implemented such that herein, when the program code is received and loaded into and executed by a machine, such as a computer, the machine becomes an apparatus for practicing the invention. When implemented on a general-purpose processor, the program code combines with the processor to provide a unique apparatus that operates analogously to specific logic circuits. The program code (software-based logic) for carrying out the method is embodied as part of the system described below.</p>
<p id="p-0063" num="0062">Overview</p>
<p id="p-0064" num="0063">The embodiment of the present invention denominated as FabricX architecture allows storage administrators to manage the components of their SAN infrastructure without interrupting the services they provide to their clients. This provides for a centralization of management allowing the storage infrastructure to be managed without requiring Host-based software or resources for this management. For example, data storage volumes can be restructured and moved across storage devices on the SAN while the Hosts accessing these volumes continue to operate undisturbed.</p>
<p id="p-0065" num="0064">The new architecture also allows for management of resources to be moved off of storage arrays themselves, allowing for more centralized management of heterogeneous data storage environments. Advantages provided include: (1) centralized management of a storage infrastructure; (2) storage consolidation and economical use of resources; (3) common replication and mobility solutions (e.g., migration) across heterogeneous storage subsystems; and (4) storage management that is non-disruptive to Hosts and storage subsystems.</p>
<p id="p-0066" num="0065">Architecture</p>
<p id="p-0067" num="0066">Referring now to <figref idref="DRAWINGS">FIG. 1</figref>, reference is now made to a data storage environment <b>10</b> including an architecture including the elements of the front-end storage area network <b>20</b> and a plurality of Hosts <b>1</b>-N shown as Hosts <b>13</b>, <b>14</b>, and <b>18</b>, wherein some Hosts may communicate through the SAN and others may communicate in a direct connect fashion, as shown. The architecture includes two intelligent multi-protocol switches (IMPS's) <b>22</b> and <b>24</b> and storage and switch controller <b>26</b> to form a combination <b>27</b> which may also be denominated as a FabricX Instance <b>27</b>. In communication with the Instance through an IP Network <b>64</b> and management interface <b>43</b> is an element management station (EMS) <b>29</b>, and back-end storage network <b>42</b>. Such back-end storage may include one or more storage systems, such as the EMC Clariion and Symmetrix data storage systems from EMC of Hopkinton, Mass.</p>
<p id="p-0068" num="0067">Generally such a data storage system includes a system memory and sets or pluralities and of multiple data storage devices or data stores. The system memory can comprise a buffer or cache memory; the storage devices in the pluralities and can comprise disk storage devices, optical storage devices and the like. However, in a preferred embodiment the storage devices are disk storage devices. The sets represent an array of storage devices in any of a variety of known configurations. In such a data storage system, a computer or Host adapter provides communications between a Host system and the system memory and disk adapters and provides pathways between the system memory and the storage device pluralities. Regarding terminology related to the preferred data storage system, the Host or Host network is sometimes referred to as the front-end and from the disk adapters toward the disks is sometimes referred to as the back-end. Since the invention includes the ability to virtualize disks using LUNs as described below, a virtual initiator may be interchanged with disk adapters. A bus interconnects the system memory, and communicates with front and back-end. As will be described below, providing such a bus with switches provides discrete access to components of the system.</p>
<p id="p-0069" num="0068">Referring again to <figref idref="DRAWINGS">FIG. 1</figref>, the Data Storage Environment <b>10</b> provides architecture in a preferred embodiment that includes what has been described above as a FabricX Instance. Pairs of the IMPS switch are provided for redundancy; however, one skilled in the art will recognize that more or less switches and processors could be provided without limiting the invention and that the Controller could also be provided in redundancy. Storage from various storage subsystems is connected to a specific set of ports on an IMPS. As illustrated, the imported storage assets and these back-end ports make up the Back-End SAN <b>41</b> with a networked plurality of data storage arrays <b>38</b>, and <b>40</b>, and which also may be directly connected to either IMPS, as shown with arrays <b>30</b>-<b>34</b> so connected to the Instance <b>27</b> through IMPS <b>24</b>, but although not shown could also be connected directly to the Storage and Switch Controller.</p>
<p id="p-0070" num="0069">It is known in SAN networks using Fibre Channel and/or SCSI protocols that such data devices as those represented by disks or storage <b>30</b>-<b>40</b> can be mapped using a protocol to a Fibre Channel logical unit (LUN) that act as virtual disks that may be presented for access to one or more Hosts, such as Hosts <b>13</b>-<b>18</b> for I/O operations. LUN's are also sometimes referred to interchangeably with data volumes which at a logical level represent physical storage such as that on storage <b>30</b>-<b>40</b>. Although SCSI protocols are discussed throughout, one skilled in the art will recognize these are applicable to iSCSI protocols or the like as well.</p>
<p id="p-0071" num="0070">Over the referred IP Network <b>64</b> and by communicating through the management interface <b>43</b>, a Storage Administrator using the EMS <b>29</b> may create virtual LUN's (Disks) that are composed of elements from the back-end storage. These virtual devices which may be represented, for example by a disk icon (not shown) grouped with the intelligent switch, are made available through targets created on a specific set of intelligent switch ports. Client Host systems connect to these &#x2018;front-end&#x2019; ports to access the created volumes. The client Host systems, the front-end ports, and the virtual LUN's all form part of the Front-End SAN <b>20</b>. Note Hosts, such as Host <b>13</b> may connect directly to the IMPS.</p>
<p id="p-0072" num="0071">The combined processing and intelligence of the switch and the FabricX Controller provide the connection between the client Hosts in the front-end SAN and the storage in the back-end SAN. The FabricX Controller runs storage applications that are presented to the client Hosts. These include the Volume Management, Data Mobility, Snapshots, Clones, and Mirrors, which are terms of art known with EMC's Clariion data storage system. In a preferred embodiment the FabricX Controller implementation is based on the CLARiiON Barracuda storage processor and the CLARiiON Flare software implementation which includes Layered drivers that are discussed below.</p>
<p id="p-0073" num="0072">Hardware Components</p>
<p id="p-0074" num="0073">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, hardware components of the architecture in the environment shown in <figref idref="DRAWINGS">FIG. 1</figref> are now described in detail. A FabricX instance <b>27</b> is comprised of several discrete hardware subsystems that are networked together. The major subsystems include a Control Path Processor (CPP) <b>58</b> and a Disk Array Enclosure (DAE) <b>54</b>, each described in more detail in <figref idref="DRAWINGS">FIGS. 3 and 4</figref>.</p>
<p id="p-0075" num="0074">The CPP provides support for storage and switch software applications and runs the software that handles exceptions that occur on the fast-path. Regarding where software runs, in the exemplary embodiment, software for management by the Storage and Switch Controller is shown running on the CPP; however, that is merely an example and any or all software may be loaded and run from the IMPS or anywhere in the networked environment. Additionally the CPP supports management interfaces used to configure and control the instance. The CPP is composed of redundant storage processors and is further described with reference to <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0076" num="0075">The DAE, together with the disks that it contains provide the persistent storage of the meta-data for the FabricX instance. The meta data includes configuration information that identifies the components of the instance, for example, the identities of the intelligent switches that make up the instance, data describing the set of exported virtual volumes, the software for the Controller, information describing what Hosts and initiators are allowed to see what volumes, etc. The DAE is further described with reference to <figref idref="DRAWINGS">FIG. 4</figref>. The IMPS <b>22</b> or <b>24</b> provide storage virtualization processing in the data-path (also known as fast-path processing), and pass control to the CPP when exceptions occur for requests that it cannot handle.</p>
<p id="p-0077" num="0076">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, each FabricX instance may be managed by an administrator or user using EMS <b>29</b>. Preferably, a given EMS is capable of managing one or more FabricX instances and communicates to the FabricX instance components through one or more IP networks.</p>
<p id="p-0078" num="0077">Referring to <figref idref="DRAWINGS">FIG. 3</figref>, CPP <b>58</b> preferably includes two storage processors (SP's) <b>72</b> and <b>74</b>, which may be two Intel Pentium IV microprocessors or similar. The two storage processors in the CPP communicate with each other via links <b>71</b>, which may be for example redundant 2 Gbps Fibre Channel links, each provided in communication with the mid-plane <b>76</b>. Each CPP contains fan modules <b>80</b> that connect directly to the mid-plane <b>76</b>. The CPP contains two power supplies <b>78</b> and <b>82</b> (Power Supply A and B). In a preferred embodiment, the power supplies are redundant, have their own line cord, power switch, and status light, and each power supply is capable of providing full power to the CPP and its DAE. During normal operation the power supplies share load current. These redundant standby power supplies provide backup power to the CPP to ensure safety and integrity of the persistent meta-data maintained by the CPP.</p>
<p id="p-0079" num="0078">Referring to <figref idref="DRAWINGS">FIG. 4</figref>, the DAE <b>54</b> is shown. A FabricX instance <b>27</b> preferably has a single DAE <b>54</b>, which is loaded with four disk drives <b>100</b> (the number of drives is a variable choice, however). These disk drives provide the persistent storage for meta-data of the instance, wherein the meta-data is used for certain management and control functions. None of this storage is directly accessible or visible to Hosts on the front-end. The meta-data on the disk drives is three-way mirrored to provide protection from disk failures. Each SP has a single arbitrated loop that provides its connection to the DAE. Each Link Control Card or LCC <b>98</b> and <b>102</b> connects the FabricX SP's to the meta-data storage devices or disk drives within the Disk Array Enclosure.</p>
<p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. 5</figref> shows a schematic illustration of the architecture and environment of <figref idref="DRAWINGS">FIG. 1</figref> in detail with preferred connectivity and in a preferred two IMPS configuration (IMPS <b>22</b> and IMPS <b>24</b>). Host Systems <b>13</b>-<b>18</b> preferably communicate with FabricX via a SCSI protocol running over Fibre Channel. Each Fibre Channel port of each IMPS is distinguished as being either a front-end port, a back-end port, a control-port, or an inter-switch port. Hosts connect to the FabricX instance <b>27</b> via front-end ports. Front-end ports support SCSI targets and preferably have virtualizing hardware to make up an intelligent port. The Host's connection to the port may be direct as in the case of labeled Host <b>1</b> or indirect such as Host <b>2</b> via Layer 2 Fibre Channel switches such as Switch 60-SW1 and Switch 62-SW2. Hosts may establish multiple paths to their storage by connecting to two or more separate front-end ports for high availability and performance; however, the preferred FabricX instance architecture allows Hosts to be configured with a single path for the sake of simplicity. In some configurations, not shown for simplicity, the switches 60-SW1 and 62-SW2 could be combined and/or integrated with the IMPS without departing from the spirit of the invention.</p>
<p id="p-0081" num="0080">An IMPS can be used to support virtual SAN's (VSAN's), to parse between front-end SAN's and back-end SAN's even if such SAN's are not physically configured. In general, switches that support VSANs allow a shared storage area network to be configured into separate logical SANs providing isolation between the components of different VSANs. The IMPS itself may be configured in accordance with specifications from such known switch vendors as Brocade and Cisco.</p>
<p id="p-0082" num="0081">Each intelligent switch preferably contains a collection of SCSI ports, such as Fibre Channel, with translation processing functions that allow a port or associated hardware to make various transformations on the SCSI command stream flowing through that port. These transformations are performed at wire-speeds and hence have little impact on the latency of the command. However, intelligent ports are only able to make translations on read and write commands. For other SCSI commands, the port blocks the request and passes control for the request to a higher-level control function. This process is referred to as faulting the request. Faulting also occurs for read and write commands when certain conditions exist within the port. For example, a common transformation performed by an intelligent port is to map the data region of a virtual volume presented to a Host to the data regions of back-end storage elements. To support this, the port maintains data that allows it to translate (map) logical block addresses of the virtual volume to logical back-end addresses on the back-end devices. If this data is not present in the port when a read or write is received, the port will fault the request to the control function. This is referred to as a map fault.</p>
<p id="p-0083" num="0082">Once the control function receives a faulted request it takes whatever actions necessary to respond to the request (for example it might load missing map data), then either responds directly to the request or resumes it. The control function supported may be implemented differently on different switches. On some vendor's switches the control function is known to be supported by a processor embedded within the blade containing the intelligent ports, on others it is known to provide it as an adjunct processor which is accessed via the backplane of the switch, a third known configuration is to support the control function as a completely independent hardware component that is accessed through a network such as Fibre Channel or IP.</p>
<p id="p-0084" num="0083">Back-end storage devices connect to FabricX via the Fibre Channel ports of the IMPSs that have been identified as back-end ports (oriented in <figref idref="DRAWINGS">FIG. 5</figref> toward the back-end SAN). Intelligent ports act as SCSI initiators and the switch routes SCSI traffic to the back-end targets <b>103</b>-<b>110</b> respectively labeled T1-TN through the back-end ports of the respective IMPS's. The back-end devices may connect directly to a back-end IMPS if there is an available port as shown by T5, or they may connect indirectly such as in the case of T1 via a Layer 2 Fibre Channel switch, such as Switch 60-SW3, and Switch 62-SW4.</p>
<p id="p-0085" num="0084">The EMS <b>29</b> connects to FabricX in a preferred embodiment through an IP network, e.g. an Ethernet network which may be accessed redundantly. The FabricX CPP <b>58</b> in a preferred embodiment has two 10/100 Mbps Ethernet NIC that is used both for connectivity to the IMPS (so that it can manage the IMPS and receive SNMP traps), and for connectivity to the EMS. It is recommended that the IP networks <b>624</b><i>a</i>-<i>b </i>provided isolation and dedicated 100 Mbps bandwidth to the IMPS and CPP.</p>
<p id="p-0086" num="0085">The EMS in a preferred embodiment is configured with IP addresses for each Processor <b>72</b>-<b>74</b> in the FabricX CPP. This allows direct connection to each processor. Each Processor preferably has its own Fibre Channel link that provides the physical path to each IMPS in the FabricX instance. Other connections may also work, such as the use of Gigabit Ethernet control path connections between the CPP and IMPS. A logical control path is established between each Processor of the CPP and each IMPS. The control paths to IMPS's are multiplexed over the physical link that connects the respective SP of the CPP to its corresponding IMPS. The IMPS provides the internal routing necessary to send and deliver Fiber Channel frames between the SP of the CPP and the respective IMPS. Other embodiments are conceivable that could use IP connectivity for the control path. In such a case the IMPS could contain logic to route IP packets to the SP.</p>
<p id="p-0087" num="0086">Software Components</p>
<p id="p-0088" num="0087">Reference is made to <figref idref="DRAWINGS">FIGS. 6 and 7</figref>, showing a functional block diagram of software comprised in modules that run on the Storage Processors (SP) <b>72</b> or <b>74</b> within Control Path Processor (CPP) <b>58</b> and on the IMPS owned by the instance. Each of these storage processors operates as a digital computer preferably running Microsoft Window XP Embedded and Hosts software components. Software Components of each SP are now described.</p>
<p id="p-0089" num="0088">The CPP-based software includes a mixture of User-Level Services <b>122</b> and Kernel-mode or Kernel services <b>128</b>. The Kernel services include Layered Drivers <b>123</b>, Common Services <b>125</b>, Switch Independent Layer (SIL) <b>126</b>, and Switch Abstraction Layer-Control Path Processor (SAL-CPP) <b>127</b>. The IMPS-based software preferably runs on a control processor within the vendor's switch. This processor may be embedded on an I/O blade within the switch or implemented as a separate hardware module.</p>
<p id="p-0090" num="0089">The SAL-CPP <b>127</b> provides a vendor-independent interface to the services provided by the IMPS's that form a FabricX instance. This software Layer creates and manages a IMPS Client for each IMPS that is part of the FabricX instance. The following services are provided by the SAL-CPP. There is a Switch Configuration and Management Services (SWITCH CONFIG &#x26; MGMT) in the SAL-CPP that provides uniform support for configuring the IMPS, zone configuration and management, name service configuration and management, discovering the ports supported by the IMPS, reporting management related events such as Registered State Change Notifications (RSCNs), and component failure notifications. The service interfaces combined with the interfaces provided by the user-level Switch Management service encapsulate the differences between different switch vendors and provide a single uniform interface to the FabricX management system. The Switch Adapter Port Driver (SAPD) of the Kernel Services <b>128</b> uses these interfaces to learn what ports are supported by the instance so that it can create the appropriate device objects representing these ports.</p>
<p id="p-0091" num="0090">Referring to <figref idref="DRAWINGS">FIG. 6</figref>, the SAL-CPP <b>127</b> provides Front-End Services (FRONT-END SVCS) that include creating and destroying virtual targets, activate and deactivate virtual targets. Activation causes the target to log into the network while deactivation causes it to log out. Storage Presentation objects represent the presentation of a volume on a particular target. These Front-End Services also include LUN mapping and/or masking on a per initiator and per target basis.</p>
<p id="p-0092" num="0091">Referring again to <figref idref="DRAWINGS">FIG. 6</figref>, the SAL-CPP <b>127</b> provides Back-End Services (BACK-END SVCS) that include discovering back-end paths and support for storage devices or elements, including creating and destroying objects representing such devices or elements. Back-End Services include managing paths to the devices and SCSI command support. These services are used by FlareX of the Layered Drivers <b>123</b> to discover the back-end devices and make them available for use, and by the Path Management of the Switch Independent Layer (SIL). The SIL is a collection of higher-level switch-oriented services including managing connectivity to storage devices. These services are implemented using the lower-level services provided by the SAL-CPP.</p>
<p id="p-0093" num="0092">SAL-CPP <b>127</b> provides a volume management (Volume MGMT) service interface that supports creating and destroying virtual volumes, associating virtual volumes with back-end Storage Elements, and composing virtual volumes for the purpose of aggregation, striping, mirroring, and/or slicing. The volume management service interface also can be used for loading all or part of the translation map for a volume to a virtualizer, quiescing and resuming IO to a virtual volume, creating and destroying permission maps for a volume, and handling map cache miss faults, permission map faults, and other back-end errors. These services are used by the volume graph Manager (VGM) in each SP to maintain the mapping from the virtual targets presented out the logical front of the instance to the Storage Elements on the back-end.</p>
<p id="p-0094" num="0093">There are other SAL-CPP modules. The SAL copy service (COPY SVCS) functions provide the ability to copy blocks of data from one virtual volume to another. The Event Dispatcher is responsible for delivering events produced from the IMPS to the registered kernel-based services such as Path Management, VGM, Switch Manager, etc.</p>
<p id="p-0095" num="0094">The Switch and Configuration Management Interface is responsible for managing the connection to an IMPS. Each Storage Processor maintains one IMPS client for each IMPS that is part of the instance. These clients are created when the Switch Manager process directs the SAL-CPP to create a session with a IMPS.</p>
<p id="p-0096" num="0095">The Switch Independent Layer (SIL) <b>126</b> is a collection of higher-level switch-oriented services. These services are implemented using the lower-level services provided by the SAL-CPP. These services include:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0096">Volume Graph Manager (VGM)&#x2014;The volume graph Manager is responsible for processing map-miss faults, permission map faults, and back-end IO errors that it receives from the SAL-CPP. The VGM maintains volume graphs that provide the complete mapping of the data areas of front-end virtual volumes to the data areas of back-end volumes. The Volume Graph Manager provides its service via a kernel DLL running within the SP.</li>
        <li id="ul0002-0002" num="0097">Data Copy Session Manager&#x2014;The Data Copy Session Manager provides high-level copy services to its clients. Using this service, clients can create sessions to control the copying of data from one virtual volume to another. The service allows its clients to control the amount of data copied in a transaction, the amount of time between transactions, sessions can be suspended, resumed, and aborted. This service builds on top of capabilities provided by the SAL-CPP's Data Copy Services. The Data Copy Session Manager provides its service as a kernel level DLL running within the SP.</li>
        <li id="ul0002-0003" num="0098">Path Management&#x2014;The path management component of the SIL is a kernel-level DLL that works in conjunction with the Path Manager. Its primary responsibility is to provide the Path Manager with access to the path management capabilities of the SAL-CPP. It registers for path change events with the SAL-CPP and delivers these events to the Path Manager running in user-mode. Note, in some embodiments, the Path Management, or any of the other services may be configured to operate elsewhere, such as being part of another driver, such as FlareX.</li>
        <li id="ul0002-0004" num="0099">Switch Management&#x2014;The switch management component of the SIL is a kernel-level DLL that works in conjunction with the Switch Manager. Its primary responsibility is to provide the Switch Manager with access to the switch management capabilities of the SAL-CPP.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0097" num="0100">The CPP also Hosts a collection of Common Services <b>125</b> that are used by the Layered application drivers. These services include:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0101">Persistent Storage Mechanism (PSM)&#x2014;This service provides a reliable persistent data storage abstraction. It is used by the Layered applications for storing their meta-data. The PSM uses storage volumes provided by FlareX that are located on the Disk Array Enclosure attached to the CPP. This storage is accessible to both SPs, and provides the persistency required to perform recovery actions for failures that occur. Flare provides data-protection to these volumes using three-way mirroring. These volumes are private to a FabricX instance and are not visible to external Hosts.</li>
        <li id="ul0004-0002" num="0102">Distributed Lock Service (DLS)&#x2014;This service provides a distributed lock abstraction to clients running on the SPs. The service allows clients running on either SP to acquire and release shared locks and ensures that at most one client has ownership of a given lock at a time. Clients use this abstraction to ensure exclusive access to shared resources such as meta-data regions managed by the PSM.</li>
        <li id="ul0004-0003" num="0103">Message Passing Service (MPS)&#x2014;This service provides two-way communication sessions, called filaments, to clients running on the SPs. The service is built on top of the CMI service and adds dynamic session creation to the capabilities provided by CMI. MPS provides communication support to kernel-mode drivers as well as user-level applications.</li>
        <li id="ul0004-0004" num="0104">Communication Manager Interface (CMI)&#x2014;CMI provides a simple two-way message passing transport to its clients. CMI manages multiple communication paths between the SPs and masks communication failures on these. The CMI transport is built on top of the SCSI protocol which runs over 2 Gbps Fibre-Channel links that connect the SPs via the mid-plane of the storage processor enclosure. CMI clients receive a reliable and fast message passing abstraction. CMI also supports communication between SPs within different instances of FabricX. This capability will be used to support mirroring data between instances of FabricX.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0098" num="0105">The CPP includes Admin Libraries that provide the management software of FabricX with access to the functionality provided by the Layered drivers such as the ability to create a mirrored volume or a snapshot. The Admin Libraries, one per managed Layer, provide an interface running in user space to communicate with the managed Layers. The CPP further includes Layered Drivers <b>123</b> providing functionality as described below for drivers denominated as Flare, FlareX (FLARE_X), Fusion, Clone/Mirror, PIT Copy, TDD, TCD, and SAPD.</p>
<p id="p-0099" num="0106">Flare provides the low-level disk management support for FabricX. It is responsible for managing the local Disk Array Enclosure used for storing the data of the PSM, the operating system and FabricX software, and initial system configuration images, packages, and the like. It provides the RAID algorithms to store this data redundantly.</p>
<p id="p-0100" num="0107">The FlareX component is responsible for discovering and managing the back-end storage that is consumed by the FabricX instance. It identifies what storage is available, the different paths to these Storage Elements, presents the Storage Elements to the management system and allows the system administrator to identify which Storage Elements belong to the instance. Additionally, FlareX may provide Path Management support to the system, rather than that service being provided by the SIL as shown. In such a case, FlareX would be responsible for establishing and managing the set of paths to the back-end devices consumed by a FabricX instance. And in such a case it would receive path related events from the Back-End Services of the SAL-CPP and responds to these events by, for example, activating new paths, reporting errors, or updating the state of a path.</p>
<p id="p-0101" num="0108">The Fusion Layered driver provides support for re-striping data across volumes and uses the capabilities of the IMPS have to implement striped and concatenated volumes. For striping, the Fusion Layer (also known as the Aggregate Layer), allows the storage administrator to identify a collection of volumes (identified by LUN) over which data for a new volume is striped. The number of volumes identified by the administrator determines the number of columns in the stripe set. Fusion then creates a new virtual volume that encapsulates the lower Layer stripe set and presents a single volume to the Layers above.</p>
<p id="p-0102" num="0109">Fusion's support for volume concatenation works in a similar way; the administrator identifies a collection of volumes to concatenate together to form a larger volume. The new larger volume aggregates these lower Layer volumes together and presents a single volume to the Layers above. The Fusion Layer supports the creation of many such striped and concatenated volumes.</p>
<p id="p-0103" num="0110">Because of its unique location in the SAN infrastructure, FabricX, can implement a truly non-disruptive migration of the dataset by using the Data Mobility Layer driver that is part of the Drivers <b>123</b>. The client Host can continue to access the virtual volume through its defined address, while FabricX moves the data and updates the volume mapping to point to the new location.</p>
<p id="p-0104" num="0111">The Clone driver provides the ability to clone volumes by synchronizing the data in a source volume with one or more clone volumes. Once the data is consistent between the source and a clone, the clone is kept up-to-date with the changes made to the source by using mirroring capabilities provided by the IMPS's. Clone volumes are owned by the same FabricX instance as the source; their storage comes from the back-end Storage Elements that support the instance.</p>
<p id="p-0105" num="0112">The Mirror driver supports a similar function to the clone driver however, mirrors are replicated between instances of FabricX. The mirror Layered driver works in conjunction with the mirror driver in another instance of FabricX. This application provides the ability to replicate a source volume on a remote FabricX instance and keep the mirror volume in synch with the source.</p>
<p id="p-0106" num="0113">The PIT (Point-In-Time) Copy driver, also known as Snap, provides the ability to create a snapshot of a volume. The snapshot logically preserves the contents of the source volume at the time the snapshot is taken. Snapshots are useful for supporting non-intrusive data backups, replicas for testing, checkpoints of a volume, and other similar uses.</p>
<p id="p-0107" num="0114">The Target Class Driver and Target Disk Driver (TCD/TDD) Layer provides SCSI Target support. In FabricX these drivers mostly handle non-read and write SCSI commands (such as INQUIRY, REPORT_LUNS, etc). The drivers are also responsible for error handling, when errors cannot be masked by the driver Layers below, the TCD/TDD is responsible for creating the SCSI error response to send back to the Host. The TCD/TDD Layer also implements support for the preferred CLARiiON functionality which provides the means of identifying what LUNs each initiator should see. This is known as LUN masking. The feature also provides for LUN mapping whereby the Host visible LUN is translated to an instance-based LUN. Additionally such functionality when combined with a Host agent provides the ability to identify which initiators belong to a Host to simplify the provisioning of LUN masking and mapping.</p>
<p id="p-0108" num="0115">The Switch Adapter Port Driver (SAPD) is presented as a Fibre-Channel Port Driver to the TCD/TDD (Target Class Driver/Target Disk Driver) drivers, but rather than interfacing with a physical port device on the SP, the driver interfaces with the SAL-CPP and creates a device object for each front-end port of each IMPS that is part of the FabricX instance. The SAPD registers with the SAL-CPP to receive non-IO SCSI commands that arrive. The SAL-CPP will deliver all non-IO SCSI commands received for LU's owned by this driver's SP to this SAPD. The SAPD runs as a kernel-mode driver.</p>
<p id="p-0109" num="0116">The following services are user based: Governor and Switch Management. The Governor is an NT Service that is responsible for starting other user-level processes and monitoring their health and restarting them upon failure. The Switch Manager controls sessions created in the SAL-CPP for each IMPS. The Switch Manager is responsible for establishing the connections to the switches under its control and for monitoring and reacting to changes in their health. Each SP Hosts a single Switch Manager that runs as a User-level process and a Kernel-mode service within the SP.</p>
<p id="p-0110" num="0117">Reference is made once again to <figref idref="DRAWINGS">FIG. 6</figref>. The Raid++ services encapsulates the legacy logic dealing with the configuration and management of the array logical components (such as storage group, LUNs, etc.) and physical components (such as cabinets, DAEs, disk drives, etc.). The Providers are plug-in modules to the CIMOM which provide the functionality for a particular set of managed objects. Providers represent the objects (class definitions and behaviors) as defined in the object model of the managed element. The Admin Libraries include an interface between the user space management tasks and the kernel mode drivers' instrumenting the management of FabricX. The Admin Libraries accept requests using a Tagged Length Data (TLD) self-describing message format from the management Layer and converts those requests into the specific IOCTL calls required to realize those requests. Responses are returned using the same format.</p>
<p id="p-0111" num="0118">The Path Management is responsible for the construction and management of paths to back-end Storage Elements and is part of Kernel-mode services. It notes when paths change state; based on these state changes it applies its path management policies to take any adjusting actions. For example, upon receiving a path failure notification, the Path Management might activate a new path to continue the level of service provided for a back-end volume.</p>
<p id="p-0112" num="0119">One function of FabricX Volume Management is to combine elements of the physical storage assets of the FabricX Instance into logical devices. The initial implementation of the FabricX Volume Manager is based on the Flare Fusion Driver. As in Flare, the basic building blocks of the volumes exported by FabricX are constructed from the back-end storage devices. Each device visible to the FabricX instance will be initially represented as an un-imported Storage Element. The storage administrator will able to bind the individual storage elements into single disk RAID Groups. From these RAID Groups the administrator can define Flare Logical Units (FLU). In the FabricX environment the FLU's will be exported by the FlareX component to the Layered drivers above.</p>
<p id="p-0113" num="0120">Flare Fusion imports FLU's and aggregates them into Aggregate Logical Units (ALU). When a logical unit or SCSI Disk is presented to a client Host it is called a Host Logical Unit (HLU). HLU's can be created by: directly exporting a FLU; exporting an ALU created by concatenating two or more FLU's; and exporting an ALU created by striping two or more FLU's.</p>
<p id="p-0114" num="0121">The FabricX Inter Process Communication Transport (FIT) provides the message passing support necessary for the SAL Agents running on the IMPS's to communicate with the SAL-CPP client instance running on each SP. This transport provides a model of asynchronous communication to its clients and is responsible for monitoring and reporting on the health of the communications network connecting the IMPSs to the CPPs. FIT uses a proprietary protocol developed on top of the SCSI/FC protocol stack to provide a control path between the SPs and the IMPS's. This protocol runs over the Fibre Channel connecting the SP to the IMPS's switch. FIT supports multiple transport protocols. In addition to SCSI/FC FIT also supports TCP/IP.</p>
<p id="p-0115" num="0122"><figref idref="DRAWINGS">FIG. 7</figref> shows the software components of the intelligent switch or IMPS <b>22</b> or <b>24</b>. IMPS Software Components include an IMPS API and FIT <b>132</b>, SAL Agent <b>130</b>, and IMPS Operating System <b>134</b>. Each switch vendor provides a software interface to the services provided by their switch that is the IMPS API and the SP's communicate with this API. This API provides support to application software that runs within the switch or blade and in some cases this interface is remote to the switch. FabricX isolates the storage application logic from these differences by defining the Switch Abstraction Layer (SAL-CPP) discussed with reference to <figref idref="DRAWINGS">FIG. 6</figref>. The SAL Agent <b>130</b> is an application from the perspective of the IMPS and works with SAL-CPP <b>127</b> (<figref idref="DRAWINGS">FIG. 6</figref>) and the FIT of the SAL-CPP and of the IMPS. The Agent <b>130</b> directly uses the native IMPS API to control and communicate with the switch. Its function is to provide access to the services of the switch to the Control Path Processors. The IMPS operating system varies from switch vendor to vendor. For example, Cisco's Intelligent Switches use Monte Vista Linux, while the Brocade switches use NetBSD.</p>
<p id="p-0116" num="0123">FabricX preferably uses the Flare architecture from CLARiiON. This architecture uses a Windows driver stack. At the lowest Layer is the code, labeled FlareX (Flare_X) that interfaces to the back-end devices. The storage applications are logically Layered above FlareX as Windows device drivers, including Fusion, Mirror_View, Snap_View, Clone_View, and TCD/TDD. These Layers provide support for features such as volume concatenation, striping, clones, snapshots, mirrors, and data migration. These Layers also define virtualized Logical Unit objects.</p>
<p id="p-0117" num="0124">This architecture includes replication Layers: Snap_View, Clone_View, and Mirror_View are Layered above the Fusion Layer of code and consume the logical units (volumes) presented by the Fusion Layer, they likewise present logical units to the Layers above. The replication Layers have no knowledge of back-end devices or data placement thereon.</p>
<p id="p-0118" num="0125">The inventors have critically recognized that prior art storage arrays had limited processes to create clones, which are replicas stored within an array, and mirrors, which are replicas stored on different arrays. This is because the front-end of an array has no way to directly access to a back-end device in some other array. Remote mirroring in such prior art configurations is processed through two different arrays, one attached to a Host and one attached to remote back-end devices. These two arrays are then connected to each other via a WAN. However, the present invention being based on a switch and storage architecture does not have such limitations. Instead, all back-end devices are equivalent irregardless of their physical location though latency of access may vary. A back-end device may be connected to a back-end port of a switch through some WAN making it physically remote. With a switch-based architecture clones, which are replicas stored within the storage managed by the instance, can be created from storage that is physically remote from the Controller and switch hardware just as easily as creating it from storage which is physically close to this hardware. Only one FabricX instance is necessary, to create clones whether on physically local back-end devices or on physically remote back-end devices. However, if it is desirable to create replicas, mirrors, between instances of FabricX that is possible. For example, one might want to create a replica on another instance for the purpose of increasing data protection and providing a means to tolerate a failure of a FabricX instance, or to replicate data using an asynchronous protocol for very long distance replication.</p>
<p id="p-0119" num="0126">Further Operation Details</p>
<p id="p-0120" num="0127"><figref idref="DRAWINGS">FIG. 8</figref> shows an example of creation and use of clones with a FabricX instance. In this example, a Source Logical Unit (LU) <b>154</b><i>a </i>is connected to a Host <b>18</b> together with two clones, Clone 1 <b>152</b><i>a </i>and Clone 2 <b>156</b><i>a </i>on FabricX Instance <b>27</b>. The connection of the Host is shown directly, but it may also be indirect and/or via a SAN. Clone 1 is made on a back-end device provided by a locally attached array. Clone 2 is made on a remote device provided by an array located at some distance and connected to the local FabricX Instance <b>27</b> which can be accomplished by means such as Asynchronous Transfer Mode (ATM) or Dense Wave Division Multiplexing (DWDM) networks. Because these replicas are logically equivalent both are referred to as Clones. Clones made using one FabricX Instance can be accessed by Hosts connected to the same FabricX Instance mapped in respective mapping areas <b>157</b> and <b>158</b> of each IMPS controlled by a respective SP. Using the mapping <b>157</b> the respective FabricX instance presents the Source LU <b>154</b><i>a </i>to the Host <b>19</b>, while maintaining the cloned replicas <b>152</b><i>b </i>and, via a WAN, the replica <b>156</b><i>b </i>related to respective arrays <b>160</b>-<b>164</b>.</p>
<p id="p-0121" num="0128"><figref idref="DRAWINGS">FIGS. 9 and 10</figref> show an example of Snap replicas created using FabricX. Snaps are constructed by using a process called Copy On First Write (COFW). When a Snap is created it represents the state and contents of a volume as it existed at the time of creation. With the COFW technique a snap is initially just a map of pointers <b>173</b><i>a</i>-<i>c </i>and <b>175</b><i>a</i>-<i>c</i>. These map pointers initially refer to the blocks <b>176</b><i>a</i>-<b>178</b><i>a </i>in the Source, a.k.a. Ancestor, volume <b>179</b><i>a</i>. Only when a block in the source volume is about to change is it necessary to actually create a copy of the data as it was when the Snap was instantiated. When a write is received for a chunk of data the sub-segment(s) that refer to the chunk are copied from the Source to another area on a back-end volume (BEV) that is assigned to the Snap.</p>
<p id="p-0122" num="0129"><figref idref="DRAWINGS">FIG. 10</figref> shows a volume consisting of three chunks of data after the Snap is created and before any write commands are received. Both the source and Snap maps point to the same three chunks of data on back-end storage, i.e., pointers <b>173</b><i>d</i>-<i>f </i>and <b>175</b><i>d</i>-<i>f </i>point to respective blocks <b>176</b><i>b</i>-<b>178</b><i>b </i>on volume <b>179</b><i>b</i>. When a write is received, say for chunk <b>2</b>, FabricX will block the write command. It will copy chunk <b>2</b> from the back-end storage to some newly allocated space on back-end storage and change the Snap's map to point to this new location. FabricX will then allow the write of chunk <b>2</b> to proceed to the old location. After the write is complete the mapping is as shown in <figref idref="DRAWINGS">FIG. 10</figref>.</p>
<p id="p-0123" num="0130"><figref idref="DRAWINGS">FIG. 11</figref> shows a schematic block diagram of software components of the architecture of <figref idref="DRAWINGS">FIG. 1</figref> showing location and relationships of such components to each other. The components reside within either the client (Host) computers or the CPP of FabricX and use various communications method and protocols. The following describe the function of each of the major components and their interfaces to associated functions.</p>
<p id="p-0124" num="0131">Several components of FabricX System Management reside on Host computers (those using the storage services of FabricX and/or those managing FabricX), these are referred to as Client Components and are shown in group <b>18</b>. One component in particular, Raid++, has both a client and server instance, shown respectively in Host/client group <b>18</b> or server group <b>210</b>. The C++ Command Line Interface CLI <b>200</b>&#x2014;referred to here as CLI++) component resides on any system where the user wishes to manage FabricX using a text based command line interface. This component in conjunction with the Java based CLI provide the user with control over FabricX. The security model for the CLI++ is Host based; the user/IP address of the Host is entered into the FabricX registry to permit/limit the user's access to system management. The CLI++ uses a client instance of Raid++ to hold the model of the FabricX instance and to manipulate the configuration of FabricX. The client resident Raid++ communicates with the server based Raid++ using a messaging scheme over TCP/IP.</p>
<p id="p-0125" num="0132">The Java CLI <b>206</b> provides commands that use a different management scheme from that used by the CLI++. The Java CLI captures the user command string, packages it into an XML/HTTP message and forwards it to the CIMOM on the server group <b>210</b>. The CIMOM directs the command to the CLI Provider which decomposes the command and calls methods in the various CIM providers, primarily the CLARiiON provider, to effect the change.</p>
<p id="p-0126" num="0133">The Java GUI <b>208</b> provides a windows based management interface. It communicates with CIMOM using standard CIM XML/HTTP protocol. The GUI effects its changes and listens for events using standard CIM XML/HTTP. The Host Agent <b>204</b> provides optional functionality by pushing down to FabricX information about the Host. The following information is forwarded by the Agent explicitly to the CPP: Initiator type, Initiator options, Host device name used for push, Hostname, Host IP address, Driver name, Host Bus Adapter (HBA) model, HBA vendor string, and Host ID.</p>
<p id="p-0127" num="0134">The Event Monitor <b>202</b> resides on a Host and can be configured to send email, page, SNMP traps, and/or use a preferred EMC Call Home feature for service and support. The configuration is performed on the CPP and the configuration information is pushed back to the Event Monitor. The Event Monitor may also run directly on the CPP but due to memory constraints may be limited in function in comparison to running on a Host computer.</p>
<p id="p-0128" num="0135">Referring again to <figref idref="DRAWINGS">FIG. 11</figref>, the server side management components shown in group <b>210</b> interact with the user interfaces and tools for administering the system configuration and operation and to report on system operation. The server side components are comprised of middleware which resides between the user and the storage management components of the system which implement FabricX storage features. The server side components are basically divided into two groups, the legacy Raid++ module which provides the majority of the management services and the CIMOM and its providers. The Raid++ module uses a proprietary transport to communicate with management clients such as the CLI++. The Raid++ module maintains an object model of the system that it uses for managing the system; it updates the model periodically and on demand by polling the system to rebuild the model. The CIMOM CLARiiON Provider is essentially a wrapper for the Raid++ classes and methods and translates GUI initiated CIM XML commands into calls to Raid++ to fulfill requests.</p>
<p id="p-0129" num="0136">The management functions not provided by Raid++ are provided by a series of CIMOM providers which are attached to a CIMOM. The CIMOM provides common infrastructure services such as XML coding/decoding and HTTP message transport. The Hosted services exclusively implemented in CIMOM providers are:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0137">Analyzer Provider&#x2014;Provides statistics about performance of traffic on ports on the switch;</li>
        <li id="ul0006-0002" num="0138">CLI Provider&#x2014;This provider implements services to allow CLI clients to access CIM managed services such as Clone, Analyzer, Fusion, and switch management;</li>
        <li id="ul0006-0003" num="0139">Clone Provider&#x2014;Provides services to manage the configuration and operation of clones;</li>
        <li id="ul0006-0004" num="0140">Data Mobility Provider&#x2014;Provides services to manage the configuration and operation of data migration between storage volumes transparently to the Host applications using the storage;</li>
        <li id="ul0006-0005" num="0141">Fusion Provider&#x2014;Provides services to configure and manage the combining of LUNs to create new LUNs of larger capacity;</li>
        <li id="ul0006-0006" num="0142">Mirror Provider&#x2014;Provides services to manage the configuration and operation of mirrors; and</li>
        <li id="ul0006-0007" num="0143">Switch Management Provider&#x2014;Provides services to configure and manage the attached intelligent switch components owned by FabricX.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0130" num="0144">The above-described providers periodically poll the system infrastructure to build a model of the existing component configuration and status. If any changes are detected in configuration or status between the existing model and the newly built model, registered observers are notified of the changes. The model is then updated with the new model and saved for queries by the provider. The services of these providers can be accessed from other providers by formulating XML requests and sending them to the CIMOM. This permits providers which require the services of other providers (such as Raid++ through the CLARiiON Provider or the CIM local services such as persistent storage, directory services, or security) to access those services. Additionally Admin STL Driver Access through the server side provides access to these providers to the drivers and services of an SP as shown in group <b>218</b>, including to the following drivers: Flare, Clones, Snaps, Fusion, and mirrors and services for switch management and data mobility.</p>
<p id="p-0131" num="0145">Other Service Providers are shown in group <b>212</b> of the server group <b>210</b>, and include the Persistent Data Service Provider, Security Provider, and Directory Service Provider. The Persistent Data Service Provider provides the CIMOM with access to the kernel-based PSM. The CIMOM uses the PSM to store meta-data relevant to the management system for example user names and passwords. The Security Provider supports authentication of users and authorization of user roles. The Directory Service Provider is used to obtain the network addresses of the systems in a domain of managed FabricX instances.</p>
<p id="p-0132" num="0146">Reference will be made below to <figref idref="DRAWINGS">FIGS. 12-20</figref> to describe a problem solved with the architecture including the software components described above with reference to <figref idref="DRAWINGS">FIGS. 1-11</figref>; however a general overview is now given. The inventors have critically recognized that Intelligent Multi-Protocol Switches (IMPS) <b>22</b> or <b>24</b> generally have limited memory resources available to support mapping virtual storage extents to physical storage extents. A typical switch today is capable of storing 10,000 maps per translation unit. Certain storage applications, such as a volume snapshot application can consume large numbers of these maps in support of a single volume. This further reduces the set of volumes that a translation unit can support.</p>
<p id="p-0133" num="0147">This problem is addressed with the architecture of the present invention by using memory with the FabricX Storage Processor <b>26</b> or <b>28</b> to supplement the memory resources of the IMPS's translation unit and more efficiently use memory of each. The translation unit's memory resources are used to store a subset of the full set of extent maps for the volumes exported by the translation unit. Maps are loaded to the translation unit from the CPP both on demand and ahead of demand in a technique denoted as being a virtualizer application which is preferably software running in a SP or on the IMPS. In this embodiment, sequential access is detected and future requests are predicted using protection maps to mark the edges of unmapped regions. Access to a marked region in combination with the detection of sequential access triggers the preloading of additional maps prior to the arrival of the actual request.</p>
<p id="p-0134" num="0148">I/O requests that arrive to find no mapped region are handled by loading the map from the control-processor. The control processor uses access data collected from the intelligent multi-protocol switch to determine which data to replace. Supported cache replacement algorithms include least-recently used, least frequently used. The IMPS hardware is used to detect need for extents prior to access and statistical data is collected on volume access to select which cache entries to replace. The mechanism further identifies volumes whose extent maps have become fragmented and triggers a process to reduce this fragmentation.</p>
<p id="p-0135" num="0149">Referring to <figref idref="DRAWINGS">FIG. 12</figref>, Virtualization Mapping from a logical volume <b>230</b> to physical storage <b>232</b>, <b>234</b>, and <b>236</b> is shown. A Host <b>18</b> write to the volume in the region 0-K is mapped to physical block segment <b>232</b> (SE1), and likewise to the volume in the region 0-(j&#x2212;k) to physical block segment <b>234</b> (SE2), and also to the volume in the region 0-(n&#x2212;k) to physical block segment <b>236</b>. In this simple example of Virtualization Mapping the Logical Volume <b>240</b><i>a </i>maps byte for byte with the storage element <b>242</b><i>a</i>. To be precise region 0 through k&#x2212;1 of <b>230</b> is mapped to SE1 (<b>232</b>). Likewise region k through j&#x2212;1 or <b>230</b> is mapped to SE2 (<b>234</b>) and region j through n&#x2212;1 is mapped to SE3 (<b>236</b>).</p>
<p id="p-0136" num="0150">Referring to <figref idref="DRAWINGS">FIG. 13</figref> and <figref idref="DRAWINGS">FIG. 14</figref>, such virtualization mapping is shown using the embodiment described above with reference to <figref idref="DRAWINGS">FIGS. 1-11</figref>, wherein <figref idref="DRAWINGS">FIG. 13</figref> shows an example case of before a Snapshot or Snap, and <figref idref="DRAWINGS">FIG. 14</figref> shows after a Snap where no writes have taken place. A source logical volume <b>240</b><i>a </i>(<figref idref="DRAWINGS">FIG. 13</figref>) is mapped to segment block <b>242</b><i>a </i>containing data &#x201c;A&#x201d; in the region 0-n. At the point when the Snap is created, the Snap Cache is merely associated with this Snap volume&#x2014;the Snap volume does not actually map portions of its storage to the Snap Cache until a write takes place.</p>
<p id="p-0137" num="0151">An example of solving the problem described in general is shown with reference to <figref idref="DRAWINGS">FIG. 15</figref>, which depicts the example of a virtualization mapping case wherein a write takes place after the Snap. This example case is described in reference to method steps shown in Flow diagrams depicted in <figref idref="DRAWINGS">FIGS. 16 and 17</figref>. In step <b>300</b>, the Host <b>18</b> writes data B to region j&#x2212;k of logical volume <b>242</b><i>b </i>(<figref idref="DRAWINGS">FIG. 17</figref>). In step <b>302</b> the write of data B is held by the storage application. The application carrying out the methodology is given control in step <b>304</b> which flows into the &#x201c;A&#x201d; connecting step <b>306</b>, which flows into its identically identified counterpart for <figref idref="DRAWINGS">FIG. 17</figref>.</p>
<p id="p-0138" num="0152">Referring to <figref idref="DRAWINGS">FIG. 17</figref>, in steps <b>308</b> and <b>310</b>, the application reads the original data A from region j&#x2212;k of segment <b>242</b><i>b </i>and writes the data to the allocated storage in the snap cache <b>246</b><i>b</i>. In step <b>312</b>, the application updates the logical mapping for the snap logical volume <b>244</b><i>b </i>to map region j&#x2212;k to the new region for A in the snap cache <b>246</b><i>b</i>. Then in step <b>314</b> the application allows the original write to proceed.</p>
<p id="p-0139" num="0153"><figref idref="DRAWINGS">FIGS. 18-20</figref> show fault extent mapping that is a further feature of the storage application. In this embodiment volumes whose extent maps have become fragmented are identified and a process to reduce this fragmentation is invoked. Referring to <figref idref="DRAWINGS">FIG. 18</figref>, volume <b>320</b> has been fragmented into N segments and volume <b>325</b> in conjunction with <b>324</b> represents a mechanism for preserving the image of <b>320</b>. I/O operations to the region covered by <b>324</b> cause a fault which yields control of the I/O to the storage application. This simplified diagram illustrates a fault region that applies to both reads and writes however the switches and the SAL Agent and the IMPS API also support the ability to create fault-regions that apply only to reads or only to writes. In the example case, the CPP running the storage application takes the fault and updates the extent map for the volume to map region 3 and a fault map <b>326</b> is created for the region <b>327</b> (<figref idref="DRAWINGS">FIG. 19</figref>). To reduce fragmentation a new fault map <b>330</b> is created and the map entries 1 and 2 are combined (<figref idref="DRAWINGS">FIG. 20</figref>). This causes a reduction in the number of entries required to support the volume.</p>
<p id="p-0140" num="0154"><figref idref="DRAWINGS">FIG. 26</figref> shows the storage software application <b>500</b> in an SP <b>72</b> or <b>74</b> and also including the software components <b>122</b>-<b>27</b> of <figref idref="DRAWINGS">FIG. 7</figref>. Computer-readable medium <b>502</b> includes program logic <b>503</b> that may include any or all of the software <b>122</b>-<b>127</b>, and <b>500</b>. Such a medium may be represented by any or all of those described at the beginning of this Detailed Description.</p>
<p id="p-0141" num="0155"><figref idref="DRAWINGS">FIGS. 21-25</figref> describe an embodiment for ensuring consistent error presentation to Hosts accessing virtualized volumes and for preserving structured error handling, and a general overview is now given. In general, it is recognized by the inventors that without this invention virtualizing intelligent switches process I/O requests from Host computers at wire speed and do not present these requests to storage applications. Errors that occur while processing these requests may present an inconsistent error behavior to the Hosts accessing the volume due to these errors originating from disparate back-end storage devices. Furthermore certain errors would be better handled and masked according to the structure and semantics of a virtual volume.</p>
<p id="p-0142" num="0156">In this embodiment the storage application (<figref idref="DRAWINGS">FIG. 26</figref>) is given control of a request when exceptional conditions occur such as an error when processing the request. This invention provides a means of handling errors in a hierarchical manner allowing storage applications to be structured in a manner consistent with the structure of a virtual volume. Errors are presented to the applications using a bottom-up delivery algorithm that allows lower-level applications to mask errors from higher Layers. Applications are given the initial context of the I/O request along with the error allowing them to only incur additional overhead when exceptional conditions arise. The storage applications process and transform errors from disparate back-end devices and unify the presentation of these errors to make the volume appear as a single device to the Hosts that access it.</p>
<p id="p-0143" num="0157">Referring to <figref idref="DRAWINGS">FIG. 21</figref>, a schematic block diagram of elements of the architecture shown in the above-described Figures is presented to show the relationship of the Layered drivers with the Device Object Graph <b>410</b> and its logical volumes in group <b>412</b>, and the relationship to Backend arrays <b>420</b> including physical volumes <b>422</b>-<b>426</b> to which the logical volumes are mapped. The Layered drivers TCD/TDD <b>400</b>, Clone <b>402</b>, Snap <b>404</b>, Fusion <b>406</b> and FlareX <b>408</b> are part of Layered drivers <b>123</b> discussed above with reference to <figref idref="DRAWINGS">FIG. 6</figref>. Generally each driver presents volumes to the Layer above it. For example the FlareX driver imports storage extents from the back-end array and presents the identity of such extents to Layered drivers above, e.g. Fusion, and Clone drivers.</p>
<p id="p-0144" num="0158">Regarding nomenclature used for presenting an example of how this embodiment functions, in the volume group <b>412</b>, V7 represents a &#x201c;top level&#x201d; volume, or one that is capable of being presented to a Host. V6 represents a cloning volume. This volume has two children, V3 and V5, where V3 is the source for the clone and V5 is its replica. Similarly V1 and V2 are children of V3 while V4 is a child of V5. It is the children at the bottommost Layer that are presented by the bottommost driver in this example (FlareX) to map the storage extents of the back-end array that are of interest.</p>
<p id="p-0145" num="0159">Referring now to <figref idref="DRAWINGS">FIG. 22</figref>, a Device Graph <b>430</b> and a Volume Graph <b>432</b> illustrate the relationship between drivers and volumes. The Volume Graph is a data structure that is the same as a known Device Graph in known Clariion architecture. The Volume Graph maps to the Device Graph in order to present information needed for virtualization by the IMPS, and provides a mechanism by which errors can be received and handled by the Device Drivers. In an effort to have the Host seamlessly communicate with software components discussed herein in the same way it would if communicating with a data storage system such as an EMC Symmetrix or Clariion. Virtualization Software in the SP accomplishes this goal and also by communicating with software in the IMPS to present information about the volumes consistently, including presentation and management of errors. The Volume Graph provides a mechanism by which an error can be introduced into the Device Graph and presented to the Host.</p>
<p id="p-0146" num="0160">Information extracted from the IMPS through its API includes the type of I/O operation (e.g., read or write), a logical volume target or in this context a virtual target (VT), a virtual LUN (v-LUN), physical target, a physical LUN, block address of a physical device, length of request, and error condition. The error condition is received and that identifies the bottommost affected volume and delivered to the bottommost affected object.</p>
<p id="p-0147" num="0161">Objects in the Volume Graph have one-to-one congruity with objects in the Device Graph, as follows. At the top Layer (V7) the Volume Graph Object G represents a slice volume. A slice volume represents a &#x201c;slice&#x201d; or partition of its child volume. It has only one child volume and presents a (possibly complete) contiguous subset of the data of this child volume. The next Layer down (V6) is congruently mapped to element F represents the mirrored volume. V3 represents an aggregated volume consisting of the concatenation of volumes V1 and V2. Volume V5 is a slice volume that presents the data of V4. Slice Volume Objects A, B, and D, are congruent with Device Graph Objects V1, V2, and V4, respectively.</p>
<p id="p-0148" num="0162">Upon receiving an error, the Bottommost Volume Graph Object will deliver the error to its corresponding Device Graph Object. The Device Graph Object as the owner of the error can decide how to handle, e.g. propagate the error up the Layer stack, or deal with it directly and not propagate it. Two goals are carried out in the error presentation: consistent presentation to the Host, and masking of errors which involves using independent capabilities of the FabricX architecture to solve problems for the Host at the unique middle-Layer level of a FabricX instance.</p>
<p id="p-0149" num="0163">Reference is made below to <figref idref="DRAWINGS">FIGS. 23-25</figref>, wherein an example of handling of an error for either consistent presentation to the Host or masking of the error while handling the error at the FabricX instance or middle switch level is shown. In the example shown in <figref idref="DRAWINGS">FIG. 24</figref>, Virtual Target (VT) LUN or VT/LUN <b>450</b> is loaded with a Volume presentation of a Clone or Clone/Volume <b>452</b> and having two Children Volumes, i.e. &#x201c;Child1&#x201d; <b>454</b> and &#x201c;Child2&#x201d; <b>456</b>. In this example case, Child1 and Child2 are synchronized, Child1 represents storage for one type of Data Storage System, for example an EMC Symmetrix, and Child2 represents storage for another type of Data Storage System, for example an EMC Clariion.</p>
<p id="p-0150" num="0164">In handling the error in this example, there are two cases for handling in keeping with the goals discussed above: (a) Case 1&#x2014;error is not delivered back to Host but rather handled by a Device Object at the FabricX instance level; or (b) Case 2&#x2014;error is transformed to have the error personality of FabricX. Each method of handling is explained in the example, continued with the flow-diagrams of <figref idref="DRAWINGS">FIGS. 24 and 25</figref>.</p>
<p id="p-0151" num="0165">Referring to <figref idref="DRAWINGS">FIG. 24</figref>, in step <b>460</b>, an I/O command in the form of a read arrives at an IMPS for the VT/LUN. In step <b>462</b>, the IMPS selects Child2 to perform the read. The IMPS forwards the read to the data storage system supporting Child2 in step <b>464</b>. That data storage system returns an error for the read in step <b>466</b>. In step <b>468</b>, the IMPS receives the error and presents the error to the SAL Agent in the IMPS in the preferred embodiment, but one skilled in the art will recognize that it could run elsewhere, including in an SP. In steps <b>468</b> and <b>470</b>, the error is propagated from the SAL Agent running on the IMPS to the SAL CPP running on the SP. The SAL CPP then delivers the error to the Volume Graph Manager. The Volume Graph Manager VGM identifies the bottommost affected volume object and delivers the error to this object in step <b>474</b>. Connecting step <b>476</b> labeled &#x201c;B&#x201d; connects with the identically numbered and labeled step on <figref idref="DRAWINGS">FIG. 25</figref>.</p>
<p id="p-0152" num="0166">Referring to <figref idref="DRAWINGS">FIG. 25</figref>, in step <b>480</b>, the bottommost object (D) delivers or presents the error and its I/O context to the Volume Graph Object Owner (V4). V4 decides it will not handle the error and indicates to D to propagate the error, which D does up to E, shown in steps <b>482</b> and <b>484</b>. E presents the error to V5 in step <b>486</b>. In step <b>490</b> V5 decides whether to mask the error (Case 1) and handle it itself at the FabricX level, or to return it to the Host (Case 2). If Case 2 is selected, then the error is transformed to have the error personality of FabricX. In this example, that means that V7 updates the error presented by the storage system and using the SAL Agent to modify the error on the switch via the IMPS API. This in turn causes the error to be returned to the Host with modified content, but from the switch or FabricX level.</p>
<p id="p-0153" num="0167"><figref idref="DRAWINGS">FIG. 27</figref> shows an architecture including a plurality of control path processors (CPC) <b>600</b>, each with a Virtualization application-specific integrated circuit (ASIC) <b>604</b> on board. An ASIC is an integrated circuit customized for a particular use, rather than a general-purpose use. As feature sizes have shrunk and design tools improved, complexity and functionality possible in ASIC's have grown from 5,000 gates to over 100 million. The new architecture leverages the new ASIC technology for advantages discussed herein.</p>
<p id="p-0154" num="0168">The CPC includes intelligence for virtualization of storage units and switch management related to switches <b>610</b>-<b>618</b> in a data storage environment. The CPC is essentially identical in functional capability to the CPP of <figref idref="DRAWINGS">FIGS. 2-3</figref>, and <b>5</b>-<b>6</b>, and capable of interacting with the DAE of <figref idref="DRAWINGS">FIG. 4</figref>, and providing similar capability of the Instance <b>27</b> of <figref idref="DRAWINGS">FIG. 1</figref>. The CPC has advantages over the CPP because it includes the Virtualization ASIC that includes the intelligence of the IMPS of <figref idref="DRAWINGS">FIGS. 1-2</figref>, <b>5</b> and <b>7</b>, essentially obviating the need for a separate IMPS from a switch vendor. The CPC has costs and reliability advantages over the architecture of the CPP. The CPC is capable of interacting with all of the components, hardware and software included, that the CPP interacts with providing all of the Functionality discussed in <figref idref="DRAWINGS">FIGS. 8-25</figref>, and can be represented, at least in part, by the Program Logic of <figref idref="DRAWINGS">FIG. 26</figref>.</p>
<p id="p-0155" num="0169">In particular, as described above, the Architecture of <figref idref="DRAWINGS">FIG. 27</figref> includes the capabilities discussed with reference to the embodiment shown in <figref idref="DRAWINGS">FIG. 1</figref>, including the following features. The new architecture also allows for management of resources to be moved off of storage arrays themselves, allowing for more centralized management of heterogeneous data storage environments. Advantages provided include: (1) centralized management of a storage infrastructure; (2) storage consolidation and economical use of resources; (3) common replication and mobility solutions (e.g., migration) across heterogeneous storage subsystems; and (4) storage management that is non-disruptive to Hosts and storage subsystems.</p>
<p id="p-0156" num="0170">The Architecture of <figref idref="DRAWINGS">FIG. 27</figref>, includes the virtualization ASIC's embedded in the CPC node. Preferably there is one ASIC per CPC node. Storage Applications, such as those discussed in the <figref idref="DRAWINGS">FIGS. 8-26</figref> port directly to the ASIC. Performance scales linearly with additional nodes, but the CPC may be managed and developed separately from the Virtualization ASIC. Low cost hardware and reduced components makes this viable for low-end solution. This embodiment is not dependent on a particular switch vendor, for a particular IMPS, and it functions well in an iSCSI or Fibre Channel protocol-directed data storage environment.</p>
<p id="p-0157" num="0171"><figref idref="DRAWINGS">FIG. 28</figref> shows another embodiment of an architecture including a plurality of control path processors <b>600</b> and paired virtualization ASIC <b>604</b> including intelligence for virtualization of storage units and onboard switch management and switches in a data storage environment including at least one data storage system <b>624</b>. The CPC can perform virtualization of logical volumes or LUNS on the storage system to present to Host A or B <b>620</b>-<b>622</b>, acting as a so-called Storage Area Network (SAN) on a controller.</p>
<p id="p-0158" num="0172"><figref idref="DRAWINGS">FIG. 29</figref> shows the <b>600</b> including paired Storage Processors (SP<sub>N</sub>) and virtualization ASIC (VA<sub>N</sub>) <b>604</b>. This view is intended to show how the CPC is scalable to meet the needs of the storage network and can grow by simply adding CPC's with paired SP<sub>N</sub>'s and VA<sub>N</sub>'s.</p>
<p id="p-0159" num="0173"><figref idref="DRAWINGS">FIG. 30</figref> shows more detail regarding functional elements of an embodiment of a SP on the CPC of <figref idref="DRAWINGS">FIG. 29</figref> paired with the virtualization ASIC. The SP <b>602</b> includes, similar to the CPP and SP shown in <figref idref="DRAWINGS">FIG. 6</figref>, User-Level Services, Kernal Services, and a Switch Abstraction Layer (SAL) Agent. The Virtualization ASIC carries out at least some functionality that had been carried out by the IMPS in <figref idref="DRAWINGS">FIG. 7</figref>, while the CPC carries out other functions that had been carried out by the IMPS.</p>
<p id="p-0160" num="0174"><figref idref="DRAWINGS">FIG. 31</figref> shows an embodiment of a complete ASIC <b>704</b> including an integrated storage processor <b>709</b> with virtualization circuitry built in to the complete ASIC. In this embodiment one ASIC embeds circuitry that carries out the complete functionality shown in <figref idref="DRAWINGS">FIGS. 6 and 7</figref>, and can carry out all the functionality described in <figref idref="DRAWINGS">FIGS. 8-25</figref>, and may be executed by the Program Logic of <figref idref="DRAWINGS">FIG. 25</figref>.</p>
<p id="p-0161" num="0175">Alternative Embodiment of Architecture</p>
<p id="p-0162" num="0176">FIGS. <b>32</b> through <b>43</b>A-G describe an alternatively embodied architecture for carrying out methodology described herein, including uses and methods described in reference to these figures. Some of the advantages of this alternative embodiment include significantly increasing the scalability to provide network-based block storage virtualization services that cover an entire enterprise data center and maintain that coverage as the data center grows. The architecture can support scale that allows an instance to grow as the data center grows. It increases the reliability and availability in a complementary and enhancement fashion to existing data storage systems, such as the preferred EMC Symmetrix or Clariion data storage system. It provides a single point of management for an instance that covers management of storage virtualization, and the network and array resources needed to provide this virtualization. It also yields an architecture that is flexible in accommodating new and disparate block storage virtualization technologies.</p>
<p id="p-0163" num="0177">Referring to <figref idref="DRAWINGS">FIG. 32</figref>, a logical structure of an alternative architectural embodiment is shown, which in a preferred embodiment can be implemented by hardware and/or software provided by EMC corporation. It is envisioned that it can be provided with EMC's preferred Invista system for storage management with virtualization. Thus, as used below, a preferred Invista entity, such as an Invista Instance may be forward looking in view of the teachings herein based on the critical recognitions of the inventors. Invista is merely one envisioned embodiment and many others will likely occur to one skilled in the art in view of the teachings herein.</p>
<p id="p-0164" num="0178"><figref idref="DRAWINGS">FIG. 32</figref> shows a logical structure of instances managed in an architecture that includes a Storage Virtualization Manager <b>720</b>, including a Host Layer <b>722</b>, Storage Presentation Layer <b>730</b>, Volume Virtualization Layer <b>732</b>, and RAID Protection Layer <b>734</b>. The Host Layer includes Hosts <b>724</b> and <b>726</b>. The Storage Presentation Layer includes SCSI Virtualization <b>729</b>, SCSI Target Virtualization <b>731</b>, and SCSI LU Virtualization <b>733</b>. The Volume Virtualization Layer includes Volume Virtualization <b>735</b> (e.g. data mobility) and Volume Virtualization <b>737</b> (e.g. remote replication). The volume virtualization layer may include fewer or more volume virtualizations with data mobility and remote replication being only two such examples. The RAID protection Layer includes physical storage with storage elements (SE) <b>736</b> and <b>738</b>. Although SCSI protocols are discussed throughout, one skilled in the art will recognize these are applicable to iSCSI protocols or the like as well.</p>
<p id="p-0165" num="0179">The Manager <b>720</b> manages two major logical layers; the Storage Presentation Layer and the Volume Virtualization Layer. These Layers are realized by preferred EMC Invista hardware components including pairs of Service Processors (SP's) forming Control Processor Clusters (CPC's), Data Path Controllers (DPC's), and Appliances (APPL's), by Layer 2 FibreChannel switches, by IP switches, and by Physical Storage Arrays. These hardware elements and their associated software are discussed in reference to Figures described above, including <figref idref="DRAWINGS">FIG. 32</figref>, and also with regard to <figref idref="DRAWINGS">FIGS. 35 and 38</figref> referenced below</p>
<p id="p-0166" num="0180">These components are managed by a highly-available management service called the preferred Invista Virtualization Manager <b>720</b> that maintains communication paths to the preferred Invista components, Fabric, and Array components via an IP service network and a SAN. Each Layer has its own requirements for the execution environments that support it. A key aspect of the architecture is its ability to realize these execution environments with different physical deployments. This allows the architecture to serve the needs of both small and large data centers with cost effective deployments for each.</p>
<p id="p-0167" num="0181">The Storage Presentation Layer is responsible for exposing virtualized block-storage to Hosts and their applications. This Layer presents LUN's to Hosts on Virtual Targets. Each virtualized volume is typically presented to one or more Hosts on two or more independent fabrics in the SAN. This is the only Layer of the system directly visible to the Hosts and from the Host's perspective this Layer acts much like an array. The Layer implements the SCSI personality of the preferred Invista instance. Structurally, there are three pieces to this Layer; SCSI Virtualization <b>729</b>, SCSI Target Virtualization <b>731</b> and SCSI LU Virtualization <b>733</b>. The SCSI Virtualization component is responsible for interfacing with the Management Server's Mediation Layer (See <figref idref="DRAWINGS">FIG. 33</figref>) for provisioning data in the SCSI Target Virtualization and SCSI LU Virtualization components.</p>
<p id="p-0168" num="0182">The SCSI Target Virtualization component is responsible for handling all requests at the SCSI Target level. Many non-read/write SCSI commands can be completely serviced at this level (in some cases after having obtained some initial information from the SCSI Virtualization and SCSI LU Virtualization components). The nature of a SCSI Target is that it exists in a single fabric. It can be reset and logged in or out of the fabric independently of other SCSI targets. The presentation of an LU to a Host must be made highly available by presenting the LU on two or more SCSI Targets that are placed in separate fabrics. While a SCSI Target should be reliable it does not need to be continuously available due to the replicated nature of the presentation.</p>
<p id="p-0169" num="0183">The Volume Virtualization Layer <b>732</b> is responsible for preserving the Hosts' data while performing various transformations on the data. Typical transformations in this Layer include local replication for data protection across arrays, data mobility for array lease rollover and ILM functions, point in time data copies for archiving, and remote replication for disaster recovery.</p>
<p id="p-0170" num="0184">Volume Virtualization typically includes the use of acceleration hardware, DPC's with ASIC's, to perform transformations on the layout of data at I/O speeds, but it can also be realized by the functions within a typical storage array or by virtualizations provided by an appliance. Some volumes may undergo multiple Volume Virtualizations. The ability to compose different network-based features together by linking these virtualizations together via routing is a key principle that achieves flexibility in the architecture and the ability to adopt new technologies quickly.</p>
<p id="p-0171" num="0185">The Execution Environment for Volume Virtualization requires redundant processing components to ensure that I/O requests can be serviced in the presence of failure. Multiple Volume Virtualization Execution Environments can be added to an instance to scale the number of volumes that it supports. A typical Volume Virtualization Execution Environment consists of a CPC formed from a pair of Storage Processors and two or more DPC's with integrated ASIC's for accelerated virtualization.</p>
<p id="p-0172" num="0186">The Volume Virtualization Layer <b>732</b> implements storage features in a Storage Area Network (SAN). It consists of Volume Virtualizers <b>735</b> and <b>737</b> which both consume and expose SCSI Logical Units (LU's), implementing one or more Volume Virtualizations (features). These Volume Virtualizations may range from simple volume management (partitioning and concatenation) to complex features such as asynchronous mirroring and snapshots. Various kinds of Volume Virtualizers may exist, with possible specialization for a specific set of tasks.</p>
<p id="p-0173" num="0187">The Volume Virtualization Layer exposes a set of LU's to the Storage Presentation layer. In other words, the Storage Presentation Layer consumes LU's exposed by the Volume Virtualization Layer. The LU's are presented to the Storage Presentation Layer via Initiator Target LUN (ITL) data structures, which provides SCSI endpoint semantics. Each ITL represents a point of access from a Host initiator to a logical unit via a given target. Here, the ITL's only function is it to provide SCSI connectivity between the two layers.</p>
<p id="p-0174" num="0188">For the Storage Presentation Layer, the Volume Virtualization appears as a SCSI Target to which it can login and send I/O. One skilled in the art will recognize that the SCSI Virtualization layer and the Volume Virtualization layer can be combined within the same processor (e.g. the DPC). In such a manifestation, the linkage between the two need not be via a SCSI routing and can be directly controlled by software (e.g. the embodiment shown in <figref idref="DRAWINGS">FIG. 38</figref>). The Volume Virtualization Target represents this entity. Depending on the type of the Volume Virtualizer, this could either be a virtual target or a physical target. Volume Virtualizers are configured by a Management Server (discussed in reference to <figref idref="DRAWINGS">FIG. 33</figref> below), which controls which LU's they expose and consume. A typical preferred Invista instance will have multiple Volume Virtualizers functioning independently, which enables a high degree of scalability and also provides failure containment. A typical volume will be routed though multiple volume virtualizations, using one or more Volume Virtualizers. That is Volume Virtualizers can consume volumes from other Volume Virtualizers also in order to realize more complex virtualizations.</p>
<p id="p-0175" num="0189">The inventors have critically recognized that Volume Virtualizers should be highly available devices, exposing volumes via multiple ports, and consuming storage using multiple paths, and well as having internal redundancy. In accordance with this recognition, this alternative architecture allows for Volume Virtualizer failover for increasing the availability of a volume.</p>
<p id="p-0176" num="0190">Volume virtualizations can be grouped into two classes: pass-though virtualizations, where the consumed LU is mapped 1:1 to the exposed LU, and the contents of the two are identical, and transforming virtualizers, where this characteristic does not hold. Pass-through virtualizations can be removed easily without affecting the Host's view of the storage. Transforming virtualizations cannot be transparently removed without data reorganization. Another characteristic of a Volume Virtualizer is whether the state of the virtualization exists completely on the SAN, or whether parts of the state exist only within the Volume Virtualizer itself. Mirrors, although do not transform data, are considered to be transforming type virtualizations in the sense that they are not 1:1 mapping of the back-end.</p>
<p id="p-0177" num="0191">Pass through virtualizations and virtualizations whose state completely exists in the SAN have the characteristic that the virtualization can be atomically moved from one Volume Virtualizer to another upon the failure of the first Volume Virtualizer without service interruption (provided that the new virtualizer can be brought online within I/O timeout limits).</p>
<p id="p-0178" num="0192">Different kinds of Volume Virtualizers will have different performance characteristics. Some Volume Virtualizers may route all I/O through general purpose processors, providing functionality at the lowest price/performance. Other Volume Virtualizers will include special purpose hardware to optimize the common data paths, including hardware similar to that used for port virtualization. A special sub-class of this type of Volume Virtualizer is capable of leveraging the port virtualization hardware itself concurrently with the port virtualization Layer. Volume virtualizers used by preferred Invista may have been created with no awareness of preferred Invista, or were created specifically for preferred Invista, or may have initially been created without preferred Invista awareness, but have been retrofitted to be more integrated with preferred Invista.</p>
<p id="p-0179" num="0193">Disk arrays consume SCSI LU's and expose SCSI LU's, and can therefore be classified as Volume Virtualizers. Appliances also can qualify as Volume Virtualizers. We can also have a Volume Virtualizer implemented as a set of DPC's controlled by CPCs. It is also possible to mix these up, so that part of the virtualization is realized on one type of Volume Virtualizer and other part is realized on another type of Volume Virtualizer.</p>
<p id="p-0180" num="0194">Based on the above discussion, Volume Virtualizers can be volume virtualizers designed to work with SCSI target virtualizers; array Volume Virtualizers; and appliance Volume Virtualizers. The preferred Invista Volume Virtualizers (IVV's) are Volume Virtualizers designed to work optimally with preferred Invista SCSI Target Virtualizers and DPC's. IVV's leverage the performance advantages inherent in a DPC while providing a consistent feature set across various DPC vendors. Their main responsibilities include: handling R/W commands; performing volume virtualization transformations; providing I/O routing to back-end and other Volume Virtualizers; and managing back-end paths.</p>
<p id="p-0181" num="0195">Additionally, storage arrays such as EMC's preferred Clariion can be used as Volume Virtualizers. These Volume Virtualizers may be used stand-alone or can be consumed by other Volume Virtualizers e.g. an preferred Invista Volume Virtualizer may use native CLARiiON to achieve snapshot capabilities. The main responsibilities include: handling R/W commands; performing Volume Virtualization transformations; and provide access to back-end storage.</p>
<p id="p-0182" num="0196">Appliance Volume Virtualizers are devices specialized to do certain virtualizations. They can be used in a stand-alone mode where the SCSI Target virtualization Layer directly connects to the appliance or can be consumed by another Volume Virtualizers e.g. an preferred Invista Volume Virtualizer may use a preferred Kashya appliance for replication available from EMC Corporation to achieve remote replication of its virtualized LU's. The main responsibilities include: handling R/W commands; performing Volume Virtualization transformations; providing I/O routing to back-end and other Volume Virtualizers; managing back-end paths</p>
<p id="p-0183" num="0197"><figref idref="DRAWINGS">FIG. 33</figref> illustrate the layers of management using Storage Virtualization Manager <b>720</b>. The layers of the preferred EMC Invista management are: the Management Server <b>740</b>, the Cluster Provider <b>742</b> and the Virtualization Nodes <b>743</b>. The Management Server <b>740</b> provides the centralized management elements organized as a collection of Web Services, these services provide: browser based GUI to achieve an active display; security services supporting embedded, Active Directory, and LDAPv3 directories to support authentication and authorization; services to manage licensing; common logging and auditing services compliant auditing of management events and actions. It also includes services to manage the configuration of zones on fabric switches; services to manage the provisioning of array storage to the Invista instance for virtualization; services to manage the configuration, provision virtual storage to hosts, configure and manage replication services (Clones and Mirrors), configure and manage Data Mobility, configure and manage Dynamic Storage Pools, and configure and manage ancillary capabilities such as Continuous Data Protection and Long Distance Replication provided by appliances, e.g. a preferred EMC-owned RecoverPoint replication appliance, which may be virtualized by the Appliance Node of the Virtualization Nodes <b>743</b>.</p>
<p id="p-0184" num="0198">The Management Server <b>740</b> is preferably based on a so-called Common Management Platform (CMP), which is an architecture that supports services, such a Switch Configuration and Array Services, Interfaces, such as a Graphical User Interface (GUI) and Command Line Interface (CLI). The CMP architecture has a single Domain Manager component for each unique device type and also supports Business Logic Software, and a Domain Manager Mediation Layer.</p>
<p id="p-0185" num="0199">The Cluster Provider <b>742</b> provides a single point of management from a preferred Invista instance to the Management Server <b>740</b> or other management entity. The interface from the Cluster Provider <b>742</b> to the Management Server <b>740</b> preferably conforms to Distributed Management Task Force (DMTF) standards for interfacing to the Common Information Model Object Manager (CIMOM). This interface presents a model conforming to the preferred EMC Common Information Model (ECIM) which is an extension to the Storage Networking Industry Association's (SNIA) Storage Management Initiative (SMI) modeled in DMTF modeling language (MOF). The Cluster Provider communicates with the various preferred Invista CPC's that are part of Virtualization Nodes <b>743</b> in the native management protocol for each CPC. These nodes are the SCSI and Volume Virtualizers of the Storage Presentation and Volume Virtualization layers.</p>
<p id="p-0186" num="0200">The Cluster Provider <b>742</b> configures each CPC from a persistent repository (not shown) included as part of the Cluster Provider, collects the management state and status information for each CPC, and transforms management commands and responses between ECIM model and CIM protocol of the Cluster Provider <b>742</b> and Management Server <b>740</b> and the native object model and protocol of a preferred Invista CPC of node <b>743</b>. The Cluster Provider also implements the CIM Indication service to pass filtered events from the Cluster Providers CIMOM to the Management Server <b>740</b> or any other registered and authorized management entity. The Cluster Provider communicates with each managed Invista CPC through an adaptor (CPC Model of <b>742</b>) which understands the protocol and object model of each type of component (CPC of <b>743</b>) as well as any version specific model differences of a particular CPC. The adaptor translates between the local object models of the CPCs and the ECIM. An Appliance Model also exists in the ECIM instance.</p>
<p id="p-0187" num="0201">Each component CPC of nodes <b>743</b> of the preferred Invista has a local management agent; that agent communicates in its own protocol (CIM/XML over HTTP as defined in DSP0200 and DSP0201 or other protocol) and presents a CPC specific object model. Each CPC persists a copy of its own configuration data locally to facilitate local operation; this copy of the configuration data is reconciled with the Cluster Provider's persistent repository upon CPC initialization and if any discrepancies are found the Cluster Provider's (of <b>742</b>) persistent version is used to reload the CPC (of <b>743</b>) local repository. The management agent on the CPC (of <b>743</b>) also sends CIM Indications (or other device specific events) to the CPC Model interface on the Cluster Provider (of <b>742</b>) to notify the Cluster Provider of CPC state changes.</p>
<p id="p-0188" num="0202">The Cluster Provider (of <b>742</b>) is highly available, it runs on two separate executions platforms to insure availability of management in the face of a software or platform failure. The Cluster Providers communicate to coordinate management operations and to coordinate initialization and recovery of a Cluster Provider. The Cluster Providers can share the workload of a preferred Invista-based instance, the relational database used for the persistent repository of the Cluster Provider is used as the coordination point between management actions on the Cluster Provider instances.</p>
<p id="p-0189" num="0203">The Management Server <b>740</b> is preferably made highly available; it runs on two or more separate execution platforms to insure availability of the management services in the face of a software or hardware failure. The Management Server instances are independent but share common persistent data. A CPC (of <b>743</b>) may or may not be highly available. In highly available configurations the management agents of the CPC coordinate the updating of the local persistent repository and share the management of the CPC. In the event of a partial CPC failure the remaining management agent is capable of fully managing the CPC instance.</p>
<p id="p-0190" num="0204"><figref idref="DRAWINGS">FIG. 34</figref> shows how volume maps get distributed among the Storage Presentation Layer <b>730</b> and Volume Virtualization Layer <b>732</b>, which itself includes Volume Virtualization <b>737</b>, which is also a logical construction. Volume Virtualization <b>737</b> is realized by two physical components: the Volume Virtualizer <b>735</b> and the Appliance Volume Virtualizer <b>741</b>.</p>
<p id="p-0191" num="0205"><figref idref="DRAWINGS">FIG. 34</figref> shows how volume maps get distributed among the Storage Presentation Layer <b>730</b> and Volume Virtualization Layer <b>732</b>. Having a logical separation between the Storage Presentation and the Volume Virtualization layers and having the ability to stack different Volume Virtualizers means that the volume maps are also distributed in these layers. Each volume map will consume a lower level as if its connected to an SE or back-end disk. The volume maps in the Storage Presentation Layer will typically be very simple as they only provide SCSI endpoint semantics. The volume maps in the volume virtualization layer are more complex as they provide I/O transformation mappings and may possibly consume volumes from other Volume Virtualizers as well. The SCSI LU Virtualizer <b>733</b> is responsible for providing the virtualization of SCSI state associated with the logical unit (LU) where this state is specific to the LU and not the target through which this LU is accessed.</p>
<p id="p-0192" num="0206">Having a logical separation between the Storage Presentation and the Volume Virtualization Layers and having the ability to stack different Volume Virtualizers means that the volume maps are also distributed in these Layers. Each volume map will consume a lower level as if its connected to an SE or back-end. The volume maps in the Storage Presentation Layer will typically be very simple as they only provide SCSI endpoint semantics. The volume maps in the Volume Virtualization Layer are more complex as they provide I/O transformation mappings and may possibly route to other Volume Virtualizers as well. The SCSI LU Virtualizer <b>733</b> is responsible for providing the virtualization of SCSI state associated with the logical unit (LU) where this state is specific to the LU and not the target through which this LU is accessed.</p>
<p id="p-0193" num="0207"><figref idref="DRAWINGS">FIG. 34</figref> shows an example of how distributed volume maps for Storage <b>736</b> and <b>738</b> would be realized on different logical entities thorough Layers <b>730</b> and <b>732</b> involving Host <b>726</b>. Typically a BEV such as the BEV1 volume <b>752</b> will connect to the TLV, such as TLV1 <b>751</b> in the consumed graph as if it were connecting to a back-end. At every Layer, except the SCSI Target Virtualization Layer, the TLV's will be exposed via mid-layer ITL's, which provide SCSI endpoint semantics. The depicted example also shows a scenario where Volume Virtualizer <b>747</b> is mirroring data to a local storage as well as to an appliance based virtualizer. The BEV1 object <b>752</b>, which represents a disk in the graph maintained within the SCSI Target Virtualizer <b>731</b>, is consuming the TLV2 object <b>754</b> from the Volume Virtualizer <b>747</b>. The SCSI Target Virtualizer is treating this TLV (Top Level Volume object) as though it were a disk. The fact that the volume virtualizer is further virtualizing the volume is of no concern to the SCSI Target Virtualizer.</p>
<p id="p-0194" num="0208">The Volume Virtualization Layer <b>732</b> includes a mirror volume <b>756</b> that is responsible for replicating write I/Os to the legs of the mirror beneath it. Read I/Os can be serviced from either leg of a synchronized mirror. BEV2 <b>758</b> and BEV3 <b>760</b> represent back-end Volumes. These are logical representations of a disk and play the same role as BEV1 in <b>731</b>. In <b>732</b>, BEV3 is bound to a virtual volume presented by an appliance. This appliance performs additional virtualizations on the volume. For example, the Appliance Volume Virtualizer <b>741</b> could be a preferred EMC RecoverPoint (from EMC-owned Kashya) appliance that provides a Continuous Data Protection virtualization on the volume. Appliance SCSI Transformations <b>762</b> exist in the Appliance Volume Virtualizer <b>737</b> for tracking changes made to the volume allowing the data of the volume at a given time to be subsequently presented to a host.</p>
<p id="p-0195" num="0209">Referring to <figref idref="DRAWINGS">FIG. 35</figref> the way in which various hardware platforms deployable within logical instance of the architecture of <figref idref="DRAWINGS">FIG. 32</figref> realize software execution environments defined below is shown. <figref idref="DRAWINGS">FIG. 35</figref> shows preferred hardware deployments including ASIC's as shown in <figref idref="DRAWINGS">FIGS. 27-31</figref> for embodying the execution environments shown in <figref idref="DRAWINGS">FIG. 35</figref>. The following enumerates the main responsibilities of the software components depicted on <figref idref="DRAWINGS">FIG. 35</figref>, and makes reference to an exemplary hardware embodiment discussed with reference to <figref idref="DRAWINGS">FIG. 38</figref> below.</p>
<p id="p-0196" num="0210">Management Server Execution Environment <b>782</b> implemented by Management Server Processor <b>834</b> provides a management interface to the user. It also coordinates Invista instance's object model and coordinates dissemination of configuration information. It provides notification of state changes in the instance and Integrates with other preferred EMC SAN management tools. Mediation Layer Execution Environment <b>744</b> implemented by either Appliance <b>800</b> or one or more of the Storage Processors <b>900</b>, <b>902</b>, <b>922</b>, or <b>925</b> mediates EMC CIM (ECIM) requests received from the Management Server and translates them to format(s) supported by the preferred Invista components. Environment <b>774</b> translates notifications received from Invista Gen2 components into events supported by ECIM object model and maintains the management object model for selected components, such as a database service, communications director, a cluster manager, and an instance manager.</p>
<p id="p-0197" num="0211">Referring again to <figref idref="DRAWINGS">FIG. 35</figref> with reference to hardware components described in reference to <figref idref="DRAWINGS">FIG. 38</figref> discussed below, a SCSI Virtualization Execution Environment <b>786</b> implemented by Storage Processor <b>900</b>, <b>902</b>, <b>922</b>, and/or <b>924</b> interfaces with the management mediation layer, coordinates provisioning and configuration of SCSI Target and Logical Units (LU) Virtualizers, and provides support for LUN mapping and masking. A SCSI Target Virtualization Execution Environment <b>796</b> implemented by DPC <b>830</b> and/or one or more of the Storage Processors exposes and manages Virtual SCSI Targets, handles logins, logouts, and other SCSI target based commands, processes SCSI non-READ/WRITE commands that do not require inter DPC coordination and handles ITL-based NACA. LU Virtualization Execution Environment <b>808</b> implemented by one or more of the Storage Processors coordinates SCSI reservations, handles non-READ/WRITE commands that require inter DPC coordination, and handles UNIT ATTENTION state. A Volume Virtualization Execution Environment <b>798</b> implemented by Array <b>790</b> handles READ/WRITE commands, performs Volume Virtualization transformations, provides I/O routing either to back-end storage or to other Volume Virtualizers, and manages back-end paths.</p>
<p id="p-0198" num="0212"><figref idref="DRAWINGS">FIG. 36</figref> illustrates an example data flow for a volume configured for high availability by Storage Virtualization Manager <b>720</b> in the presence of the loss of a Volume Virtualizer. In this example, the SCSI Target Virtualization <b>731</b> is presenting volume V1 to host <b>726</b>. Volume V1 <b>850</b> is bound to volume M <b>852</b> in the preferred Invista Volume Virtualization layer <b>842</b> and <b>846</b> by the Storage Presentation Layer. Volume M is a mirroring volume that is replicating writes to volumes V2 <b>854</b> and V3 <b>856</b>. V2 and V3 are virtualizing volumes that each maintain the same data on separate physical storage. In this example, V2 maintains the data of the volume on the back-end array volumes in RAID Protection Layer <b>734</b> SE1 and SE2 of Array 1 <b>946</b>, by treating the storage of SE2 as a logical concatenation or extension of the storage on SE1. V3 is maintaining its copy of the data on SE3 of Array 2 <b>948</b>. The capacity of SE3 is equal to the combined capacity of SE1 and SE2. At this point in the scenario SE3 has the identical data of SE1 and SE2 combined.</p>
<p id="p-0199" num="0213"><figref idref="DRAWINGS">FIG. 37</figref> shows the data flow when an error has occurred on a Volume Virtualizer and M is no longer accessible from V1. In this situation the Storage Presentation Layer rebinds the data flow from V1 to V3 bypassing M. In this manner access to the Host's data can continue despite the failure of the Volume Virtualizer.</p>
<p id="p-0200" num="0214">An alternative failure scenario (not shown) is a failure of the Volume Virtualizer including V3. In this scenario, the mirroring volume M, detects an inability to reach V3 and stops replicating writes to V3, all access is subsequently directed purely to V2 and the Host is able to continue accessing data despite the failure of another Volume Virtualizer.</p>
<p id="p-0201" num="0215"><figref idref="DRAWINGS">FIG. 38</figref> shows an example hardware and software deployment of the alternative embodiment architecture of <figref idref="DRAWINGS">FIG. 32</figref>. In this deployment a Data Path Controller (DPC) <b>830</b> is hosting both the SCSI Target Virtualization Service <b>910</b> and the Volume Virtualization Acceleration Service <b>912</b>. DPC <b>831</b> hosts <b>911</b> and <b>913</b>, which are identical in function to <b>910</b> and <b>912</b>, respectively. In this example each Data Path Controller is realized by Application Specific Integrated Circuits (ASIC's), such as the ASIC's described in reference to <figref idref="DRAWINGS">FIGS. 27-31</figref> above, and that are managed by a line card within a FibreChannel Switch (not shown). The SCSI LU Virtualization Service <b>926</b> and <b>928</b> are running as highly available distributed applications on a pair of Service Processors; SVS SP A and SVS SP B, <b>922</b> and <b>924</b>, respectively. The SCSI Virtualization Service <b>930</b> and <b>932</b> is also provided on the Service Processors for acting as a coordinator of management functions between the SCSI LU Virtualization Service, SCSI Target Virtualization Service, and the Virtualization Manager.</p>
<p id="p-0202" num="0216">The Volume Virtualization Controller Service is also running as a highly available distributed application on a pair of Service Processors; VCS SP A and VCS SP B, <b>900</b> and <b>902</b>, respectively. Other possible embodiments include having CIMOM on dedicated service processors and the CIMOM sharing SP with any one of the Virtualization services. Each of these components is managed by the preferred Invista Management Software running within the preferred Invista CIMOM <b>916</b> and <b>920</b> on a pair of Management Server Processors A and B, <b>906</b> and <b>908</b>, respectively. The preferred Invista CIMOM processes are part of a highly available distributed application that operates within the framework of the Common Management Platform (CMP) <b>914</b> and <b>918</b>.</p>
<p id="p-0203" num="0217"><figref idref="DRAWINGS">FIG. 39</figref> illustrates a host <b>726</b> that is accessing a volume, LU1 <b>005</b> that is being virtualized. In this example, the volume maps entirely onto a single volume, SE1, presented by a storage array called Array 1 <b>946</b> on the RAID Layer <b>734</b>. L1 maps its contents byte for byte to the entire contents of SE1. At this point the Storage Presentation layer by providing SCSI Virtualization <b>840</b> is presenting LU1 to the host with the preferred Invista SCSI personality and a location independent identifier for the volume. This personality is independent of the storage arrays that contain the physical storage. Non-read and write SCSI commands from the host that act on the logical unit are serviced directly by the Storage Presentation layer and are not presented to the back-end array. Commands requiring coordination among multiple DPC's, such as Reservation requests, are serviced by the SCSI LU Virtualizer running on the CPC <b>940</b>, which is the same or similar entity as item <b>58</b> in <figref idref="DRAWINGS">FIG. 3</figref>, and may include item <b>54</b> from <figref idref="DRAWINGS">FIG. 4</figref>. The SCSI Target Virtualizer running on the DPC <b>904</b> immediately services non-read/write commands that commands that do not require coordination among its peers. For Reads and Writes the SCSI Virtualization logic on the DPC passes these through to the array which services these with the data flowing between the host and the DPC and between the DPC and the array. Use for Array 2 <b>948</b> present in the Raid Layer <b>734</b> is explained in reference to an example case shown in <figref idref="DRAWINGS">FIG. 40</figref> below.</p>
<p id="p-0204" num="0218"><figref idref="DRAWINGS">FIG. 40</figref> shows an example case wherein the same virtual volume, LU1, as discussed in reference to <figref idref="DRAWINGS">FIG. 39</figref> is being presented to the host <b>726</b>. At this point, in the example case, the storage administrator could be involved with moving data residing on volume SE1, from Array 1 to Array 2. The administrator wants to move this data between arrays without interrupting the access to this data from the host. Using preferred Invista's Data Mobility application, the administrator selects the new location for the data to be on volume SE3 which is presented to the preferred Invista Instance from Array 2. SE3 is same capacity as the source volume, SE1. To accomplish the data movement, the Data Mobility application inserts a Volume Virtualization layer <b>842</b> into the data path between LU1 and SE1. The Data Mobility application consists of software that runs on the DPC <b>904</b> and on the CPC <b>940</b> in this Volume Virtualization layer <b>842</b>. This software inserts a mirroring volume M into the data path and starts to replicate write I/Os that arrive to both SE1 and SE3. Read I/Os are directed to SE1. The Data Mobility layer also starts a series of copy operations that read data from SE1 and write the data to SE3, with the purpose being to copy the entire volume SE1 to SE3. The Data Mobility layer performs this synchronization in a manner that prevents writes from these copy operations from overwriting more recent data written by the host. The Data Mobility layer consists of software running on the DPC and CPC of the Volume Virtualization layer. This software forms a part of the Volume Virtualization Acceleration Service and the Volume Virtualization Controller Service. The insertion of the volume virtualization layer is done transparently to the host in a manner allows I/Os from the host to flow to volume without the host observing any interruption of service.</p>
<p id="p-0205" num="0219"><figref idref="DRAWINGS">FIG. 41</figref> shows more in the example case, wherein upon completion of the synchronization of data from SE1 to SE3, the storage administrator is giving the options to either abort the operation or accept the new location of the data by committing the operation. In this example, the administrator accepts the new location and commits the tasks. Once the storage administrator has committed the job, the volume SE1 is no longer needed for this data and updates to volume SE1 from the host are no longer necessary. At this point the Data Mobility application stops the mirroring of data and directs all read and write traffic that it receives to volume SE3.</p>
<p id="p-0206" num="0220"><figref idref="DRAWINGS">FIG. 42</figref> shows the final completion stage of the Data Mobility job. Since the data has been moved to its new location, and the storage administrator has accepted this new location, the virtualization being performed by the Volume Virtualization Layer is now complete. As a final step the Data Mobility application rebinds the volume LU1 to volume SE3 directly short circuiting the VAS volume. This step removes the overhead of the Volume Virtualization Layer since it now no longer has any useful service to provide to this volume.</p>
<p id="p-0207" num="0221"><figref idref="DRAWINGS">FIGS. 43A-G</figref> depict a continuing example case where the alternative embodiment architecture enables high availability of function and services in the event of failures and problems. <figref idref="DRAWINGS">FIG. 43A</figref> shows Host <b>726</b> and Storage Presentation Layer <b>730</b> and Volume Virtualization Layer <b>732</b>. RAID Protection Layer <b>734</b> includes storage elements SE1 and SE2. In the example case SCSI LU Virtualization Service (SLVS) <b>964</b> presents V1 <b>954</b>A-B to the Host on dual Fabrics A-B <b>950</b>A-B employing the SCSI Target Virtualization Service (STVS) 1-2 <b>952</b>A-B. Volume Virtualization Controller Service (VCS) <b>966</b> coordinates virtualization on the Virtualization Accelerator Services VAS1-2 <b>956</b>A-B. The SLVS, STVS, VCS and VAS discussed in <figref idref="DRAWINGS">FIGS. 43A-G</figref> are identical or similar to those discussed in reference to <figref idref="DRAWINGS">FIG. 38</figref>, but have different reference numbers because, in the presented example cases, these elements are functioning present in a slightly different context. A VAS is also referred to as an Accelerator herein. All outstanding writes to Mirror M <b>958</b>A-B are recorded in the Write Intent Log (WIL) stored in the RAID Protection Layer <b>734</b>, and removed from WIL upon completion of I/O. A reservation key of READ ANY (RA) indicates that any mirror leg (V2 <b>960</b>A-B or V3 <b>962</b>A-B) can be used to service reads.</p>
<p id="p-0208" num="0222"><figref idref="DRAWINGS">FIGS. 43B-D</figref> show high availability in the presence of VCS failures is shown, by partitioning an accelerator from its controller. Referring to <figref idref="DRAWINGS">FIG. 43B</figref>, on loss of a communication link with its controller VAS1 sets Primary Read (PR) reservation key on SE1 and SE2. All Reads received by VAS1 are then directed to the primary volume SE1, while writes continue to both. Write errors are recorded in a Dirty Region Log (DRL) stored in the RAID protection layer. For these errors the WIL is updated as in the normal case. <figref idref="DRAWINGS">FIG. 43C</figref> shows that on loss of a communication link with Accelerator VCS, it sets DPCLoss reservation key (DL) on SE1 and SE2. DL key overrides the PR key. So all reads on VAS2 are directed to the primary volume (SE1). VAS2 updates DRL on write errors and the WIL is updated as in the normal case. <figref idref="DRAWINGS">FIG. 43D</figref> shows that on observing DL reservation key, VAS1 stops servicing the volume and returns CHECK CONDITION with a sense code of NOT READY to all subsequent IO's from the host.</p>
<p id="p-0209" num="0223"><figref idref="DRAWINGS">FIG. 43E</figref> shows high availability in the presence of a VCS failure in a different scenario then that described above. <figref idref="DRAWINGS">FIG. 43E</figref> shows that on loss of communication link with the controller, VAS1-2 each set PR reservation key on SE1 and SE2. All reads received by VAS1-2 are directed to the primary volume SE, and writes continue to both. Both of these Accelerators update DRL on write errors and update WIL as normal.</p>
<p id="p-0210" num="0224"><figref idref="DRAWINGS">FIGS. 43F-G</figref> show Recovery that augments high availability in the presence of VCS failures. <figref idref="DRAWINGS">FIG. 43F</figref> shows that on restoration of connections to VAS1 and VAS2, the VCS consolidates the out of synch region information recorded in the DRL's and synchronizes these regions between SE1 and SE2. <figref idref="DRAWINGS">FIG. 43G</figref> shows that upon completion of the synchronization, the full mirror service is restored to V1, the DRL is cleared, and the reservation key is set back to READ ANY.</p>
<p id="p-0211" num="0225">A system, method, and architecture has been described for managing one or more data storage networks at a middle-Layer level and for managing failure. Method may be performed by Program Logic shown in <figref idref="DRAWINGS">FIG. 36</figref> for any of the methods described above. Having described a preferred embodiment of the present invention, it may occur to skilled artisans to incorporate these concepts into other embodiments. Nevertheless, this invention should not be limited to the disclosed embodiment, but rather only by the spirit and scope of the following claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A system for managing a plurality of storage area networks including a plurality of data storage volumes and one or more hosts, wherein the volumes are in a storage network in the storage area networks, the system comprising:
<claim-text>one or more processors in communication with network connectivity capability for the storage network, wherein the one or more processors include program logic for carrying out</claim-text>
<claim-text>a volume virtualization controller service for controlling virtualization of volumes in the storage network and</claim-text>
<claim-text>a SCSI virtualization service that manages virtual SCSI-related targets and virtual logical units (LU's) wherein the volume virtualization controller service supports virtualization enabling virtual Storage Area Networks (SAN)s by parsing between front-end SANs and back-end SANs regardless of physical configuration of the front and back-end SANs, to enable mapping from virtual volumes to back-end storage elements across virtual SANs.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the volume virtualization controller service handles I/O commands including routing of the I/O commands.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the volume virtualization controller service further performs accelerated volume virtualization transformations by performing the transformations on the one or more processors.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a SCSI target virtualization service manages Virtual SCSI Targets, handles logins, logouts, and other SCSI target based commands.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising a SCSI logical unit (LU) virtualization service that provides support for LU mapping and masking.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the SCSI virtualization service manages virtual SCSI targets and the virtual SCSI LU's by coordinating functions of a SCSI target virtualization service and the SCSI LU virtualization service.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising a SCSI target virtualization service manages Virtual SCSI Targets, handles logins, logouts, and other SCSI target based commands.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising a SCSI logical unit (LU) virtualization service that provides support for LU mapping and masking.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the SCSI virtualization service manages virtual SCSI targets and the virtual SCSI LU's by coordinating functions of a SCSI target virtualization service and the SCSI LU virtualization service.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>managing Virtual SCSI Targets, handles logins, logouts, and other SCSI target based commands by a SCSI target virtualization service.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising: providing support for LU mapping and masking via a SCSI logical unit (LU) virtualization service.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A method for managing a plurality of storage area networks including a plurality of data storage volumes and one or more hosts, wherein the volumes are in a storage network, wherein the network includes one or more processors in the network that include program logic capable of carrying out steps of the method comprising:
<claim-text>controlling virtualization of volumes in the storage network; and</claim-text>
<claim-text>managing a SCSI virtualization service; wherein the volume virtualization controller service supports virtualization enabling virtual Storage Area Networks (SAN)s by parsing between front-end SANs and back-end SANs regardless of physical configuration of the front and back-end SANs, to enable mapping from virtual volumes to back-end storage elements across virtual SANs.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the managing of the SCSI virtualization services includes managing virtual SCSI-related targets and virtual logical units (LU's) by handling I/O commands including routing of the I/O commands.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the controlling virtualization of volumes includes performing virtual transformations on the one or more processors.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A program product for managing a plurality of storage area networks including a plurality of data storage volumes and one or more hosts, wherein the volumes are in a storage network, wherein the network includes one or more processors in the network, the program product comprising:
<claim-text>a computer-readable non-transitory storage medium encoded with computer-executable program code enabling:</claim-text>
<claim-text>controlling virtualization of volumes in the storage network; and</claim-text>
<claim-text>managing a SCSI virtualization service; wherein the volume virtualization controller service supports virtualization enabling virtual Storage Area Networks (SAN)s by parsing between front-end SANs and back-end SANs regardless of physical configuration of the front and back-end SANs, to enable mapping from virtual volumes to back-end storage elements across virtual SANs.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the managing of the SCSI virtualization services includes managing virtual SCSI-related targets and virtual logical units (LU's) by handling I/O commands including routing of the I/O commands.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the controlling virtualization of volumes includes performing virtual transformations on the one or more processors.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, computer-executable program code further enabling:
<claim-text>managing Virtual SCSI Targets, handles logins, logouts, and other SCSI target based commands by a SCSI target virtualization service.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The program product of <claim-ref idref="CLM-00015">claim 15</claim-ref>, computer-executable program code further enabling:
<claim-text>providing support for LU mapping and masking via a SCSI logical unit (LU) virtualization service.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The program product of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the SCSI virtualization service manages virtual SCSI targets and the virtual SCSI LU's by coordinating functions of a SCSI target virtualization service and the SCSI LU virtualization service.</claim-text>
</claim>
</claims>
</us-patent-grant>
