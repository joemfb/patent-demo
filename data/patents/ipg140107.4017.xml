<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625086-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625086</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12735582</doc-number>
<date>20080212</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>581</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>18</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>01</class>
<subclass>S</subclass>
<main-group>3</main-group>
<subgroup>783</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>3561415</main-classification>
<further-classification>348142</further-classification>
</classification-national>
<invention-title id="d2e53">Determining coordinates of a target in relation to a survey instrument having a camera</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5949548</doc-number>
<kind>A</kind>
<name>Shirai et al.</name>
<date>19990900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6384902</doc-number>
<kind>B1</kind>
<name>Schneider</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6411372</doc-number>
<kind>B1</kind>
<name>Donath et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6453569</doc-number>
<kind>B1</kind>
<name>Kumagai et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6559931</doc-number>
<kind>B2</kind>
<name>Kawamura</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6688010</doc-number>
<kind>B1</kind>
<name>Schwaerzler</name>
<date>20040200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7200945</doc-number>
<kind>B2</kind>
<name>Endo</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7503123</doc-number>
<kind>B2</kind>
<name>Matsuo et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification> 33290</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7508980</doc-number>
<kind>B2</kind>
<name>Otani et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>7697749</doc-number>
<kind>B2</kind>
<name>Ogawa</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2003/0048438</doc-number>
<kind>A1</kind>
<name>Kawamura et al.</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2004/0131248</doc-number>
<kind>A1</kind>
<name>Ito et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2005/0057745</doc-number>
<kind>A1</kind>
<name>Bontje</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2006/0013474</doc-number>
<kind>A1</kind>
<name>Kochi et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2006/0017938</doc-number>
<kind>A1</kind>
<name>Ohtomo et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2006/0021236</doc-number>
<kind>A1</kind>
<name>Endo</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2006/0167648</doc-number>
<kind>A1</kind>
<name>Ohtani</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2007/0065004</doc-number>
<kind>A1</kind>
<name>Kochi et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382162</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2007/0104353</doc-number>
<kind>A1</kind>
<name>Vogel</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2008/0120855</doc-number>
<kind>A1</kind>
<name>Matsuo et al.</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2010/0074532</doc-number>
<kind>A1</kind>
<name>Gordon et al.</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382203</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>CN</country>
<doc-number>1727845</doc-number>
<kind>A</kind>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>CN</country>
<doc-number>101101210</doc-number>
<date>20070100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>CN</country>
<doc-number>101101210</doc-number>
<kind>A</kind>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>DE</country>
<doc-number>144 967</doc-number>
<date>19801100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>DE</country>
<doc-number>10 2007 030 784</doc-number>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>EP</country>
<doc-number>0 971 207</doc-number>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>EP</country>
<doc-number>0 997 704</doc-number>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>EP</country>
<doc-number>1 139 062</doc-number>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>EP</country>
<doc-number>1 607 718</doc-number>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>EP</country>
<doc-number>1 655 573</doc-number>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>JP</country>
<doc-number>4-198809</doc-number>
<date>19920700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>JP</country>
<doc-number>2000-013060</doc-number>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>JP</country>
<doc-number>2000131060</doc-number>
<kind>A</kind>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>JP</country>
<doc-number>2005-017262</doc-number>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>JP</country>
<doc-number>2007-147422</doc-number>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>WO</country>
<doc-number>WO 2004/057269</doc-number>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>WO</country>
<doc-number>WO2005/059473</doc-number>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>WO</country>
<doc-number>WO 2009/100774</doc-number>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>WO</country>
<doc-number>WO 2009/100773</doc-number>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>WO</country>
<doc-number>WO 2009/106141</doc-number>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00042">
<othercit>Zhang et al.: &#x201c;Photo Total Station System an Integration of Metric Digital Camera and Total Station&#x201d; Conference on Optical 3-D Measurement Techniques (vol. I, pp. 176-182) Sep. 22, 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00043">
<othercit>International Preliminary Report on Patentability and Written Opinion dated Apr. 15, 2010 issued in related application No. PCT/EP2008/052531.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00044">
<othercit>International Search Report dated Dec. 9, 2008 in related application No. PCT/EP2008/052531.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00045">
<othercit>International Search Report dated Nov. 12, 2008 in related application No. PCT/EP2008/058183.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00046">
<othercit>International Search Report dated Nov. 6, 2008 in related application No. PCT/EP2008/058175.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00047">
<othercit>Chinese Office Action dated Mar. 12, 2012 issued in corresponding Chinese Application No. 200880125829.7 and English translation thereof.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00048">
<othercit>Zhang, Z. &#x201c;Determing the Epipolar Geometry and its Uncertainty: A Review.&#x201d; <i>International Journal of Computer Vision</i>, 27(2), 161-198, 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00049">
<othercit>U.S. Office Action dated Feb. 26, 2013 for co-pending U.S. Appl. No. 12/735,279.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00050">
<othercit>Chinese Office Action dated Jun. 13, 2013 for Chinese Application No. 200880125829.7.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>19</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>348142</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>7</number-of-drawing-sheets>
<number-of-figures>12</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20110007154</doc-number>
<kind>A1</kind>
<date>20110113</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Vogel</last-name>
<first-name>Michael</first-name>
<address>
<city>Schleifreisen</city>
<country>DE</country>
</address>
</addressbook>
<residence>
<country>DE</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Svanholm</last-name>
<first-name>Set</first-name>
<address>
<city>Sollentuna</city>
<country>SE</country>
</address>
</addressbook>
<residence>
<country>SE</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Gr&#xe4;sser</last-name>
<first-name>Christian</first-name>
<address>
<city>Vallentuna</city>
<country>SE</country>
</address>
</addressbook>
<residence>
<country>SE</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Vogel</last-name>
<first-name>Michael</first-name>
<address>
<city>Schleifreisen</city>
<country>DE</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Svanholm</last-name>
<first-name>Set</first-name>
<address>
<city>Sollentuna</city>
<country>SE</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Gr&#xe4;sser</last-name>
<first-name>Christian</first-name>
<address>
<city>Vallentuna</city>
<country>SE</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Harness, Dickey &#x26; Pierce, P.L.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Trimble AB</orgname>
<role>03</role>
<address>
<city>Danderyd</city>
<country>SE</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Perungavoor</last-name>
<first-name>Sath V</first-name>
<department>2488</department>
</primary-examiner>
<assistant-examiner>
<last-name>Brown, Jr.</last-name>
<first-name>Howard D</first-name>
</assistant-examiner>
</examiners>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/EP2008/001053</doc-number>
<kind>00</kind>
<date>20080212</date>
</document-id>
<us-371c124-date>
<date>20100928</date>
</us-371c124-date>
</pct-or-regional-filing-data>
<pct-or-regional-publishing-data>
<document-id>
<country>WO</country>
<doc-number>WO2009/100728</doc-number>
<kind>A </kind>
<date>20090820</date>
</document-id>
</pct-or-regional-publishing-data>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method is disclosed for determining coordinates of a target in relation to a surveying instrument wherein a first image is captured using a camera in a first camera position and orientation, a target is selected by identifying an object point in the first image, and first image coordinates of the object point in the first image are measured. The surveying instrument is then rotated around the rotation center so that the camera is moved from the first camera position and orientation to a second camera position and orientation, while retaining the rotation center of the surveying instrument in a fixed position. A second image is captured using the camera in the second camera position and orientation, the object point identified in the first image is identified in the second image, and second image coordinates of the object point in the second image are measured. Target coordinates in relation to the rotation center of the surveying instrument are then determined based on the first camera position and orientation, the first image coordinates, the second camera position and orientation, the second image coordinates, and camera calibration data. Furthermore, a surveying instrument for performing the method is disclosed.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="204.72mm" wi="169.84mm" file="US08625086-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="225.72mm" wi="163.07mm" file="US08625086-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="215.14mm" wi="152.82mm" file="US08625086-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="224.20mm" wi="161.21mm" file="US08625086-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="226.31mm" wi="162.31mm" file="US08625086-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="249.94mm" wi="151.55mm" file="US08625086-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="209.47mm" wi="124.38mm" file="US08625086-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="214.63mm" wi="172.30mm" file="US08625086-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">The present invention relates to surveying instruments, such as total stations. More particularly, the present invention relates to a method for determining coordinates of a target in relation to a surveying instrument having a camera for capturing a field of view and an identified target point within the field of view.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">In surveying, the use of a camera in a geodetic instrument may provide for improved user convenience and new functions. Particularly, a view such as an image or a video feed provided by the camera and shown on a display of the instrument may be used for assisting target selection and for providing the user with an overview of potential points of interest.</p>
<p id="p-0004" num="0003">One apparent difference between a traditional eyepiece and a video image shown on a display, is that the display image may have an active overlay of information. Various information may be given together with the captured image to facilitate instrument usage.</p>
<p id="p-0005" num="0004">For a general background of total stations comprising a camera for capturing an image or a video feed of the view towards which the total station is aimed, reference is made to WO 2005/059473.</p>
<p id="p-0006" num="0005">Surveying instruments of this kind, i.e. which includes a camera, are sometimes referred to as video-theodolites.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0007" num="0006">In the field of video-theodolites, or more generally for geodetic instruments provided with a camera, problems arise when the camera center and the instrument rotation center do not coincide. Directions to targets from the camera center determined based on the camera image will normally not apply directly as directions to the target from the instrument rotation center.</p>
<p id="p-0008" num="0007">The present invention provides a method for determining, in relation to a surveying instrument, target coordinates of a point of interest, or target, identified in two images captured by a camera in the surveying instrument.</p>
<p id="p-0009" num="0008">When the camera center or perspective center and the rotation center of a surveying instrument are non-coincident (eccentric), it is generally not possible to directly determine a correct direction from the rotation center towards an arbitrary point identified or indicated in an image captured by the camera. In other words, if a direction from a camera to a target is determined from an image captured by the camera, the direction towards the target from the rotation center that is not coinciding with the camera center, will typically not be adequately determined by approximation using the direction from the camera. Only if the distance to the point of interest is known would it be possible to derive the correct direction from the rotation center to the target from the image. It is thus desirable to determine coordinates of the target with respect to the rotational center of the surveying instrument.</p>
<p id="p-0010" num="0009">The present invention provides a method for determining coordinates of a target in relation to a surveying instrument wherein a first image is captured using a camera in a first camera position and orientation, a target is selected by identifying an object point in the first image, and, first image coordinates of the object point in the first image are measured. The surveying instrument is rotated around the rotation center so that the camera is moved from the first camera position and orientation to a second camera position and orientation. A second image is captured using the camera in the second position and orientation, the object point identified in the first image is identified also in the second image, and second image coordinates of the object point in the second image are measured. Even though a rotation is undertaken prior to capturing the second image, the rotation center of the surveying instrument is retained in a fixed position when the second image is captured, i.e. in the same position as when the first image was captured. Finally, coordinates of the target in relation to the rotation center of the surveying instrument are determined based on the first camera position and orientation, the first image coordinates, the second camera position and orientation, the second image coordinates, and camera calibration data.</p>
<p id="p-0011" num="0010">The present invention makes use of an understanding that the coordinates of the point of interest from the surveying instrument can be determined by using the eccentricity of a camera in the surveying instrument for capturing two images from the camera in two known camera positions and orientations with intermediate rotation of the surveying instrument and identification of an object point and the coordinates thereof in the two images together with camera calibration data. When target coordinates with respect to the rotational center of the instrument have been determined, distance from the rotation center to the target can be determined. However, target coordinates may be used for other purposes than determining distance. For instance, it may be useful to measure coordinates of a ground mark very accurately for surveying instrument stationing.</p>
<p id="p-0012" num="0011">The present invention also provides a total station which comprises various means for carrying out the above-described method.</p>
<p id="p-0013" num="0012">In addition, the present invention can be implemented in a computer program that, when executed, performs the inventive method in a surveying instrument. The computer program may, for example, be downloaded into a surveying instrument as an upgrade. As will be understood, the inventive method can be implemented for a surveying instrument using software, hardware or firmware, or a combination thereof, as desired in view of the particular circumstances.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0014" num="0013">In the following detailed description, reference is made to the accompanying drawings, on which:</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic diagram showing, from the side, a situation for a surveying instrument having a camera that is eccentric to the instrument rotation center.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 2</figref> schematically shows, from above, how the eccentricity between the camera and the instrument center leads to angle differences relative to an object point also in a situation where the instrument sight line and the camera line of sight are coaxial.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 3</figref> schematically shows an image or video frame that may be captured by the camera and presented on a screen of the instrument.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 4</figref> is a general outline of the inventive method.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> are schematic diagrams showing, from the side, a first implementation of a surveying instrument according to the invention having a camera that is eccentric to the instrument rotation center. Images are captured by the camera in two different positions using rotation of the surveying instrument.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 6</figref> is an outline of an implementation of the inventive method.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIGS. 7</figref><i>a </i>and <b>7</b><i>b </i>illustrate an approach for determining whether accuracy of a derived target distance and/or direction is considered good enough.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 8</figref> illustrate an alternative approach for determining whether accuracy of a derived target distance and/or direction is considered good enough.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 9</figref> schematically illustrates the capturing of two images from a camera using rotation of a surveying instrument between the capturing of a first image and a second image, and identification of an object point along a section of an epipolar line in the second image.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 10</figref> shows schematically a geodetic instrument according to the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 1</figref> illustrates schematically a situation where an object point relating to a target P is identified in an image captured by a camera in a surveying instrument. In the figure, there is shown a camera sensor <b>101</b>, such as a CMOS camera or a CCD, for capturing an image centered about a camera axis <b>102</b>. An image is formed on the camera sensor <b>101</b> by means of an optical system <b>103</b>. The surveying instrument can be aimed at a desired target by rotation over horizontal and vertical angles about a rotation center <b>104</b> of the instrument. As illustrated in the figure, the optical system <b>103</b> for the camera is eccentric with respect to the rotation center of the instrument (separation e<sub>q </sub>and e<sub>l</sub>, respectively, from the rotation center of the instrument). The camera axis <b>102</b> (center line for the camera view) is thus not collinear with the optical axis <b>105</b> of the instrument (i.e. optical line of sight), as illustrated in the figure. The camera optical axis <b>102</b> should ideally be perpendicular to the plane of the camera sensor <b>101</b> and the optical system <b>103</b> should be free from distortions or aberrations, which is not the case in practice.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 3</figref> schematically shows an image captured by the camera. Such image may be presented to an operator using the surveying instrument, such that the operator may select the image object point of a target P by clicking in the image or otherwise indicating the desired object point. The optical axis (i.e. the optical line of sight) of the instrument is directed roughly towards the image center, which is indicated by the cross-hair in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0027" num="0026">For determining the direction from the surveying instrument towards a selected target P, there are a number of functions implemented in the instrument. For example, if the direction to a target from the camera is expressed as horizontal and vertical angles (&#x3b8;<sup>x</sup>,&#x3b8;<sup>y</sup>) from the camera axis, a function is provided in the instrument (or in an associated control unit) that determines the direction to a target from the camera by calculating the horizontal and the vertical angles (&#x3b8;<sup>x</sup>,&#x3b8;<sup>y</sup>) based on pixel coordinates in the image or video feed captured by the camera. Hence, the function &#x192; that calculates the horizontal and vertical angles (&#x3b8;<sup>x</sup>,&#x3b8;<sup>y</sup>) based on image pixels can be described as
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(&#x3b8;<sup>x</sup>,&#x3b8;<sup>y</sup>)=&#x192;(<i>x,y,C</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where x, y are the number of pixels (or more generally, pixel coordinates) of the desired target in the coordinate system of the camera, and C are calibration factors to be determined for each system. The calibration factors C include details about the camera, such as but not limited to, its eccentricity e<sub>q </sub>and e<sub>l </sub>and its focal length f<sub>cam </sub>(<figref idref="DRAWINGS">FIG. 1</figref>). For a general description of how to calculate horizontal and vertical angles based on an image captured by a camera in the instrument, reference is made to the above-mentioned WO 2005/059473.
</p>
<p id="p-0028" num="0027">In order to find the direction to a target captured with an eccentric camera from an instrument using one image, e.g. for aiming the instrument towards the desired target P, it is necessary to know the distance to the target. To this end, it may be useful to study the schematic diagram shown in <figref idref="DRAWINGS">FIG. 2</figref>. <figref idref="DRAWINGS">FIG. 2</figref> is a plan view taken from above, and illustrates the angles between the current line of sight (optical as well as EDM) and the desired target (P) for both the camera (cam) and the instrument (instr) center. The camera calibration data are used for determining the camera angle &#x3b8;<sub>c </sub>to the object point P, but the instrument angle &#x3b8;<sub>i </sub>to the object point P will be slightly different as indicated in <figref idref="DRAWINGS">FIG. 2</figref>. Hence, in order to find the direction to a target from an instrument using one image, e.g. for aiming the instrument towards the desired object point P, it is necessary to know the distance to the target. It should be understood that <figref idref="DRAWINGS">FIG. 2</figref> is only a schematic illustration.</p>
<p id="p-0029" num="0028">However, if two images are captured from a camera in a surveying instrument, where a first image is captured using the camera in a first camera position and orientation, said first camera position being eccentric to the rotation center of the surveying instrument, and a second image is captured using the camera in a second camera position and orientation, said second position being eccentric to the rotation center of the surveying instrument, by rotation of the surveying instrument around its rotation center so that the camera is moved from the first camera position and orientation to the second camera position and orientation, an object point corresponding to a target is identified in the first image and in the second image, and image coordinates for the object point in the first image and second image are measured, coordinates of the target with respect to the rotation center of the surveying instrument can be determined using camera calibration data.</p>
<p id="p-0030" num="0029">Determining the direction towards the target from the rotation center of the surveying instrument is useful e.g. for aiming the surveying instrument towards the target for subsequent distance measuring. Determining the target distance from the rotation center of the surveying instrument is useful e.g. for determining a sample interval for a distance measuring instrument using time of flight measurements, focusing capability of EDM, and auto focus for a further camera or for a user.</p>
<p id="p-0031" num="0030">General steps of the method according to the present invention are outlined in <figref idref="DRAWINGS">FIG. 4</figref>. The method is performed in a surveying instrument comprising a camera and starts with capturing in a step S<b>401</b> of a first image using said camera in a first camera position and orientation, the first camera position being eccentric to the rotation center of the surveying instrument, after which a target selection is performed in a step S<b>402</b>. An object point relating to the target, is identified in the first image which may be a snapshot or a frame of a video feed captured by the camera. For example, the target may be identified by selection by an operator clicking on a screen showing the image captured by the camera, or in any other suitable way of indicating the selected target with respect to the captured image. It is also envisaged that the target identification can be made in other ways, such as using prisms located at the target, edge detection, identification of target features (e.g. arrows), etc. In order to facilitate the target selection, it is preferred that it is indicated in the screen image the point at which the instrument line of sight is currently aimed, for example using a cross-hair, a dot or similar.</p>
<p id="p-0032" num="0031">Once the target has been selected by identification of the object point in the first image in the step S<b>402</b>, first image coordinates of the object point in the first image are measured in a step S<b>403</b>.</p>
<p id="p-0033" num="0032">In a step S<b>404</b> the surveying instrument is rotated around the rotation center so that the camera is moved from the first camera position and orientation to a second camera position and orientation. The second camera position is eccentric to the rotation center of the surveying instrument. The first camera position and orientation is given by camera calibration data as well as the horizontal and vertical direction of the camera center to the instrument rotation center. Thus, the first camera position and orientation is determined by taking into account the horizontal and vertical direction of the camera center to the instrument rotation center, i.e. camera calibration data in the form of eccentricity parameters e<sub>q </sub>and e<sub>l</sub>, such that camera coordinates are related to surveying instrument coordinates. Further, as will be discussed in the following, other calibration data can be considered, for example camera position independent parameters such as the so-called camera constant representing distance between the camera center and image plane and/or parameter of distortion, which depends on the image position. The second camera position and orientation can be derived from the rotation together with camera calibration data.</p>
<p id="p-0034" num="0033">The method then proceeds with capturing in a step S<b>405</b> a second image using the camera in the second camera position and orientation, after which the object point identified in the first image is identified in a step S<b>406</b> in the second image. For example, the object point may be identified by selection by an operator clicking on a screen showing the image captured by the camera, or preferably by means of digital image processing.</p>
<p id="p-0035" num="0034">Once the object point has been identified in the second image in step S<b>406</b>, second image coordinates of the object point in the second image are measured in a step S<b>407</b>.</p>
<p id="p-0036" num="0035">Finally, coordinates of the target in relation to the rotation center of the surveying instrument are determined in a step S<b>408</b> based on the first camera position and orientation, the first image coordinates, the second camera position and orientation, the second image coordinates, and camera calibration data.</p>
<p id="p-0037" num="0036">A first implementation of a surveying instrument is schematically shown in <figref idref="DRAWINGS">FIGS. 5A and 5B</figref>. For simplicity, the surveying instrument is shown in two dimensions. <figref idref="DRAWINGS">FIGS. 5A and 5B</figref> can be seen as either top or side views. However, it will be apparent to a skilled person how the teachings of the invention should be expanded to the three-dimensional case.</p>
<p id="p-0038" num="0037">The surveying instrument <b>500</b> comprises a camera comprising a camera sensor <b>501</b> and an optical system <b>502</b>. The camera has a camera center, or projection center, (indicated by O&#x2032; in <figref idref="DRAWINGS">FIGS. 5A</figref> and O&#x2033; in <figref idref="DRAWINGS">FIG. 5B</figref>) that is eccentric to the location O of the rotation center of the instrument <b>500</b>. The camera may further be moved to different positions by rotation of the surveying instrument <b>500</b> around its rotation center. It is to be noted that the surveying is shown having a single camera since only one camera is required. However, the surveying instrument may well comprise more than one camera, e.g. a second camera used for a separate feature. It is to be noted that the camera is located above the line of sight of the instrument in the example <figref idref="DRAWINGS">FIGS. 5A and 5B</figref>. Of course the camera may have an arbitrary eccentric location with respect to the instrument rotation center.</p>
<p id="p-0039" num="0038">In <figref idref="DRAWINGS">FIG. 5A</figref>, a first image is captured by the camera sensor <b>501</b>, when the camera center is located in a first camera position O&#x2032; eccentric to the location O of the rotation center of the surveying instrument <b>500</b> and having a first camera orientation. A target in position P is selected by identifying an object point in a position P&#x2032; in the first image.</p>
<p id="p-0040" num="0039">Once the target has been selected by identification of the object point in the first image, first image coordinates of the object point in the first image are measured.</p>
<p id="p-0041" num="0040">In <figref idref="DRAWINGS">FIG. 5B</figref>, the camera center has been moved from the first position O&#x2032; and first camera orientation to a second camera position O&#x2033; and second camera rotation by rotation of the instrument <b>500</b> around the rotation center by a rotation angle &#x3b4;. A second image is captured by the camera sensor <b>501</b>, when the camera center is located in the second camera position O&#x2033; eccentric to the location O of the rotation center of the surveying instrument <b>500</b> and having the second camera orientation. The object point identified in the first image is identified also in the second image in a position P&#x2033;.</p>
<p id="p-0042" num="0041">Once the object point has been identified in the second image, second image coordinates of the object point in position P&#x2033; in the second image are measured.</p>
<p id="p-0043" num="0042">Finally, coordinates of the target P in relation to the rotation center O of the surveying instrument are determined based on the first camera position and orientation, the first image coordinates, the second camera position and orientation, the second image coordinates, and camera calibration data.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 6</figref> is an outline of an implementation of the inventive method performed in a surveying instrument comprising a camera such as schematically disclosed in <figref idref="DRAWINGS">FIGS. 5A-B</figref>. Input to the method are camera calibration data comprising camera and surveying instrument parameters. The method starts in a step S<b>601</b> with capturing of a first image using the camera in a first camera position and orientation, said first camera position being eccentric to a rotation center of the surveying instrument, after which in a step S<b>602</b> a target is selected by identifying an object point in the first image corresponding to the target. For example, the target may be identified by selection by an operator clicking on a screen showing the image captured by the camera. Furthermore, image coordinates for the object point in the first image are measured.</p>
<p id="p-0045" num="0044">After the object point has been identified, it is determined in a step S<b>603</b> if the object point can be identified using image processing and recognition software. If it cannot it is determined if a pattern in the vicinity of the point is detectable. If this is not the case the uncertainty is displayed in a step S<b>605</b> and suitable alternative method is used in a step S<b>606</b>.</p>
<p id="p-0046" num="0045">If a pattern can be detected or if the point can be identified by software it is determined in a step S<b>607</b> if the object point is close enough to a closest corner of the image, e.g. less than a threshold distance from the closest corner of the first image. This is to ensure that the method produces an accurate enough output. In alternative it may be determined if the object point is far enough from a point in the image corresponding to the center of the camera detector, e.g. more than a threshold distance from the point in the image corresponding to the center of the camera detector.</p>
<p id="p-0047" num="0046">If the point is not close enough to a closest corner of the first image, in a step S<b>608</b> a shortest distance (corresponding to a largest difference between the direction from eccentric positioned camera to target and the rotation center of the surveying instrument) is assumed and rotation angles are calculated which will result in the object point being located closer than a threshold to a closest corner in a revised first image but still within the revised first image. The surveying instrument is then rotated in accordance with the rotation angles and the revised first image is captured.</p>
<p id="p-0048" num="0047">Using the first camera position and orientation, the second camera position and orientation, the first image coordinates, and camera calibration data the so called epipolar line can be identified in the revised first image on which epipolar line the object point should be located in the revised first image. In a step S<b>609</b> the object point is identified by searching along the epipolar line in the revised first image and the first image coordinates are revised as the measured image coordinates of the object point in the revised first image.</p>
<p id="p-0049" num="0048">In a step S<b>610</b> rotation angles are calculated based on camera calibration data, the first camera position and orientation, and the first image coordinates, which rotation angles will result in the object point being located closer than a threshold to a corner of a second image but still within the second image, said corner of the second image being different than the corner of the first image to which the object point is closer than a threshold in the first image and preferably diametrically opposed to that corner.</p>
<p id="p-0050" num="0049">The surveying instrument is then rotated in a step S<b>611</b> according to the rotation angles and the second image is captured using the camera in a second camera position and orientation resulting from the rotation.</p>
<p id="p-0051" num="0050">The object point identified in the first image is identified S<b>612</b> in the second image. Using the first camera position and orientation (or revised camera position and orientation), the second camera position and orientation, the first image coordinates, and camera calibration data the so called epipolar line can be identified in the second image on which epipolar line the object point is located in the second image. The object point is identified by searching along the epipolar line in the second image. For example, the object point may be identified by selection by an operator clicking on a screen showing the second image, or preferably by means of digital image processing.</p>
<p id="p-0052" num="0051">Once the object point has been identified in the second image, second coordinates of the object point in the second image are measured in a step S<b>612</b>. Coordinates of the target in relation to the rotation center of the surveying instrument are determined based on the first camera position and orientation (or revised first camera position and orientation), the first image coordinates, the second camera position and orientation, the second image coordinates, and camera calibration data. The coordinates of the target in relation to the rotation center of the surveying instrument are determined based on the first camera position and orientation, the first image coordinates, the second camera position and orientation, the second image coordinates, and camera calibration data. A target direction from the rotation center of the surveying instrument is determined based on the determined target coordinates in relation to the rotation center of the surveying instrument. That is, the coordinates of the target are expressed in relation to the coordinate system of the surveying instrument. Hence, the target coordinates are typically related to a coordinate system having its origin at the rotation center of the surveying instrument.</p>
<p id="p-0053" num="0052">In a step S<b>613</b> the distance from the instrument rotation center to the target is determined based on the target coordinates in relation to the rotation center of the surveying instrument.</p>
<p id="p-0054" num="0053">In a step S<b>614</b> it is determined if the accuracy of the distance and/or the direction is enough. If it is not, suitable rotation angles for additional measurement is determined in a step S<b>615</b>, the surveying instrument is rotated according to the suitable rotation angles and a revised second image is capture in a step S<b>616</b>, and the method proceeds to the step S<b>612</b>.</p>
<p id="p-0055" num="0054">If the accuracy is enough the surveying instrument is rotated so that the line of sight of the instrument rotation center is aimed in the target direction determined in the step S<b>617</b>.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIGS. 7A and 7B</figref> show two different images indicating image corners <b>701</b>, an object point <b>702</b> and four areas <b>703</b> in each image where the accuracy of the determined target distance and/or direction is considered to be good enough. Referring again to <figref idref="DRAWINGS">FIG. 6</figref>, it is determined in step S<b>607</b> if the object point <b>702</b> is close enough to a closest corner <b>701</b> of the image, e.g. less than a threshold distance from the closest corner of the first image in order to ensure that an accurate enough result is produced. In <figref idref="DRAWINGS">FIG. 7A</figref>, the object point <b>702</b> is considered to be close enough (i.e. within area <b>703</b>) to the corner <b>701</b>. To the contrary, <figref idref="DRAWINGS">FIG. 7B</figref> illustrates a situation where the object point <b>702</b> not is located in one of the four areas <b>703</b>, wherein the accuracy not is considered to be good enough.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an alternative to the thresholding approach of <figref idref="DRAWINGS">FIGS. 7A and 7B</figref>. In the approach of <figref idref="DRAWINGS">FIG. 8</figref>, it may be determined if the object point <b>802</b> is far enough from a point in the image corresponding to the center of the camera detector, e.g. more than a threshold distance from the point in the image corresponding to the center of the camera detector. In <figref idref="DRAWINGS">FIG. 8</figref>, the shaded area <b>803</b> indicates a far enough distance from the center. If so, the accuracy is considered to be good enough.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 9</figref> schematically illustrates the capturing of two images from a camera using rotation of a surveying instrument between the capturing of a first image and a second image, and identification of an object point along a section of an epipolar line in the second image.</p>
<p id="p-0059" num="0058">The camera has a camera center, or projection center, that is eccentric to the location O of the rotation center of the instrument. The camera may further be moved to different camera positions and orientations by rotation of the surveying instrument around its rotation center. For each rotation the camera center will be located on a sphere.</p>
<p id="p-0060" num="0059">A first image is captured by the camera, when the camera center is located in a first position O&#x2032; eccentric to the location O of the rotation center of the surveying instrument and the camera having a first camera orientation. A target in position P is selected by identifying an object point in a position P&#x2032;(x&#x2032;, y&#x2032;) in the first image.</p>
<p id="p-0061" num="0060">The camera center is then moved from the first camera position O&#x2032; and orientation to a second camera position O&#x2033; and orientation by rotation of the instrument around the rotation center. The distance between the first and the second camera position is denoted d<sub>c</sub>. A second image is captured by the camera, when the camera center is located in the second position O&#x2033; eccentric to the location O of the rotation center of the surveying instrument and the camera having the second camera orientation. The object point identified in the first image is identified in the second image in a position P&#x2033;(x&#x2033;, y&#x2033;).</p>
<p id="p-0062" num="0061">The so called collinearity equations are used to estimate the coordinates of P. For a background of the deriving of the collinearity equations and of calibration of a camera reference is made to WO 2005/059473.</p>
<p id="p-0063" num="0062">The measured coordinates P&#x2032; in the first image can be defined by the equations:</p>
<p id="p-0064" num="0063">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
  <mrow>
    <msup>
      <mi>x</mi>
      <mi>&#x2032;</mi>
    </msup>
    <mo>=</mo>
    <mrow>
      <msub>
        <mi>x</mi>
        <mn>0</mn>
      </msub>
      <mo>-</mo>
      <mrow>
        <msub>
          <mi>c</mi>
          <mi>K</mi>
        </msub>
        <mo>&#x2062;</mo>
        <mfrac>
          <mrow>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>11</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>X</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>X</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>21</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Y</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Y</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>31</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Z</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Z</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>13</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>X</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>X</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>23</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Y</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Y</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>33</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Z</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Z</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
      <mo>+</mo>
      <mrow>
        <mi>&#x394;</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>x</mi>
      </mrow>
    </mrow>
  </mrow>
</math>
</maths>
<maths id="MATH-US-00001-2" num="00001.2">
<math overflow="scroll">
  <mrow>
    <msup>
      <mi>y</mi>
      <mi>&#x2032;</mi>
    </msup>
    <mo>=</mo>
    <mrow>
      <msub>
        <mi>y</mi>
        <mn>0</mn>
      </msub>
      <mo>-</mo>
      <mrow>
        <msub>
          <mi>c</mi>
          <mi>K</mi>
        </msub>
        <mo>&#x2062;</mo>
        <mfrac>
          <mrow>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>12</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>X</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>X</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>22</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Y</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Y</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>32</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Z</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Z</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>13</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>X</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>X</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>23</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Y</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Y</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>33</mn>
                  <mi>&#x2032;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Z</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Z</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2032;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
      <mo>+</mo>
      <mrow>
        <mi>&#x394;</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>x</mi>
      </mrow>
    </mrow>
  </mrow>
</math>
</maths>
<br/>
The measured coordinates P&#x2033; in the second image can be defined by the equations:
</p>
<p id="p-0065" num="0064">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
  <mrow>
    <msup>
      <mi>x</mi>
      <mi>&#x2033;</mi>
    </msup>
    <mo>=</mo>
    <mrow>
      <msub>
        <mi>x</mi>
        <mn>0</mn>
      </msub>
      <mo>-</mo>
      <mrow>
        <msub>
          <mi>c</mi>
          <mi>K</mi>
        </msub>
        <mo>&#x2062;</mo>
        <mfrac>
          <mrow>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>11</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>X</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>X</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>21</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Y</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Y</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>31</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Z</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Z</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>13</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>X</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>X</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>23</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Y</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Y</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>33</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Z</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Z</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
      <mo>+</mo>
      <mrow>
        <mi>&#x394;</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>x</mi>
      </mrow>
    </mrow>
  </mrow>
</math>
</maths>
<maths id="MATH-US-00002-2" num="00002.2">
<math overflow="scroll">
  <mrow>
    <msup>
      <mi>y</mi>
      <mi>&#x2033;</mi>
    </msup>
    <mo>=</mo>
    <mrow>
      <msub>
        <mi>y</mi>
        <mn>0</mn>
      </msub>
      <mo>-</mo>
      <mrow>
        <msub>
          <mi>c</mi>
          <mi>K</mi>
        </msub>
        <mo>&#x2062;</mo>
        <mfrac>
          <mrow>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>12</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>X</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>X</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>22</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Y</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Y</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>32</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Z</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Z</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
          <mrow>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>13</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>X</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>X</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>23</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Y</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Y</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>+</mo>
            <mrow>
              <msub>
                <mi>r</mi>
                <msup>
                  <mn>33</mn>
                  <mi>&#x2033;</mi>
                </msup>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Z</mi>
                  <mo>-</mo>
                  <msub>
                    <mi>Z</mi>
                    <msup>
                      <mn>0</mn>
                      <mi>&#x2033;</mi>
                    </msup>
                  </msub>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mfrac>
      </mrow>
      <mo>+</mo>
      <mrow>
        <mi>&#x394;</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>y</mi>
      </mrow>
    </mrow>
  </mrow>
</math>
</maths>
<br/>
The following parameters are known from calibration:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0065">X<sub>0</sub>; Y<sub>0</sub>; Z<sub>0</sub>: coordinates from camera position, &#x2032;=first image; &#x2033;=second image.</li>
        <li id="ul0002-0002" num="0066">r<sub>ij</sub>: elements from rotation matrix (i=1 . . . 3; j=1 . . . 3), &#x2032;=first image; &#x2033;=second image.</li>
        <li id="ul0002-0003" num="0067">x<sub>0</sub>; y<sub>0</sub>: coordinates from the principle point (constant, independent of camera position)</li>
        <li id="ul0002-0004" num="0068">c<sub>K</sub>: camera constant (constant, independent of camera position)</li>
        <li id="ul0002-0005" num="0069">&#x394;x, &#x394;y: parameter of distortion. The distortion is known as a polynomial of a higher degree. The distortion depends of the image position and is independent of the camera position.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0066" num="0070">All parameters are measured or known from camera calibration except from X, Y, Z which are the coordinates of P. Hence, there are three unknowns and four equations. X, Y, Z can be determined with a least square method. It should be noted that there is no limitation to two images to estimate the location of point P, but at least two images are necessary. Hence, two or more images may be used.</p>
<p id="p-0067" num="0071">It should be noted that once the target has been selected by identification of the object point in the first image, a first direction r&#x2032; to the target from the first position O&#x2032; of the camera center can determined.</p>
<p id="p-0068" num="0072">Using this first direction r&#x2032; together with a maximum distance D<sub>max </sub>(which may be selected as infinity), and a minimum distance D<sub>min </sub>(which may be selected as the minimum distance of the surveying instrument) along the direction r&#x2032;, a section of r&#x2032; can then be transformed into a section of a so called epipolar line el&#x2033; in the second image on which the object point should be located in the second image. Hence, if a pattern has been recognized in the first image around the position P&#x2032; of the object point in the first image, this pattern may be recognized in the second image, e.g. by means of automatic image processing, by searching along the section of the epipolar line in the second image or in an area around that section. If in alternative, an operator is to identify the target in the second image, the identification can be simplified by graphical indication of the section of the epipolar line in the second image.</p>
<p id="p-0069" num="0073">In an alternative embodiment, a first image is captured by the camera, when the camera center is located in a first position O&#x2032; eccentric to the location O of the rotation center of the surveying instrument and the camera having a first camera orientation. A target in position P is selected by identifying an object point in a position P&#x2032;(x&#x2032;, y&#x2032;) in the first image.</p>
<p id="p-0070" num="0074">The camera center is then moved from the first camera position O&#x2032; and orientation to a second camera position O&#x2033; and orientation by a 180-degree rotation around one rotational axis of the rotation center, e.g. the horizontal axis, followed by a further 180-degree rotation around the other rotational axis of the rotation center, i.e. the vertical axis.</p>
<p id="p-0071" num="0075">It can theoretically be shown that these two 180-degree rotations result in a maximal base, i.e. a maximal distance d<sub>c</sub>, between the first and the second camera position. A second image is captured by the camera, when the camera center is located in the second position O&#x2033; eccentric to the location O of the rotation center of the surveying instrument and the camera having the second camera orientation. The object point identified in the first image is identified in the second image in a position P&#x2033;(x&#x2033;, y&#x2033;). The target coordinates are subsequently determined in line with the above description.</p>
<p id="p-0072" num="0076">In <figref idref="DRAWINGS">FIG. 10</figref>, there is shown one example of a total station according to the present invention. In many aspects, the total station comprises features known from previous instruments. For example, the total station <b>1000</b> shown in <figref idref="DRAWINGS">FIG. 10</figref> comprises an alidade <b>1001</b> mounted on a base <b>1002</b>, and has a mounting support structure in the form of a tripod <b>1003</b>. The alidade <b>1001</b> can be rotated about a vertically oriented rotation axis V, in order to aim the instrument in any desired horizontal direction. In the alidade, there is arranged a center unit <b>1004</b>, which can be rotated about a horizontally oriented rotation axis H, in order to aim the instrument in any desired vertical direction. Measurements made using the total station <b>1000</b> are typically related to an origin of coordinates located at the intersection between the vertically oriented and the horizontally oriented rotation axes V and H.</p>
<p id="p-0073" num="0077">For rotation of the alidade about the vertically oriented rotation axis to aim the instrument in any desired horizontal direction, there is provided drive means <b>1005</b>. The rotational position of the alidade <b>1001</b> is tracked by means of a graduated disc <b>1006</b> and a corresponding angle encoder or sensor <b>1007</b>. For rotation of the center unit <b>1004</b> about the horizontally oriented rotation axis, there are provided similar drive means <b>1008</b>, graduated disc <b>1009</b> and sensor <b>1010</b>. Moreover, the instrument has an optical plummet <b>1012</b>, which gives a downwards view along the vertically oriented rotation axis. The optical plummet is used by the operator to center or position the instrument above any desired point on the ground.</p>
<p id="p-0074" num="0078">As mentioned above, the instrument line of sight is centered at the intersection between the vertical and the horizontal rotation axes, and this can be seen in the figure where these axes cross in the center of a telescope <b>1013</b> in the center unit <b>1004</b>.</p>
<p id="p-0075" num="0079">Above the telescope, in the center unit, there is provided a camera <b>1014</b> for capturing an image or a video feed generally in the direction of the instrument line of sight. However, as shown, the camera <b>1014</b> is eccentric from the center of the telescope <b>1013</b>; in this case, the camera is mounted vertically above the telescope. The instrument also comprises a display device for showing the image captured by the camera. The display may be an integral part of the instrument, but more preferably, the display is included in a removable control panel that can be used for remote control of the instrument via short range radio. It is even conceivable that the instrument is fully remote controlled, wherein the display may be in the form of a computer screen located far away from the total station, and wherein information to and from the instrument are transferred over a wireless computer or radio telephone network.</p>
<p id="p-0076" num="0080">The instrument can also be manually operated for aiming towards a desired target using vertical and horizontal motion servo knobs <b>1015</b> and <b>1016</b>.</p>
<p id="p-0077" num="0081">The camera of the instrument is operable to capture images from different positions and orientations eccentric to a rotation center of the surveying instrument.</p>
<p id="p-0078" num="0082">According to the present invention, the instrument further comprises means for identifying an object point corresponding to a selected target in the displayed image; means for determining, based on the position of the object point in the displayed images, directions towards the target from the camera in the different positions; means for measuring image coordinates of the object point in the displayed images, and means for determining target coordinates of the target in relation to the rotation center of the surveying instrument, based on the first camera position and orientation, the first image coordinates, the second camera position and orientation, the second image coordinates, and camera calibration data.</p>
<p id="p-0079" num="0083">The means for identifying the object point in the captured image may take the form of a cursor that can be moved across the display. Alternatively, the display may be a touch display, where the object point is identified by simply clicking or tapping on the display. This is preferred for the identification of the object point in the first image.</p>
<p id="p-0080" num="0084">The means for identifying the object point may include further functionality implemented in the form of image processing software. In such case, the further functionality would be that object points may be identified based on certain features in the captured image, for example markers or patterns located at the desired target. For example, an object point identified in a first image may be automatically identified in a second image based on patterns identified in the first image. In case the object point is automatically identified by the instrument, the user may be given the option to give a confirmation that the identified object point is correct before or during the aiming and measuring procedures. This is preferred for the identification of the object point in the second image.</p>
<p id="p-0081" num="0085">The means for measuring image coordinates of the object point in the displayed images, and the means for determining target coordinates of the target in relation to the rotation center of the surveying instrument are preferably implemented in the form of computer program code that is executed in a processor. However, implementation may also be made in dedicated hardware, such as in a special purpose microprocessor or a digital signal processor (DSP), firmware or similar.</p>
<p id="p-0082" num="0086">The means for rotating the instrument is preferably implemented in association with the servo control system for the instrument for controlled activation of the drive motors <b>1005</b> and <b>1008</b> (see <figref idref="DRAWINGS">FIG. 10</figref>).</p>
<p id="p-0083" num="0087">While specific embodiments have been described, the skilled person will understand that various modifications and alterations are conceivable within the scope as defined in the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US08625086-20140107-M00001.NB">
<img id="EMI-M00001" he="13.38mm" wi="76.20mm" file="US08625086-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US08625086-20140107-M00002.NB">
<img id="EMI-M00002" he="13.38mm" wi="76.20mm" file="US08625086-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for determining coordinates of a target in relation to a surveying instrument comprising a camera, comprising:
<claim-text>capturing a first image using the camera in a first camera position and orientation, said first camera position being eccentric to a rotation center of the surveying instrument;</claim-text>
<claim-text>selecting a target by identifying an object point in the first image;</claim-text>
<claim-text>measuring first image coordinates of the object point in the first image;</claim-text>
<claim-text>rotating the surveying instrument around the rotation center so that the camera is moved from the first camera position and orientation to a second camera position and orientation, while retaining the rotation center of the surveying instrument in a fixed position, said second camera position being eccentric to the rotation center of the surveying instrument;</claim-text>
<claim-text>capturing a second image using the camera in the second camera position and orientation;</claim-text>
<claim-text>identifying, in the second image, the object point identified in the first image, wherein the identifying includes,
<claim-text>selecting a minimum distance and a maximum distance from the first camera position along an imaging ray of the target in the first image, between which minimum distance and maximum distance the target is located,</claim-text>
<claim-text>determining a section of an epipolar line in the second image on which the object point is located based on the maximum distance, the minimum distance, the first camera position and orientation, the second camera position and orientation, the first image coordinates, and camera calibration data, and</claim-text>
<claim-text>identifying, along the section of the epipolar line in the second image, the object point identified in the first image;</claim-text>
</claim-text>
<claim-text>measuring second image coordinates of the object point in the second image; and</claim-text>
<claim-text>determining target coordinates of the target in relation to the rotation center of the surveying instrument based on the first camera position and orientation, the first image coordinates, the second camera position and orientation, the second image coordinates, and camera calibration data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the minimum distance is selected as the minimum distance of the surveying instrument.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein if the identified object point is not closer than a threshold distance from a closest corner of the first image the following steps are performed:
<claim-text>determining, based on the position of the object point in the first image, a revised rotation of the surveying instrument around the rotation center resulting in the object point being closer than a threshold distance from a closest corner of an image captured with the camera after the revised rotation;</claim-text>
<claim-text>rotating the surveying instrument around the rotation center by the revised rotation so that the camera is moved from the first camera position and orientation to a revised first camera position and orientation;</claim-text>
<claim-text>capturing a revised first image using the camera in the revised first position and orientation;</claim-text>
<claim-text>identifying, in the revised first image, the object point identified in the first image;</claim-text>
<claim-text>revising the first image coordinates to measured image coordinates of the object point in the revised first image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein identifying, in the revised first image, the object point identified in the first image comprises:
<claim-text>determining an epipolar line in the revised first image on which the object point is located, based on the first camera position and orientation, the revised camera position and orientation, the first image coordinates and camera calibration data; and</claim-text>
<claim-text>identifying, along the epipolar line in the revised first image, the object point identified in the first image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein identifying, in the revised first image, the object point identified in the first image comprises:
<claim-text>selecting a minimum distance and a maximum distance from the first camera position along the imaging ray of the target in the first image, between which minimum distance and maximum distance the target is located,</claim-text>
<claim-text>determining, based on the maximum distance, and the minimum distance, a section of the epipolar line in the revised first image on which the object point is located; and</claim-text>
<claim-text>identifying, along the section of the epipolar line in the revised first image, the object point identified in the first image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object point is identified in the first image by operator selection.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object point is identified in the first image by automatic image analysis.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object point identified in the first image is identified in the second image by automatic image analysis.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object point identified in the first image is identified in the second image by user selection.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>determining a direction from the rotation center of the surveying instrument towards the target based on the determined coordinates of the target.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:
<claim-text>rotating the surveying instrument so that a line of sight of the surveying instrument is directed in the direction towards the target.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:
<claim-text>measuring a distance to the target using distance measuring capabilities within the surveying instrument.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>determining a distance from the rotation center of the surveying instrument to the target based on the determined target coordinates.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said rotation of the surveying instrument around the rotation center so that the camera is moved from the first camera position and orientation to a second camera position and orientation comprises:
<claim-text>rotating the surveying instrument 180 degrees around a first of the two rotational axes of the rotation center; and</claim-text>
<claim-text>rotating the surveying instrument 180 degrees around a second of the two rotational axes of the rotation center.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A computer program product stored on a non-transitory computer-readable medium, the computer program product comprising computer program code portions configured to perform the method of <claim-ref idref="CLM-00001">claim 1</claim-ref> when loaded and executed in a computer.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. A surveying instrument, comprising:
<claim-text>a camera configured to capture images from a first camera position and orientation and a second camera position and orientation different from the first camera position and orientation, the first and second camera positions and orientations being eccentric to a rotation center of the surveying instrument, while retaining the rotation center of the surveying instrument in a fixed position;</claim-text>
<claim-text>a display configured to display images captured by said camera from the different positions and orientations;</claim-text>
<claim-text>an identifying mechanism configured to identify an object point corresponding to a selected target in the displayed images, said identifying mechanism being configured to,
<claim-text>select a minimum distance and a maximum distance from the first camera position along an imaging ray of the target in the first image, between which minimum distance and maximum distance the target is located,</claim-text>
<claim-text>determine a section of an epipolar line in the second image on which the object point is located based on the maximum distance, the minimum distance, the first camera position and orientation, the second camera position and orientation, the first image coordinates, and camera calibration data, and</claim-text>
</claim-text>
<claim-text>identify, along the section of the epipolar line in the second image, the object point identified in the first image;</claim-text>
<claim-text>a measuring mechanism configured to measure image coordinates of the object point in the displayed images;</claim-text>
<claim-text>a rotating mechanism configured to rotate the surveying instrument around the rotation center of the surveying instrument; and</claim-text>
<claim-text>a determining mechanism configured to determine coordinates of the target in relation to the rotation center of the surveying instrument, based on the first camera position and orientation, the first image coordinates, the second camera position and orientation, the second image coordinates, and camera calibration data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The instrument of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the identifying mechanism is implemented as a cursor movable across the displayed images.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The instrument of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the identifying mechanism is implemented using a touch display, where the object point can be identified by clicking or tapping on the display.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The instrument of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the identifying mechanism is implemented using image processing software. </claim-text>
</claim>
</claims>
</us-patent-grant>
