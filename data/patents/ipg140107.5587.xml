<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626687-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626687</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13201287</doc-number>
<date>20100219</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>FR</country>
<doc-number>09 51086</doc-number>
<date>20090219</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>188</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>3</main-group>
<subgroup>08</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classifications-cpc>
<main-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>3</main-group>
<subgroup>08</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</main-cpc>
<further-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</further-cpc>
</classifications-cpc>
<classification-national>
<country>US</country>
<main-classification>706 25</main-classification>
<further-classification>382181</further-classification>
</classification-national>
<invention-title id="d2e71">Method for the selection of attributes for statistical Learning for object detection and recognition</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7564994</doc-number>
<kind>B1</kind>
<name>Steinberg et al.</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>2007/0237387</doc-number>
<kind>A1</kind>
<name>Avidan et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2010/0040285</doc-number>
<kind>A1</kind>
<name>Csurka et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382170</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2010/0092085</doc-number>
<kind>A1</kind>
<name>Marchesotti</name>
<date>20100400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382173</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2010/0111396</doc-number>
<kind>A1</kind>
<name>Boucheron</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382133</main-classification></classification-national>
</us-citation>
<us-citation>
<nplcit num="00006">
<othercit>Perrotton et al; &#x201c;Automatic Object Detection on Aerial Images Using Local Descriptors and Image Synthesis&#x201d;, Computer Vision Systems, May 12, 2008, vol. 5008, pp. 302-311, ISBN: 978-3-540-79546-9.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00007">
<othercit>Torralba et al; &#x201c;Sharing Visual Features for Multiclass and Multiview Object Detection&#x201d;, IEEE Transactions on Pattern Analysis and Machine Intelligence, May 1, 2007, vol. 29, No. 5, pp. 854-869, ISSN: 0162-8828.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00008">
<othercit>Porway et al; &#x201c;A hierarchical and contextual model for aerial image understanding&#x201d;, Computer Vision and Pattern Recognition, Jun. 23, 2008 pp. 1-8, ISBN: 978-1-4244-2242-5.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00009">
<othercit>Wang; &#x201c;From Bag of Categories to Tree of Object Recognition&#x201d;, International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, Feb. 4, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00010">
<othercit>French Preliminary Search Report for FR 09 51086 dated Sep. 18, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00011">
<othercit>International Search Report for PCT/EP2010/052110 dated Apr. 29, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>5</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>3</number-of-drawing-sheets>
<number-of-figures>9</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120072383</doc-number>
<kind>A1</kind>
<date>20120322</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Perrotton</last-name>
<first-name>Xavier</first-name>
<address>
<city>Paris</city>
<country>FR</country>
</address>
</addressbook>
<residence>
<country>FR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sturzel</last-name>
<first-name>Marc</first-name>
<address>
<city>Montrouge</city>
<country>FR</country>
</address>
</addressbook>
<residence>
<country>FR</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Perrotton</last-name>
<first-name>Xavier</first-name>
<address>
<city>Paris</city>
<country>FR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Sturzel</last-name>
<first-name>Marc</first-name>
<address>
<city>Montrouge</city>
<country>FR</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Pearne &#x26; Gordon LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>European Aeronautic Defence and Space Company-Eads France</orgname>
<role>03</role>
<address>
<city>Paris</city>
<country>FR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Chen</last-name>
<first-name>Alan</first-name>
<department>2129</department>
</primary-examiner>
</examiners>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/EP2010/052110</doc-number>
<kind>00</kind>
<date>20100219</date>
</document-id>
<us-371c124-date>
<date>20110908</date>
</us-371c124-date>
</pct-or-regional-filing-data>
<pct-or-regional-publishing-data>
<document-id>
<country>WO</country>
<doc-number>WO2010/094759</doc-number>
<kind>A </kind>
<date>20100826</date>
</document-id>
</pct-or-regional-publishing-data>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">The invention relates to an attribute selection method for making statistical learning of descriptors intended to enable automatic recognition and/or detection of an object from a set of images, method characterized by the following steps:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0000">obtain a mask of the object in each image containing said object to be recognized,</li>
        <li id="ul0002-0002" num="0000">define and select at least one set of descriptors as a function of their geometric shape and/or apparent specific physical characteristics,</li>
        <li id="ul0002-0003" num="0000">calculate attributes associated with this shape and said specific physical characteristics,</li>
        <li id="ul0002-0004" num="0000">sort the descriptors as a function of their respective scores,</li>
        <li id="ul0002-0005" num="0000">select descriptors with the highest scores to perform said statistical learning.</li>
    </ul>
    </li>
</ul>
</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="80.52mm" wi="148.84mm" file="US08626687-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="246.72mm" wi="182.63mm" file="US08626687-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="240.96mm" wi="143.51mm" file="US08626687-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="240.96mm" wi="140.38mm" file="US08626687-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">The invention relates to supervised statistical learning applied to image processing and more specifically concerns a method for the selection of attributes to be used for statistical learning of descriptors intended to enable automatic recognition and/or detection of an object from a set of images.</p>
<p id="p-0003" num="0002">The invention also relates to an attribute selection device for making statistical learning of descriptors intended to enable automatic recognition and/or detection of an object from a set of images.</p>
<p id="p-0004" num="0003">The invention also relates to a computer program stored in a recording medium that, when run by a computer, will implement the method according to the invention.</p>
<heading id="h-0002" level="1">STATE OF PRIOR ART</heading>
<p id="p-0005" num="0004">In known supervised statistical learning techniques, there is usually a set of learning data for example composed of an extended set of positive and negative example images, and a single or multi-class learning algorithm that uses descriptors calculated locally on these images, and that selects the most discriminating descriptors.</p>
<p id="p-0006" num="0005">One problem with these techniques is due to the fact that there are far too many possible descriptors for an exhaustive search, such that the algorithm has to use only a limited number of possible solutions.</p>
<p id="p-0007" num="0006">Known solutions for solving this problem consist of defining families of possible descriptors and processing all possible descriptors in these families. For example, for Haar filters, available filters correspond to predefined geometric patterns and all instantiations of these patterns are then tested in the learning images. On the other hand, any geometric patterns not initially defined will be ignored. In such an approach, it is essential to limit possible patterns, otherwise the number of descriptors to be tested becomes completely prohibitive.</p>
<p id="p-0008" num="0007">The purpose of the invention is to overcome the disadvantages of prior art by means of a method for selection of attributes making it possible to make use of segmentation data in learning images and leaving the algorithm define geometric patterns that are the most relevant and the most discriminating as a function of the learning data used.</p>
<heading id="h-0003" level="1">PRESENTATION OF THE INVENTION</heading>
<p id="p-0009" num="0008">The invention then recommends a method for the selection of attributes to perform statistical learning of descriptors intended to enable automatic recognition and/or detection of an object from a set of images comprising the following steps:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0009">obtain a mask of the object in each image containing said object to be recognised,</li>
        <li id="ul0004-0002" num="0010">define at least one set of descriptors using their geometric shape and/or apparent specific physical characteristics,</li>
        <li id="ul0004-0003" num="0011">select at least one set of descriptors as a function of their geometric shape and/or apparent specific physical characteristics,</li>
        <li id="ul0004-0004" num="0012">calculate the attributes associated with this shape and said specific physical characteristics,</li>
        <li id="ul0004-0005" num="0013">select at least one set of descriptors as a function of their shape and attributes calculated on this shape,</li>
        <li id="ul0004-0006" num="0014">for each descriptor and for each image, define a semantic conformity score starting from previously calculated masks,</li>
        <li id="ul0004-0007" num="0015">sort the descriptors as a function of their respective scores.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0010" num="0016">Preferably, the semantic conformity score of a descriptor is defined as a function of the conformity level of the geometric shape of said descriptor with the mask of the object to be recognised in the image.</p>
<p id="p-0011" num="0017">According to another characteristic of the invention, said descriptors measure a statistical property on a combination of adjacent geometric shapes and non-adjacent geometric shapes.</p>
<p id="p-0012" num="0018">In one variant embodiment, said geometric shapes are rectangles.</p>
<p id="p-0013" num="0019">For example, the statistical property measurement may be a histogram difference.</p>
<p id="p-0014" num="0020">In one embodiment of the invention, the mask of the object to be recognised in the image is obtained by image synthesis.</p>
<p id="p-0015" num="0021">In this embodiment, the semantic conformity score of a descriptor is defined as a function of the conformity level of the geometric shape of said descriptor with the mask of the object to be recognised in the image.</p>
<p id="p-0016" num="0022">The method according to the invention is implemented by means of an attribute selection device to perform statistical learning of descriptors for automatic recognition and/or detection of an object starting from a set of images, this device comprises:
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0023">means for segmenting each image containing said object to be recognised,</li>
        <li id="ul0006-0002" num="0024">means for defining at least one set of descriptors as a function of their geometric shape and/or apparent specific physical characteristics such as the colour or the appearance of the object to be recognised,</li>
        <li id="ul0006-0003" num="0025">means for calculating attributes associated with this shape and said specific physical characteristics,</li>
        <li id="ul0006-0004" num="0026">means for selecting at least one set of descriptors as a function of their shape and attributes calculated on this shape,</li>
        <li id="ul0006-0005" num="0027">means for defining a semantic conformity score for each descriptor and for each image,</li>
        <li id="ul0006-0006" num="0028">means for sorting descriptors as a function of their respective scores,</li>
        <li id="ul0006-0007" num="0029">means for selecting descriptors with the highest scores to perform said statistical learning.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0017" num="0030">The method according to the invention is implemented using a computer program stored in a recording medium and comprising:
<ul id="ul0007" list-style="none">
    <li id="ul0007-0001" num="0000">
    <ul id="ul0008" list-style="none">
        <li id="ul0008-0001" num="0031">instructions for obtaining a mask of the object in each image containing said object to be recognised,</li>
        <li id="ul0008-0002" num="0032">instructions for defining at least one set of descriptors as a function of their geometric shape and/or apparent specific physical characteristics (colour, appearance, etc.),</li>
        <li id="ul0008-0003" num="0033">instructions for calculating attributes associated with this shape and said specific physical characteristics,</li>
        <li id="ul0008-0004" num="0034">instructions for selecting at least one set of descriptors as a function of their shape and attributes calculated on this shape,</li>
        <li id="ul0008-0005" num="0035">instructions for defining a semantic conformity score from the previously calculated masks, for each descriptor and for each image,</li>
        <li id="ul0008-0006" num="0036">instructions for sorting the descriptors as a function of their respective scores,</li>
        <li id="ul0008-0007" num="0037">instructions for selecting the descriptors with the highest score to perform said statistical learning.</li>
    </ul>
    </li>
</ul>
</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0018" num="0038">Other characteristics and advantages of the invention will become clear from the following description taken as a non-limitative example with reference to the appended figures in which:</p>
<p id="p-0019" num="0039"><figref idref="DRAWINGS">FIG. 1</figref> diagrammatically shows three classes to be recognised by the method according to the invention;</p>
<p id="p-0020" num="0040"><figref idref="DRAWINGS">FIG. 2</figref> diagrammatically shows a contour mask of an image of an aircraft in one of the classes in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0021" num="0041"><figref idref="DRAWINGS">FIG. 3</figref> diagrammatically shows an example of the descriptors considered;</p>
<p id="p-0022" num="0042"><figref idref="DRAWINGS">FIG. 4</figref> diagrammatically shows discriminating regions common to the three classes in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0023" num="0043"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> diagrammatically show discriminating regions characteristic of a subset of two classes in <figref idref="DRAWINGS">FIG. 1</figref>;</p>
<p id="p-0024" num="0044"><figref idref="DRAWINGS">FIGS. 6A to 6C</figref> diagrammatically show characteristic discriminating regions for each class in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED PRESENTATION OF PARTICULAR EMBODIMENTS</heading>
<p id="p-0025" num="0045"><figref idref="DRAWINGS">FIG. 1</figref> diagrammatically shows the silhouettes of three classes to be recognised, class <b>1</b>, class <b>2</b> and class <b>3</b>, corresponding to three aircrafts <b>2</b>, <b>4</b> and <b>6</b> respectively of the same size obtained using a camera associated with the device according to the invention.</p>
<p id="p-0026" num="0046">Note that the method according to the invention is implemented by a learning algorithm that focuses on the most specific zones of each class in order to select attributes for each class that will be used for statistical learning of descriptors to discriminate each of the aircrafts <b>2</b>, <b>4</b> or <b>6</b> in an extended set of images.</p>
<p id="p-0027" num="0047">A mask is extracted representing contours of the object for each real image in the learning base. An example mask for class <b>1</b> is shown in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0028" num="0048">In this example, we consider Haar filters that make a difference in contrast between regions of the image. <figref idref="DRAWINGS">FIG. 3</figref> presents example shapes representing rectangles with two different contrast levels +1 and &#x2212;1.</p>
<p id="p-0029" num="0049">The next step is to calculate descriptors at any scale and position in the learning image. The result is then several thousand descriptors, and a semantic conformity score is defined for each descriptor and for each image, starting from the previously calculated masks.</p>
<p id="p-0030" num="0050">Thus, a descriptor will have a higher semantic conformity score if it can semantically differentiate the background from the target.</p>
<p id="p-0031" num="0051">In the example in <figref idref="DRAWINGS">FIG. 1</figref>, the score will be maximum and equal to 1 when each region is placed on a single region of the image, the background or the target, and these regions are opposite. Conversely, when the two regions are in the same zone, for example the background zone, the score will be minimum and zero.</p>
<p id="p-0032" num="0052">The next step is to perform a classical sort of descriptors as a function of their respective scores from the lowest to the highest, and for example, the hundred descriptors with the highest scores will be selected.</p>
<p id="p-0033" num="0053">In the example in <figref idref="DRAWINGS">FIG. 1</figref>, any descriptors calculating a distance between rectangular regions along the x and y axes is considered, for algorithmic optimisation choices. Regions associated with the background are cross-hatched vertically and regions associated with the object are cross-hatched horizontally.</p>
<p id="p-0034" num="0054"><figref idref="DRAWINGS">FIG. 4</figref> shows the overlap of segmentation masks demonstrating zones <b>10</b> and <b>12</b> common to the three classes <b>2</b>, <b>4</b> and <b>6</b>. The combination of rectangles in zones <b>10</b> and <b>12</b> makes it possible of efficiently discriminating images that probably contain an object of one of the three aircraft classes in the background zones <b>12</b> common to the three classes.</p>
<p id="p-0035" num="0055">In <figref idref="DRAWINGS">FIG. 5</figref>, zones specific to groups of two classes are considered, avoiding zones already considered by the previously defined descriptors.</p>
<p id="p-0036" num="0056">Thus, <figref idref="DRAWINGS">FIG. 5A</figref> considers zones <b>20</b> and <b>22</b> common to classes <b>4</b> and <b>6</b> and <figref idref="DRAWINGS">FIG. 5B</figref> considers zones <b>24</b>, <b>26</b> common to classes <b>2</b> and <b>4</b>.</p>
<p id="p-0037" num="0057">As shown in <figref idref="DRAWINGS">FIG. 6</figref>, the algorithm focuses on the most specific zones in each of the classes as iterations in the learning phase continue, which makes it possible to eliminate background zones <b>12</b> at the beginning of the processing, to make the discrimination between classes at the end of the processing.</p>
<p id="p-0038" num="0058">Thus, zones <b>40</b> specific to class <b>2</b> are isolated in <figref idref="DRAWINGS">FIG. 6A</figref>, zones <b>50</b> specific to class <b>4</b> are isolated in <figref idref="DRAWINGS">FIG. 6B</figref>, and zones <b>60</b> specific to class <b>6</b> are isolated in <figref idref="DRAWINGS">FIG. 6C</figref>.</p>
<p id="p-0039" num="0059">Note that the algorithm will reuse previously defined zones as much as possible in order to optimise execution calculation times.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. Attribute selection method for statistical learning of descriptors intended to enable automatic recognition and/or automatic detection of an object from a set of images, method characterized by the following steps:
<claim-text>obtain a mask of the object in each image containing said object to be recognised,</claim-text>
<claim-text>define at least one set of descriptors using their geometric shape and/or apparent specific physical characteristics,</claim-text>
<claim-text>select at least one set of descriptors as a function of their geometric shape and/or apparent specific physical characteristics,</claim-text>
<claim-text>calculate attributes associated with these descriptors and said specific physical characteristics,</claim-text>
<claim-text>for each descriptor and for each image, define a semantic conformity score with the mask of the object to be recognised in the image previously calculated representing the conformity level of the geometric shape of said descriptor with the mask of the object to be recognised in the image,</claim-text>
<claim-text>sort the descriptors as a function of their corresponding scores,</claim-text>
<claim-text>select descriptors with the highest scores to perform said statistical learning,</claim-text>
<claim-text>measure a statistical property on a combination of adjacent geometric shapes and non-adjacent geometric shapes, using said descriptors,</claim-text>
<claim-text>focus on the most specific zones in each of the classes as iterations in the learning phase continue, in order to eliminate background zones (<b>12</b>) at the beginning of the processing and to discriminate between classes at the end of the processing.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. Method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which said geometric shapes are rectangles.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. Method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, in which the statistical property measurement is obtained from a histogram difference.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. Method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which the mask of the object to be recognised in the image is obtained by image synthesis.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A non-transitory recording medium containing a computer program that, when run by a computer, performs the method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>. </claim-text>
</claim>
</claims>
</us-patent-grant>
