<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625903-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625903</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13753672</doc-number>
<date>20130130</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2008-027710</doc-number>
<date>20080207</date>
</priority-claim>
</priority-claims>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>G</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382195</main-classification>
<further-classification>382100</further-classification>
<further-classification>345582</further-classification>
</classification-national>
<invention-title id="d2e61">Pose estimation</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6793350</doc-number>
<kind>B1</kind>
<name>Raskar et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>353121</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>7149345</doc-number>
<kind>B2</kind>
<name>Fujiwara</name>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382154</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7227973</doc-number>
<kind>B2</kind>
<name>Ishiyama</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382100</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382104</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382154</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382128</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382131</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382254</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382115</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382118</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382168</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382151</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382155</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382173</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382181</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382190</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382201</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382203</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382195</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382199</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382232</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382276</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382312</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345418</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345419</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345421</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345420</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345426</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345582</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>30</number-of-drawing-sheets>
<number-of-figures>30</number-of-figures>
</figures>
<us-related-documents>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12367178</doc-number>
<date>20090206</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8401295</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13753672</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130148900</doc-number>
<kind>A1</kind>
<date>20130613</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>NEC Corporation</orgname>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Shiba</last-name>
<first-name>Hisashi</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>NEC Corporation</orgname>
<role>03</role>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Chawan</last-name>
<first-name>Sheela</first-name>
<department>2669</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">In a pose estimation for estimating the pose of an object of pose estimation with respect to a reference surface that serves as a reference for estimating a pose, a data processing device: extracts pose parameters from a binarized image; identifies a combination of pose parameters for which the number of cross surfaces of parameter surfaces that accord with surface parameter formulas, which are numerical formulas for expressing a reference surface, is a maximum; finds a slope weighting for each of cross pixels, which are pixels on each candidate surface and which are pixels within a prescribed range, that is identified based on the angles of the tangent plane at the cross pixel and based on planes formed by each of the axes of parameter space; and identifies the significant candidate surface for which a number, which is the sum of slope weightings, is a maximum, as the actual surface that is the reference surface that actually exists in the image.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="185.50mm" wi="113.11mm" file="US08625903-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="149.44mm" wi="146.22mm" file="US08625903-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="137.16mm" wi="153.16mm" file="US08625903-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="122.51mm" wi="139.95mm" file="US08625903-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="153.92mm" wi="144.95mm" file="US08625903-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="121.24mm" wi="143.09mm" file="US08625903-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="149.44mm" wi="148.17mm" file="US08625903-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="110.74mm" wi="94.66mm" file="US08625903-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="58.67mm" wi="96.69mm" file="US08625903-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="170.43mm" wi="155.11mm" file="US08625903-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="196.85mm" wi="155.70mm" file="US08625903-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="80.77mm" wi="144.86mm" file="US08625903-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="194.14mm" wi="159.60mm" file="US08625903-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="169.08mm" wi="123.36mm" file="US08625903-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="80.60mm" wi="134.45mm" file="US08625903-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="203.12mm" wi="153.84mm" file="US08625903-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="139.02mm" wi="142.92mm" file="US08625903-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="144.86mm" wi="141.65mm" file="US08625903-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="194.82mm" wi="117.94mm" file="US08625903-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="196.77mm" wi="153.16mm" file="US08625903-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="212.09mm" wi="166.62mm" file="US08625903-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="200.58mm" wi="156.38mm" file="US08625903-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="194.82mm" wi="157.65mm" file="US08625903-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="210.82mm" wi="159.60mm" file="US08625903-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="207.01mm" wi="118.53mm" file="US08625903-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="201.25mm" wi="159.60mm" file="US08625903-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="210.82mm" wi="115.99mm" file="US08625903-20140107-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="178.14mm" wi="155.11mm" file="US08625903-20140107-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="195.41mm" wi="99.31mm" file="US08625903-20140107-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="219.20mm" wi="158.92mm" file="US08625903-20140107-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="198.63mm" wi="153.16mm" file="US08625903-20140107-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<p id="p-0002" num="0001">This application is based upon and claims the benefit of priority from Japanese patent application No. 2008-027710, filed on Feb. 7, 2008, the disclosure of which is incorporated herein in its entirety by reference.</p>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to a data processing device for estimating the pose of a device, a pose estimation system, a pose estimation method, and a program.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">There are many methods of measuring pitch, roll, and yaw, which are parameters that indicate the attitude of an air vehicle that flies through the air or space or a underwater vehicle that travels through water by a specific coordinate system of the ground surface, ocean surface, or ocean floor, and methods of measuring the height from the ground surface, ocean surface, or ocean floor or depth from the ocean surface, including measurement methods that use pose estimation sensors. Small unmanned air vehicle and underwater vehicle that are seeing increased use in recent years tend to incorporate various sensors that are specially directed to navigation and inspection in addition to pose measurement sensors. In particular, there is an increasing need for mounting cameras, radar, and sonar to obtain images. Sensors that are primarily directed to the capture of such images are referred to as &#x201c;image sensors.&#x201d;</p>
<p id="p-0007" num="0006">However, increase in the variety of mounted sensors results in greater complexity of the air vehicle or underwater vehicle systems (including systems for measuring the above-described parameters). As a result, not only do the design, fabrication, and maintenance of air vehicle or underwater vehicle systems entail considerable time and effort, but there is also a danger of an increase in the frequency of malfunctions.</p>
<p id="p-0008" num="0007">In addition, the air vehicle and underwater vehicle systems become bulkier and heavier, and further, consume more power. It is desirable both from the standpoint of utility as well as from the standpoints of size and energy efficiency to decrease the variety of sensors mounted in air vehicle and underwater vehicle systems.</p>
<p id="p-0009" num="0008">However, when image acquisition is the object of the air vehicle or underwater vehicle, image sensors are indispensable constituent elements. On the other hand, sensors for pose measurement of the air vehicle or underwater vehicle are also indispensable constituent elements for safe and precise navigation. Nevertheless, image sensors that are directed to the acquisition of images that can also substitute for pose measurement sensors can allow the omission of pose measurement sensors and can be considered effective for decreasing size and weight. In addition, although the widely used pose measurement sensors that use inertia may suffer a severe drop in accuracy in the presence of vibration, this problem can be circumvented if pose can be measured by the image sensors.</p>
<p id="p-0010" num="0009">In addition, as methods of estimating the pose of an air vehicle or underwater vehicle that appears in images based on the acquired images, various techniques have been proposed for estimating pose from the appearance of known geographical features or landmarks. However, such techniques cannot be applied in locations in which geographical features or landmarks are difficult to recognize such as over a desert or ice field, in clouds or mist, or in water; in locations that lack beacons or precise topographical maps; or in locations in which geographical features have greatly changed or landmarks have been lost due to, for example, a disaster. Under these circumstances, the use of basic information such as the ground surface or ocean surface as a reference is extremely effective when figuring pose parameters.</p>
<p id="p-0011" num="0010">However, when a substantially flat surface such as the surface of the earth or a surface having a high degree of symmetry such as a spherical surface is the standard, yaw is extremely difficult to find based on the appearance of the surface.</p>
<p id="p-0012" num="0011">Nevertheless, the other principal pose parameters, i.e., altitude or depth, pitch, and roll, can be found. Yaw is not absolutely necessary when only minimum control of pose is necessary such as for avoiding collision with the surface of the earth or floor of the ocean or jumping out of from the ocean surface.</p>
<p id="p-0013" num="0012">A position on a plane such as the surface of the earth, the ocean surface, or the ocean floor can be uniquely found if three points' positions on the plane are known. In a camera, irradiation of, for example, a laser allows the coordinates of a point on the plane to be found based on the principles of triangulation from the positional relation of the laser and camera. However, under noisy conditions such as at night or during bad weather, the use of only three points does not allow sufficient accuracy. In the case of radar or sonar, high accuracy is obtained by combining the data of as many points as possible due to the high level of noise caused by reflections from the ground surface, ocean surface, or ocean floor. For example, a plane is preferably identified by using the reflections of the entire ground surface in radar and by using the reflections of the entire ocean surface in sonar.</p>
<p id="p-0014" num="0013">As a method of finding, from a multiplicity of points, a surface that is made up from this multiplicity of points, a method can be considered of first fitting a plane by means of the least squares method. This method, however, tends to be diverted toward values that greatly deviate from the average. In addition, proper fitting cannot be achieved when the ocean surface and ocean floor are simultaneously visible and the reverberations of each intermix such as in a shallow water.</p>
<p id="p-0015" num="0014">The use of a three-dimensional Hough transform has been proposed as a method of finding a plane even in conditions of a high degree of noise. For easy understanding in the following explanation, a case is described of finding a straight line or curve by means of a Hough transform for a two-dimensional image having a high level of noise as an example.</p>
<p id="p-0016" num="0015">In a Hough transform, an image (in this example, a two-dimensional image) is first subjected to binarization. The binarized image is next divided between a &#x201c;candidate region&#x201d; that is a region in which the existence of a straight line or a curve is predicted and a &#x201c;background region&#x201d; that is a region in which straight lines and curves are not predicted to occur. Next, taking as parameters the coefficients of formulas that express the lines that are to be detected, all combinations of parameters are found for all lines that can pass through each point contained in the candidate region. All combinations of parameters are plotted in parameter space that takes the parameters as axes to find straight lines or curves (hereinbelow referred to as &#x201c;parameter lines&#x201d;). Parameter lines are drawn for each point of the candidate region, and the points at which many parameter lines intersect are the parameter combinations that are to be detected.</p>
<p id="p-0017" num="0016">Explanation next regards a case of straight-line detection that is widely used in the field of image processing. In the example that is next described, as shown in <figref idref="DRAWINGS">FIG. 1</figref>, for a straight line on an image, the distance &#x3c1; from the origin to any point (x, y) on the straight line can be expressed as &#x3c1;=x cos &#x3b8;+y sin &#x3b8; using point (x, y) and angle <b>8</b> formed by a vector having the origin as its starting point and having point (x, y) as an endpoint with respect to the x-axis.</p>
<p id="p-0018" num="0017">Although there are an enormous number of straight lines that can pass through point (x<sub>i</sub>, y<sub>i</sub>) located in a candidate region on an image, this enormous number of straight lines becomes one parameter line in the parameter space that takes the above-described angle &#x3b8; and distance &#x3c1; as axes (the &#x3b8;&#x2212;&#x3c1; plane) and can be expressed by the curve &#x3c1;=x<sub>i </sub>cos &#x3b8;+y<sub>i </sub>sin &#x3b8;. For example, a case is considered in which points A, B, and C located on one straight line exist within a candidate region as shown in <figref idref="DRAWINGS">FIG. 2</figref>. When the parameter lines that correspond to each of points A, B, and C are detected in this state, the point where the three parameter lines intersect (&#x3c1;<sub>0</sub>, &#x3b8;<sub>0</sub>) is the parameter located on the straight line that is to be detected.</p>
<p id="p-0019" num="0018">Therefore, the straight line that is to be detected is expressed by &#x3c1;<sub>0</sub>=x cos &#x3b8;<sub>0</sub>+y sin &#x3b8;<sub>0 </sub>when the above-described relational expression is applied and distance &#x3c1;<sub>0 </sub>and angle &#x3b8;<sub>o </sub>are used.</p>
<p id="p-0020" num="0019">As disclosed in JP-1995-271978A, JP-1997-081755A, JP-1998-096607A, and JP-2003-271975A, various techniques have been proposed for detecting a plane by applying this method to three dimensions.</p>
<p id="p-0021" num="0020">However, in the technique described in JP-1995-271978A, JP-1997-081755A, and JP-1998-096607A, once straight lines have been found in a plane, these straight lines are bundled to find the plane. As a result, these methods have a first problem that is the complexity of the processing for finding a plane. In addition, the restrictive conditions of a straight line are weaker than for a plane. As a result, when higher levels of noise components are included, straight lines which are strongly influenced by the noise components are detected, raising the concern of a decrease in processing efficiency.</p>
<p id="p-0022" num="0021">In the technique disclosed in JP-2003-271975A, surfaces formed by three points to vote are found at first. This case also has a second problem in that the processing for finding a surface is complex. When there is higher levels of noise components, more points is preferably combined together to find a plane. In this case, the concern arises of a marked increase in the amount of processing in the technique disclosed in JP-2003-271975A.</p>
<p id="p-0023" num="0022">In addition, as a problem common to typical techniques, there is a third problem that, when a plane is found, the transformation of parameters contained in the relational expression for expressing the plane that has been found and of pose parameters (for example, depth, altitude, pitch, and roll) that relate to the object of pose estimation is non-linear, whereby the accuracy of the pose parameters cannot be specified in advance. In other words, even when the accuracy (discretization widths) for the parameters of a formula that describes a plane in the parameter space of a Hough transform are each set identically, the accuracy for each pose parameter differ according to the values of the pose parameters and are not fixed on the same value. For example, when the relational expression that indicates coordinate z in the z direction of a point located on a surface is expressed by z=ax+by+c, using slope &#x201c;a&#x201d; for the x direction of the surface in the parameters of the Hough transform results in a nonlinear relation as regards slope &#x201c;a&#x201d; and pitch and roll, which are angles, and altitude, which is length. As a result, implementing a Hough transform by setting slope &#x201c;a&#x201d; at fixed periods does not result in fixed periods in the pose parameters.</p>
<p id="p-0024" num="0023">In a Hough transform, a fourth problem exists of a danger that the potential for mistakenly identifying a false surface resulting from noise components as the &#x201c;true surface&#x201d; that is the surface to be detected changes (increases or decreases) according to the coordinates in the three-dimensional image. The potential for mistakenly identifying a false surface resulting from noise components as the &#x201c;true surface&#x201d; changes according to the coordinates in the three-dimensional image because a group of nearby pixels within a three-dimensional image that originally belong to a different surface are, due to their location within the three-dimensional image, recognized as having the same parameters, resulting in an increase in the number of pixels that are detected, and the increased number of pixels may become greater than the total number of detected pixel groups that belong to the true surface and that are located at other coordinates. To facilitate understanding of this type of phenomenon, a case will be explained in which a Hough transform is performed on a two-dimensional image.</p>
<p id="p-0025" num="0024">A case will here be considered in which a straight line on a plane is detected by a Hough transform. For example, it is assumed that straight lines that pass through the point (x<sub>0</sub>, y<sub>0</sub>) are expressed by &#x3c1;=x<sub>0 </sub>cos &#x3b8;+y<sub>0 </sub>sin &#x3b8;. In this case, a straight line that passes through (x<sub>0</sub>+&#x394;x, y<sub>0</sub>+&#x394;y) located in the vicinity of the point (x<sub>0</sub>, y<sub>0</sub>) is expressed by &#x3c1;+&#x394;&#x3c1;=(x<sub>0</sub>+&#x394;x)cos(&#x3b8;+&#x394;&#x3b8;)+(y<sub>0</sub>+&#x394;y)sin(&#x3b8;+&#x394;&#x3b8;). Still further, when approximation that ignores expansions relating to &#x394;x, &#x394;y, and &#x394;&#x3b8; and terms that are secondary or greater is applied to the relational expression that indicates this straight line, the straight line is expressed by &#x394;&#x3c1;&#x2248;(&#x394;x+y<sub>0</sub>&#x394;&#x3b8;)cos &#x3b8;+(&#x394;y&#x2212;x<sub>0</sub>&#x394;&#x3b8;)sin &#x3b8;. This means that even when &#x394;x, &#x394;y, and &#x394;&#x3b8; are fixed values, when point (x<sub>0</sub>, y<sub>0</sub>) that represents a position on an image differs, &#x394;&#x3c1; also differs accordingly.</p>
<p id="p-0026" num="0025">This problem will next be explained by means of a specific example. As shown in <figref idref="DRAWINGS">FIG. 3</figref><i>a</i>, a case is described in which there are two lines to be detected, line &#x3b1; and line &#x3b2;, point A being detected on line &#x3b1;, and point B and point C being detected on line &#x3b2;. In addition, as shown in <figref idref="DRAWINGS">FIG. 3</figref><i>b</i>, parameter lines in parameter space that correspond to each of points A, B, and C intersect with each other at different coordinates.</p>
<p id="p-0027" num="0026">On the other hand, in the example shown in <figref idref="DRAWINGS">FIG. 4</figref><i>a</i>, the objects of detection are the two lines line &#x3b1;&#x2032; and line &#x3b2;&#x2032;, point A&#x2032; being detected on line &#x3b1;&#x2032; and point B&#x2032; and point C&#x2032; being detected on line &#x3b2;&#x2032;. The relative positions between each of points A&#x2032;, B&#x2032;, and C&#x2032; shown in <figref idref="DRAWINGS">FIG. 4</figref><i>a </i>are assumed to be identical to the relative positions between each of points A, B, and C shown in <figref idref="DRAWINGS">FIG. 3</figref><i>a</i>. In this case, the parameter line that corresponds to point A&#x2032;, the parameter line that corresponds to point B&#x2032;, and the parameter line that corresponds to point C&#x2032; intersect with each other at the same coordinates, as shown in <figref idref="DRAWINGS">FIG. 4</figref><i>b</i>. In other words, the fourth problem is that, when the positions of each of the detected points differ, the form of intersection of the parameter lines that correspond to each of the points differs. Although a method has been proposed for circumventing this fourth problem regarding straight lines on a two-dimensional image, a method of circumventing the fourth problem for plane detection in a three-dimensional image has yet to be proposed.</p>
<p id="p-0028" num="0027">In addition, there is a fifth problem that the problem of changes (increase or decrease) in the potential for mistakenly identifying a false surface resulting from noise components as the &#x201c;true surface&#x201d; according to the coordinates in a three-dimensional image is not limited to the case of detecting a plane within a three-dimensional image but can also occur when detecting a typical curved surface within a three-dimensional image.</p>
<p id="p-0029" num="0028">In a typical method for computing area by counting the number of pixels contained in an image, the area included in the surface will be computed differently when the angle of view changes, even when the same surface is viewed from the object of pose estimation. As a result, the area included in the &#x201c;true surface&#x201d; after the angle of view has changed will in some cases be smaller than the threshold value for determining &#x201c;false surfaces&#x201d; that result from noise components. In such cases, a sixth problem arises that the &#x201c;true surface&#x201d; and a &#x201c;false surfaces&#x201d; resulting from noise components cannot be distinguished, resulting in the danger of the inability to detect the &#x201c;true surface&#x201d; after a change of angle. Counting the &#x201c;crossing frequency (the &#x201c;cross surface number&#x201d; that will be described hereinbelow)&#x201d; that indicates the crossing frequency of parameter surfaces in parameter space is equivalent to counting the number of pixels that belong to each surface detected on an image. However, when a surface is counted by pixels, the area of the surface becomes equal to that of the largest case of projecting this surface upon plane x-y, plane y-z, or plane z-x. In the interest of facilitating understanding, a two-dimensional Hough transform is here considered. Explanation here regards the case of a straight line having a length of 8 pixels, which is the length of a straight line on a simple plane rather than the area of a surface in three dimensions.</p>
<p id="p-0030" num="0029">When the length of pixels aligned in the horizontal direction is taken as a reference, the straight line formed by a series of pixels shown in <figref idref="DRAWINGS">FIG. 5</figref><i>a </i>and the straight line formed of a series of pixels shown in <figref idref="DRAWINGS">FIG. 5</figref><i>b </i>have the same length (the length of eight pixels). However, the straight line shown in <figref idref="DRAWINGS">FIG. 5</figref><i>a </i>has a length of approximately 11.3 pixels according to length measured based on Euclidean distance.</p>
<p id="p-0031" num="0030">The sixth problem is next explained taking the example of a surface in a three-dimensional image. The example shown in <figref idref="DRAWINGS">FIG. 6</figref> compares plane A that is tilted by exactly angle &#x3b8; with respect to plane z-x and plane B that forms an angle of &#x201c;0&#x201d; with respect to plane z-x. In this example, each of planes A and B are assumed to have regions in which boundaries are set by each of the sides of rectangles. The measurement of the area of a surface is the measurement of the number of pixels, and when surface A that is tilted by an angle &#x3b8; is projected onto plane z-x and matches with plane B, the surface of plane A is discerned to be equal to the surface of plane B. However, when the area included in plane A is calculated based on the length in the vertical direction and the length in the horizontal direction as measured by Euclidean distance, the area of plane A is greater than the area of plane B. This application of the latter area measurement method is appropriate both from the standpoint of the visual confirmation of the user and from the standpoint of finding the area based on the actual physical object.</p>
<p id="p-0032" num="0031">In a typical Hough transform, these standpoints are not taken into consideration. As a result, although the area of a plane that is to be detected in excessive noise differs from the area of a false plane resulting from noise components can be clearly distinguished by means of a threshold value, the angle of view may sometimes cause the area of the &#x201c;true surface&#x201d; to fall below this threshold value. In other words, even when a threshold value is provided for distinguishing a &#x201c;true surface&#x201d; from &#x201c;false surfaces&#x201d;, the danger remains that the orientation of the surface may cause the area of the true plane that is to be detected to fall below the threshold value and thus prevent detection of the true plane.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0033" num="0032">It is an object of the present invention to provide a data processing device, a pose estimation system, a pose estimation method, and a program that can solve the above-described problems.</p>
<p id="p-0034" num="0033">The data processing device of the present invention for solving the above-described problems is a data processing device for, based on an image received as input, estimating the pose of an object of pose estimation with respect to a reference surface that is the reference for estimating pose and includes: a binarization unit for dividing the image into a candidate region that is a candidate for the reference surface and a background region that is a region other than the candidate region; a surface parameter formula expression unit for extracting pose parameters that indicate the pose of the object of pose estimation with respect to a reference surface that appears in the image received as input and, based on a combination of values obtained by implementing a transform by a prescribed function upon, of the pose parameters that were extracted, parameters that indicate the direction in which the object of pose estimation is directed and pose parameters other than the parameters that indicate direction, finding surface parameter formulas that are numerical formulas that express the reference surface; a parameter computation unit for, based on pose parameters that were extracted by the surface parameter formula expression unit, computing the combination of pose parameters that make up the surface parameter formulas that express all surfaces that pass through each pixel contained in the candidate region that was divided by the binarization unit; a parameter surface drawing unit for, based on the combination of pose parameters computed by the parameter computation unit, drawing parameter surfaces in accordance with the surface parameter formulas on parameter space, which is space that includes each of the parameters belonging to the pose parameters as the axes of base vectors; an intersection detection unit for finding cross point coordinates, which are the coordinates of intersections through which pass a plurality of the parameter surfaces that were drawn by the parameter surface drawing unit, and a cross surface number, which is the number of parameter surfaces that pass through the cross point coordinates; a significant candidate extraction unit for, when the cross surface number at each cross point that was found by the intersection detection unit is compared with the cross surface numbers at other cross point coordinates in prescribed neighboring regions that take as a reference the cross point coordinates corresponding to the cross surface number and is thus determined to be a maximum, identifying the combination of pose parameters that make up the parameter surface that corresponds to the determined cross surface number as a significant candidate that is an optimum combination of pose parameters for expressing the reference surface; a significant candidate surface drawing unit for, based on a significant candidate identified by the significant candidate extraction unit and a combination of pose parameters that were computed by the parameter computation unit for which distance to the significant candidate in parameter space in prescribed neighboring regions that take the significant candidate as a reference is no greater than a prescribed value, drawing significant candidate surfaces on the image; a cross determination unit for, regarding each significant candidate surface that was drawn by the significant candidate surface drawing unit, identifying pixels that are located on, of the candidate region, the significant candidate surface, and pixels that are located within a prescribed range from the significant candidate surface as cross pixels and finding coordinates of the cross pixels and tangent planes at the cross pixels; a weighting computation unit for, based on angles that are formed by tangent planes in cross pixels that were found by the cross determination unit and planes that are formed by the axes of each base vector included in the parameter space, finding a slope weighting for each cross pixel that was identified by the cross determination unit; a number measurement unit for computing for each of the significant candidate surfaces a number that is the value obtained by adding up the slope weightings of cross pixels that were found by the weighting computation unit and that were contained in each of the significant candidates; and an actual surface determination unit for, when the number computed by the number measurement unit is compared with a number belonging to other significant candidate surfaces in neighboring regions that take as a standard the significant candidate surface that has this number and that thus is determined to be a maximum, identifying the significant candidate surface having the determined number as an actual surface that is a reference surface that actually exists in the image.</p>
<p id="p-0035" num="0034">Alternatively, the data processing device of the present invention is a data processing device for, based on an image that is received as input, estimating the pose of an object of pose estimation with respect to a reference surface that is the reference for estimating pose, the data processing device including: a binarization unit for dividing the image into a candidate region that is a candidate for the reference surface and a background region that is a region other than the candidate region; a surface parameter formula expression unit for extracting pose parameters that indicate the pose of the object of pose estimation with respect to a reference surface that appears in the image received as input and, based on the combination of values obtained by implementing a transform by a prescribed function upon, of the pose parameters that were extracted, parameters that indicate the direction in which the object of pose estimation is directed and pose parameters other than the parameters that indicates direction, finding surface parameter formulas that are numerical formulas that express the reference surface; an all-candidate surface drawing unit for, based on the combination of all pose parameters in parameter space, which is space having, as the axes of base vectors, each of the parameters that belong to the pose parameters, drawing candidate surfaces that accord with the surface parameter formulas on the image in the parameter space; a cross determination unit for, as regards each candidate surface that was drawn by the all-candidate surface drawing unit, identifying pixels that are located on, of the candidate region, the candidate surface, and pixels that are located within a prescribed range from the candidate surface as cross pixels and finding coordinates of the cross pixels and tangent planes at the cross pixels; a weighting computation unit for, based on angles that are formed by tangent planes at the cross pixels that were found by the cross determination unit and planes that are formed by the axes of each base vector included in the parameter space, finding a slope weighting for each cross pixel that was identified by the cross determination unit; a number measurement unit for computing for each of the candidate surfaces a number that is the value obtained by adding up the slope weightings of cross pixels that were found by the weighting computation unit and that were contained in each of the candidate surfaces; and an actual surface determination unit for, based on the number computed by the number measurement unit, identifying from among candidate surfaces drawn by the all-candidate surface drawing unit an actual surface that is a reference surface that actually exists in the image.</p>
<p id="p-0036" num="0035">In addition, the pose estimation system of the present invention for solving the above-described problems is a pose estimation system provided with the data processing device according to claim <b>1</b> and the data processing device according to claim <b>9</b>, the pose estimation system further including a data process switching device for, based on the results of comparing a parameter space calculation amount, which is a calculation amount indicating the volume of arithmetic processing for the data processing device according to claim <b>1</b> to identify an actual surface that is the reference surface, and an image space calculation amount, which is a calculation amount indicating the volume of arithmetic processing for the data processing device according to claim <b>9</b> to identify an actual surface, selecting the data processing device according to claim <b>1</b> or the data processing device according to claim <b>9</b> and causing the selected data processing device to identify the actual surface.</p>
<p id="p-0037" num="0036">In addition, the pose estimation method of the present invention for solving the above-described problems is a pose estimation method for, based on an image that is received as input, estimating the pose of an object of pose estimation with respect to a reference surface that is the reference for estimating the pose and includes: a binarization process for dividing the image into a candidate region that is a candidate for the reference surface and a background region that is a region other than the candidate region; a surface parameter formula expression process for extracting pose parameters that indicate the pose of an object of pose estimation with respect to a reference surface that appears in the image that is received as input and, based on the combination of values obtained by applying a transform by a prescribed function upon, of pose parameters that have been extracted, parameters that indicate direction in which the object of pose estimation is directed and pose parameters other than parameter that indicate direction, finding surface parameter formulas that are numerical formulas that express the reference surface; a parameter computation process for, based on pose parameters that were extracted in the surface parameter formula expression process, computing the combination of pose parameters that make up the surface parameter formulas that express all surfaces that pass through each pixel contained in the candidate region that was divided in the binarization process; a parameter surface drawing process for, based on the combination of pose parameters computed in the parameter computation process, drawing parameter surfaces that accord with the surface parameter formulas in parameter space, which is space that includes, as the axes of base vectors, each of the parameters belonging to the pose parameters; an intersection detection process for finding cross point coordinates, which are the coordinates of intersections through which pass a plurality of the parameter surfaces that were drawn in the parameter surface drawing process, and the cross surface number, which is the number of parameter surfaces that pass through the cross point coordinates; a significant candidate extraction process for, when the cross surface number at each cross point that was found in the intersection detection process is compared with the cross surface numbers at other cross point coordinates in prescribed neighboring regions that take the cross point coordinates corresponding to the cross surface number as a reference and is thus determined to be a maximum, identifying combinations of pose parameters that make up the parameter surface that corresponds to the determined cross surface number as significant candidates, which are the optimum combinations of pose parameters for expressing the reference surface; a significant candidate surface drawing process for, based on a significant candidate identified in the significant candidate extraction process and a combination of pose parameters that were computed in the parameter computation process, for which a distance to the significant candidate in the parameter space in prescribed neighboring regions that take the significant candidate as a reference is no greater than a prescribed value, drawing significant candidate surfaces on the image; a cross determination process for, regarding each significant candidate surface that was drawn in the significant candidate surface drawing process, identifying pixels that are located on, of the candidate region, the significant candidate surface, and pixels that are located within a prescribed range from the significant candidate surface as cross pixels and finding coordinates of the cross pixels and tangent planes at the cross pixels; a weighting computation process for, based on angles that are formed by tangent planes at the cross pixels that were found in the cross determination process and planes that are formed by the axes of each base vector included in the parameter space, finding a slope weighting for each cross pixel that was identified in the cross determination process; a number measurement process for computing for each of the significant candidate surfaces a number that is the value obtained by adding up the slope weightings of cross pixels that were found in the weighting computation process and that were contained in each of the significant candidates; and an actual surface determination process for, when the number computed in the number measurement process is compared with a number belonging to other significant candidate surfaces in neighboring regions that take as a standard the significant candidate surface that has this number and thus is determined to be a maximum, identifying the significant candidate surface having the determined number as an actual surface that is a reference surface that actually exists in the image.</p>
<p id="p-0038" num="0037">Alternatively, the pose estimation method of the present invention is a pose estimation method for, based on an image that is received as input, estimating the pose of an object of pose estimation with respect to a reference surface that is the reference for estimating pose, the pose estimation method including: a binarization process for dividing the image into a candidate region that is a candidate for the reference surface and a background region that is a region other than the candidate region; a surface parameter formula expression process for extracting pose parameters that indicate the pose of the object of pose estimation with respect to a reference surface that appears in the image received as input and, based on the combination of values obtained by implementing a transform by a prescribed function upon, of the pose parameters that were extracted, parameters that indicate the direction in which the object of pose estimation is directed and pose parameters other than the parameters that indicate direction, finding surface parameter formulas that are numerical formulas that express the reference surface; an all-candidate surface drawing process for, based on the combination of all pose parameters in parameter space, which is space having, as the axes of base vectors, each of the parameters belonging to the pose parameters, drawing candidate surfaces that accord with the surface parameter formulas on the image in the parameter space; a cross determination process for, as regards each candidate surface that was drawn in the all-candidate surface drawing process, identifying pixels that are located on, of the candidate region, the candidate surface, and pixels that are located within a prescribed range from the candidate surface as cross pixels and finding coordinates of the cross pixels and tangent planes at the cross pixels; a weighting computation process for, based on angles that are formed by tangent planes at the cross pixels that were found in the cross determination process and planes that are formed by the axes of each base vector included in the parameter space, finding a slope weighting for each cross pixel that was identified in the cross determination process; a number measurement process for computing for each of the candidate surfaces a number that is the value obtained by adding up the slope weightings of cross pixels that were found in the weighting computation process and that were contained in each of the candidate surfaces; and an actual surface determination process for, based on the numbers computed in the number measurement process, identifying an actual surface that is a reference surface that actually exists in the image from among candidate surfaces drawn in the all-candidate surface drawing process.</p>
<p id="p-0039" num="0038">In addition, the pose estimation method of the present invention includes a data process switching process for, based on the result of comparing the parameter space calculation amount, which is an amount of calculation that indicates the amount of arithmetic processing carried out using the pose estimation method described in claim <b>21</b> to identify an actual surface that is the reference surface and the image space calculation amount, which is an amount of calculation that indicates the amount of arithmetic processing carried out using the pose estimation method described in claim <b>29</b> to identify an actual surface, selecting one of the pose estimation methods, among described in claims <b>21</b> and <b>29</b> and using the selected pose estimation method to identify the actual surface.</p>
<p id="p-0040" num="0039">Still further, the program according to the present invention is a record medium having recorded there on a computer-readable program for causing execution by a computer, the program causing a computer to execute: a binarization procedure for dividing an image that is received as input into a candidate region that is a candidate for a reference surface that is the reference for estimating pose and a background region that is a region other than the candidate region; a surface parameter formula expression procedure for extracting pose parameters that indicate the pose of an object of pose estimation with respect to a reference surface that appears in the image received as input and, based on a combination of values obtained by implementing a transform by a prescribed function upon, of pose parameters that were extracted, parameters that indicate the direction in which the object of pose estimation is directed and pose parameters other than the parameters that indicate direction, finding surface parameter formulas that are numerical formulas that express the reference surface; a parameter computation procedure for, based on pose parameters that were extracted in the surface parameter formula expression procedure, computing a combination of pose parameters that make up the surface parameter formulas that express all surfaces that pass through each pixel contained in the candidate region that was divided in the binarization procedure; a parameter surface drawing procedure for, based on the combination of pose parameters computed in the parameter computation procedure, drawing parameter surfaces that accord with the surface parameter formulas in parameter space, which is space that includes, as the axes of base vectors, each of the parameters belonging to the pose parameters; an intersection detection procedure for finding cross point coordinates, which are the coordinates of intersections through which pass a plurality of the parameter surfaces that were drawn in the parameter surface drawing procedure, and cross surface numbers, which are the numbers of parameter surfaces that pass through the cross point coordinates; a significant candidate extraction procedure for, when the cross surface number at each cross point that was found in the intersection detection procedure is compared with the cross surface numbers at other cross point coordinates in prescribed neighboring regions that take the cross point coordinate corresponding to the cross surface number as a reference and is thus determined to be a maximum, identifying the combination of pose parameters that make up the parameter surface that corresponds to the determined cross surface number as a significant candidate, which is the optimum combination of pose parameters for expressing the reference surface; a significant candidate surface drawing procedure for, based on a significant candidate identified in the significant candidate extraction procedure and the combination of pose parameters that were computed in the parameter computation procedure for which distance to the significant candidate in the parameter space in prescribed neighboring regions that take the significant candidate as a reference is no greater than a prescribed value, drawing significant candidate surfaces on the image; a cross determination procedure for, as regards each significant candidate surface that was drawn in the significant candidate surface drawing procedure, identifying pixels that are located on, of the candidate region, the significant candidate surface, and pixels that are located within a prescribed range from the significant candidate surface as cross pixels and finding coordinates of the cross pixels and tangent planes at the cross pixels; a weighting computation procedure for, based on angles that are formed by tangent planes at the cross pixels that were found in the cross determination procedure and planes that are formed by the axes of each base vector included in the parameter space, finding a slope weighting for each cross pixel that was identified in the cross determination procedure; a number measurement procedure for computing for each of the significant candidate surfaces a number that is a value obtained by adding up the slope weightings of cross pixels that were found in the weighting computation procedure and contained in each of the significant candidates; and an actual surface determination procedure for, when the number computed in the number measurement procedure is compared with numbers belonging to other significant candidate surfaces in neighboring regions that take as a reference the significant candidate surface that has this number and thus determined to be a maximum, identifying the significant candidate surface having the determined number as an actual surface that is a reference surface that actually exists in the image.</p>
<p id="p-0041" num="0040">Alternatively, the program of the present invention is a record medium having recorded there on a computer-readable program for causing execution by a computer, the program causing a computer to execute: a binarization procedure for dividing an image received as input into a candidate region that is a candidate for the reference surface that is the reference for estimating pose and a background region that is a region other than the candidate region; a surface parameter formula expression procedure for extracting pose parameters that indicate the pose of an object of pose estimation with respect to a reference surface that appears in the image received as input and, based on a combination of values obtained by implementing a transform by a prescribed function upon, of pose parameters that were extracted, parameters that indicate the direction in which the object of pose estimation is directed and pose parameters other than the parameters that indicate direction, finding surface parameter formulas that are numerical formulas that express the reference surface; an all-candidate surface drawing procedure for, based on the combination of all pose parameters in parameter space, which is space having, as the axes of base vectors, each of the parameters belonging to the pose parameters, drawing candidate surfaces that accord with the surface parameter formulas on the image in the parameter space; a cross determination procedure for, as regards each candidate surface that was drawn in the all-candidate surface drawing procedure, identifying pixels that are located on, of the candidate region, the candidate surface, and pixels that are located within a prescribed range from the candidate surface as cross pixels and finding coordinates of the cross pixels and tangent planes at the cross pixels; a weighting computation procedure for, based on angles that are formed by tangent planes at the cross pixels that were found in the cross determination procedure and planes that are formed by the axes of each base vector included in the parameter space, finding a slope weighting for each cross pixel that was identified in the cross determination procedure; a number measurement procedure for computing for each of the candidate surfaces a number that is a value obtained by adding up the slope weightings of cross pixels that were found in the weighting computation procedure and that were contained in each of the candidate surfaces; and an actual surface determination procedure for, based on the numbers computed in the number measurement procedure, identifying from among candidate surfaces drawn in the all-candidate surface drawing procedure an actual surface that is a reference surface that actually exists in the image.</p>
<p id="p-0042" num="0041">In addition, the program of the present invention is a record medium having recorded there on a computer-readable program for causing execution by a computer of: a data process switching procedure for, based on the result of comparing the parameter space calculation amount, which is a calculation amount that indicates the amount of arithmetic processing carried out by executing the program described in claim <b>41</b> to identify an actual surface that is the reference surface and the image space calculation amount, which is a calculation amount that indicates the amount of arithmetic processing carried out by executing the program described in claim <b>49</b> to identify the actual surface, selecting one of the program described in claim <b>41</b> and the program described in claim <b>49</b> and executing the selected program to identify the actual surface.</p>
<p id="p-0043" num="0042">The data processing device according to the present invention is a data processing device for, based on an image that is received as input, estimating pose of an object of pose estimation with respect to a reference surface that is the reference of estimating pose, the data processing device: dividing the image into a candidate region that is a candidate of the reference surface and a background region that is the region other than the candidate region; extracting pose parameters that indicate the pose of the object of pose estimation with respect to the reference surface that appears in the image that is received as input; based on a combination of values obtained by implementing a transform by a prescribed function upon, of the pose parameters that were extracted, parameters that indicate the direction in which the object of pose estimation is directed and pose parameters other than the parameters that indicate direction, finding surface parameter formulas that are numerical formulas for expressing the reference surface; based on the pose parameters that were extracted, calculating a combination of pose parameters that make up surface parameter formulas that express all surfaces that pass through each pixel that is contained in the candidate region that was divided; based on the combination of pose parameters that were calculated, drawing parameter surfaces that accord with the surface parameter formulas in parameter space, which is space having, as the axes of base vectors, each of the parameters that belong to the pose parameters; finding the cross point coordinates that are the coordinates of intersections through which pass a plurality of parameter surfaces that were drawn and a cross surface number that is the number of parameter surfaces that pass through cross point coordinates; when the cross surface number at each cross point that was found is compared with a cross surface number at other cross point coordinates in prescribed neighboring regions that take the cross point coordinate corresponding to the cross surface number as a reference and thus determined to be a maximum, identifying, as significant candidates that are the combination of optimum pose parameters for expressing the reference surface, the combination of pose parameters that make up parameter surfaces that correspond to the cross surface number that was determined; based on the combination of the significant candidates that were identified and the pose parameters for which the distance to significant candidates in prescribed neighboring regions that take the significant candidates as a reference is no greater than a prescribed value, drawing significant candidate surfaces on the image; regarding each significant candidate surface that was drawn, identifying as cross pixels those pixels located on, of the candidate region, the significant candidate surface, and pixels located within a prescribed range from significant candidate surface; finding the coordinates of cross pixels and tangent planes at cross pixels; based on angles formed by tangent planes at cross pixels that were found and surfaces that are formed by the axes of each base vector belonging to parameter space, finding a slope weighting for each cross pixel; calculating for each significant candidate surface a number that is the value obtained by adding up the slope weightings of cross pixels that are contained in each significant candidate; and, when a number that is calculated is compared with numbers belonging to other significant candidate surfaces in neighboring regions that take as reference the significant candidate surface that has the number and that thus is determined to be a maximum, identifying the significant candidate surface having the number that was determined as an actual surface that is a reference surface that actually exists in the image.</p>
<p id="p-0044" num="0043">By means of this configuration, a surface that is the reference of a pose parameter can be accurately and easily detected to realize pose estimation.</p>
<p id="p-0045" num="0044">The above and other objects, features, and advantages of the present invention will become apparent from the following description with reference to the accompanying drawings which illustrate an example of the present invention.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 1</figref> shows an example of a straight line on a plane for explaining a Hough transform;</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 2</figref> shows an example of parameter lines obtained in parameter space when points are detected on a straight line;</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 3</figref><i>a </i>shows a first example in which three points located on lines are detected on lines that are objects of detection;</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 3</figref><i>b </i>shows an example in which parameter lines that correspond to each point intersect at different coordinates;</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 4</figref><i>a </i>shows a second example in which three points located on lines are detected on lines that are objects of detection;</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 4</figref><i>b </i>shows an example in which parameter lines that correspond to each point intersect at the same coordinates;</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 5</figref><i>a </i>shows a first example of a pixel series arranged in linear form;</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 5</figref><i>b </i>shows a second example of a pixel series arranged in linear form;</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 6</figref> shows an example in which the area of a plane is the same as the area projected on plane z-x;</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 7</figref> shows the configuration of a pose estimation system according to the first embodiment of the present invention;</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 8</figref> shows the configuration of the input device shown in <figref idref="DRAWINGS">FIG. 7</figref>;</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 9</figref> shows the configuration of the data processing device shown in <figref idref="DRAWINGS">FIG. 7</figref>;</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. 10</figref> shows the configuration of the memory device shown in <figref idref="DRAWINGS">FIG. 7</figref>;</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 11</figref> shows an example of the data structure of the intersection list shown in <figref idref="DRAWINGS">FIG. 10</figref>;</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 12</figref> is a flow chart showing the operations for identifying an actual surface;</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 13</figref> shows an example of the pose and positional relation of an air vehicle or underwater vehicle with respect to a plane that is a reference;</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 14</figref> shows an example of the pose and positional relation of an air vehicle or underwater vehicle with respect to a spherical surface that is a reference;</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 15</figref> shows the configuration of a data processing device in the second embodiment;</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 16</figref> shows the configuration of a pose estimation system according to the third embodiment;</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 17</figref> shows the configuration of the data process switching device shown in <figref idref="DRAWINGS">FIG. 16</figref>;</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 18</figref> shows an example of the configuration of the data processing device in the fourth embodiment;</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 19</figref> shows an example of the configuration of the data processing device in the fifth embodiment;</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. 20</figref> shows a first example of the configuration of the data processing device in the sixth embodiment;</p>
<p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. 21</figref> shows a second example of the configuration of the data processing device in the sixth embodiment;</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 22</figref> shows a first example of the configuration of the data processing device in the seventh embodiment;</p>
<p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. 23</figref> shows a second example of the configuration of the data processing device in the seventh example;</p>
<p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. 24</figref> shows a first example of the configuration of the data processing device in the eighth embodiment;</p>
<p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. 25</figref> shows a second example of the configuration of the data processing device in the eighth embodiment;</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. 26</figref> shows an example of the configuration of the data process switching device in the eighth embodiment; and</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 27</figref> shows the configuration of the data processing device in the ninth embodiment.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">EXEMPLARY EMBODIMENT</heading>
<heading id="h-0005" level="1">First Embodiment</heading>
<p id="p-0076" num="0075">Explanation first regards pose estimation system <b>1</b> (including a data processing device, pose estimation method, and program) according to the first embodiment of the present invention.</p>
<p id="p-0077" num="0076">The configuration of pose estimation system <b>1</b> of the first embodiment is first explained. Pose estimation system <b>1</b> is mounted in, for example, an air vehicle or underwater vehicle that is the object of pose estimation together with image acquisition device <b>700</b> and pose control system <b>800</b> shown in <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0078" num="0077">As shown in <figref idref="DRAWINGS">FIG. 7</figref>, pose estimation system <b>1</b> is made up from input device <b>2</b>, data processing device <b>3</b>, memory device <b>4</b>, and communication device <b>5</b>.</p>
<p id="p-0079" num="0078">Input device <b>2</b> includes an image input function and a character input function. Input device <b>2</b> receives images from external image acquisition device <b>700</b> and receives the images as input. In addition, input device <b>2</b> receives character information according to the operation of the user. External image acquisition device <b>700</b> is made up from, for example, a stereo camera, a laser, a rangefinder, radar, sonar, or lidar, and acquires images by capturing and generating the image of a subject.</p>
<p id="p-0080" num="0079">Data processing device <b>3</b> is made up from a prescribed information processing device that operates in accordance with program control and executes various data processing for detecting a plane or curved surface.</p>
<p id="p-0081" num="0080">Memory device <b>4</b> stores various types of information. For example, memory device <b>4</b> stores a program for controlling data processing device <b>3</b>, data for executing operations by data processing device <b>3</b>, and the results of operation by data processing device <b>3</b>.</p>
<p id="p-0082" num="0081">Communication device <b>5</b> transmits pose parameters computed by data processing device <b>3</b> to external pose control system <b>800</b>. Communication device <b>5</b> executes &#x201c;data transmission processing,&#x201d; and transmits pose parameters by way of a cable or wireless network (not shown) to, for example, external pose control system <b>800</b>. In addition, pose control system <b>800</b> controls the pose of the object of pose estimation in which the pose estimation system is mounted based on the pose parameters that are transmitted in from communication device <b>5</b>.</p>
<p id="p-0083" num="0082">Explanation next regards the details of input device <b>2</b>. As shown in <figref idref="DRAWINGS">FIG. 8</figref>, input device <b>2</b> includes external data input unit <b>21</b> and character input unit <b>22</b>.</p>
<p id="p-0084" num="0083">External data input unit <b>21</b> executes &#x201c;external data input processing&#x201d; and receives as input three-dimensional images received from image acquisition device <b>700</b>. In this case, no particular limitations apply regarding the three-dimensional images that external data input unit <b>21</b> receives. For example, the three-dimensional image may be a still three-dimensional image or a moving three-dimensional image. In addition, the three-dimensional image may be a three-dimensional image that represents by high dimensions: a physical amount that indicates a temporal size or a spatial size such as a velocity field or a magnetic field, an image characteristic amount that is obtained by various operations such as convolution by a specific function, or temporal change of an image characteristic amount.</p>
<p id="p-0085" num="0084">External data input unit <b>21</b> is not limited to direct reception by way of data lines of an image acquired by image acquisition device <b>700</b> and may receive an image from the outside by reading an image from a recording medium that records an image acquired by image acquisition device <b>700</b>.</p>
<p id="p-0086" num="0085">Character input unit <b>22</b> is made up from, for example, a keyboard, a mouse, or a touch-panel, and receives character information according to the manipulation of the user by the execution of &#x201c;character input processing.&#x201d;</p>
<p id="p-0087" num="0086">Explanation next regards the details of data processing device <b>3</b>. As shown in <figref idref="DRAWINGS">FIG. 9</figref>, data processing device <b>3</b> includes: binarization unit <b>31</b>, surface parameter formula expression unit <b>32</b>, parameter computation unit <b>33</b>, parameter surface drawing unit <b>34</b>, intersection detection unit <b>35</b>, significant candidate extraction unit <b>36</b>, significant candidate surface drawing unit <b>37</b>, cross determination unit <b>38</b>, weighting computation unit <b>39</b>, number measurement unit <b>310</b>, and actual surface determination unit <b>311</b>.</p>
<p id="p-0088" num="0087">Binarization unit <b>31</b> executes a &#x201c;binarization process&#x201d; to divide a three-dimensional image received as input from external data input unit <b>21</b> into prescribed regions. Based on the pixel values of the three-dimensional image, binarization unit <b>31</b> divides the three-dimensional image into two regions, a &#x201c;candidate region CR&#x201d; that is the region that is a candidate for the &#x201c;reference surface&#x201d; (plane or curved surface) that is the object of detection, and &#x201c;background region BR&#x201d; that is a region that is not a candidate for the reference surface (a region other than candidate region CR). In addition, the three-dimensional image that is divided into two regions generated by the binarization process of binarization unit <b>31</b> is referred to as a &#x201c;binarized image.&#x201d; Surface parameter formula expression unit <b>32</b> executes a &#x201c;surface parameter formula expression process,&#x201d; and based on the three-dimensional image received as input by external data input unit <b>21</b>, extracts &#x201c;pose parameters&#x201d; that indicate the pose of the object of pose estimation with respect to a plane or curved surface that appears in the three-dimensional image. Surface parameter formula expression unit <b>32</b> then subjects, of the extracted pose parameters, the parameters that indicate direction to conversion by &#x201c;a prescribed function.&#x201d;</p>
<p id="p-0089" num="0088">Here, the &#x201c;object of pose estimation&#x201d; is, for example, an air vehicle or a underwater vehicle. In addition, of the pose parameters, &#x201c;parameters that indicate direction&#x201d; are, for example, the angle of an object of pose estimation with respect to the reference plane for estimating pose, or the direction (for example, north, east, south, or west) of the object of pose estimation with respect to the reference surface. In addition, the pose parameters include, for example, the distance between the object of pose estimation and the reference surface (a plane or curved surface) that is the reference for estimating the pose of the object of pose estimation.</p>
<p id="p-0090" num="0089">When the &#x201c;parameters that indicate direction&#x201d; are angles, a transform is executed that uses a trigonometric function as the &#x201c;prescribed function.&#x201d; This transform is carried out to correct the length of a pixel series to the distance that is measured by Euclidean distance, as was explained in the previously described example. In the following explanation, the parameter that indicates direction is assumed to be an angle.</p>
<p id="p-0091" num="0090">Surface parameter formula expression unit <b>32</b> further executes a &#x201c;surface parameter formula expression process&#x201d; and combines a value obtained by subjecting an angle to conversion by a trigonometric function with a pose parameter other than an angle (direction) to find a &#x201c;surface parameter formula,&#x201d; which is a numerical expression for expressing any surface (plane or curved surface). In addition, surface parameter formula expression unit <b>32</b> saves the &#x201c;surface parameter formula&#x201d; that was found. Parameter computation unit <b>33</b> executes a &#x201c;parameter computation process&#x201d; and, at each pixel of the &#x201c;candidate region CR&#x201d; that was divided by binarization unit <b>31</b>, computes a combination of pose parameters that satisfy formulas that express all planes or curved surfaces that can pass through these pixels based on the pose parameters that were extracted by the surface parameter formula expression unit. Parameter surface drawing unit <b>34</b> executes a &#x201c;parameter surface drawing process,&#x201d; and using the combination of all pose parameters that were computed by parameter computation unit <b>33</b>, draws &#x201c;parameter surfaces&#x201d; (in planar form or in the form of a curved surface) in &#x201c;parameter space,&#x201d; which is space that includes each of the parameters (such as angle and distance) that belong to the pose parameters as the axes of base vectors.</p>
<p id="p-0092" num="0091">Intersection detection unit <b>35</b> executes a &#x201c;intersection detection process,&#x201d; and, based on the parameter surfaces drawn by parameter surface drawing unit <b>34</b>, finds &#x201c;cross point coordinates&#x201d; and &#x201c;cross surface numbers&#x201d; relating to all pixels contained in candidate region CR.</p>
<p id="p-0093" num="0092">Here, &#x201c;cross point coordinates&#x201d; refers to the coordinates of cross points (intersections) through which a plurality of parameters pass due to the intersection of a plurality of parameter surfaces in parameter space. In addition, a &#x201c;cross surface number&#x201d; is the number of parameter surfaces that intersect at a &#x201c;cross point coordinate&#x201d; and that pass through a cross point (intersection).</p>
<p id="p-0094" num="0093">Significant candidate extraction unit <b>36</b> executes a &#x201c;significant candidate extraction process,&#x201d; and extracts as &#x201c;significant candidates&#x201d; the combination of pose parameters that are most suitable for expressing planes and curved surfaces from among the combination of pose parameters computed by parameter computation unit <b>33</b>.</p>
<p id="p-0095" num="0094">The conditions for identification as a significant candidate are; that the cross surface number at cross point coordinates detected by intersection detection unit <b>35</b> be greater than a first threshold value, and moreover, that the cross surface number at cross point coordinates that are currently being determined is the largest in prescribed neighboring regions that take these cross point coordinates as a reference.</p>
<p id="p-0096" num="0095">For this purpose, significant candidate extraction unit <b>36</b> compares the first threshold value with the cross surface number at the cross point coordinates that were detected by intersection detection unit <b>35</b>. The first threshold value may be a value stored in threshold value memory unit <b>41</b>. If, as a result of comparison, the cross surface number is greater than the first threshold value at each cross point, it is determined that the conditions for significant candidate have been met.</p>
<p id="p-0097" num="0096">In addition, significant candidate extraction unit <b>36</b> further determines whether a cross surface number that has been determined to be greater than the first threshold value is the maximum in prescribed neighboring regions that take the cross point coordinates that correspond to this cross surface number as a reference. Then, when it is determined that there are no intersections within the prescribed neighboring regions that have a larger cross surface number than currently being determined (when it is determined that the cross surface number is a maximum), the combination of pose parameters that make up the parameter surface that corresponds to the cross surface number is identified as the &#x201c;significant candidate&#x201d; that is the combination of pose parameters most suitable for expressing a plane or curved surface. Significant candidate extraction unit <b>36</b> further, regarding the combination of pose parameters that has been identified as a significant candidate, converts &#x201c;significant candidate surface bit BC,&#x201d; which indicates that the combination is a significant candidate, from &#x201c;0&#x201d; to &#x201c;1.&#x201d; Significant candidate bit BC is registered in intersection list <b>421</b> that is stored by memory device <b>4</b> and that will be described hereinbelow.</p>
<p id="p-0098" num="0097">Significant candidate surface drawing unit <b>37</b> executes a &#x201c;significant candidate surface drawing process&#x201d; and, based on a significant candidate that was extracted by significant candidate extraction unit <b>36</b> and the combination of pose parameters for which distance from the significant candidate in parameter space is no greater than a prescribed value in prescribed neighboring regions that take this significant candidate as a reference, draws &#x201c;significant candidate surfaces&#x201d; (planes or curved surfaces). Significant candidate surface drawing unit <b>37</b> draws &#x201c;significant candidate surfaces&#x201d; to overlie the three-dimensional image.</p>
<p id="p-0099" num="0098">Cross determination unit <b>38</b> executes a &#x201c;cross determination process&#x201d; and, for each significant candidate surface that was drawn by significant candidate surface drawing unit <b>37</b>, identifies as &#x201c;cross pixels&#x201d; pixels through which significant candidate surfaces pass in candidate region CR of the three-dimensional image and pixels located within a prescribed range that takes the significant candidate surface as a reference. In addition, Cross determination unit <b>38</b> finds the coordinates in parameter space at which the identified &#x201c;cross pixels&#x201d; are located. Cross determination unit <b>38</b> further registers the coordinates of the &#x201c;cross pixels&#x201d; that were found in a cross pixel list (to be described) that is stored by parameter space memory unit <b>42</b>.</p>
<p id="p-0100" num="0099">Cross determination unit <b>38</b> further finds tangent planes of significant candidate surfaces at cross pixels.</p>
<p id="p-0101" num="0100">Weighting computation unit <b>39</b> executes a &#x201c;weighting computation process.&#x201d; In the &#x201c;weighting computation process,&#x201d; weighting computation unit <b>39</b> finds each of the angles formed by tangent planes that were found by cross determination unit <b>38</b> and surfaces formed by the axes belonging to parameter space (plane x-y, plane y-z, and plane z-x). In other words, weighting computation unit <b>39</b> finds the angles formed by tangent planes that were found by cross determination unit <b>38</b> and plane x-y, the angles that the tangent planes form with plane y-z, and the angle that the tangent planes form with plane z-x. Weighting computation unit <b>39</b> then finds the &#x201c;minimum angle,&#x201d; which is the angle for which the absolute value among the angles that were found is a minimum.</p>
<p id="p-0102" num="0101">Weighting computation unit <b>39</b> further finds a &#x201c;slope weighting&#x201d; that expresses the ratio of the length of the pixel series to the length measured by Euclidean distance for each pixel through which significant candidate surfaces pass. The &#x201c;slope weighting&#x201d; in this example is found by the absolute value of the reciprocal of the cosine for the minimum angle. For example, if the minimum angle that is found is the minimum angle &#x3b8;, the &#x201c;slope weighting&#x201d; is 1/|cos &#x3b8;|.</p>
<p id="p-0103" num="0102">Number measurement unit <b>310</b> executes a &#x201c;number measurement process&#x201d; and finds a &#x201c;number,&#x201d; which is the value obtained by adding up the slope weightings of all cross pixels for each significant candidate surface.</p>
<p id="p-0104" num="0103">Actual surface determination unit <b>311</b> executes an &#x201c;actual surface determination process&#x201d; and based on the numbers found by number measurement unit <b>310</b> identifies from among the significant candidate surfaces an &#x201c;actual surface,&#x201d; which is a surface that actually exists in the three-dimensional image.</p>
<p id="p-0105" num="0104">As to the conditions for identification as an actual surface, the number found by number measurement unit <b>310</b> must be larger than a second threshold value, and further, the cross surface number at cross point coordinates that are currently being determined must be the maximum in prescribed neighboring regions that take these cross point coordinates as a reference.</p>
<p id="p-0106" num="0105">To this end, actual surface determination unit <b>311</b> compares the numbers found by number measurement unit <b>310</b> with a second threshold value. The second threshold value may be a value stored by threshold value memory unit <b>41</b>. If, as a result of the comparison, the number is found to be greater than the second threshold value, actual surface determination unit <b>311</b> determines that the conditions for an actual surface are satisfied.</p>
<p id="p-0107" num="0106">Actual surface determination unit <b>311</b> further determines whether the number belonging to a significant candidate surface that has been determined to be greater than the second threshold value is the largest when compared with numbers belonging to other significant candidate surfaces included in neighboring regions that take the significant candidate surface having this number as a reference. Then, if it is determined that there are no significant candidate surfaces in prescribed neighboring regions larger than the number that is currently being determined (if it is determined that the number is the maximum), a significant candidate surface having this number is identified as an &#x201c;actual surface&#x201d; that actually exists in the three-dimensional image. Actual surface determination unit <b>311</b> further, for the significant candidate surface (combination of pose parameters) that has been identified as an actual surface, converts the &#x201c;actual surface bit BR&#x201d; that indicates a significant candidate from &#x201c;0&#x201d; to &#x201c;1.&#x201d; The significant candidate surface bit BR is further registered in intersection list <b>421</b> (to be described) that is stored by memory device <b>4</b>.</p>
<p id="p-0108" num="0107">Actual surface determination unit <b>311</b> further supplies the pose parameters that make up the actual surface that has been identified to external pose control system <b>800</b> by way of communication device <b>5</b>.</p>
<p id="p-0109" num="0108">Explanation next regards the details of memory device <b>4</b>. As shown in <figref idref="DRAWINGS">FIG. 10</figref>, memory device <b>4</b> includes threshold value memory unit <b>41</b>, parameter space memory unit <b>42</b>, and image memory unit <b>43</b>.</p>
<p id="p-0110" num="0109">Threshold value memory unit <b>41</b> executes a &#x201c;threshold value storage process&#x201d; and stores various threshold values that have been received as input from character input unit <b>22</b>. Threshold value memory unit <b>41</b> stores, for example, the above-described first threshold value, the range of values, and the discretization width of each axis of parameter space that is set in advance.</p>
<p id="p-0111" num="0110">Parameter space memory unit <b>42</b> executes a &#x201c;parameter space storage process&#x201d; and stores, for example, lists of detection candidate surfaces or parameter space used with threshold value memory unit <b>41</b> when detecting surfaces.</p>
<p id="p-0112" num="0111">Parameter space memory unit <b>42</b> stores, for example, &#x201c;intersection list <b>421</b>&#x201d; in which are registered cross points (intersections) located at the cross point coordinates that were detected by intersection detection unit <b>35</b>.</p>
<p id="p-0113" num="0112">As shown in <figref idref="DRAWINGS">FIG. 11</figref>, intersection list <b>421</b> places in association the combination of pose parameters, significant candidate surface bit BC, and actual surface bit BR. Significant candidate surface bit BC indicates that the corresponding combination of pose parameters is a significant candidate when its value is set to &#x201c;1.&#x201d; In addition, actual surface bit BR indicates that the corresponding combination of pose parameters makes up an actual surface when its value is set to &#x201c;1.&#x201d;</p>
<p id="p-0114" num="0113">In addition, parameter space memory unit <b>42</b> stores, for example, &#x201c;cross pixel list <b>422</b>&#x201d; that registers the coordinates of cross pixels. Parameter space memory unit <b>42</b> further stores &#x201c;slope weightings&#x201d; that are found by weighting computation unit <b>39</b>.</p>
<p id="p-0115" num="0114">Image memory unit <b>43</b> executes an &#x201c;image storage process&#x201d; and stores, for example, the input image that is received as input from external data input unit <b>21</b> and the images during processing or the images that result from processing by each of the constituent elements of data processing device <b>3</b>.</p>
<p id="p-0116" num="0115">Explanation next regards the pose estimation operations that are carried out in pose estimation system <b>1</b> of the first embodiment that has the above-described configuration.</p>
<p id="p-0117" num="0116">As shown in <figref idref="DRAWINGS">FIG. 12</figref>, external data input unit <b>21</b> of input device <b>2</b> receives a three-dimensional image by means of the &#x201c;external data input process&#x201d; of Step <b>11</b>. Image memory unit <b>43</b> stores the three-dimensional image that is received as input.</p>
<p id="p-0118" num="0117">By means of the &#x201c;binarization process&#x201d; of Step <b>12</b>, binarization unit <b>31</b> refers to image memory unit <b>43</b> and, based on the pixel values of the three-dimensional image, divides the three-dimensional image into two regions: a region that is a candidate for the planes or curved surfaces that are the objects of detection (candidate region CR) and a region other than the candidate region CR (background region BR). In this way, binarization unit <b>31</b> generates a binarized image.</p>
<p id="p-0119" num="0118">The method by which binarization unit <b>31</b> divides a three-dimensional image into two regions is the same as a typical method for dividing an image (two-dimensional image) into two regions. There are many typical methods for binarization of an image that divides an image into two regions.</p>
<p id="p-0120" num="0119">For example, a P-tile method may be applied in which a prescribed ratio of a candidate region to the number of all pixels is determined as a threshold value in advance and saved in threshold value memory unit <b>41</b>, and the three-dimensional image then subjected to binarization in accordance with this threshold value.</p>
<p id="p-0121" num="0120">Alternatively, a mode method may be employed in which, for example, a histogram is created with the horizontal axis as pixel values and the vertical axis as frequency, and, assuming the histogram has a double-peak form, binarization is carried out with the valley as the threshold value. Still further, a discriminant-analysis method may be applied in which, for example, binarization is carried out by determining a threshold value such that the dispersion of pixel values in each of candidate region CR and background region BR is a minimum, and moreover, the dispersion between candidate region CR and background region BR is increased. Further, a fixed threshold value method may be applied in which, for example, a threshold value of pixel values is determined in advance and saved in threshold value memory unit <b>41</b>, and the image then subjected to binarization based on whether a pixel value is greater than this threshold value. Finally, a dynamic threshold value method may be applied in which, for example, the image is divided into small regions of a prescribed size, and the P-tile method, mode method, or discriminant-analysis method then used for each small region.</p>
<p id="p-0122" num="0121">Image memory unit <b>43</b> executes an image storage process and stores the binarized image generated by binarization unit <b>31</b>.</p>
<p id="p-0123" num="0122">Surface parameter formula expression unit <b>32</b> executes the &#x201c;surface parameter formula expression process&#x201d; of Step <b>13</b>, and finds &#x201c;surface parameter formulas&#x201d; that are numerical formulas indicating surfaces in the three-dimensional image. An actual surface parameter formula that is calculated by surface parameter formula expression unit <b>32</b> in this &#x201c;surface parameter formula expression process&#x201d; is next described.</p>
<p id="p-0124" num="0123">For example, a case is considered in which, as shown in <figref idref="DRAWINGS">FIG. 13</figref>: the &#x201c;reference surface&#x201d; that is the surface taken as a reference is a plane; roll, which is the rotation angle that takes as its axis the &#x201c;direction of the line of sight (for example, the direction of the optical axis of the image-capture lens)&#x201d; when image acquisition of device <b>700</b> (image acquisition means) captures an image, is &#x3b1; toward the right; pitch, which is the angle formed by the direction of the line of sight of image acquisition device <b>700</b> (image acquisition means) with respect to the plane, is pitch &#x3b2;; the plane that is the reference surface is positioned above image acquisition device <b>700</b> (image acquisition means); and the distance between the plane and image acquisition device <b>700</b> is distance d. This case corresponds to, for example, estimating the pose of a craft that travels through water at a depth d that corresponds to distance d based on the ocean surface that corresponds to a reference surface that is positioned above. In this example, the projection onto plane x-y of the direction of the line of sight of the image acquisition unit is assumed to match the y-axis.</p>
<p id="p-0125" num="0124">If the position of image acquisition device <b>700</b> is taken as the origin in this state, when viewed from a coordinate system that is fixed at image acquisition device <b>700</b> that takes the direction of the line of sight as the y&#x2032;-axis, the horizontal direction of image acquisition device <b>700</b> as the x&#x2032;-axis, and the vertical direction as the z&#x2032;-axis, the conversion of the coordinate system is represented by the following formula 1:</p>
<p id="p-0126" num="0125">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Formula</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>1</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>(</mo>
          <mtable>
            <mtr>
              <mtd>
                <msup>
                  <mi>x</mi>
                  <mi>&#x2032;</mi>
                </msup>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msup>
                  <mi>y</mi>
                  <mi>&#x2032;</mi>
                </msup>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msup>
                  <mi>z</mi>
                  <mi>&#x2032;</mi>
                </msup>
              </mtd>
            </mtr>
          </mtable>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mo>(</mo>
            <mtable>
              <mtr>
                <mtd>
                  <mrow>
                    <mi>cos</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b1;</mi>
                  </mrow>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mrow>
                    <mrow>
                      <mo>-</mo>
                      <mi>sin</mi>
                    </mrow>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b1;</mi>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mrow>
                    <mi>sin</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b1;</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>sin</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b2;</mi>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow>
                    <mi>cos</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b2;</mi>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow>
                    <mi>cos</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b1;</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>sin</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b2;</mi>
                  </mrow>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mrow>
                    <mi>sin</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b2;</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>cos</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b2;</mi>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow>
                    <mrow>
                      <mo>-</mo>
                      <mi>sin</mi>
                    </mrow>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b2;</mi>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow>
                    <mi>cos</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b1;</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>cos</mi>
                    <mo>&#x2062;</mo>
                    <mstyle>
                      <mspace width="0.6em" height="0.6ex"/>
                    </mstyle>
                    <mo>&#x2062;</mo>
                    <mi>&#x3b2;</mi>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
            <mo>)</mo>
          </mrow>
          <mo>&#x2062;</mo>
          <mrow>
            <mo>(</mo>
            <mtable>
              <mtr>
                <mtd>
                  <mi>x</mi>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>y</mi>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>z</mi>
                </mtd>
              </mtr>
            </mtable>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mi>Formula1</mi>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0127" num="0126">In addition, the plane that is taken as the reference surface is represented by the following formula 2:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>[Formula 2]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>d=&#x2212;x</i>&#x2032; sin &#x3b1;+<i>y</i>&#x2032; cos &#x3b1; sin &#x3b2;+<i>z</i>&#x2032; cos &#x3b1; cos &#x3b2;&#x2003;&#x2003;(Formula2)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0128" num="0127">Surface parameter formula expression unit <b>32</b> subjects the three-dimensional image to a Hough transform that takes the above-described &#x3b1;, &#x3b2;, and distance d as the axes of parameter space. On the other hand, a case may be considered in which the reference surface is a sphere. In this case, it is useful to seek the pose parameters with the center coordinate of the spherical surface as the base point, similar to a case of estimating the pose of a flying craft on a satellite orbit.</p>
<p id="p-0129" num="0128">As a result, a spherical surface is assumed to be positioned vertically above or vertically below a flying craft that is the object of pose estimation as shown in <figref idref="DRAWINGS">FIG. 14</figref>. When the center of the spherical surface is positioned a distance d as measured from above the air vehicle and the radius of the spherical surface is known to be R, the relation that represents the spherical surface in a coordinate system that takes as axes the x-direction, y-direction, and z-direction in which roll and pitch are both &#x201c;0&#x201d; is represented by the following Formula 3:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>[Formula 3]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i><sup>2</sup><i>+y</i><sup>2</sup>+(<i>z&#x2212;d</i>)<sup>2</sup><i>=R</i><sup>2</sup>&#x2003;&#x2003;(Formula3)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0130" num="0129">In addition, in a coordinate system that takes as axes x&#x2032;, y&#x2032; and z&#x2032; in which roll is &#x3b1; and pitch is &#x3b2;, a spherical surface is represented by the following Formula 4 that uses Formula 1:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>[Formula 4]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i>=(<i>y</i>&#x2032; sin &#x3b2;+<i>z</i>&#x2032; cos &#x3b2;)cos &#x3b1;&#x2212;<i>x&#x2032;</i> sin &#x3b1;&#xb1;&#x221a;{square root over (<i>R</i><sup>2</sup><i>&#x2212;A</i>)}&#x2003;&#x2003;(Formula4)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0131" num="0130">Here, A in Formula 4 is defined by the following Formula 5:</p>
<p id="p-0132" num="0131">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Formula</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>5</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi>A</mi>
        <mo>=</mo>
        <mrow>
          <mrow>
            <msup>
              <mi>y</mi>
              <mi>&#x2032;2</mi>
            </msup>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <msup>
                    <mi>sin</mi>
                    <mn>2</mn>
                  </msup>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <mi>&#x3b1;</mi>
                    <mo>&#xb7;</mo>
                    <msup>
                      <mi>sin</mi>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                  <mo>&#x2062;</mo>
                  <mi>&#x3b2;</mi>
                </mrow>
                <mo>+</mo>
                <mrow>
                  <msup>
                    <mi>cos</mi>
                    <mn>2</mn>
                  </msup>
                  <mo>&#x2062;</mo>
                  <mi>&#x3b2;</mi>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mn>2</mn>
            <mo>&#x2062;</mo>
            <msup>
              <mi>y</mi>
              <mi>&#x2032;</mi>
            </msup>
            <mo>&#x2062;</mo>
            <msup>
              <mi>z</mi>
              <mi>&#x2032;</mi>
            </msup>
            <mo>&#x2062;</mo>
            <msup>
              <mi>cos</mi>
              <mn>2</mn>
            </msup>
            <mo>&#x2062;</mo>
            <mi>&#x3b1;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>sin</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b2;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>cos</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b2;</mi>
          </mrow>
          <mo>+</mo>
          <mrow>
            <msup>
              <mi>z</mi>
              <mi>&#x2032;2</mi>
            </msup>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <msup>
                    <mi>sin</mi>
                    <mn>2</mn>
                  </msup>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <mi>&#x3b1;</mi>
                    <mo>&#xb7;</mo>
                    <msup>
                      <mi>cos</mi>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                  <mo>&#x2062;</mo>
                  <mi>&#x3b2;</mi>
                </mrow>
                <mo>+</mo>
                <mrow>
                  <msup>
                    <mi>sin</mi>
                    <mn>2</mn>
                  </msup>
                  <mo>&#x2062;</mo>
                  <mi>&#x3b2;</mi>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mn>2</mn>
            <mo>&#x2062;</mo>
            <msup>
              <mi>x</mi>
              <mi>&#x2032;</mi>
            </msup>
            <mo>&#x2062;</mo>
            <msup>
              <mi>y</mi>
              <mi>&#x2032;</mi>
            </msup>
            <mo>&#x2062;</mo>
            <mi>sin</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b1;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>cos</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b2;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>sin</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3d5;</mi>
          </mrow>
          <mo>+</mo>
          <mrow>
            <mn>2</mn>
            <mo>&#x2062;</mo>
            <msup>
              <mi>z</mi>
              <mi>&#x2032;</mi>
            </msup>
            <mo>&#x2062;</mo>
            <msup>
              <mi>x</mi>
              <mi>&#x2032;</mi>
            </msup>
            <mo>&#x2062;</mo>
            <mi>sin</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b1;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>cos</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b1;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>cos</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b2;</mi>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mi>Formula5</mi>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0133" num="0132">Surface parameter formula expression unit <b>32</b> subjects the three-dimensional image to a Hough transform that takes this &#x3b1;, &#x3b2;, and d as the axes of parameter space. Parameter computation unit <b>33</b> executes the &#x201c;parameter computation process&#x201d; of Step <b>14</b> and, at each pixel included in the &#x201c;candidate region&#x201d; of the three-dimensional image, computes the combination of parameters of surface parameter formulas (equations) that represent all planes and curved surfaces that can pass through these pixels.</p>
<p id="p-0134" num="0133">For example, in the case of the plane shown in formula 2, &#x3b1; and &#x3b2; are subjected to discretization by a width that has been provided in advance, and distance d is then found from the relation of formula 2 for each combination of &#x3b1; and &#x3b2; within a range that has been provided in advance. For example, if the coordinates of pixels of a candidate region that is in a three-dimensional image are (x<sub>0</sub>, y<sub>0</sub>, z<sub>0</sub>), distance d is found by the following formula 6:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>[Formula 6]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>d=&#x2212;x</i><sub>0 </sub>sin &#x3b1;+<i>y</i><sub>0 </sub>cos &#x3b1; sin &#x3b2;+<i>z</i><sub>0 </sub>cos &#x3b1; cos &#x3b2;&#x2003;&#x2003;(Formula6)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0135" num="0134">In the case of the spherical surface shown in Formula 4, &#x3b1; and &#x3b2; are subjected to discretization by a width that is provided in advance, and distance d is found from the relation of Formula 4 for each combination of &#x3b1; and &#x3b2; in a range that is provided in advance. For example, if the coordinates at which a pixel included in candidate region CR in a three-dimensional image are (x<sub>0</sub>, y<sub>0</sub>, z<sub>0</sub>), distance d is represented by the following Formula 7:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>[Formula 7]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i>=(<i>y</i><sub>0 </sub>sin &#x3b2;+<i>z</i><sub>0 </sub>cos &#x3b2;)cos &#x3b1;&#x2212;<i>x</i><sub>0 </sub>sin &#x3b1;&#xb1;&#x221a;{square root over (<i>R</i><sup>2</sup><i>&#x2212;B</i>)}&#x2003;&#x2003;(Formula7)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0136" num="0135">In this case, &#x201c;B&#x201d; in Formula 7 is found by the following Formula 8:</p>
<p id="p-0137" num="0136">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Formula</mi>
          <mo>&#x2062;</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>8</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi>B</mi>
        <mo>=</mo>
        <mrow>
          <mrow>
            <msubsup>
              <mi>y</mi>
              <mn>0</mn>
              <mn>2</mn>
            </msubsup>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <msup>
                    <mi>sin</mi>
                    <mn>2</mn>
                  </msup>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <mi>&#x3b1;</mi>
                    <mo>&#xb7;</mo>
                    <msup>
                      <mi>sin</mi>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                  <mo>&#x2062;</mo>
                  <mi>&#x3b2;</mi>
                </mrow>
                <mo>+</mo>
                <mrow>
                  <msup>
                    <mi>cos</mi>
                    <mn>2</mn>
                  </msup>
                  <mo>&#x2062;</mo>
                  <mi>&#x3b2;</mi>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mn>2</mn>
            <mo>&#x2062;</mo>
            <msub>
              <mi>y</mi>
              <mn>0</mn>
            </msub>
            <mo>&#x2062;</mo>
            <msub>
              <mi>z</mi>
              <mn>0</mn>
            </msub>
            <mo>&#x2062;</mo>
            <msup>
              <mi>cos</mi>
              <mn>2</mn>
            </msup>
            <mo>&#x2062;</mo>
            <mi>&#x3b1;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>sin</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b2;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>cos</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b2;</mi>
          </mrow>
          <mo>+</mo>
          <mrow>
            <msubsup>
              <mi>z</mi>
              <mn>0</mn>
              <mn>2</mn>
            </msubsup>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mrow>
                  <msup>
                    <mi>sin</mi>
                    <mn>2</mn>
                  </msup>
                  <mo>&#x2062;</mo>
                  <mrow>
                    <mi>&#x3b1;</mi>
                    <mo>&#xb7;</mo>
                    <msup>
                      <mi>cos</mi>
                      <mn>2</mn>
                    </msup>
                  </mrow>
                  <mo>&#x2062;</mo>
                  <mi>&#x3b2;</mi>
                </mrow>
                <mo>+</mo>
                <mrow>
                  <msup>
                    <mi>sin</mi>
                    <mn>2</mn>
                  </msup>
                  <mo>&#x2062;</mo>
                  <mi>&#x3b2;</mi>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mn>2</mn>
            <mo>&#x2062;</mo>
            <msub>
              <mi>x</mi>
              <mn>0</mn>
            </msub>
            <mo>&#x2062;</mo>
            <msub>
              <mi>y</mi>
              <mn>0</mn>
            </msub>
            <mo>&#x2062;</mo>
            <mi>sin</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b1;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>cos</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.3em" height="0.3ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b2;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>sin</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3d5;</mi>
          </mrow>
          <mo>+</mo>
          <mrow>
            <mn>2</mn>
            <mo>&#x2062;</mo>
            <msub>
              <mi>z</mi>
              <mn>0</mn>
            </msub>
            <mo>&#x2062;</mo>
            <msub>
              <mi>x</mi>
              <mn>0</mn>
            </msub>
            <mo>&#x2062;</mo>
            <mi>sin</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b1;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>cos</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b1;</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>cos</mi>
            <mo>&#x2062;</mo>
            <mstyle>
              <mspace width="0.6em" height="0.6ex"/>
            </mstyle>
            <mo>&#x2062;</mo>
            <mi>&#x3b2;</mi>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mi>Formula8</mi>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0138" num="0137">By means of the &#x201c;parameter computation process&#x201d; of Step <b>14</b>, parameter computation unit <b>33</b> finds the set of parameters that express all surfaces that can pass through all pixels contained in candidate region CR. Parameter space memory unit <b>42</b> executes a &#x201c;parameter space storage process&#x201d; and stores the set of all parameters that were found by parameter computation unit <b>33</b>. A surface that is formed in parameter space by the set of parameter sets that express all surfaces that can pass through a single pixel is hereinbelow referred to as a &#x201c;parameter surface.&#x201d;</p>
<p id="p-0139" num="0138">Parameter surface drawing unit <b>34</b> executes the &#x201c;parameter surface drawing process&#x201d; of Step <b>15</b>, and for example, saves the parameter space for drawing parameter surfaces in parameter space memory unit <b>42</b> and draws the set of all possible parameters. Threshold value memory unit <b>41</b> stores the discretization width and range of values of each axis of parameter space that have been set in advance by means of a &#x201c;threshold value storage process.&#x201d;</p>
<p id="p-0140" num="0139">As the parameter surface drawing method, for example, values put at elements at which nothing is drawn are set to &#x201c;0&#x201d; and values are increased by &#x201c;1&#x201d; at a time for elements at which parameter surfaces that are drawn match coordinates in parameter space. The value of increase is not limited to &#x201c;1&#x201d; and may be any value as long as it is fixed. The shape that is formed by elements obtained by increasing the values in this way is a &#x201c;parameter surface.&#x201d; One parameter surface is thus generated for each pixel in candidate region CR. Conversely, the values of elements may be reduced and focus then placed on the reduced elements.</p>
<p id="p-0141" num="0140">Intersection detection unit <b>35</b> executes the &#x201c;intersection detection process&#x201d; of Step <b>16</b>, refers to, for example, parameter space, and reads the values of each combination of pose parameters. When the number of times a read value crosses is &#x201c;2&#x201d; or more, the point is an intersection at which parameter surfaces intersect. Intersection detection unit <b>35</b> therefore registers all of the read values (cross surface numbers) and coordinates in parameter space (cross point coordinates) in intersection list <b>421</b>. Parameter space memory unit <b>42</b> further stores intersection list <b>421</b> that is registered by intersection detection unit <b>35</b>.</p>
<p id="p-0142" num="0141">Significant candidate extraction unit <b>36</b> executes the &#x201c;significant candidate extraction process&#x201d; of Step <b>17</b> and extracts significant candidates. For example, of the intersections that are registered in intersection list <b>421</b>, parameter combinations for which the cross surface number is greater than a first threshold value, and moreover, for which the cross surface number is a maximum in prescribed neighboring regions are identified as significant candidates and the &#x201c;significant candidate surface bit BC&#x201d; is changed from &#x201c;0&#x201d; to &#x201c;1.&#x201d;</p>
<p id="p-0143" num="0142">Significant candidate surface bit BC is initialized to &#x201c;0&#x201d; at the time intersection list <b>421</b> is generated. Alternatively, the significant candidate surface bit may be set to &#x201c;1&#x201d; when intersection list <b>421</b> is generated and changed to &#x201c;0&#x201d; when determined to be a significant candidate surface. The value that is set to the significant candidate surface bit is not limited to &#x201c;0&#x201d; or &#x201c;1&#x201d; and may be any other numerical value. When performing such operations as the determination of whether the cross surface number at the cross point coordinates that are currently being determined is a maximum or not or the recognition of the size of regions that are set as neighboring regions, various values that are stored in threshold value memory unit <b>41</b> are consulted.</p>
<p id="p-0144" num="0143">In a typical Hough transform, cases occur in which, in neighboring regions that take a prescribed surface as a reference, many surfaces are detected that have a shape that resembles the prescribed surface. However, according to pose estimation system <b>1</b> of the present invention, significant candidate extraction unit <b>36</b> searches for other cross points at which the cross surface number is a maximum in neighboring regions, whereby the concentrated detection of a plurality of similar surface can be controlled.</p>
<p id="p-0145" num="0144">Significant candidate surface drawing unit <b>37</b> executes the &#x201c;significant candidate surface drawing process&#x201d; of Step <b>18</b> and draws significant candidate surfaces. Significant candidate surface drawing unit <b>37</b> draws &#x201c;significant candidate surfaces&#x201d; to cross the three-dimensional image based on, of intersection list <b>421</b>, combinations of parameters that are identified as significant candidate surfaces by the setting of the significant candidate surface bit to &#x201c;1.&#x201d;</p>
<p id="p-0146" num="0145">As an example of the drawing method of a significant candidate surface, image regions having a size identical to that of the three-dimensional image are saved in image memory unit <b>43</b> as significant candidate surface images, all pixel values are made &#x201c;0,&#x201d; and values are set to &#x201c;1&#x201d; for all pixels for which coordinates match with the surface that is drawn from the formulas of a plane or curved surface. In addition, values may be changed to enable discernment for each significant candidate surface.</p>
<p id="p-0147" num="0146">Alternatively, drawing may be on a binarized image rather than on a significant candidate surface image. When a significant candidate surface is drawn on a binarized image, the pixel values of the significant candidate surface are set to values that differ from the pixel values of the candidate region CR, and moreover, that also differ from pixel values of the background region BR.</p>
<p id="p-0148" num="0147">Cross determination unit <b>38</b> executes the &#x201c;cross determination process&#x201d; of Step <b>19</b>, refers to the significant candidate surface drawn by significant candidate surface drawing unit <b>37</b> and identifies the pixels that make up significant candidate surfaces (for example, pixels for which the pixel value is &#x201c;1&#x201d;) as &#x201c;cross pixels.&#x201d;</p>
<p id="p-0149" num="0148">Cross determination unit <b>38</b> then finds tangent planes for the significant candidate surface at the positions of &#x201c;cross pixels (pixels of interest)&#x201d; that have been identified and the normals to these tangent planes. Relating to the direction of the normal that is found, the coordinates are found of a plurality of pixels for which the distance from the significant candidate surface is no greater than a prescribed value that is stored in threshold value memory unit <b>41</b>. If any one of the pixels on the binarized image that are located at the same relative position (same coordinates) as each of the pixels that were found is included in candidate region CR, this pixel is identified as a &#x201c;cross pixel.&#x201d; Parameter space memory unit <b>42</b> stores the coordinates of all cross pixels that are identified by cross determination unit <b>38</b> in cross pixel list <b>422</b>.</p>
<p id="p-0150" num="0149">The method by which cross determination unit <b>38</b> identifies cross pixels is not limited to a method based on the normal direction in coordinates of cross pixels, and determination may also be realized depending on whether a candidate region CR on a binarized image is contained in a neighboring region made up of pixels (L&#xd7;M&#xd7;N pixels) in which the number of pixels in the x-direction is L, the number of pixels in the y-direction is M, and the number of pixels in the z-direction that take cross pixels as center is N. At this time, each of pixel numbers L, M, and N may be set in advance in threshold value memory unit <b>41</b>.</p>
<p id="p-0151" num="0150">Weighting computation unit <b>39</b> executes the &#x201c;weighting computation process&#x201d; of Step <b>110</b>. In the &#x201c;weighting computation process,&#x201d; weighting computation unit <b>39</b> finds the angles that a tangent plane found by cross determination unit <b>38</b> form with plane x-y, plane y-z, and plane z-x. Weighting computation unit <b>39</b> then determines the angle having the smallest absolute value of each of the angles that were found as the minimum angle, and then finds the &#x201c;slope weighting,&#x201d; which is the absolute value of the reciprocal of the cosine of the minimum angle that was determined. For example, if there is a minimum angle &#x3b8;, the &#x201c;slope weighting&#x201d; is 1/|cos &#x3b8;|. As a result, if the minimum angle &#x3b8; is 45&#xb0;, the slope weighting is found as approximately 1.41.</p>
<p id="p-0152" num="0151">Weighting computation unit <b>39</b> calculates each &#x201c;slope weighting&#x201d; for all cross pixels that are contained in each significant candidate surface that is drawn by significant candidate surface drawing unit <b>37</b>. Parameter space memory unit <b>42</b> stores the &#x201c;slope weightings&#x201d; that were found by weighting computation unit <b>39</b>.</p>
<p id="p-0153" num="0152">Number measurement unit <b>310</b> executes the &#x201c;number measurement process&#x201d; of Step <b>111</b> and adds up the slope weightings for each cross pixel located on each significant candidate surface. Parameter space memory unit <b>42</b> stores the results found by number measurement unit <b>310</b>.</p>
<p id="p-0154" num="0153">By means of the &#x201c;actual surface determination process&#x201d; of Step <b>112</b>, actual surface determination unit <b>311</b> distinguishes the significant candidate surface for which the &#x201c;number,&#x201d; which is the value obtained by adding up slope weightings calculated by weighting computation unit <b>39</b>, is larger than the second threshold value, and moreover, wherein this number is the greatest in prescribed neighboring regions, as the significant candidate surface that is an actual surface from among the significant candidate surfaces that were drawn by significant candidate surface drawing unit <b>37</b>. Actual surface determination unit <b>311</b> further changes &#x201c;actual surface bit BR&#x201d; that is registered in intersection list <b>421</b> from &#x201c;0&#x201d; to &#x201c;1&#x201d; for the significant candidate surface that was identified as an actual surface. Actual surface bit BR is initialized to &#x201c;0&#x201d; when intersection list <b>421</b> is generated. In addition, when intersection list <b>421</b> is generated, actual surface bit BR may also be set to &#x201c;1&#x201d; and changed to &#x201c;0&#x201d; upon determining an actual surface. Still further, the value set to actual surface bit BR is not limited to &#x201c;0&#x201d; or &#x201c;1&#x201d; and may be set to any other numerical value. Regarding the size of the neighboring regions that are used when determining whether a number is a maximum, reference is made to values stored by threshold value memory unit <b>41</b>. Still further, actual surface determination unit <b>311</b> can suppress the concentrated detection of a plurality of similar actual surfaces by searching for other cross points for which the &#x201c;number&#x201d; is a maximum in neighboring regions that take the actual surface as a reference.</p>
<p id="p-0155" num="0154">In addition, actual surface determination unit <b>311</b> uses communication device <b>5</b> to transmit by way of a cable or wireless network to external pose control system <b>800</b> each pose parameter registered in intersection list <b>421</b> that is stored by parameter space memory unit <b>42</b>.</p>
<p id="p-0156" num="0155">Image memory unit <b>43</b> executes an &#x201c;image storage process&#x201d; and stores a binarized image or a three-dimensional image in which surfaces are drawn. When the user instructs, an image may be displayed by, for example, a display, a projector, or a printer.</p>
<heading id="h-0006" level="1">Second Embodiment</heading>
<p id="p-0157" num="0156">Explanation next regards the details of the second embodiment of the present invention. The pose estimation system of the second embodiment has a configuration that is fundamentally identical to that of the example shown in <figref idref="DRAWINGS">FIG. 7</figref>.</p>
<p id="p-0158" num="0157">However, the configuration of data processing device <b>3</b><i>a </i>differs in the second embodiment. Explanation next regards data processing device <b>3</b><i>a. </i></p>
<p id="p-0159" num="0158">As shown in <figref idref="DRAWINGS">FIG. 15</figref>, data processing device <b>3</b><i>a </i>of the second embodiment is provided with: binarization unit <b>31</b><i>a</i>, surface parameter formula expression unit <b>32</b><i>a</i>, all-candidate surface drawing unit <b>33</b><i>a</i>, cross determination unit <b>34</b><i>a</i>, weighting computation unit <b>35</b><i>a</i>, number measurement unit <b>36</b><i>a</i>, and actual surface determination unit <b>37</b><i>a. </i></p>
<p id="p-0160" num="0159">The functions realized by binarization unit <b>31</b><i>a </i>and surface parameter formula expression unit <b>32</b><i>a </i>are identical to those of the corresponding constituent elements of the first embodiment.</p>
<p id="p-0161" num="0160">All-candidate surface drawing unit <b>33</b><i>a </i>executes an &#x201c;all-candidate surface drawing process,&#x201d; and, for all parameter combinations in parameter space that takes as axes the parameters of formulas that expresses the surfaces on an image, draws planes or curved surfaces in the image in accordance with these parameters. All-candidate surface drawing unit <b>33</b><i>a </i>differs from significant candidate surface drawing unit <b>37</b> of the first embodiment in that it draws planes and curved surfaces for all parameters instead of for specific parameter combinations. However, all-candidate surface drawing unit <b>33</b><i>a </i>is identical to significant candidate surface drawing unit <b>37</b> of the first embodiment regarding, for example, the methods for drawing these other surfaces.</p>
<p id="p-0162" num="0161">The configurations and functions of cross determination unit <b>34</b><i>a</i>, weighting computation unit <b>35</b><i>a</i>, number measurement unit <b>36</b><i>a</i>, and actual surface determination unit <b>37</b><i>a </i>are identical to the corresponding constituent elements of the first embodiment. However, cross determination unit <b>34</b><i>a</i>, weighting computation unit <b>35</b><i>a</i>, number measurement unit <b>36</b><i>a</i>, and actual surface determination unit <b>37</b><i>a </i>of the second embodiment take as the object of processing surfaces that are drawn by all-candidate surface drawing unit <b>33</b><i>a. </i></p>
<p id="p-0163" num="0162">The difference between the first embodiment and the second embodiment is the difference in the calculation amount of data processing device <b>3</b> and the calculation amount of data processing device <b>3</b><i>a. </i></p>
<p id="p-0164" num="0163">The number of pose parameter combinations increases to the extent that the discretization width decreases. As a result, the amount of calculation in data processing device <b>3</b><i>a </i>of the second embodiment is greater than the amount of calculation in the first embodiment. In contrast, as the discretization width increases, the amount of image space calculation that data processing device <b>3</b><i>a </i>must compute becomes less than the amount of calculation of data processing device <b>3</b>.</p>
<p id="p-0165" num="0164">In addition, decrease of the proportion of candidate region CR following binarization with respect to the entire three-dimensional image efficiently limits parameter space and reduces the amount of calculation in the first embodiment. In contrast, increase in the proportion of candidate region CR with respect to the entire three-dimensional image necessitates calculation relating to the drawing of parameter surfaces in the first embodiment that is not carried out in the second embodiment. As a result, the amount of calculation may increase in the first embodiment.</p>
<heading id="h-0007" level="1">Third Embodiment</heading>
<p id="p-0166" num="0165">Explanation next regards the details of the third embodiment of the present invention. As shown in <figref idref="DRAWINGS">FIG. 16</figref>, pose estimation system <b>1</b><i>a </i>of the third embodiment includes, in contrast to the first and second embodiments, a plurality of data processing devices (first data processing device <b>3</b><i>b </i>and second data processing device <b>3</b><i>c</i>). In addition, pose estimation system <b>1</b><i>a </i>further includes data processing switching device <b>6</b> for switching the data processing device that executes processing and for selectively operating first data processing device <b>3</b><i>b </i>and second data processing device <b>3</b><i>c. </i></p>
<p id="p-0167" num="0166">First data processing device <b>3</b><i>b </i>has the same configuration as data processing device <b>3</b> of the first embodiment shown in <figref idref="DRAWINGS">FIG. 9</figref>. Second data processing device <b>3</b><i>c </i>has the same configuration as data processing device <b>3</b><i>a </i>of the second embodiment shown in <figref idref="DRAWINGS">FIG. 15</figref>.</p>
<p id="p-0168" num="0167">As previously described, the discretization width of parameter space and the ratio of the candidate region of the binarized image determine which of the amount of calculation by first data processing device <b>3</b><i>b </i>and the amount of calculation by second data processing device <b>3</b><i>c </i>is the greater.</p>
<p id="p-0169" num="0168">As a result, by means of a &#x201c;data processing switching process,&#x201d; data processing switching device <b>6</b> compares these amounts of calculation to select as the data processing device that executes processing the one of first data processing device <b>3</b><i>b </i>and second data processing device <b>3</b><i>c </i>that has a smaller amount of calculation. Data processing switching device <b>6</b> further finds &#x201c;parameter space calculation amount&#x201d; that is the amount of calculation for first data processing device <b>3</b><i>b </i>to identify the actual surface that is the reference surface and &#x201c;image space calculation amount&#x201d; that is the amount of calculation for second data processing device <b>3</b><i>c </i>to identify the actual surface. Data processing switching device <b>6</b> then, by comparing the parameter space calculation amount and the image space calculation amount, causes the data processing device for which the amount of calculation for processing is less to execute the computation of actual surfaces.</p>
<p id="p-0170" num="0169">Explanation next regards the details of data processing switching device <b>6</b>. As shown in <figref idref="DRAWINGS">FIG. 17</figref>, data processing switching device <b>6</b> includes: binarization unit <b>61</b>, surface parameter formula expression unit <b>62</b>, parameter computation calculation amount estimation unit <b>63</b>, parameter surface drawing calculation amount estimation unit <b>64</b>, intersection detection calculation amount estimation unit <b>65</b>, significant candidate extraction calculation amount estimation unit <b>66</b>, significant candidate surface drawing calculation amount estimation unit <b>67</b>, cross determination calculation amount estimation unit <b>68</b>, weighting computation calculation amount estimation unit <b>69</b>, number measurement calculation amount estimation unit <b>610</b>, actual surface determination calculation amount estimation unit <b>611</b>, all-candidate surface drawing calculation amount estimation unit <b>612</b>, all-cross determination calculation amount estimation unit <b>613</b>, all-weighting computation calculation amount estimation unit <b>614</b>, all-number measurement calculation amount estimation unit <b>615</b>, all-actual surface determination calculation amount estimation unit <b>616</b>, and method selection unit <b>617</b>.</p>
<p id="p-0171" num="0170">Binarization unit <b>61</b> and surface parameter formula expression unit <b>62</b> both have the same configuration and serve the same functions as binarization unit <b>31</b> and surface parameter formula expression unit <b>32</b>, respectively, shown in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0172" num="0171">Constituent elements <b>63</b>-<b>611</b> estimate the &#x201c;calculation amount&#x201d; (calculation cost) of processing by each constituent element of first data processing device <b>3</b><i>b </i>based on the ratio of candidate region CR in the three-dimensional image that has undergone binarization by binarization unit <b>61</b> and the discretization width of each axis of parameter space.</p>
<p id="p-0173" num="0172">Here, the calculation amount (calculation cost) is an &#x201c;amount that indicates the volume of arithmetic processing&#x201d; that data processing device <b>3</b> or <b>3</b><i>a </i>must carry out when finding values that are the objects of calculation. The form of the calculation amount is not subject to any particular limitation, and may be, for example, the number of times an arithmetic process is executed or the required time interval necessary for all arithmetic processing until the values that are the object of calculation are found.</p>
<p id="p-0174" num="0173">Parameter computation calculation amount estimation unit <b>63</b> executes a &#x201c;parameter computation calculation amount estimation process&#x201d; and estimates the calculation amount when parameter computation unit <b>33</b> is processing. Based on the surface parameters formulas, the range of pose parameters, and the discretization width of parameter space for drawing surfaces in a three-dimensional image, parameter computation calculation amount estimation unit <b>63</b> computes the number of operations when computing the pose parameter combination that makes up surface parameter formulas (equations) that express all planes and curved surfaces for one pixel in candidate region CR. Parameter computation calculation amount estimation unit <b>63</b> then multiplies the number of operations that have been calculated and the number of pixels that are contained in the candidate region, estimates the obtained value as the calculation amount of processing by parameter computation unit <b>33</b>, and takes this value as the &#x201c;parameter computation calculation amount.&#x201d; In addition, the number of operations when calculating the combination of pose parameters that make up the surface parameter formulas is the number of times that the combination of pose parameters is changed (rewritten) in space parameters when finding the combination of pose parameters that make up the surface parameter formulas for each pixel.</p>
<p id="p-0175" num="0174">Parameter surface drawing calculation amount estimation unit <b>64</b> executes a &#x201c;parameter surface drawing calculation amount estimation process&#x201d; and estimates the calculation amount when parameter surface drawing unit <b>34</b> is processing. For example, parameter surface drawing calculation amount estimation unit <b>64</b> multiplies the number of times the pose parameter combination is changed in parameter space that is obtained in the course of processing by parameter computation calculation amount estimation unit <b>63</b> by the number of operations necessary to change (rewrite) the pose parameter combination one time. Parameter surface drawing calculation amount estimation unit <b>64</b> then estimates the value obtained by this multiplication as the calculation amount of processing by parameter surface drawing unit <b>34</b>, and takes this value as &#x201c;parameter surface drawing calculation amount.&#x201d; Intersection detection calculation amount estimation unit <b>65</b> executes an &#x201c;intersection detection calculation amount estimation process&#x201d; and estimates the calculation amount when intersection detection unit <b>35</b> is processing. For example, intersection detection calculation amount estimation unit <b>65</b> first sets the number of pixels of the candidate region to the number of parameter surfaces, and then finds the mean area of the parameter surface by multiplying the number of parameter surfaces that has been set by the area of one pixel. Then, when parameter surfaces are randomly arranged in parameter space, intersection detection calculation amount estimation unit <b>65</b> estimates the number of intersections at which parameter surfaces intersect with each other. Intersection detection calculation amount estimation unit <b>65</b> multiplies the number of intersections that was estimated by the number of operations necessary for finding the coordinates in parameter space for one intersection, and estimates the value obtained as the calculation amount of processing by intersection detection unit <b>35</b> and takes this value as the &#x201c;intersection detection calculation amount.&#x201d;</p>
<p id="p-0176" num="0175">Significant candidate extraction calculation amount estimation unit <b>66</b> executes a &#x201c;significant candidate extraction calculation amount estimation process&#x201d; and estimates the calculation amount when significant candidate extraction unit <b>36</b> is processing. For example, significant candidate extraction calculation amount estimation unit <b>66</b> multiplies the number of intersections found by intersection detection calculation amount estimation unit <b>65</b>, the number of operations necessary for reading the cross surface number relating to a single intersection, and the number of operations necessary when comparing the cross surface number and the first threshold value relating to a single intersection. Significant candidate extraction calculation amount estimation unit <b>66</b> then estimates the value obtained by multiplication as the calculation amount of processing by significant candidate extraction unit <b>36</b> and takes the value as the &#x201c;significant candidate extraction calculation amount.&#x201d;</p>
<p id="p-0177" num="0176">Significant candidate surface drawing calculation amount estimation unit <b>67</b> executes a &#x201c;significant candidate surface drawing calculation amount estimation process&#x201d; and estimates the calculation amount when significant candidate surface drawing unit <b>37</b> is processing. For example, significant candidate surface drawing calculation amount estimation unit <b>67</b> estimates the number of pose parameter combinations that are identified as significant candidates that are greater than the first threshold value based on the number of intersections obtained by intersection detection calculation amount estimation unit <b>65</b>, the mean value of the pose parameters values at each intersection, and the distribution of pose parameter values. In addition, significant candidate surface drawing calculation amount estimation unit <b>67</b> estimates the number of operations when one significant candidate surface is drawn according to a surface parameter formula for drawing a surface in a three-dimensional image. Significant candidate surface drawing calculation amount estimation unit <b>67</b> then multiplies the number of operations when drawing a significant candidate surface that was estimated and the number of pose parameter combinations that are identified as significant candidates that were estimated, estimates the obtained value as the calculation amount of processing by significant candidate surface drawing unit <b>37</b>, and takes this value as the &#x201c;significant candidate surface drawing calculation amount.&#x201d;</p>
<p id="p-0178" num="0177">Cross determination calculation amount estimation unit <b>68</b> executes a &#x201c;cross determination calculation amount estimation process&#x201d; and estimates the calculation amount when cross determination unit <b>38</b> is processing. Based on, for example, the number of pose parameter combinations that are identified as significant candidates that were estimated by significant candidate surface drawing calculation amount estimation unit <b>67</b> (the number of significant candidate surfaces), cross determination calculation amount estimation unit <b>68</b> finds the number of times that a significant candidate surface crosses one pixel in a candidate region when significant candidate surfaces are randomly arranged in a binarized image. Cross determination calculation amount estimation unit <b>68</b> then multiplies the number of times of cross of a pixel that was found and the number of operations required to obtain the coordinates of one cross pixel, estimates the obtained value as the calculation amount that cross determination unit <b>38</b> processes, and takes this value as the &#x201c;cross determination calculation amount.&#x201d;</p>
<p id="p-0179" num="0178">Weighting computation calculation amount estimation unit <b>69</b> executes a &#x201c;weighting computation calculation amount estimation process,&#x201d; and estimates the calculation amount when weighting computation unit <b>39</b> is processing. For example, weighting computation calculation amount estimation unit <b>69</b> estimates the number of operations for computing one tangent plane in accordance with a surface parameter formula. Weighting computation calculation amount estimation unit <b>69</b> then adds the number of operations when computing the tangent plane that was estimated, the number of operations when finding the smallest angle from among each of the angles formed by the tangent plane with plane x-y, plane y-z, and plane z-x, and the number of operations for computing the slope weighting from the smallest angle. Weighting computation calculation amount estimation unit <b>69</b> further multiplies the value obtained by the addition, the mean area of a significant candidate surface, and the number of surfaces, estimates the obtained value as the calculation amount of processing by weighting computation unit <b>39</b>, and takes this value as the &#x201c;weighting computation calculation amount.&#x201d;</p>
<p id="p-0180" num="0179">Although no limitations are here imposed on the method of computing the mean area of significant candidate surfaces by weighting computation calculation amount estimation unit <b>69</b>, a method may be applied that is identical to the method of calculating the mean area of parameter surfaces by intersection detection calculation amount estimation unit <b>65</b>.</p>
<p id="p-0181" num="0180">Number measurement calculation amount estimation unit <b>610</b> executes a &#x201c;number measurement calculation amount estimation process&#x201d; and estimates the calculation amount when number measurement unit <b>310</b> carries out processing. Number measurement calculation amount estimation unit <b>610</b>, for example, multiplies the mean area of the significant candidate surfaces that were found by weighting computation calculation amount estimation unit <b>69</b>, the number of significant candidate surfaces, and the number of operations necessary for one multiplication of slope weighting, then estimates the obtained value as the calculation amount of processing by number measurement unit <b>310</b>, and takes this value as the &#x201c;number measurement calculation amount.&#x201d;</p>
<p id="p-0182" num="0181">Actual surface determination calculation amount estimation unit <b>611</b> executes an &#x201c;actual surface determination calculation amount estimation process&#x201d; and estimates the calculation amount during processing by actual surface determination unit <b>311</b>. For example, actual surface determination calculation amount estimation unit <b>611</b> multiplies the number of significant candidate surfaces that was estimated, the number of operations required to compare the slope weighting total (number) relating to one significant candidate surface and the second threshold value, and the calculation amount required for determining whether the number is a maximum in a prescribed neighboring region. Actual surface determination calculation amount estimation unit <b>611</b> then estimates the obtained value as the calculation amount of processing by actual surface determination unit <b>311</b> and takes this value as the &#x201c;actual surface determination calculation amount.&#x201d;</p>
<p id="p-0183" num="0182">Constituent elements <b>612</b>-<b>616</b> each estimate the calculation amount (calculation costs) of processing by each constituent element of second data processing device <b>3</b><i>c. </i></p>
<p id="p-0184" num="0183">All-candidate surface drawing calculation amount estimation unit <b>612</b> executes an &#x201c;all-candidate surface drawing calculation amount estimation process&#x201d; and estimates the calculation amount when all-candidate surface drawing unit <b>33</b><i>a </i>carries out processing. All-candidate surface drawing calculation amount estimation unit <b>612</b>, for example, estimates the number of operations for one drawing of a candidate surface from a surface parameter formula that indicates a surface in a three-dimensional image. All-candidate surface drawing calculation amount estimation unit <b>612</b> then multiplies the number of operations necessary for the drawing of one candidate surface that was estimated and the number of all pose parameter combinations in parameter space, estimates the obtained value as the calculation amount of processing by all-candidate surface drawing unit <b>33</b><i>a</i>, and takes this value as the &#x201c;all-candidate surface drawing calculation amount.&#x201d;</p>
<p id="p-0185" num="0184">All-cross determination calculation amount estimation unit <b>613</b> executes an &#x201c;all-cross determination calculation amount estimation process&#x201d; and estimates the calculation amount when cross determination unit <b>34</b><i>a </i>carries out processing. All-cross determination calculation amount estimation unit <b>613</b> finds the &#x201c;all-cross determination calculation amount&#x201d; by the same process as cross determination calculation amount estimation unit <b>68</b>.</p>
<p id="p-0186" num="0185">All-weighting computation calculation amount estimation unit <b>614</b> executes an &#x201c;all-weighting computation calculation amount estimation process&#x201d; and estimates the calculation amount when weighting computation unit <b>35</b><i>a </i>carries out processing. All-weighting computation calculation amount estimation unit <b>614</b> finds the &#x201c;all-weighting computation calculation amount&#x201d; by the same process as weighting computation calculation amount estimation unit <b>69</b>.</p>
<p id="p-0187" num="0186">All-number measurement calculation amount estimation unit <b>615</b> executes an &#x201c;all-number measurement calculation amount estimation process&#x201d; and estimates the calculation amount when number measurement unit <b>36</b><i>a </i>is processing. All-number measurement calculation amount estimation unit <b>615</b> finds the all-number measurement calculation amount by the same process as number measurement calculation amount estimation unit <b>610</b>.</p>
<p id="p-0188" num="0187">All-actual surface determination calculation amount estimation unit <b>616</b> executes an &#x201c;all-actual surface determination calculation amount estimation process&#x201d; and estimates the calculation amount when actual surface determination unit <b>37</b><i>a </i>is processing. All-actual surface determination calculation amount estimation unit <b>616</b> finds the all-actual surface determination calculation amount by means of the same process as actual surface determination calculation amount estimation unit <b>611</b>.</p>
<p id="p-0189" num="0188">In first data processing device <b>3</b><i>b</i>, a limiting of the significant candidate surfaces is first carried out before significant candidate surface drawing unit <b>37</b>, cross determination unit <b>38</b>, weighting computation unit <b>39</b>, number measurement unit <b>310</b>, and actual surface determination unit <b>311</b> execute each process. Constituent elements <b>37</b>-<b>311</b> then each draw only the limited significant candidate surfaces on the three-dimensional image to continue processing.</p>
<p id="p-0190" num="0189">On the other hand, in second data processing device <b>3</b><i>c</i>, all-candidate surface drawing unit <b>33</b><i>a</i>, cross determination unit <b>34</b><i>a</i>, weighting computation unit <b>35</b><i>a</i>, number measurement unit <b>36</b><i>a</i>, and actual surface determination unit <b>37</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. 15</figref> do not execute limiting of the significant candidate surfaces. As a result, each of constituent elements <b>33</b><i>a</i>-<b>37</b><i>a </i>draws surfaces on an image for all parameter combinations and continue processing. In addition, the presence or absence of limiting of significant candidate surfaces is the only difference between the processing by each of constituent elements <b>37</b>-<b>311</b> and processing by each of constituent elements <b>33</b><i>a</i>-<b>37</b><i>a</i>, and the calculation methods are the same.</p>
<p id="p-0191" num="0190">Method selection unit <b>617</b> compares the &#x201c;parameter space calculation amount&#x201d; and the &#x201c;image space calculation amount&#x201d; and, based on the result of comparison, selects a data processing device for executing processing from among first data processing device <b>3</b><i>b </i>and second data processing device <b>3</b><i>c. </i></p>
<p id="p-0192" num="0191">The &#x201c;parameter space calculation amount&#x201d; is the sum of: the parameter computation calculation amount, the parameter surface drawing calculation amount, the intersection detection calculation amount, the significant candidate extraction calculation amount, the significant candidate surface drawing calculation amount, the cross determination calculation amount, the weighting computation calculation amount, the number measurement calculation amount, and the actual surface determination calculation amount.</p>
<p id="p-0193" num="0192">The &#x201c;image space calculation amount&#x201d; is the sum of: the all-candidate surface calculation amount, the all-cross determination calculation amount, the all-weighting computation calculation amount, the all-number measurement calculation amount, and the all-actual surface determination calculation amount.</p>
<p id="p-0194" num="0193">Method selection unit <b>617</b> selects first data processing device <b>3</b><i>b </i>when the parameter space calculation amount is determined to be smaller than the image space calculation amount, and selects second data processing device <b>3</b><i>c </i>when the image space calculation amount is determined to be smaller than the parameter space calculation amount. When the parameter space calculation amount and the image space calculation amount are determined to be equal, method selection unit <b>617</b> selects the data processing device in accordance with a predetermined rule.</p>
<p id="p-0195" num="0194">In addition, when estimating the calculation amounts of processing by each of the constituent elements, the arrangement of surfaces may be determined stochastically and a prescribed probability distribution given to the calculation amounts themselves. For example, the estimated values may be expressed with error added. In such a case, when operations of numerical values that have probability distributions are carried out when finding the parameter space calculation amount or the image space calculation amount, the propagation of error may be taken into consideration and distributions therefore also given to the numerical values following the operations.</p>
<heading id="h-0008" level="1">Fourth Embodiment</heading>
<p id="p-0196" num="0195">Explanation next regards the details of the fourth embodiment of the present invention. As shown in <figref idref="DRAWINGS">FIG. 18</figref>, data processing device <b>3</b><i>d </i>of the fourth embodiment includes &#x201c;smoothing unit <b>312</b>&#x201d; that is provided in the preceding section (input side) of binarization unit <b>31</b> belonging to data processing device <b>3</b> of the first embodiment. This smoothing unit <b>312</b> executes a &#x201c;smoothing process&#x201d; and has the function of subjecting a three-dimensional image to a process for smoothing the three-dimensional image.</p>
<p id="p-0197" num="0196">No particular limitations are placed on the method of smoothing a three-dimensional image. For example, a typical smoothing method may be applied such as a method of convolution of a mask in which weighting is a three-dimensional Gaussian on a three-dimensional image, a method of taking the average within a local region, a method of taking an intermediate value in a local region, and a method of setting a model of the spatial distribution of pixel values in advance and then implementing minimum square fitting. This smoothing process enables a reduction of the influence of the noise component contained in the three-dimensional image.</p>
<p id="p-0198" num="0197">In addition, smoothing unit <b>312</b> is not limited to addition in a section preceding binarization unit <b>31</b> in data processing device <b>3</b> of the first embodiment but may be added immediately before the binarization unit in the second or third embodiment.</p>
<heading id="h-0009" level="1">Fifth Embodiment</heading>
<p id="p-0199" num="0198">Explanation next regards the details of the fifth embodiment of the present invention. As shown in <figref idref="DRAWINGS">FIG. 19</figref>, data processing device <b>3</b><i>e </i>of the fifth embodiment includes &#x201c;dilation/erosion unit <b>313</b>&#x201d; provided in the section following (output side of) binarization unit <b>31</b> that is included in data processing device <b>3</b> of the first embodiment.</p>
<p id="p-0200" num="0199">Dilation/erosion unit <b>313</b> executes a &#x201c;dilation/erosion process,&#x201d; which is a &#x201c;dilation process&#x201d; of dilating candidate region CR or an &#x201c;erosion process&#x201d; of eroding candidate region CR of a binarized image.</p>
<p id="p-0201" num="0200">The dilation/erosion process of a three-dimensional image is a process that applies a method typically used in a two-dimensional image to a three-dimensional image.</p>
<p id="p-0202" num="0201">For example, in the &#x201c;dilation process,&#x201d; a pixel of interest is regarded as the candidate region if there is a pixel of candidate region CR in even one of the six neighborhoods in three-dimensional space for the pixel of interest. In the &#x201c;erosion process,&#x201d; a pixel of interest is regarded as the background region if there is a pixel belonging to background region BR in even one of the six neighborhoods for the pixel of interest. The dilation process or erosion process can reduce noise and to a certain degree can connect interrupted surfaces.</p>
<p id="p-0203" num="0202">The order of executing the dilation process and erosion process is arbitrary, and the order of executing the processes can be interchanged. In addition, the execution of dilation and erosion may be iterated a prescribed number of times.</p>
<p id="p-0204" num="0203">Dilation/erosion unit <b>313</b> is not limited to addition at the succeeding stage of binarization unit <b>31</b> included in data processing device <b>3</b> of the first embodiment and may also be added immediately before the binarization unit that is included in the first, second, or third embodiment.</p>
<heading id="h-0010" level="1">Sixth Embodiment</heading>
<p id="p-0205" num="0204">Explanation next regards the details of the sixth embodiment of the present invention. As shown in <figref idref="DRAWINGS">FIG. 20</figref>, data processing device <b>3</b><i>f </i>of the sixth embodiment includes &#x201c;image size conversion unit <b>314</b>&#x201d; and &#x201c;parameter micro-adjustment unit <b>315</b>&#x201d; in addition to the configuration of data processing device <b>3</b> of the first embodiment.</p>
<p id="p-0206" num="0205">Image size conversion unit <b>314</b> executes an &#x201c;image size conversion process&#x201d; and converts the size of any image by reducing or enlarging the size of the image. Image size conversion unit <b>314</b> generates a reduced image of a three-dimensional image by reducing the three-dimensional image according to a prescribed reduction ratio and actual surface determination unit <b>311</b> finds an actual surface based on the reduced image.</p>
<p id="p-0207" num="0206">Once an actual surface has been found by actual surface determination unit <b>311</b>, image size conversion unit <b>314</b> further, by converting the reduced image obtained by first reducing the original three-dimensional image (for example, an image reduced to one-fourth of the original three-dimensional image) to a size that is smaller than the original three-dimensional image but that is larger than the reduced image (the image that is one-fourth the original), generates a new image (for example, an image that is one-half the original three-dimensional image).</p>
<p id="p-0208" num="0207">Parameter micro-adjustment unit <b>315</b> executes a &#x201c;parameter micro-adjustment process&#x201d; that takes an actual surface that has been found as a new significant candidate surface to find a new actual surface in a neighborhood in parameter space. Parameter micro-adjustment unit <b>315</b> causes alternate and repeated execution of conversion (reduction) of image size of different magnifications by image size conversion unit <b>314</b> and identification of new actual surfaces by actual surface determination unit <b>311</b> until reaching a prescribed number of iterations or until obtaining a prescribed accuracy.</p>
<p id="p-0209" num="0208">According to the sixth embodiment, by setting an image to successively higher definitions, actual surfaces can be detected with higher accuracy by means of arithmetic processing having a lower calculation amount.</p>
<p id="p-0210" num="0209">Explanation next regards the details of operations when image size conversion unit <b>314</b> changes the size of a three-dimensional image (the reduction ratio or enlargement ratio of a three-dimensional image) in successive increments. Before actual surface determination unit <b>311</b> identifies an actual surface, processing is first carried out in binarization unit <b>31</b>, followed by parameter computation unit <b>33</b> that has referred to surface parameter formula expression unit <b>32</b>, and then parameter surface drawing unit <b>34</b>, intersection detection unit <b>35</b>, and significant candidate extraction unit <b>36</b>, and then in significant candidate surface drawing unit <b>37</b> and succeeding sections. In the processing of this series of operations, operation results (actual surfaces corresponding to a prescribed image reduction ratio) are obtained for narrowing the region from all of parameter space by means of a Hough transform by means of the processing of each of surface parameter formula expression unit <b>32</b>, parameter computation unit <b>33</b>, parameter surface drawing unit <b>34</b>, intersection detection unit <b>35</b>, and significant candidate extraction unit <b>36</b>.</p>
<p id="p-0211" num="0210">If actual surfaces are first found in an image that has been reduced by a prescribed reduction ratio, these actual surfaces can then be used as significant candidate surfaces and the vicinities of these actual surfaces searched to achieve higher precision of parameters. As a result, in the second and succeeding operations in which a first operation is executed repeatedly, the processing by each of constituent elements <b>32</b>-<b>36</b> is unnecessary and the process of binarization unit <b>31</b> is executed, followed by the process of significant candidate surface drawing unit <b>37</b>.</p>
<p id="p-0212" num="0211">Image size conversion unit <b>314</b> and parameter micro-adjustment unit <b>315</b> can be added not only to data processing device <b>3</b> of the first embodiment, but also to the configurations of the second, third, fourth, and fifth embodiments.</p>
<p id="p-0213" num="0212">As a result, the following explanation regards an example in which image size conversion unit <b>314</b> and parameter micro-adjustment unit <b>315</b> are added to data processing device <b>3</b><i>g </i>that has a configuration fundamentally identical to that of the second embodiment.</p>
<p id="p-0214" num="0213">In addition, as shown in <figref idref="DRAWINGS">FIG. 21</figref>, data processing device <b>3</b><i>g </i>includes all-candidate surface significant candidate surface drawing unit <b>316</b> in place of all-candidate surface drawing unit <b>33</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. 15</figref>.</p>
<p id="p-0215" num="0214">All-candidate surface significant candidate surface drawing unit <b>316</b> executes an &#x201c;all-candidate surface significant candidate surface drawing process&#x201d; and draws significant candidate surfaces in a three-dimensional image for all parameter combinations in parameter space in a first operation that is executed before computation of actual surfaces. However, after actual surfaces have once been found, when seeking actual surfaces in the reduced image of the next size, actual surfaces are drawn based on combinations of parameters that are included in neighboring regions of the actual surfaces that were specified in parameter space.</p>
<heading id="h-0011" level="1">Seventh Embodiment</heading>
<p id="p-0216" num="0215">Explanation next regards the details of the seventh embodiment of the present invention. As shown in <figref idref="DRAWINGS">FIG. 22</figref>, data processing device <b>3</b><i>h </i>of the seventh embodiment of the present invention includes &#x201c;parameter space coarse-graining unit <b>317</b>&#x201d; and &#x201c;parameter micro-adjustment unit <b>315</b>&#x201d; that are provided in a stage succeeding (on the output side of) surface parameter formula expression unit <b>32</b> in the configuration of data processing device <b>3</b> of the first embodiment.</p>
<p id="p-0217" num="0216">Parameter space coarse-graining unit <b>317</b> executes a &#x201c;parameter space coarse-graining process&#x201d; and coarsens parameter space by altering (for example, increasing) the discretization width of parameter space to a prescribed width and thus finds actual surfaces. After finding actual surfaces, parameter space coarse-graining unit <b>317</b> discretizes parameter space more finely.</p>
<p id="p-0218" num="0217">Parameter micro-adjustment unit <b>315</b> executes a &#x201c;parameter micro-adjustment process&#x201d; and finds new actual surfaces in neighboring regions that take as a reference the actual surfaces that have once been found in parameter space that has been discretized (fractionalized) by parameter space coarse-graining unit <b>317</b>.</p>
<p id="p-0219" num="0218">Parameter micro-adjustment unit <b>315</b> repeatedly executes the derivation of new actual surfaces in parameter space of differing discretization width until a &#x201c;prescribed number of processes&#x201d; is reached, or repeatedly executes the computation of new actual surfaces until the discretization width reaches a prescribed value.</p>
<p id="p-0220" num="0219">The seventh embodiment enables the extraction of surfaces with higher accuracy and with a lower amount of calculation by making the parameter space successively higher definitions.</p>
<p id="p-0221" num="0220">Explanation next regards the details of operations when the discretization width of parameter space is repeatedly decreased in stages. In the first operation, following the process of surface parameter formula expression unit <b>32</b>, parameter space coarse-graining unit <b>317</b> changes the discretization width of parameter space, following which the processing by each of constituent elements <b>32</b>-<b>36</b> is executed. The operations of each of constituent elements <b>32</b>-<b>36</b> in the first operation are carried out to constrict the region from all parameter space by means of a Hough transform.</p>
<p id="p-0222" num="0221">As a result, in the second and succeeding operations, the process by significant candidate surface drawing unit <b>37</b> is executed following the process of parameter space coarse-graining unit <b>317</b>. Once an actual surface has been obtained at a particular parameter space discretization width, this actual surface is used without alteration as a significant candidate surface to search the neighborhood to achieve higher precision of parameters.</p>
<p id="p-0223" num="0222">In addition, parameter space coarse-graining unit <b>317</b> and parameter micro-adjustment unit <b>315</b> can be added not only to data processing device <b>3</b> of the first embodiment but also to the configurations of the second to sixth embodiments. Explanation therefore next regards an example in which parameter space coarse-graining unit <b>317</b> and parameter micro-adjustment unit <b>315</b> are added to data processing device <b>3</b><i>i </i>having fundamentally the same configuration as the second embodiment.</p>
<p id="p-0224" num="0223">As shown in <figref idref="DRAWINGS">FIG. 23</figref>, data processing device <b>3</b><i>i </i>includes parameter space coarse-graining unit <b>317</b> and parameter micro-adjustment unit <b>315</b>. Data processing device <b>3</b><i>i </i>further includes all-candidate surface significant candidate surface drawing unit <b>315</b> in place of all-candidate surface drawing unit <b>33</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. 15</figref>. All-candidate surface significant candidate surface drawing unit <b>316</b> draws surfaces for all parameter combinations in parameter space in a three-dimensional image in a first operation that is executed before computation of actual surfaces. However, when seeking actual surfaces in an image of reduced size in the second and succeeding operations after an actual surface has once been computed, actual surfaces are drawn based on parameter combinations in neighboring regions in parameter space that take the computed actual surface as a reference.</p>
<heading id="h-0012" level="1">Eighth Embodiment</heading>
<p id="p-0225" num="0224">Explanation next regards the eighth embodiment of the present invention. As shown in <figref idref="DRAWINGS">FIG. 24</figref>, data processing device <b>3</b><i>j </i>of the eighth embodiment both includes &#x201c;pixel value-weighted parameter surface drawing unit <b>318</b>&#x201d; in place of parameter surface drawing unit <b>34</b> shown in <figref idref="DRAWINGS">FIG. 9</figref> and includes &#x201c;pixel value-weighted significant candidate extraction unit <b>319</b>&#x201d; in place of significant candidate extraction unit <b>36</b> shown in <figref idref="DRAWINGS">FIG. 9</figref>. The data processing device further includes &#x201c;pixel value-weighted weighting computation unit <b>320</b>&#x201d; in place of weighting computation unit <b>39</b> shown in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0226" num="0225">When drawing a parameter surface in parameter space, pixel value-weighted parameter surface drawing unit <b>318</b> computes weightings based on pixel values of a three-dimensional image before binarization to find &#x201c;pixel value weightings.&#x201d; Actual surfaces that clearly appear in a three-dimensional image but that have small area can be detected by applying &#x201c;pixel value weightings.&#x201d;</p>
<p id="p-0227" num="0226">As the &#x201c;pixel value weightings,&#x201d; pixel values may be used without alteration or values may be used that are obtained by a logarithmic transformation of pixel values or that are obtained by conversion of pixel values by prescribed functions. In other words, pixel value-weighted parameter surface drawing unit <b>318</b> draws parameter surfaces that accord with surface parameter formulas based on pixel value-weighted parameters that are found by multiplication of pose parameter combinations that have been computed by parameter computation unit <b>33</b> and values computed based on pixel values of an image before binarization by binarization unit <b>31</b>.</p>
<p id="p-0228" num="0227">When, at an intersection, the sum of the number of cross surfaces at cross point coordinates at which parameter surfaces intersect with each other and pixel value weightings is greater than a prescribed value, and moreover, the cross surface number is a maximum in neighboring regions that take these cross point coordinates as a reference, pixel value-weighted significant candidate extraction unit <b>319</b> identifies the parameter combinations at these cross point coordinates as a significant candidate of a parameter combination that form a reference surface that is the object of detection.</p>
<p id="p-0229" num="0228">Based on the angle formed by the tangent planes at the cross pixels that were found by cross determination unit <b>38</b> and surfaces formed by each of the axes of the parameter space, pixel value-weighted weighting computation unit <b>320</b> finds a slope weighting for each cross pixel identified by cross determination unit <b>38</b>, multiplies the &#x201c;slope weightings&#x201d; that were found and &#x201c;pixel value weightings&#x201d; that were computed based on the pixel values of the three-dimensional image before the binarization process by binarization unit <b>31</b>, and finds &#x201c;pixel value-weighted slope weightings,&#x201d; which are new slope weightings.</p>
<p id="p-0230" num="0229">Each of constituent elements <b>318</b>-<b>320</b> that take pixel value weightings into consideration may be added not only to data processing device <b>3</b> of the first embodiment, but also to the configurations of the second to seventh embodiments. When added to data processing device <b>3</b><i>a </i>of the second embodiment, pixel value-weighted weighting computation unit <b>320</b> may be provided in place of weighting computation unit <b>35</b><i>a </i>shown in <figref idref="DRAWINGS">FIG. 15</figref>, as in data processing device <b>3</b><i>k </i>shown in <figref idref="DRAWINGS">FIG. 25</figref>.</p>
<p id="p-0231" num="0230">Alternatively, when each of constituent elements <b>318</b>-<b>320</b> are applied in the third embodiment, data processing device <b>3</b><i>j </i>shown in <figref idref="DRAWINGS">FIG. 24</figref> is applied as the first data processing device and data processing device <b>3</b><i>k </i>shown in <figref idref="DRAWINGS">FIG. 25</figref> is applied as the second data processing device.</p>
<p id="p-0232" num="0231">At this time, data processing switching device <b>6</b><i>a </i>includes &#x201c;pixel value-weighted parameter surface drawing calculation amount estimation unit <b>618</b>&#x201d; in place of parameter surface drawing calculation amount estimation unit <b>64</b> shown in <figref idref="DRAWINGS">FIG. 17</figref>, as shown in <figref idref="DRAWINGS">FIG. 26</figref>. In addition, data processing switching device <b>6</b><i>a </i>includes &#x201c;pixel value-weighted significant candidate extraction calculation amount estimation unit <b>619</b>&#x201d; in place of significant candidate extraction calculation amount estimation unit <b>66</b> shown in <figref idref="DRAWINGS">FIG. 17</figref>. Still further, data processing switching device <b>6</b><i>a </i>includes &#x201c;pixel value-weighted weighting computation calculation amount estimation unit <b>620</b>&#x201d; in place of weighting computation calculation amount estimation unit <b>69</b> shown in FIG. <b>17</b>. Finally, data processing switching device <b>6</b><i>a </i>includes &#x201c;all-pixel value-weighted weighting computation calculation amount estimation unit <b>621</b>&#x201d; in place of all-weighting computation calculation amount estimation unit <b>614</b> shown in <figref idref="DRAWINGS">FIG. 17</figref>. Constituent elements <b>618</b>-<b>621</b> enable the estimation of calculation amount while taking pixel value weightings into consideration, and based on the estimated amount of calculation, data processing switching device <b>6</b><i>a </i>selectively activates the one of first data processing device <b>3</b><i>j </i>and second data processing device <b>3</b><i>k </i>that entails a lower amount of calculation.</p>
<heading id="h-0013" level="1">Ninth Embodiment</heading>
<p id="p-0233" num="0232">Explanation next regards the details of the ninth embodiment of the present invention. As shown in <figref idref="DRAWINGS">FIG. 27</figref>, data processing device <b>3</b><i>l </i>of the ninth embodiment includes &#x201c;positional relation estimation unit <b>321</b>&#x201d; and &#x201c;pose significance unit <b>322</b>&#x201d; in addition to the configuration of data processing device <b>3</b> shown in <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0234" num="0233">Positional relation estimation unit <b>321</b> executes a &#x201c;positional relation estimation process&#x201d; and estimates the positional relation of each actual surface when a plurality of actual surfaces appear in a three-dimensional image that is applied as input by external data input unit <b>21</b>. For example, when two actual surfaces appear, positional relation estimation unit <b>321</b> estimates the actual surface of the two actual surfaces that is positioned above. In addition, positional relation estimation unit <b>321</b> identifies an actual surface by placing the actual surface in association with a physical entity (such as the ground surface, ocean surface, or ocean floor) that actually corresponds to the actual surface.</p>
<p id="p-0235" num="0234">A case in which a plurality of actual surfaces appear in a three-dimensional image that has been acquired by external image acquisition device <b>700</b> is, for example, a case in which SONAR is used in the ocean to capture a three-dimensional image in which two surfaces that correspond to the ocean surface and the ocean floor appear.</p>
<p id="p-0236" num="0235">Positional relation estimation unit <b>321</b> executes a &#x201c;positional relation estimation process&#x201d; and holds history such as the pose, position, and speed up to the current time. Positional relation estimation unit <b>321</b> then estimates the relative positional relation between each of the actual surfaces that appear in the three-dimensional image based on each item of history that is held and information obtained by external sensors (such as geomagnetism, atmospheric pressure, temperature, water pressure, gravity, etc.).</p>
<p id="p-0237" num="0236">Pose significance unit <b>322</b> executes a &#x201c;pose significance process&#x201d; and executes &#x201c;significance conferring,&#x201d; which is a process of determining the pose of the object of pose estimation with respect to a reference surface by placing pose parameters and reference surface in correspondence.</p>
<p id="p-0238" num="0237">For example, pose significance unit <b>322</b> discriminates the surface that is the ocean surface and the surface that is the ocean floor based on the positional relation of the ocean surface and ocean floor, and discriminates the pose parameters for each surface (for example, pose parameters for the ocean surface and pose parameters for the ocean floor), whereby the process of conferring significance of the pose parameters is realized. This &#x201c;significance conferring&#x201d; means resolving the type of reference surface to which each pose parameter corresponds. Pose significance unit <b>322</b> further discriminates surfaces based on the shape of a surface or the distribution of pixel values of a surface and confers significance of pose parameters for each surface that has been discriminated. Pose significance unit <b>322</b> further discriminates surfaces that appear in an image based on positional relation and surface shape or the distribution of pixel values of a surface and confers significance of pose parameters for each surface that has been discriminated. Pose significance unit <b>322</b> further supplies the pose parameters to which significance has been conferred to pose control system <b>800</b> by way of communication device <b>5</b>. Positional relation estimation unit <b>321</b> and pose significance unit <b>322</b> clarify the identities of the object reference surfaces of pose parameters that were found, and thus enable more accurate control of the pose of an air vehicle or underwater vehicle.</p>
<p id="p-0239" num="0238">In addition, positional relation estimation unit <b>321</b> and pose significance unit <b>322</b> may be added not only to data processing device <b>3</b> of the first embodiment, but also to the configuration of the second to eighth embodiments.</p>
<p id="p-0240" num="0239">As described hereinabove, pose estimation system <b>1</b> of the present invention features the following effects.</p>
<p id="p-0241" num="0240">As the first effect, a surface that serves as the reference of pose parameters can be detected with high accuracy even in a three-dimensional image having an abundance of noise, whereby pose can be estimated with higher accuracy. This effect can be realized because the use of all pixels that make up a surface in three-dimensional space decreases the influence of the noise component.</p>
<p id="p-0242" num="0241">As the second effect, a surface that serves as the reference of pose parameters can be detected by a simple method, whereby the processing time when improving accuracy can be shortened. This effect can be realized because an entire surface is directly computed without computing, for example, straight lines that make up a surface or a plurality of partial surfaces (patches) that are portions of surfaces.</p>
<p id="p-0243" num="0242">As a third effect, of pose parameters, pose parameters that are indicated by angles can be estimated with a uniform accuracy without regard to the values of the parameters, and further, accuracy can be easily designated. This effect can be realized because formulas that describe surfaces are constructed to enable input of pose parameters without alteration as the parameters of a Hough transform.</p>
<p id="p-0244" num="0243">As the fourth effect, the detection accuracy of a surface can be made uniform in a three-dimensional image. This effect can be realized because surfaces that correspond to each parameter are drawn in a three-dimensional image, whereby only one surface is specified in a three-dimensional image for a parameter set in parameter space. In addition, increase of calculation load can be suppressed because this process is realized by only the simple process of drawing surfaces in a three-dimensional image, and regarding the number of these surfaces that are drawn, the number is limited to surfaces that exceed a threshold value and other surfaces in the vicinity having a greater number or number of times.</p>
<p id="p-0245" num="0244">As the fifth effect, detection accuracy can be realized uniformly on a three-dimensional image not just for planes but for curved surfaces as well. This effect can be realized because the process involves simply drawing surfaces in accordance with parameters so that parameter sets in parameter space and surfaces in a three-dimensional image can be placed in one-to-one correspondence. When described by parameters, the detection accuracy of a surface does not depend on the shape of the surface.</p>
<p id="p-0246" num="0245">As a sixth effect, detection accuracy can be fixed regardless of the direction of the surface. This effect can be realized because the number of layers of parameter surfaces counted up in parameter space according to the slope of a tangent plane of the surface of the object of detection with respect to the plane x-y, the plane y-z, and the plane z-x is adjusted to a value obtained when the area of a surface is measured based on Euclidean distance in a three-dimensional image.</p>
<p id="p-0247" num="0246">As a seventh effect, increase in the amount of calculation can be suppressed even when the ratio of the candidate region after binarization with respect to the entire image is high. This effect can be realized because, in a configuration that discovers a parameter set that makes up the surface that is to be detected by first carrying out an inverse Hough transform rather than implementing a Hough transform, it is determined before beginning execution of the surface detection process whether to carry out a Hough transform based on the results of estimating the calculation amount resulting from each method.</p>
<p id="p-0248" num="0247">In addition to a form in which the processing in pose estimation system <b>1</b> is realized by the above-described dedicated hardware, the present invention may also be realized by recording a program for realizing these functions on a recording medium that is readable by pose estimation system <b>1</b>, and causing this program that has been recorded on a recording medium to be read to and executed by pose estimation system <b>1</b>. The recording medium that is readable by pose estimation system <b>1</b> refers to, for example, a relocatable recording medium such as a floppy disk (registered trademark), a magneto-optic disk, a DVD, or a CD, or to an HDD that is incorporated in pose estimation system <b>1</b>. The program that is recorded on this recording medium is read to, for example, data processing device <b>3</b> that is included in pose estimation system <b>1</b>, and under the control of data processing device <b>3</b>, the same processes as described hereinabove are carried out.</p>
<p id="p-0249" num="0248">In this case, data processing device <b>3</b> that is included in pose estimation system <b>1</b> operates as a computer that executes a program that has been read from a recording medium on which the program has been recorded.</p>
<p id="p-0250" num="0249">While the invention has been particularly shown and described with reference to exemplary embodiments thereof, the invention in not limited to these embodiments. It will be understood by those of ordinary skill in the art that various changes in form and details may be made therein without departing from the spirit and scope of the present invention as defined by the claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625903-20140107-M00001.NB">
<img id="EMI-M00001" he="17.27mm" wi="76.20mm" file="US08625903-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625903-20140107-M00002.NB">
<img id="EMI-M00002" he="15.92mm" wi="76.20mm" file="US08625903-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08625903-20140107-M00003.NB">
<img id="EMI-M00003" he="16.59mm" wi="76.20mm" file="US08625903-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A data processing device for, based on an image received as input, estimating pose of an object of pose estimation with respect to a reference surface that serves as a reference for estimating pose, said data processing device comprising:
<claim-text>a binarization unit for dividing said image into a candidate region that is a candidate for said reference surface and a background region that is a region other than the candidate region;</claim-text>
<claim-text>a surface parameter formula expression unit for extracting pose parameters that indicate the pose of an object of pose estimation with respect to a reference surface that appears in said image that was received as input and, based on the combination of a value obtained by implementing transform by a prescribed function upon, of the pose parameters that were extracted, parameters that indicate a direction in which the object of the pose estimation is directed and pose parameters other than the parameter that indicate direction, finding surface parameter formulas that are numerical formulas that express said reference surface;</claim-text>
<claim-text>an all-candidate surface drawing unit for, based on the combination of all pose parameters in parameter space, which is space having, as the axes of base vectors, each of the parameters included in said pose parameters, drawing candidate surfaces that accord with said surface parameter formulas on said image in said parameter space;</claim-text>
<claim-text>a cross determination unit for, as regards each candidate surface that was drawn by said all-candidate surface drawing unit, identifying pixels that are located on, of said candidate region, the candidate surface, and pixels that are located within a prescribed range from the candidate surface as cross pixels and finding coordinates of the cross pixels and tangent planes at said cross pixels;</claim-text>
<claim-text>a weighting computation unit for, based on angles that are formed by tangent planes at the cross pixels that were found by said cross determination unit and planes that are formed by the axes of each base vector included in said parameter space, finding a slope weighting for each cross pixel that was identified by said cross determination unit;</claim-text>
<claim-text>a number measurement unit for computing for each of said candidate surfaces a number that is the value obtained by adding up the slope weightings of cross pixels that were found by said weighting computation unit that were contained in each of the candidate surfaces; and</claim-text>
<claim-text>an actual surface determination unit for, based on the number computed by said number measurement unit, identifying from among candidate surfaces drawn by said all-candidate surface drawing unit an actual surface that is a reference surface that actually exists in said image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The data processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a smoothing unit for implementing a process to smooth said image received as input, wherein said binarization unit, based on an image that has undergone smoothing by said smoothing unit, generates a binarized image.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The data processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a dilation/erosion unit for subjecting a candidate region of a binarized image generated by said binarization unit to a prescribed number of dilation process for dilating the candidate region or erosion processes for eroding the candidate region.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The data processing device according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:
<claim-text>an image size conversion unit for changing the image size by reducing or enlarging the size of an image; and</claim-text>
<claim-text>a parameter micro-adjustment unit for causing the alternate and repeated execution of changes of image sizes of differing magnifications by said image size conversion unit and identification of a new actual surface by said actual surface determination unit until a prescribed number is reached or until a prescribed accuracy is obtained;</claim-text>
</claim-text>
<claim-text>wherein said image size conversion unit:
<claim-text>before said actual surface determination unit identifies an actual surface, generates a reduced image by reducing at a prescribed reduction ratio said image received as input; and</claim-text>
<claim-text>after said actual surface determination unit identifies an actual surface, generates a new image by changing said reduced image that was generated to a size that is smaller than said image received as input, and moreover, larger than the reduced image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The data processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>a parameter space coarse-graining unit for increasing the discretization width of said parameter space to a predetermined width to coarsen the parameter space and find an actual surface, and after once finding an actual surface, changing discretization to make the parameter space finer; and</claim-text>
<claim-text>a parameter micro-adjustment unit for repeatedly executing derivation of new actual surfaces in the parameter space of different discretization widths in neighboring regions of an actual surface that has been found in parameter space that has undergone said discretization until reaching a prescribed number of processes or until the discretization width reaches a prescribed value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The data processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a pixel value-weighted weighting computation unit for finding pixel value-weighted slope weightings by multiplying said slope weightings and values computed based on pixel value of an image before binarization by said binarization unit.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The data processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>a positional relation estimation unit for, when a plurality of actual surfaces appear in said image that is received as input, estimating both the positional relation between each of said actual surfaces and identifying the actual surfaces by placing said actual surfaces in correspondence with physical entities; and</claim-text>
<claim-text>a pose significance unit for carrying out a significance process, which is a process of, based on the positional relation of said plurality of surfaces estimated by said positional relation estimation unit, placing said pose parameters in correspondence with said actual surfaces to determine the pose of an object of pose estimation with respect to the actual surfaces.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A data processing device comprising:
<claim-text>a binarization unit for dividing an image received as input into a candidate region that is a candidate for a reference surface that serves as a reference for estimating pose and a background region that is a region other than the candidate region;</claim-text>
<claim-text>a surface parameter formula expression unit for extracting pose parameters that indicate pose of an object of pose estimation with respect to a reference surface that appears in said image that was received as input and, based on a combination of values obtained by implementing a transform by a prescribed function upon, of the pose parameters that were extracted, parameters that indicate the direction in which the object of pose estimation is directed and pose parameters other than the parameters that indicate direction, finding surface parameter formulas that are numerical formulas that express said reference surface;</claim-text>
<claim-text>a parameter computation unit for, based on pose parameters that were extracted by said surface parameter formula expression unit, computing the combination of pose parameters that make up said surface parameter formulas that express all surfaces that pass through each pixel contained in the candidate region that was divided by said binarization unit;</claim-text>
<claim-text>a parameter surface drawing unit for, based on the combination of pose parameters computed by said parameter computation unit, drawing parameter surfaces in accordance with said surface parameter formulas on parameter space, which is space that includes each of the parameters belonging to said pose parameters as the axes of base vectors;</claim-text>
<claim-text>an intersection detection unit for finding cross point coordinates, which are the coordinates of intersections through which pass a plurality of said parameter surface that were drawn by said parameter surface drawing unit, and a cross surface number, which is the number of parameter surfaces that pass through the cross point coordinates;</claim-text>
<claim-text>a significant candidate extraction unit for, when the cross surface number at each cross point that was found by said intersection detection unit is compared with cross surface numbers at other cross point coordinate corresponding to the cross surface number and is thus determined to be a maximum, identifying the combination of pose parameters that make up the parameter surface that corresponds to the determined cross surface number as a significant candidate that is an optimum combination of pose parameters for expressing said reference surface;</claim-text>
<claim-text>a significant candidate surface drawing unit for, based on a significant candidate identified by said significant candidate extraction unit and a combination of pose parameters that were computed by said parameter computation unit for which distance to said significant candidate in parameter space in prescribed neighboring regions that take the significant candidate as a reference is no greater than a prescribed value, drawing significant candidate surfaces on said image;</claim-text>
<claim-text>a cross determination unit for, regarding each significant candidate surface that was drawn by said significant candidate surface drawing unit, identifying pixels that are located on, of said candidate region, the significant candidate surface, and pixels that are located within a prescribed range from the significant candidate surface as cross pixels and finding coordinates of the cross pixels and tangent planes at said cross pixels;</claim-text>
<claim-text>a weighting computation unit for, based on angels that are formed by tangent planes in cross pixels that were found by said cross determination unit and planes that are formed by the axes of each base vector included in said parameter space, finding a slope weighting for each cross pixel that was identified by said cross determination unit;</claim-text>
<claim-text>a number measurement unit for computing for each of said significant candidate surfaces a number that is the value obtained by adding up the slope weightings of cross pixels that were found by said weighting computation unit and that were contained in each of the significant candidates; and</claim-text>
</claim-text>
<claim-text>an actual surface determination unit for, when the number computed by said number measurement unit is compared with a number belonging to other significant candidate surfaces in neighboring regions that take as a reference that significant candidate surface that has this number and that thus is determined to be a maximum, identifying the significant candidate surface having said determined number as an actual surface that is a reference surface that actually exists in said image, further comprising:
<claim-text>a pixel value-weighted parameter surface drawing unit for, based on a pixel value weighting found by multiplication of a combination of pose parameters that was computed by said parameter computation unit and a value computed based on pixel values of an image before binarization by said binarization unit, drawing on said parameter space parameter surfaces that accord with said surface parameter formulas;</claim-text>
<claim-text>a pixel value-weighted significant candidate extraction unit for, when at said cross point coordinates, the sum of the number of cross surfaces at which said parameter surfaces intersect each other and pixel value weightings is greater than a prescribed value, and moreover, when the number of cross surfaces are a maximum in neighboring regions that take the cross point coordinates as a reference, identifying the combination of parameters at the cross point coordinates as a significant candidate of a combination of parameters that form a reference surface that is the object of detection; and</claim-text>
<claim-text>a pixel value-weighted weighting computation unit for, based on angles formed by tangent planes at cross pixels that were found by said cross determination unit and surfaces that were formed by axes of each base vector included in said parameter space, finding a slope weighting for each cross pixel that was identified by said cross determination unit, and, by multiplying these slope weightings that were found and values that were computed based on pixel values of an image before binarization by said binarization unit, finding pixel value-weighted slope weightings.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A pose estimation method for, based on an image that is received as input, estimating the pose of an object of pose estimation with respect to a reference surface that serves as the reference for estimating pose, said pose estimation method comprising:
<claim-text>a binarization process for dividing said image into a candidate region that is a candidate for said reference surface and a background region that is a region other than the candidate region;</claim-text>
<claim-text>a surface parameter formula expression process for extracting pose parameters that indicate the pose of an object of pose estimation with respect to a reference surface that appears in said image received as input and, based on the combination of values obtained by implementing a transform by a prescribed function upon, of the pose parameters that were extracted, parameters that indicate a direction in which the object of pose estimation is directed and pose parameters other than the parameters that indicate direction, finding surface parameter formulas that are numerical formulas that express said reference surface;</claim-text>
<claim-text>an all-candidate surface drawing process for, based on the combination of all pose parameters in a parameter space, which is space having, as the axes of base vectors, each of the parameters belonging to said pose parameters, drawing candidate surfaces that accord with said surface parameter formulas on the image in said parameter space;</claim-text>
<claim-text>a cross determination process for, as regards each candidate surface that was drawn in said all-candidate surface drawing process, identifying pixels that are located on, of said candidate region, the candidate surface, and pixels that are located within a prescribed range from the candidate surface as cross pixels;</claim-text>
<claim-text>a weighting computation process for, based on angles that are formed by tangent planes at cross pixels that were found in said cross determination process and planes that are formed by the axes of each base vector included in said parameter space, finding a slope weighting for each cross pixel that was identified in said cross determination process;</claim-text>
<claim-text>a number measurement process for computing for each of said candidate surface a number that is a value obtained by adding up the slope weightings of cross pixels that were found in said weighting computation process and that were contained in each of the candidate surfaces; and</claim-text>
<claim-text>an actual surface determination process for, based on the numbers computed in said number measurement process, identifying an actual surface that is a reference surface that actually exists in said image from among candidate surfaces drawn in and all-candidate surface drawing process.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The pose estimation method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising a smoothing process for implementing a process to smooth said image received as input, wherein, in said binarization process, a binarized image is generated based on an image that has undergone smoothing in said smoothing process.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The pose estimation method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising a dilation/erosion process for subjecting a candidate region of a binarized image generated in said binarization process to a prescribed number of dilation processes for dilating the candidate region or erosion processes for eroding the candidate region.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The pose estimation method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:
<claim-text>an image size conversion process for changing image size by reducing or enlarging the size of any image; and</claim-text>
<claim-text>a parameter micro-adjustment process for causing alternate and repeated execution of change of image sizes of differing magnifications by said image size conversion process and identification of a new actual surface by said actual surface determinate process until reaching a prescribed number of times is reached or until a prescribed accuracy is obtained;</claim-text>
</claim-text>
<claim-text>wherein, in said image size conversion process:
<claim-text>before an actual surface is identified in said actual surface determination process, a reduced image is generated by reducing at a prescribed reduction ratio said image received as input; and</claim-text>
<claim-text>after an actual surface is identified in said actual surface determination process, a new image is generated by changing said reduced image that was generated to a size that is smaller than said image received as input, and moreover, that is larger than the reduced image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The pose estimation method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:
<claim-text>a parameter space coarse-graining process for increasing the discretization width of said parameter space to a predetermined width to coarsen the parameter space and find an actual surface, and after an actual surface has once been found, changing the discretization to make parameter space finer; and</claim-text>
<claim-text>a parameter micro-adjustment process for repeatedly executing derivation of new actual surfaces in parameter space of difference discretization widths in neighboring regions of an actual surface that has once been found in parameter space that has undergone said discretization until reaching a prescribed number of processes is reached or until discretization width reaches a prescribed value.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The pose estimation method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:
<claim-text>a positional relation estimation process for, when a plurality of actual surfaces appear in said image that is received as input, estimating both the positional relation between each of said actual surfaces and identifying the actual surfaces by placing said actual surfaces in correspondence with physical entities; and</claim-text>
<claim-text>a pose significance process for carrying out a significance process, which is a process for, based on the positional relation of said plurality of surfaces estimated in said positional relation estimation process, placing said pose parameters in correspondence with said actual surfaces to determine the pose of an object of pose estimation with respect to the actual surfaces.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. A pose estimation method for, based on an image that is received as input, estimating the pose of an object of pose estimation with respect to a reference surface that serves as a reference for estimating the pose, said pose estimation method comprising;</claim-text>
<claim-text>a binarization process for dividing said image into a candidate region that is a candidate for said reference surface and a background region that is a region other than the candidate region;</claim-text>
<claim-text>a surface parameter formula expression process for extracting pose parameters that indicate the pose of an object of pose estimation with respect to a reference surface that appears in said image that is received as input and, based on the combination of values obtained by applying a transform by a prescribed function upon, of pose parameters that have been extracted, parameters that indicate direction in which an object of pose estimation is directed and pose parameters other than the parameters that indicate direction, finding surface parameter formulas that are numerical formulas that express said reference surface;</claim-text>
<claim-text>a parameter computation process for, based on pose parameters that were extracted in said surface parameter formula expression process, computing a combination of pose parameters that make up said surface parameter formulas that express all surfaces that pass through each pixel contained in a candidate region that was divided in said binarization process;</claim-text>
<claim-text>a parameter surface drawing process for, based on the combination of pose parameters computed in said parameter computation process, drawing parameter surfaces that accord with said surface parameter formulas on parameter space, which is space that includes each of the parameters belonging to said pose parameters as the axes of base vectors;</claim-text>
<claim-text>an intersection detection process for finding cross point coordinates, which are the coordinates of intersections through which pass a plurality of said parameter surfaces that were drawn in said parameter surface drawing process, and cross surface numbers, which are the numbers of parameter surfaces that pass through the cross point coordinates;
<claim-text>a significant candidate extraction process for, when the cross surface number at each cross point that was found in said intersection detection process is compared with the cross surface numbers at other cross point coordinates in prescribed neighboring regions that take the cross point coordinates corresponding to the cross surface number as a reference and is thus determined to be a maximum, identifying the combinations of pose parameters that make up the parameter surface that corresponds to the determined cross surface number as significant candidates, which are optimum combinations of pose parameters for expressing said reference surface;</claim-text>
</claim-text>
<claim-text>a significant candidate surface drawing process for, based on a significant candidate identified in said significant candidate extraction process and a combination of pose parameters that were computed in said parameter computation process, for which a distance to said significant candidate in said parameter space in prescribed neighboring regions that take the significant candidate as a reference is no greater than a prescribed value, drawing a significant candidate surfaces on said image;</claim-text>
<claim-text>a cross determination process for, regarding each significant candidate surface that was drawn in said significant candidate surface drawing process, identifying pixels that are located on, of said candidate region, the significant candidate surface, and pixels that are located within a prescribed range from the significant candidate surface as cross pixels and finding coordinates of the cross pixels and tangent planes at said cross pixels;</claim-text>
<claim-text>a weighting computation process for, based on angles that are formed by tangent planes at the cross pixels that were found in said cross determination process and planes that are formed by the axes of each base vector included in said parameter space, finding a slope weighting for each cross pixel that was identified in said cross determination process;</claim-text>
<claim-text>a number measurement process for computing for each of said significant candidate surfaces a number that is the value obtained by adding up the slope weightings of cross pixels that were found in said weighting computation process and that were contained in each of the significant candidates; and</claim-text>
<claim-text>an actual surface determination process for, when the number computed in said number measurement process is compared with a number belonging to other significant candidate surfaces in neighboring regions that take as a reference the significant candidate surface that has this number and thus is determined to be a maximum, identifying the significant candidate surface having the determined number as an actual surface that is a reference surface that actually exists in said image, further comprising:
<claim-text>a pixel value-weighted parameter surface drawing process for, based on pixel value weighting found by multiplication of a combination of pose parameters computed in said parameter computation process and a value that was computed based on pixel values of an image before binarization in said binarization process. drawing on said parameter space parameter surfaces that accord with said surface parameter formulas;</claim-text>
<claim-text>a pixel value-weighted significant candidate extraction process for, when at said cross point coordinates, the sum of the number of cross surfaces at which said parameter surfaces intersect each other and pixel value weightings is greater than a prescribed value, and moreover, when the number of cross surfaces is a maximum in neighboring regions that take the cross point coordinates as a reference, identifying the combination of parameters at the cross point coordinates as a significant candidate of a combination of parameters that form a reference surface that is the object of detection; and</claim-text>
<claim-text>a pixel value-weighted weighting computation process for, based on angles formed by tangent planes at cross pixels that were found in said cross determination process and surfaces formed by axes of each base vector included in said parameter space, finding a slope weighting for each cross pixel that was identified in said cross determination process, and, by multiplying these slope weightings that were found and values computed based on pixel values of an image before binarization in said binarization process, finding pixel value-weighted slope weightings.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The pose estimation method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising a pixel value-weighted weighting computation process for finding pixel value-weighted slope weightings by multiplying said slope weightings and values that are computed based on pixel values of an image before binarization in said binarization process.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A non-transitory computer readable recording record medium</claim-text>
<claim-text>having recorded there on a computer-readable program for causing a computer to execute:
<claim-text>a binarization procedure for dividing an image received as input into a candidate region that is a candidate for a reference surface that is a reference for estimating pose and a background region that is a region other than the candidate region;</claim-text>
<claim-text>a surface parameter formula expression procedure for extracting pose parameters that indicate the pose of an object of pose estimation with respect to a reference surface that appears in said image received as input and, based on a combination of values obtained by implementing a transform by a prescribed function upon, of pose parameters that were extracted, parameters that indicate the direction in which an object of pose estimation is directed and pose parameters other than parameter that indicate direction, finding surface parameter formulas that are numerical formulas that express said reference surface;</claim-text>
<claim-text>an all-candidate surface drawing procedure for, based on a combination of all pose parameters in parameter space, which is space having, as axes of base vectors, each of the parameters belonging to said pose parameters, drawing candidate surfaces that accord with said surface parameter formulas on said image in said parameter space;</claim-text>
<claim-text>a cross determination procedure for, as regards each candidate surface that was drawn in said all-candidate surface drawing procedure, identifying pixels that are located on, of said candidate region, the candidate surface, and pixels that are located within a prescribed range from the candidate surface as cross pixels and finding coordinates of the cross pixels and tangent planes at said cross pixels;</claim-text>
<claim-text>a weighting computation procedure for, based on angles that are formed by tangent planes at cross pixels that were found in said cross determination procedure and planes that are formed by the axes of each base vector included in said parameter space, finding a slope weighting for each cross pixel that was identified in said cross determination procedure;</claim-text>
<claim-text>a number measurement procedure for computing for each of said candidate surfaces a number that is a value obtained by adding up the slope weightings of cross pixels that were found in said weighting computation procedure and that were contained in each of the candidate surfaces; and</claim-text>
<claim-text>an actual surface determination procedure for, based on the numbers computed in said number measurement procedure, identifying from among candidate surfaces drawn in said all-candidate surface drawing procedure an actual surface that is a reference surface that actually exists in said image.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The non-transitory computer readable recording record medium according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising a smoothing procedure for implementing a process for smoothing said image received as input, wherein, in said binarization procedure, a binarized image is generated based on an image that has undergone smoothing in said smoothing procedure.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The non-transitory computer readable recording record medium according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising a dilation/erosion procedure for subjecting a candidate region of a binarized image generated in said binarization procedure to a prescribed number of dilation procedures for dilating the candidate region or to erosion procedures for eroding the candidate region.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The non-transitory computer readable recording record medium according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising: a parameter space coarse-graining procedure for increasing the discretization width of said parameter space to a predetermined width to coarsen parameter space and find an actual surface, and after an actual surface has once been found, changing discretization to make the parameter space finer; and
<claim-text>a parameter micro-adjustment procedure for repeatedly executing derivation of new actual surfaces in the parameter space of different discretization widths in neighboring regions of an actual surface that has once been found in parameter space that has undergone said discretization until reaching a prescribed number of processes is reached or until the discretization width reaches a prescribed value. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
