<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624709-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624709</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13458525</doc-number>
<date>20120427</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>08</class>
<subclass>B</subclass>
<main-group>5</main-group>
<subgroup>22</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>340  81</main-classification>
<further-classification>34053922</further-classification>
<further-classification>34053925</further-classification>
<further-classification>340549</further-classification>
<further-classification>340619</further-classification>
<further-classification>340937</further-classification>
<further-classification>348 36</further-classification>
<further-classification>348143</further-classification>
<further-classification>382103</further-classification>
<further-classification>345426</further-classification>
</classification-national>
<invention-title id="d2e43">System and method for camera control in a surveillance system</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4992866</doc-number>
<kind>A</kind>
<name>Morgan</name>
<date>19910200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5610875</doc-number>
<kind>A</kind>
<name>Gaiser</name>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>367 75</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5834158</doc-number>
<kind>A</kind>
<name>Hillmer</name>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>430296</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5854632</doc-number>
<kind>A</kind>
<name>Steiner</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345426</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5872594</doc-number>
<kind>A</kind>
<name>Thompson</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348213</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5926209</doc-number>
<kind>A</kind>
<name>Glatt</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6018350</doc-number>
<kind>A</kind>
<name>Lee et al.</name>
<date>20000100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345426</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6266100</doc-number>
<kind>B1</kind>
<name>Gloudemans et al.</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348587</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6597406</doc-number>
<kind>B2</kind>
<name>Gloudemans et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348587</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6680746</doc-number>
<kind>B2</kind>
<name>Kawai et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>3482119</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6909458</doc-number>
<kind>B1</kind>
<name>Suzuki et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6998987</doc-number>
<kind>B2</kind>
<name>Lin</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3405731</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>7019644</doc-number>
<kind>B2</kind>
<name>Barrie</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34053913</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>7151562</doc-number>
<kind>B1</kind>
<name>Trajkovic</name>
<date>20061200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34821113</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>7263207</doc-number>
<kind>B2</kind>
<name>Lee et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>7280134</doc-number>
<kind>B1</kind>
<name>Henderson et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348117</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>7385626</doc-number>
<kind>B2</kind>
<name>Aggarwal et al.</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>7425986</doc-number>
<kind>B2</kind>
<name>Kawai et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482113</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>7522186</doc-number>
<kind>B2</kind>
<name>Arpa et al.</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348153</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>7583815</doc-number>
<kind>B2</kind>
<name>Zhang et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>7629995</doc-number>
<kind>B2</kind>
<name>Salivar et al.</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>7750936</doc-number>
<kind>B2</kind>
<name>Provinsal et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>7884849</doc-number>
<kind>B2</kind>
<name>Yin et al.</name>
<date>20110200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>7929016</doc-number>
<kind>B2</kind>
<name>Yoshida et al.</name>
<date>20110400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>7940299</doc-number>
<kind>B2</kind>
<name>Geng</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>7956891</doc-number>
<kind>B2</kind>
<name>Uchihara</name>
<date>20110600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>7990422</doc-number>
<kind>B2</kind>
<name>Ahiska et al.</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482181</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>8194912</doc-number>
<kind>B2</kind>
<name>Kitaura et al.</name>
<date>20120600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>8237791</doc-number>
<kind>B2</kind>
<name>Chen et al.</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2002/0050988</doc-number>
<kind>A1</kind>
<name>Petrov et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345418</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2003/0071891</doc-number>
<kind>A1</kind>
<name>Geng</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 39</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2004/0119819</doc-number>
<kind>A1</kind>
<name>Aggarwal et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2004/0227656</doc-number>
<kind>A1</kind>
<name>Asakura et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>341176</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2005/0024206</doc-number>
<kind>A1</kind>
<name>Samarasekera et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>340541</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2005/0036036</doc-number>
<kind>A1</kind>
<name>Stevenson et al.</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34821199</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2005/0146605</doc-number>
<kind>A1</kind>
<name>Lipton et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2006/0028548</doc-number>
<kind>A1</kind>
<name>Salivar et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2006/0028550</doc-number>
<kind>A1</kind>
<name>Palmer et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348155</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2006/0033813</doc-number>
<kind>A1</kind>
<name>Provinsal et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2006/0229885</doc-number>
<kind>A1</kind>
<name>Kodama</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2007/0024706</doc-number>
<kind>A1</kind>
<name>Brannon et al.</name>
<date>20070200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348142</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>0070/1920070</doc-number>
<name>Yin et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>348 36</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2007/0070190</doc-number>
<kind>A1</kind>
<name>Yin et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 36</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2007/0122058</doc-number>
<kind>A1</kind>
<name>Kitaura et al.</name>
<date>20070500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382284</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2007/0146484</doc-number>
<kind>A1</kind>
<name>Horton et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2007/0236570</doc-number>
<kind>A1</kind>
<name>Sun et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2007/0291104</doc-number>
<kind>A1</kind>
<name>Petersen et al.</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 1401</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2008/0031493</doc-number>
<kind>A1</kind>
<name>Brogren et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2008/0049099</doc-number>
<kind>A1</kind>
<name>Shih et al.</name>
<date>20080200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 38</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2008/0088706</doc-number>
<kind>A1</kind>
<name>Girgensohn et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34820799</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2008/0211929</doc-number>
<kind>A1</kind>
<name>Uchihara</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34823199</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2008/0231708</doc-number>
<kind>A1</kind>
<name>Morimoto et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2008/0243908</doc-number>
<kind>A1</kind>
<name>Aasman et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707102</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2008/0291278</doc-number>
<kind>A1</kind>
<name>Zhang et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2008/0291279</doc-number>
<kind>A1</kind>
<name>Samarasekera et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>2009/0010493</doc-number>
<kind>A1</kind>
<name>Gornick et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>2009/0040302</doc-number>
<kind>A1</kind>
<name>Thompson</name>
<date>20090200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>2009/0167867</doc-number>
<kind>A1</kind>
<name>Lin et al.</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348169</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>2009/0237510</doc-number>
<kind>A1</kind>
<name>Chen et al.</name>
<date>20090900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>2009/0262195</doc-number>
<kind>A1</kind>
<name>Yoshida et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>US</country>
<doc-number>2009/0262196</doc-number>
<kind>A1</kind>
<name>Salivar et al.</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00062">
<document-id>
<country>US</country>
<doc-number>2010/0013917</doc-number>
<kind>A1</kind>
<name>Hanna et al.</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00063">
<document-id>
<country>US</country>
<doc-number>2010/0020178</doc-number>
<kind>A1</kind>
<name>Kleihorst</name>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348175</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00064">
<document-id>
<country>US</country>
<doc-number>2010/0033567</doc-number>
<kind>A1</kind>
<name>Gupta et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00065">
<document-id>
<country>US</country>
<doc-number>2010/0141767</doc-number>
<kind>A1</kind>
<name>Mohanty et al.</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00066">
<document-id>
<country>US</country>
<doc-number>2010/0142401</doc-number>
<kind>A1</kind>
<name>Morris</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>370254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00067">
<document-id>
<country>US</country>
<doc-number>2010/0208941</doc-number>
<kind>A1</kind>
<name>Broaddus et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00068">
<document-id>
<country>US</country>
<doc-number>2010/0245588</doc-number>
<kind>A1</kind>
<name>Waehner et al.</name>
<date>20100900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348169</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00069">
<document-id>
<country>US</country>
<doc-number>2010/0263709</doc-number>
<kind>A1</kind>
<name>Norman et al.</name>
<date>20101000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>136246</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00070">
<document-id>
<country>US</country>
<doc-number>2010/0321473</doc-number>
<kind>A1</kind>
<name>An</name>
<date>20101200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 47</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00071">
<document-id>
<country>US</country>
<doc-number>2011/0109747</doc-number>
<kind>A1</kind>
<name>Forrester et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348152</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00072">
<document-id>
<country>US</country>
<doc-number>2011/0122250</doc-number>
<kind>A1</kind>
<name>Lee et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348155</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00073">
<document-id>
<country>US</country>
<doc-number>2011/0188762</doc-number>
<kind>A1</kind>
<name>Grindstaff et al.</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382218</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00074">
<document-id>
<country>US</country>
<doc-number>2011/0193935</doc-number>
<kind>A1</kind>
<name>Gorzynski</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 1408</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00075">
<document-id>
<country>US</country>
<doc-number>2011/0193936</doc-number>
<kind>A1</kind>
<name>Gai</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 36</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00076">
<document-id>
<country>US</country>
<doc-number>2011/0199461</doc-number>
<kind>A1</kind>
<name>Horio et al.</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348 46</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00077">
<document-id>
<country>US</country>
<doc-number>2011/0199484</doc-number>
<kind>A1</kind>
<name>Uchihara</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348143</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00078">
<document-id>
<country>US</country>
<doc-number>2012/0206607</doc-number>
<kind>A1</kind>
<name>Morioka</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348159</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00079">
<document-id>
<country>EP</country>
<doc-number>0747739</doc-number>
<date>19961200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-cpc-text>G06B 6/124</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00080">
<document-id>
<country>EP</country>
<doc-number>747739</doc-number>
<kind>A2</kind>
<date>19961200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-cpc-text>G02B 6/124</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00081">
<document-id>
<country>JP</country>
<doc-number>2006214735</doc-number>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-cpc-text>G01B 11/00</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00082">
<document-id>
<country>JP</country>
<doc-number>2006214735</doc-number>
<kind>A</kind>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00083">
<document-id>
<country>KR</country>
<doc-number>1020090050379</doc-number>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-cpc-text>H04N 7/18</classification-cpc-text>
</us-citation>
<us-citation>
<patcit num="00084">
<document-id>
<country>KR</country>
<doc-number>1020100005378</doc-number>
<date>20100100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
<classification-cpc-text>H04N 7/18</classification-cpc-text>
</us-citation>
<us-citation>
<nplcit num="00085">
<othercit>Ashcraft, &#x201c;Projecting an Arbitrary Latitude and Longitude onto a Tangent Plane,&#x201d; Brigham and Young University, Department of Electrical and Computer Engineering, 4 pages, Jan. 21, 1999.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00086">
<othercit>Bourke, &#x201c;Determining If a Point Lies on the Interior of a Polygon,&#x201d; accessible at the following link: http://local.wasp.uwa.edu.auk-pbourke/geometry/insidepoly/, 6 pages, Nov. 20, 1987.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00087">
<othercit>Chen et al., &#x201c;An Efficient Approach for the Calibration of Multiple PTZ Cameras,&#x201d; IEEE Transactions on Automation Science and Engineering, vol. 4, No. 2, pp. 286-293, Apr. 2007.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00088">
<othercit>Eigensystems, &#x201c;Jacobi Transformations of a Symmetric Matrix,&#x201d; Eigensystems, pp. 463-469,1992.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00089">
<othercit>Horn, &#x201c;Closed-form solution of absolute orientation using unit quaternions,&#x201d; Journal of the Optical of America A, vol. 4, pp. 629-642, Apr. 1987.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00090">
<othercit>Junejo et al, &#x201c;Refining PTZ Camera Calibration,&#x201d; Pattern Recognition, 4 pages, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00091">
<othercit>Robinault et al., &#x201c;Self-calibration and control of a PTZ camera based on a spherical mirror,&#x201d; IEEE, Advanced Video and Signal Based Surveillance, pp. 244-249, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00092">
<othercit>Thirthala et al., Calibration of Pan-Tilt-Zoom (PTZ) Camera and Omni-directional Cameras, IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1 page, 2005.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00093">
<othercit>International Searching Authority, International Search Report&#x2014;International Application No. PCT/US2010/056691, dated Aug. 3, 2011, together with the Written Opinion of the International Searching Authority, 9 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>24</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>340  81</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340937</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34053922</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34053925</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340549</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>340619</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 36- 39</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348143-159</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>15</number-of-drawing-sheets>
<number-of-figures>20</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12946413</doc-number>
<date>20101115</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8193909</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13458525</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>PCT/US2010/056691</doc-number>
<date>20101115</date>
</document-id>
<parent-status>PENDING</parent-status>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12946413</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120212611</doc-number>
<kind>A1</kind>
<date>20120823</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Estes</last-name>
<first-name>Andrew D.</first-name>
<address>
<city>Huntsville</city>
<state>AL</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Frederick</last-name>
<first-name>Johnny E.</first-name>
<address>
<city>Meridianville</city>
<state>AL</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Estes</last-name>
<first-name>Andrew D.</first-name>
<address>
<city>Huntsville</city>
<state>AL</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Frederick</last-name>
<first-name>Johnny E.</first-name>
<address>
<city>Meridianville</city>
<state>AL</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Sunstein Kann Murphy &#x26; Timbers LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Intergraph Technologies Company</orgname>
<role>02</role>
<address>
<city>Las Vegas</city>
<state>NV</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Bugg</last-name>
<first-name>George</first-name>
<department>2682</department>
</primary-examiner>
<assistant-examiner>
<last-name>Obiniyi</last-name>
<first-name>Paul</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method and a system for calibrating a camera in a surveillance system. The method and system use a mathematical rotation between a first coordinate system and a second coordinate system in order to calibrate a camera with a map of an area. In some embodiments, the calibration can be used to control the camera and/or to display a view cone on the map.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="136.14mm" wi="148.76mm" file="US08624709-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="175.43mm" wi="147.74mm" file="US08624709-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="210.57mm" wi="151.55mm" file="US08624709-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="211.33mm" wi="131.23mm" file="US08624709-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="202.27mm" wi="126.24mm" file="US08624709-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="99.14mm" wi="163.83mm" file="US08624709-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="193.97mm" wi="148.76mm" file="US08624709-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="221.23mm" wi="157.14mm" orientation="landscape" file="US08624709-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="179.92mm" wi="151.05mm" file="US08624709-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="156.38mm" wi="135.72mm" file="US08624709-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="224.03mm" wi="153.84mm" orientation="landscape" file="US08624709-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="221.23mm" wi="155.45mm" orientation="landscape" file="US08624709-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="171.37mm" wi="143.00mm" file="US08624709-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="210.65mm" wi="142.24mm" file="US08624709-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="170.69mm" wi="138.26mm" file="US08624709-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="95.59mm" wi="138.51mm" file="US08624709-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<p id="p-0002" num="0001">The present application is a continuation of U.S. patent application Ser. No. 12/946,413, filed Nov. 15, 2010, now U.S. Pat. No. 8,193,909, which is a continuation of PCT Application No. PCT/US 10/56691, filed Nov. 15, 2010, the full disclosures of which are incorporated by reference herein in their entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The present invention relates to methods of camera control, and more particularly to methods of camera control in a surveillance system.</p>
<heading id="h-0002" level="1">BACKGROUND ART</heading>
<p id="p-0004" num="0003">Surveillance systems use cameras to survey areas of interest. The cameras produce video feeds that can be displayed to a user. In this manner, the user can remotely observe an area of interest. In some cases, the cameras in the surveillance system are fixed and the user cannot change their field of view. In other cases, however, the cameras and their field of views can be adjusted through user control. Typically, the control is performed using a manual control, such as a joystick. Thus, if the video feed includes a moving object, the user manually controls the camera to follow the moving object. This manual control of the camera is not very efficient, particularly if the user has a limited time period to capture and/or respond to the moving object. These problems are further compounded if the camera is not accurately and properly mounted (e.g., to be level with the ground). In another example, image analysis is used to automatically control the camera to follow an object within the field of view. The shortcoming with this approach is that the image analysis cannot be performed unless the object is within the field of view of the camera. If the moving object is not within the field of view, the user must manually guide the camera to capture the moving object within its field of view.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0005" num="0004">Illustrative embodiments of the present invention are directed to a method, a system, and a computer readable medium encoded with instructions for calibrating a surveillance system. Illustrative embodiments of the present invention include displaying a video feed from a camera that has an orientation characterized by pan, zoom, and tilt coordinates. Exemplary embodiments further include displaying a map of an area that is characterized by geospatial coordinates. A user selects at least three pairs of points using an input device. A first point of the pair is selected in the map and a second point of the pair is selected from the video feed. The first point and the second point correspond to the same geographic location. The three points selected in the map are converted from geospatial coordinates into Cartesian coordinates defined by a first coordinate system. The three points selected in the video feed are converted from pan, zoom, and tilt coordinates into Cartesian coordinates defined by a second coordinate system. A mathematical rotation between the first coordinate system and the second coordinate system is then determined based upon the Cartesian coordinates for the three pairs of points.</p>
<p id="p-0006" num="0005">Various embodiments of the present invention also include allowing the user to select at least one point in the map using the input device. The geospatial coordinates for the selected point are converted into Cartesian coordinates defined by the first coordinate system and the mathematical rotation is applied to those Cartesian coordinates to determine Cartesian coordinates defined by the second coordinate system. The Cartesian coordinates defined by the second coordinate system are then further converted into pan and tilt coordinates for the selected point. Orientation instructions are then provided to the camera based upon the pan and tilt coordinates for the selected point.</p>
<p id="p-0007" num="0006">In additional or alternative embodiments of the present invention, coordinates from a sensor for a target are received and, if the coordinates for the target are not Cartesian coordinates defined by the first coordinate system, then they are converted into Cartesian coordinates defined by the first coordinate system. The mathematical rotation is applied to the Cartesian coordinates defined by the first Cartesian coordinate system to determine Cartesian coordinates defined by the second Cartesian coordinate system. The Cartesian coordinates defined by the second Cartesian coordinate system are converted into pan and tilt coordinates and orientation instructions are provided to the camera based upon the pan and tilt coordinates.</p>
<p id="p-0008" num="0007">In another illustrative embodiment of the present invention, the video feed has upper left, upper right, lower left, and lower right corners. Exemplary embodiments of the invention further include determining effective pan and tilt angles for at least the lower left and lower right corners of the video feed based upon the pan, zoom, and tilt coordinates for the camera orientation. The effective pan and tilt angles for at least the lower left and lower right corners of the video feed are converted into Cartesian coordinates defined by the second coordinate system. The mathematical rotation is applied to the Cartesian coordinates defined by the second coordinate system to determine Cartesian coordinates defined by the first coordinate system for at least the lower left and lower right corners of the video feed. A view cone is determined using the Cartesian coordinates defined by the first coordinate system for at least the lower left and lower right corners of the video feed and based upon the upper left and upper right corners of the video feed. The view cone is displayed on the map.</p>
<p id="p-0009" num="0008">Illustrative embodiments of the present invention are directed to a method, a system, and a computer readable medium encoded with instructions for controlling a camera that has an orientation characterized by pan, zoom, and tilt coordinates. Exemplary embodiments of the invention include displaying a map of an area that is characterized by geospatial coordinates. A user selects at least one point in the map using the input device. The geospatial coordinates for the selected point are converted into Cartesian coordinates defined by a first coordinate system that characterizes the map. A mathematical rotation is applied to the Cartesian coordinates for the selected point to determine Cartesian coordinates defined by a second coordinate system that characterizes a video feed from the camera. The mathematical rotation provides a conversion between the first coordinate system and the second coordinate system. The Cartesian coordinates defined by the second coordinate system are converted into pan and tilt coordinates for the selected point and orientation instructions are provided to the camera based upon at least the pan and tilt coordinates for the selected point. The video feed from the camera is then displayed according to the pan and tilt coordinates.</p>
<p id="p-0010" num="0009">Illustrative embodiments of the present invention are directed to a method, a system, and a computer readable medium encoded with instructions for prioritizing video feeds from a plurality of cameras that have locations characterized by coordinates. Exemplary embodiment of the invention include receiving coordinates for a point of interest and determining whether the point of interest is within the viewing range of any of the plurality of the cameras. If more than one camera is within viewing range of the point of interest, the distances between the cameras and the point of interest are determined. Various embodiments of the present invention further include determining which of the cameras is the least distant from the point of interest. Orientation instructions are provided to the least distant camera based upon the coordinates of the point of interest and a video feed is displayed from the least distant camera.</p>
<p id="p-0011" num="0010">Illustrative embodiments of the present invention are directed to a method, a system, and a computer readable medium encoded with instructions for tracking at least one target using a sensor and a camera that has an orientation characterized by pan, zoom, and tilt coordinates. Exemplary embodiments of the invention include receiving coordinates from the sensor for the target. If the coordinates for the target are not Cartesian coordinates defined by a first coordinate system characterizing the map, then the coordinates are converted into Cartesian coordinates defined by the first coordinate system. A mathematical rotation is applied to the Cartesian coordinates defined by the first coordinate system in order to determine Cartesian coordinates defined by a second coordinate system that characterizes a video feed from the camera. The mathematical rotation provides a conversion between the first coordinate system and the second coordinate system. The Cartesian coordinates defined by the second Cartesian coordinate system are converted into pan and tilt coordinates and orientation instructions are provided to the camera based upon the pan and tilt coordinates. The video feed from the camera is then displayed according to the pan and tilt coordinates.</p>
<p id="p-0012" num="0011">Illustrative embodiments of the present invention are directed to a method, a system, and a computer readable medium encoded with instructions for displaying a view cone for a camera that has an orientation characterized by pan, zoom, and tilt coordinates. Exemplary embodiments of the present invention include displaying a map of an area, determining the view cone based upon the pan, zoom, and tilt coordinates for the camera, and displaying the view cone on the map.</p>
<p id="p-0013" num="0012">Illustrative embodiments of the present invention are directed to a method, a system, and a computer readable medium encoded with instructions for displaying a view cone for a camera that has an orientation characterized by pan, zoom, and tilt coordinates. Exemplary embodiments of the present invention include displaying a map of an area that is characterized by geospatial coordinates. Effective pan and tilt angles are determined for at least the lower left and lower right corners of the video feed based upon the pan, zoom, and tilt coordinates for the camera orientation. The effective pan and tilt angles for at least the lower left and lower right corners of the video feed are converted into Cartesian coordinates defined by a second coordinate system that characterizes the video feed. A mathematical rotation is applied to the Cartesian coordinates for at least the lower left and lower right corners of the video feed to determine Cartesian coordinates defined by a first coordinate system that characterizes the map. The mathematical rotation provides a conversion between the first coordinate system and the second coordinate system. A view cone is then determined based upon the Cartesian coordinates, defined by the first coordinate system, for at least the lower left and lower right corners and based upon the upper left and upper right corners of the video feed. The view cone is then displayed on the map.</p>
<p id="p-0014" num="0013">When the tilt coordinate for the camera is below the horizon, determining the view cone based upon the upper left and upper right corners of the video feed further includes determining effective pan and tilt angles for the upper left and upper right corners of the video feed based upon the pan, zoom, and tilt coordinates for the camera orientation. The effective pan and tilt angles for the upper left and upper right corners of the video feed are converted into Cartesian coordinates defined by the second coordinate system. The mathematical rotation is applied to the Cartesian coordinates to determine Cartesian coordinates defined by the first coordinate system for the upper left and upper right corners of the video feed. The view cone is determined based upon the Cartesian coordinates, defined by the first coordinate system, for the upper left, upper right, lower left and lower right corners.</p>
<p id="p-0015" num="0014">When the tilt coordinate for the camera is above the horizon, determining the view cone based upon the upper left and upper right corners of the video feed includes determining effective tilt angles for the upper left and upper right corners of the video feed based upon the pan, zoom, and tilt coordinates for the camera. Coordinates, defined by the first coordinate system, for the upper left and upper right corners of the video feed are determined based upon a resolvable distance of the camera. The view cone is determined based upon the Cartesian coordinates, defined by the first coordinate system, for the upper left, upper right, lower left and lower right corners.</p>
<p id="p-0016" num="0015">In further exemplary embodiments, the view cone is a polygon and the Cartesian coordinates, defined by the first coordinate system, for the upper left, upper right, lower left and lower right corners are the vertices of the polygon.</p>
<p id="p-0017" num="0016">Illustrative embodiments of the present invention are directed to a method, a calibrated system, and a computer readable medium encoded with instructions for controlling a camera. Exemplary embodiments of the present invention include displaying a map of an area and determining Cartesian coordinates for a point of interest. The Cartesian coordinates are defined by a first coordinate system characterizing the map. Exemplary embodiments further include applying a mathematical rotation to the Cartesian coordinates for the point of interest to determine Cartesian coordinates defined by a second coordinate system that characterizes a video feed from the camera. The mathematical rotation provides a conversion between the first coordinate system and the second coordinate system. The Cartesian coordinates defined by the second coordinate system are converted into at least pan and tilt coordinates for the point of interest and orientation instructions are provided to the camera based upon at least the pan and tilt coordinates for the point of interest. The video feed from the camera is displayed according to the orientation instructions.</p>
<p id="p-0018" num="0017">In all or some of the above described embodiments, the geospatial coordinates are latitude, longitude, and altitude coordinates. The input device is one or more of a mouse, a cursor, a crosshair, a touch screen, and a keyboard. The sensor is one or more of a camera, a radar, and a motion detector.</p>
<p id="p-0019" num="0018">Furthermore, in all or some of the above described embodiments, the map of an area is displayed and the map is characterized by geospatial coordinates. In some embodiments, the location of at least one camera is displayed on the map. In additional or alternative embodiments, the location of at least one sensor is displayed on the map. Also, in some embodiments, the location of at least one target is displayed on the map.</p>
<p id="p-0020" num="0019">In all or some of the above described embodiments, the mathematical rotation is determined by displaying a map of an area that is characterized by geospatial coordinates. A user selects at least three pairs of points using an input device. The first point of the pair is selected in the map and a second point of the pair being selected from the video feed. The first point and the second point corresponding to the same geographic location. The three points selected in the map are converted from geospatial coordinates into Cartesian coordinates defined by the first coordinate system. The at least three points selected in the video feed are converted from pan, zoom, and tilt coordinates into Cartesian coordinates defined by the second coordinate system. The mathematical rotation between the first coordinate system and the second coordinate system is determined based upon the Cartesian coordinates for the at least three pairs of points. In some embodiments, the mathematical rotation is a matrix.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0021" num="0020">The foregoing features of the invention will be more readily understood by reference to the following detailed description, taken with reference to the accompanying drawings, in which:</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 1</figref> shows a surveillance system in accordance with one embodiment of the present invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 2</figref> shows a method for calibrating a surveillance system in accordance with one embodiment of the present invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 3A</figref> shows a video feed from a camera in accordance with one embodiment of the present invention;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 3B</figref> shows a map of a building floor plan in accordance with one embodiment of the present invention;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 4A</figref> shows a video feed from a camera in accordance with one embodiment of the present invention;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 4B</figref> shows a video feed from a camera in accordance with one embodiment of the present invention;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 5</figref> shows a table of selected points in accordance with one embodiment of the present invention;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 6</figref> shows a method for controlling at least one camera in accordance with one embodiment of the present invention;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 7</figref> shows a map of an outdoor terrain in accordance with one embodiment of the present invention;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 8</figref> shows a method for tracking a target in accordance with one embodiment of the present invention;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 9</figref> shows a method for prioritizing video feeds from a plurality of cameras in accordance with one embodiment of the present invention;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 10</figref> shows a map of an outdoor terrain with a plurality of cameras in accordance with one embodiment of the present invention;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 11</figref> shows a viewing range of a camera in accordance with one embodiment of the present invention;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 12</figref> shows a method for displaying a view cone for at least one camera in accordance with one embodiment of the present invention;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 13</figref> shows a view cone and a camera icon in accordance with one embodiment of the present invention.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 14</figref> shows a view cone and camera icon as applied to a map in accordance with one embodiment of the present invention;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 15</figref> shows a video feed and a corresponding view cone in accordance with one embodiment of the present invention;</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 16</figref> shows a video image transposed into a polygonal view cone in accordance with one embodiment of the present invention;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 17</figref> shows how an observable distance is calculated using a right triangle in accordance with one embodiment of the present invention; and</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 18</figref> shows a view cone in accordance with one embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF SPECIFIC EMBODIMENTS</heading>
<p id="p-0042" num="0041">Illustrative embodiments of the present invention are directed to a surveillance system. <figref idref="DRAWINGS">FIG. 1</figref> shows a surveillance system <b>100</b> in accordance with one embodiment of the present invention. Illustrative embodiments of the surveillance system include at least one camera <b>102</b>. In other embodiments, such as the one shown in <figref idref="DRAWINGS">FIG. 1</figref>, the system includes a plurality of cameras <b>102</b>, <b>104</b>, <b>106</b>. The cameras <b>102</b>, <b>104</b>, <b>106</b> are in electronic communication with a processor (e.g., a camera server) via, for example, a communications network <b>110</b>. The processor <b>108</b> is also in communication with at least one display device such as a monitor <b>112</b>. The display device <b>112</b> allows a user to view video feeds from the cameras <b>102</b>, <b>104</b>, <b>106</b>. The system <b>100</b> also includes an input device <b>114</b> such as a computer mouse, a cursor, a crosshair, a keyboard, a menu, and/or a joy stick. In various embodiments, the display device <b>112</b> is also the input <b>114</b> device (e.g., touch screen). The input device <b>114</b> is in electronic communication with the processor <b>108</b> and allows a user to control the cameras <b>102</b>, <b>104</b>, <b>106</b> and to select points in the video feeds from each of the cameras.</p>
<p id="p-0043" num="0042">In illustrative embodiments, the surveillance system also includes a geographic information system <b>116</b>. The geographic information system <b>116</b> includes a database of maps of geographic areas (e.g., building floor plans and/or outdoor terrains). Each map includes geospatial coordinates definable by a geographic coordinate system (e.g., latitude, longitude, and altitude). In additional or alternative embodiments, the geospatial coordinates are definable a Cartesian Coordinate system.</p>
<p id="p-0044" num="0043">In some embodiments, the processor <b>108</b> itself includes the geographic information system <b>116</b>, however, in other embodiments, the geographic information system may be in communication with the processor via the communications network (e.g., internet) <b>110</b>. The display device <b>112</b> is used to display the map and the input device <b>114</b> is used to select a points on the map. In various exemplary embodiments, a first display device may be used to display video feeds from the cameras <b>102</b>, <b>104</b>, <b>106</b> and a second display device is used to display the map. Additionally or alternatively, the surveillance system <b>100</b> may include at least one sensor <b>118</b> such as a camera, a radar, and/or a motion detector. The sensor <b>118</b> is in electronic communication with the processor <b>108</b> via, for example, the computer network <b>110</b>.</p>
<p id="p-0045" num="0044">Calibration</p>
<p id="p-0046" num="0045">Illustrative embodiments of the present invention are directed to a method for calibrating a surveillance system (such as the one shown in <figref idref="DRAWINGS">FIG. 1</figref>.). <figref idref="DRAWINGS">FIG. 2</figref> shows a method <b>200</b> for calibrating a surveillance system in accordance with one embodiment of the present invention. The method includes displaying a video feed from a camera using a display device <b>202</b>. <figref idref="DRAWINGS">FIG. 3A</figref> shows a video feed <b>301</b> from a camera in accordance with one embodiment of the present invention. In the present example, the video feed <b>301</b> shows the breeze way of a building. In illustrative embodiments, the camera is a PTZ camera with an orientation that characterized by pan, zoom, and tilt coordinates, however, the use of other cameras is also within the scope of the present invention.</p>
<p id="p-0047" num="0046">The method also includes using the display device to display a map of an area <b>204</b>. <figref idref="DRAWINGS">FIG. 3B</figref> shows a map <b>300</b> of a building floor plan in accordance with one embodiment of the present invention. The floor plan includes a hallway that is located between two rooms. The map <b>300</b> is characterized by geospatial coordinates. In other words, each point on the map <b>300</b> can be defined in terms of a geospatial coordinates such as latitude and longitude. In some embodiments of the present invention, the map <b>300</b> does not include a coordinate for altitude. In such an embodiment, the altitude is assumed to be &#x201c;0&#x201d;. In other illustrative embodiment of the present invention, every point on the map <b>300</b> is defined by an altitude coordinate and, therefore, every point on the map is defined by three coordinates: latitude, longitude, and altitude. In yet another embodiment of the invention, only some of the points on the map <b>300</b> are defined in terms of altitude. In such an embodiment, the altitude for points that are not defined can be extrapolated from adjacent points that do have defined altitudes. For example, if a first point at 30.000 degrees latitude and 20.000 degrees longitude does not have a defined altitude, then points closest to the first point can be used to extrapolate the altitude for the first point. In such an example, the method uses a second point, at 30.000 degrees latitude and 19.999 degrees longitude with an altitude of 4 feet, and a third point, at 30.000 degrees latitude and 20.001 degrees longitude with an altitude of 10 feet, to assume that the altitude for the first point is about 7 feet. In illustrative embodiments, various other extrapolation techniques can also be used. For example, in some embodiments, algorithms for smoothing the extrapolated altitudes may be used.</p>
<p id="p-0048" num="0047">Illustrative embodiments of the map <b>300</b> also show the location of a camera on the map using a camera icon <b>302</b>. In this manner, a user can view the map <b>300</b> and understand the vantage point and perspective of the video feed from the camera. In <figref idref="DRAWINGS">FIG. 3B</figref>, a camera icon <b>302</b> shows the location of the camera at the end of the hallway.</p>
<p id="p-0049" num="0048">The map <b>300</b> also displays an input device <b>304</b> so that the user can select points on the map <b>300</b>. In <figref idref="DRAWINGS">FIG. 3B</figref>, the input device <b>304</b> is a crosshair, however, in other embodiments the input device can take the form of, for example, an arrow, a touch screen, and/or a menu.</p>
<p id="p-0050" num="0049">Illustrative embodiments of the calibration method allow the user to select at least three pairs of points using the input device <b>304</b>, <b>206</b>. A first point of the pair is selected in the map <b>300</b> using the input device <b>304</b>. In <figref idref="DRAWINGS">FIG. 3B</figref>, the user selects a left corner <b>306</b> in the hallway using the crosshair <b>304</b>. The processor registers the particular geospatial coordinates for the selected point. Once the user selects the first point in the map <b>300</b>, he then selects the second point of the pair from a video feed. <figref idref="DRAWINGS">FIG. 4A</figref> shows a video feed <b>400</b> in accordance with one embodiment of the present invention. The video feed <b>400</b> and the map <b>300</b> may be simultaneously displayed in separate windows on one display device or each separately on its own display device. In <figref idref="DRAWINGS">FIG. 4A</figref>, the user uses a crosshair <b>402</b> to select the left corner <b>306</b> in the hallway in the video feed <b>400</b>. When the user selects a point in the video feed <b>400</b>, a processor queries the camera for its orientation (e.g., pan angle, tilt angle, and zoom). Once the orientation is received from the camera, the processor registers the particular orientation of the camera. The first and second points in the pair should be selected so that they correspond to the same geographic location. In this manner, the method appropriately calibrates the video feed <b>400</b> to the map <b>300</b>.</p>
<p id="p-0051" num="0050">Once the first pair of points is selected, the user selects a second pair of points. In <figref idref="DRAWINGS">FIG. 3B</figref>, the user selects a first point of the pair as a right corner <b>306</b> in the hallway. The second point of the pair is then selected in the video feed <b>400</b>. <figref idref="DRAWINGS">FIG. 4B</figref> shows the video feed <b>400</b> in accordance with one embodiment of the present invention. In <figref idref="DRAWINGS">FIG. 4B</figref>, the user has shifted the orientation of the camera and, therefore, the video feed <b>400</b> displays a different part of the hallway. The user uses the crosshair <b>402</b> to select the right corner <b>308</b> of the hallway as it appears in the video feed <b>400</b>.</p>
<p id="p-0052" num="0051">Once the second pair of points is selected, the user selects a third pair of points. In illustrative embodiments of the present invention, the user selects three pairs of points, however, in other exemplary embodiments, the user may selects 4, 5, or more pairs of points for the calibration. In some exemplary embodiments, using more pairs of points results in a more accurate calibration because any error within the coordinates of one point is averaged over a greater number of points.</p>
<p id="p-0053" num="0052">In illustrative embodiments of the present invention, as described above, the user selects the first point of the pair in the map <b>300</b> and then the second point of the pair in the video feed <b>400</b>. Once the first pair of points is selected, the user then selects the next pair of points. The embodiments of the present invention, however, are not limited to this particular order. In fact, the points can be selected in any order so long as there are at least three pairs of points selected. For example, in one illustrative embodiment, the user first selects a point of the pair in the video feed <b>400</b> and then selects the other point of the pair in the map <b>300</b>. In yet another illustrative embodiment, the user first selects all three points in the video feed <b>400</b> and then selects corresponding points in the map <b>300</b>.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 5</figref> shows a table <b>500</b> of selected points in accordance with one embodiment of the present invention. The table includes 6 pairs of points and lists the particular coordinates for each of the pairs. Columns <b>502</b> and <b>504</b> list the geospatial coordinates for the points selected in the map <b>301</b>. Columns <b>506</b> and <b>508</b> list the camera orientations for the points selected in the video feed <b>400</b>. The geospatial coordinates in the table <b>500</b> are latitude, longitude, and altitude (altitude not displayed), while the coordinates for the camera coordinates are pan angle, tilt angle, and zoom (zoom not displayed).</p>
<p id="p-0055" num="0054">In various embodiments of the calibration method, the pan, tilt, and zoom orientation of the camera for each point in the video feed are converted into effective pan and tilt angles for the points themselves. The processor initially registers the orientation of the camera itself, but these initial pan and tilt angles are representative of the center of the video feed. When a user selects points away from the center of the video feed <b>400</b>, the effective pan and tilt angles for the points will be different from the pan and tilt orientation of the camera itself. To determine the effective pan and tilt angles for the points, a horizontal angular field of view (H<sub>FOV</sub>) and vertical angular field of view (V<sub>FOV</sub>) is determined using Equations 1 and 2:</p>
<p id="p-0056" num="0055">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>H</mi>
          <mi>FOV</mi>
        </msub>
        <mo>=</mo>
        <mi>Zoom</mi>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>V</mi>
          <mi>FOV</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <mi>Zoom</mi>
          <mo>&#xd7;</mo>
          <mfrac>
            <mn>1</mn>
            <mi>AspectRatio</mi>
          </mfrac>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0057" num="0056">The effective pan angle (&#x3b8;) can be calculated using Equation 3,</p>
<p id="p-0058" num="0057">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>&#x3b8;</mi>
        <mo>=</mo>
        <mrow>
          <mrow>
            <msub>
              <mi>H</mi>
              <mi>FOV</mi>
            </msub>
            <mo>&#xd7;</mo>
            <mfrac>
              <msub>
                <mi>H</mi>
                <mi>D</mi>
              </msub>
              <msub>
                <mi>W</mi>
                <mi>Videofeed</mi>
              </msub>
            </mfrac>
          </mrow>
          <mo>+</mo>
          <mi>Pan</mi>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0059" num="0058">wherein H<sub>D </sub>is the horizontal distance (e.g., in pixels) from the center of the video feed <b>400</b> to the point, and W<sub>Videofeed </sub>is the width of the video feed (e.g., in pixels). Also, the effective tilt angle (&#x3c6;) can be calculated using Equation 4,</p>
<p id="p-0060" num="0059">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>&#x3c6;</mi>
        <mo>=</mo>
        <mrow>
          <mrow>
            <msub>
              <mi>V</mi>
              <mi>FOV</mi>
            </msub>
            <mo>&#xd7;</mo>
            <mfrac>
              <msub>
                <mi>V</mi>
                <mi>D</mi>
              </msub>
              <msub>
                <mi>H</mi>
                <mi>Videofeed</mi>
              </msub>
            </mfrac>
          </mrow>
          <mo>+</mo>
          <mi>Tilt</mi>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>4</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
wherein V<sub>D </sub>is the vertical distance (e.g., in pixels) from the center of the video feed <b>400</b> to the point, and H<sub>Videofeed </sub>is the height of the video feed (e.g., in pixels).
</p>
<p id="p-0061" num="0060">Illustrative embodiments of the calibration method also include converting the at least three points selected in the video feed <b>400</b> from effective pan and tilt angles to Cartesian coordinates defined by a Cartesian coordinate system <b>210</b>. In various embodiments of the calibration method, the effective pan and tilt angles are converted to unit vectors on a &#x201c;camera&#x201d; Cartesian coordinate system that has its origin at the camera's location. The effective pan and tilt angles can be converted into Cartesian coordinates using Equations 5, 6, and 7:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>=cos(&#x3c6;)&#xd7;sin(&#x3b8;)&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i>=cos(&#x3c6;)&#xd7;cos(&#x3b8;)&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>w</i>=sin(&#x3c6;)&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0062" num="0061">In this manner, Cartesian coordinates are calculated for the effective pan (&#x3b8;) and tilt (&#x3c6;) angles of each point selected in the video feed <b>400</b>.</p>
<p id="p-0063" num="0062">Illustrative embodiments of the calibration method further include converting the three points selected in the map <b>300</b> from geospatial coordinates into Cartesian coordinates defined by a coordinate system <b>208</b>. The coordinate system can be created using a geospatial location for a point (P<sub>lat</sub>, P<sub>lon</sub>, P<sub>alt</sub>) and a geospatial location for the camera (C<sub>lat</sub>, C<sub>lon</sub>, C<sub>alt</sub>). The origin of the &#x201c;map&#x201d; Cartesian coordinate system is set as the location of the camera. In illustrative embodiments, the x-axis of the coordinate system points east, the y-axis points north, and the z-axis points in a direction away from and normal to the Earth.</p>
<p id="p-0064" num="0063">The distance from the camera to the point along the x-axis can be calculated using Equation 8:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>P</sub><i>=R</i><sub>E</sub>&#xd7;sin(<i>P</i><sub>lat</sub><i>&#x2212;C</i><sub>lat</sub>)+(<i>R</i><sub>E</sub>&#xd7;cos(<i>P</i><sub>lat</sub>))&#xd7;(1&#x2212;cos(<i>P</i><sub>lon</sub><i>&#xd7;C</i><sub>lon</sub>))&#xd7;sin(<i>C</i><sub>lat</sub>)&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0065" num="0064">The distance from the camera to the point along the y-axis can be calculated using Equation 9:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Y</i><sub>P</sub><i>=R</i><sub>E</sub>&#xd7;cos(<i>P</i><sub>lat</sub>))&#xd7;sin(<i>P</i><sub>lon</sub><i>&#x2212;C</i><sub>lon</sub>))&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0066" num="0065">The distance from the camera to the point along the z-axis can be calculated using Equation 10:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Z</i><sub>P</sub><i>=&#x2212;C</i><sub>alt</sub><i>=&#x2212;P</i><sub>alt</sub>&#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0067" num="0066">In each of Equations 8, 9, and 10, R<sub>E </sub>is the semi-major axis of the earth, which can be calculated according to Equation 11,
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>E</sub>=(1&#x2212;<i>K</i><sub>flat</sub>)&#xd7;sin<sup>2</sup>(<i>P</i><sub>lat</sub>)&#xd7;<i>R</i><sub>a</sub>&#x2003;&#x2003;(11)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0068" num="0067">wherein K<sub>flat </sub>is the Earth flatness constant, which is approximately 1/298.257, and R<sub>a </sub>is the radius of the Earth, which is approximately 6,378,137 meters. Accordingly, using Equations 8, 9, and 10, the Cartesian coordinates for the point are (X<sub>P</sub>, Y<sub>P</sub>, Z<sub>P</sub>) and the Cartesian coordinates for the camera are (0, 0, 0).</p>
<p id="p-0069" num="0068">In illustrative embodiments, the three points selected from the video feed are then converted into unit vectors. The unit vectors can be determined using Equations 12, 13, and 14:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>u=X</i><sub>P</sub>/&#x221a;{square root over (<i>X</i><sub>P</sub><sup>2</sup><i>Y</i><sub>P</sub><sup>2</sup><i>Z</i><sub>P</sub><sup>2</sup>)}&#x2003;&#x2003;(12)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>v=Y</i><sub>P</sub>/&#x221a;{square root over (<i>X</i><sub>P</sub><sup>2</sup><i>Y</i><sub>P</sub><sup>2</sup><i>Z</i><sub>P</sub><sup>2</sup>)}&#x2003;&#x2003;(13)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>w=Z</i><sub>P</sub>/&#x221a;{square root over (<i>X</i><sub>P</sub><sup>2</sup><i>Y</i><sub>P</sub><sup>2</sup><i>Z</i><sub>P</sub><sup>2</sup>)}&#x2003;&#x2003;(14)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0070" num="0069">Accordingly, each of the three points selected from the map is defined as a unit vector (u, v, w) of the &#x201c;map&#x201d; Cartesian coordinate system.</p>
<p id="p-0071" num="0070">In the illustrative embodiments discussed above, the map <b>300</b> is characterized by geospatial coordinates such as latitude, longitude, and altitude. In other embodiments, however, the map is already characterized by a Cartesian coordinate system (before the above described calibration process) and, therefore, a conversion from geospatial coordinates to a Cartesian coordinate system may not be necessary.</p>
<p id="p-0072" num="0071">Illustrative embodiments of the present invention further include determining a mathematical rotation between the &#x201c;map&#x201d; coordinate system and the &#x201c;camera&#x201d; coordinate system based upon the Cartesian coordinates for the at least three pairs of points selected from the map and the video feed <b>212</b>. The mathematical rotation provides a conversion from points in the &#x201c;map&#x201d; coordinate system to corresponding points in the &#x201c;camera&#x201d; coordinate system. The transform of the rotation provides a conversion from points in the &#x201c;camera&#x201d; coordinate system to corresponding points in the &#x201c;map&#x201d; coordinate system. Or vice versa, a transform of the mathematical rotation provides a conversion from points in the &#x201c;camera&#x201d; coordinate system to corresponding points in the &#x201c;map&#x201d; coordinate system, and its transform provides a conversion from points in the &#x201c;map&#x201d; coordinate system to corresponding points in the &#x201c;camera&#x201d; coordinate system. As used in the present application, the term &#x201c;rotation&#x201d; also includes a transform of the rotation.</p>
<p id="p-0073" num="0072">It is known in the art how to develop a mathematical rotation (and its transform) between a first Cartesian coordinate system and a second Cartesian Coordinate system. For example, in various embodiments, the mathematical rotation is a four-by-four matrix that is applied to unit vectors that define a point in one of the &#x201c;camera&#x201d; and &#x201c;map&#x201d; coordinate systems. The four-by-four rotation is determined by developing an initial four-by-four matrix. The initial four-by-four matrix is developed according to, for example, Section 4 of Berthold K. P. Horn, &#x201c;Closed-form Solution of Absolute Orientation Using Unit Quaternions&#x201d; 4 Journal of the Optical Society of America 629 (April 1987). The Horn article is hereby incorporated by reference in its entirety. Eigenvectors for the initial four-by-four matrix are then calculated using a Jacobi Eigenvalue algorithm as known in the art. The calculation of the Eigenvalues can be software coded as disclosed in, for example, Section 11.1 of William H. Press et al., &#x201c;Numerical Recipes in C&#x201d; (Cambridge University Press 2d ed.) (1992). Section 11.1 of that book is also hereby incorporated by reference. Once the eigenvectors are calculated, an eigenvector corresponding to the most positive eigenvalue is selected as the rotation matrix. In illustrative embodiments, the rotation matrix is stored by the processor as a quaternion.</p>
<p id="p-0074" num="0073">Illustrative embodiments of the present invention are advantageous over many prior art systems because there is no need for the camera to be accurately aligned with other reference points. For example, many prior art systems require pan-zoom-tilt cameras to be level with the ground when installed and/or to be properly aligned with a particular coordinate system. If not properly aligned, the camera will not accurately shift and capture points of interest. The inventors have overcome this problem by calibrating the camera using a mathematical rotation between a first Cartesian coordinate system and a second Cartesian coordinate system. In illustrative embodiments of the present invention, the inventors discovered that the above disclosed calibration accounts for any misalignment of the camera with the ground or coordinate system. In the past, calibration of a camera with a map of an area was not approached as a problem that could be solved through the use of a mathematical rotation between two Cartesian coordinate systems, perhaps, because latitude-longitude coordinates and the motion of a pan-zoom-tilt camera does not correspond with a Cartesian coordinate system.</p>
<p id="p-0075" num="0074">Camera Control Using the Map</p>
<p id="p-0076" num="0075">Illustrative embodiments of the present invention are also directed to a computer-implemented method for controlling a camera in a surveillance system (such as the system shown in <figref idref="DRAWINGS">FIG. 1</figref>). <figref idref="DRAWINGS">FIG. 6</figref> shows a method <b>600</b> for controlling at least one camera in accordance with one embodiment of the present invention. The method includes displaying a map of an area using a display device <b>602</b>. <figref idref="DRAWINGS">FIG. 7</figref> shows a map <b>700</b> of an outdoor terrain in accordance with one embodiment of the present invention. The map <b>700</b> is characterized by geospatial coordinates such as latitude, longitude, and altitude. The method further includes allowing a user to select a point of interest in the map <b>700</b> using an input device <b>604</b>. In the embodiment of <figref idref="DRAWINGS">FIG. 6</figref>, the input device is a crosshair <b>702</b>. In illustrative embodiments of the present invention, the orientation of a camera <b>704</b> then shifts so that the camera observes the point of interest. In this manner, the user controls the camera <b>704</b> and its orientation by simply selecting a point in the map <b>700</b> using the crosshair <b>702</b>. Thus, the user can use a geospatial interface to automate camera control. In the example of <figref idref="DRAWINGS">FIG. 7</figref>, the user is interested in activity on a road <b>706</b> that runs through the map <b>700</b>. By selecting a point on the road <b>706</b> in the map <b>700</b>, the camera <b>704</b> automatically shifts its orientation to the road and the user can view the activity on the road via the video feed from the camera.</p>
<p id="p-0077" num="0076">In illustrative embodiments, once the user selects the point on the map, a processor converts the geospatial coordinates for the selected point into Cartesian coordinates defined by a coordinate system <b>606</b>. In exemplary embodiments, the conversion is performed using Equations 8, 9, 10, 11, 12, 13, and 14, as described in the &#x201c;Calibration&#x201d; section of present application. Accordingly, the geospatial coordinates for the point are defined as unit vectors of a &#x201c;map&#x201d; Cartesian coordinate system. The processor <b>108</b> then applies a mathematical rotation to the &#x201c;map&#x201d; Cartesian coordinates for the selected point to determine Cartesian coordinates defined by a &#x201c;camera&#x201d; Cartesian coordinate system <b>608</b>. The &#x201c;camera&#x201d; coordinate system characterizes a video feed from the camera and a mathematical rotation (or its transform) provides a conversion between the &#x201c;map&#x201d; coordinate system and the &#x201c;camera&#x201d; coordinate system. In illustrative embodiments, the mathematical rotation is developed according to the method described in the &#x201c;Calibration&#x201d; section of the present application.</p>
<p id="p-0078" num="0077">The Cartesian coordinates for the selected point (e.g., unit vectors u, v, w) are then defined in terms of pan, zoom, and tilt coordinates (e.g., pan (&#x3b8;) and tilt (&#x3c6;)) <b>610</b>. The Cartesian coordinates (defined by the &#x201c;camera&#x201d; coordinate system) can be converted to pan and tilt angles using Equations 15 and 16:</p>
<p id="p-0079" num="0078">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>&#x3b8;</mi>
        <mo>=</mo>
        <mrow>
          <msup>
            <mi>atan</mi>
            <mn>2</mn>
          </msup>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>u</mi>
              <mo>,</mo>
              <mi>v</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>15</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi>&#x3c6;</mi>
        <mo>=</mo>
        <mrow>
          <mi>atan</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>(</mo>
            <mfrac>
              <mi>w</mi>
              <msqrt>
                <mrow>
                  <msup>
                    <mi>u</mi>
                    <mn>2</mn>
                  </msup>
                  <mo>+</mo>
                  <msup>
                    <mi>v</mi>
                    <mn>2</mn>
                  </msup>
                </mrow>
              </msqrt>
            </mfrac>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>16</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0080" num="0079">Accordingly, the selected point is defined in terms of a pan and tilt angles of the camera. Based on these pan and tilt angles, the processor can provide orientation instructions to the camera so that the camera shifts to view the selected point and the video feed of the selected point is thus displayed to the user <b>612</b>, <b>614</b>.</p>
<p id="p-0081" num="0080">Target Tracking</p>
<p id="p-0082" num="0081">Illustrative embodiments of the present invention are also directed to a computer-implemented method for tracking a target using at least one camera and at least one sensor (e.g., such as those shown in <figref idref="DRAWINGS">FIG. 1</figref>). <figref idref="DRAWINGS">FIG. 8</figref> shows a method <b>800</b> for tracking a target in accordance with one embodiment of the present invention. The method includes receiving coordinates for the target at a processor from the sensor <b>802</b>. In some embodiments, the coordinates for the target that are received from the sensor and the coordinates are already defined in terms of a Cartesian coordinate system that characterizes a map of an area. For example, in some embodiments, the Cartesian coordinates for a motion detector in the map are determined when the detector is placed in the area. Therefore, if the motion detector is set-off, then the processor can assume that the target is located near the coordinates of the motion detector and uses the coordinates of the sensor defined by the &#x201c;map&#x201d; coordinate system to determine an appropriate camera action. In other embodiments, however, the coordinates for the target that are received from the sensor are defined in terms of a geospatial coordinate system. In such an embodiment, the geospatial coordinates are converted into Cartesian coordinates defined by the &#x201c;map&#x201d; coordinate system <b>804</b>. In exemplary embodiments, the conversion is performed using Equations 8, 9, 10, 11, 12, 13, and 14, as described in the &#x201c;Calibration&#x201d; section of present application. In this manner, the geospatial coordinates for the target are defined as unit vectors (u, v, w) of the &#x201c;map&#x201d; Cartesian coordinate system.</p>
<p id="p-0083" num="0082">The method for target tracking also includes applying a mathematical rotation (or its transform) to the &#x201c;map&#x201d; Cartesian coordinates for the target to determine Cartesian coordinates defined by a &#x201c;camera&#x201d; Cartesian coordinate system <b>806</b>. The &#x201c;camera&#x201d; coordinate system defines a video feed from the camera and the mathematical rotation provides a conversion between the &#x201c;map&#x201d; coordinate system and the &#x201c;camera&#x201d; coordinate system. In illustrative embodiments, the mathematical rotation is developed according to the method described in the &#x201c;Calibration&#x201d; section of the present application.</p>
<p id="p-0084" num="0083">The Cartesian coordinates for the target (u, v, w) are then defined in terms of pan, zoom, and tilt coordinates (e.g., pan &#x201c;&#x3b8;&#x201d; and tilt &#x201c;&#x3c6;&#x201d;) for the camera <b>808</b>. In various illustrative embodiments, the Cartesian coordinates are converted to pan and tilt angles using Equations 15 and 16, as described in the &#x201c;Camera Control Using the Map&#x201d; section of present application. Accordingly, the target is defined in terms of a pan and tilt angles of the camera. Based on these pan and tilt angles, the processor provides orientation instructions to the camera so that the camera shifts to view the target <b>810</b>, <b>812</b>.</p>
<p id="p-0085" num="0084">In illustrative embodiments, the processor continuously receives coordinates from the sensor and iteratively determines camera instructions based upon the new coordinates. In this manner, the processor and the camera automatically track the target as it moves through the area.</p>
<p id="p-0086" num="0085">Illustrative embodiments of the target tracking method may also display a map that shows the location of the sensor using a sensor icon. Furthermore, in additional or alternative embodiments, the map displays the location of the target in the area using a target icon. The processor can use the geospatial and/or &#x201c;camera&#x201d; Cartesian coordinates for the sensor and/or target to display them on the map. By displaying the target, sensor, and cameras on the map, the user can better understand the location of the target in relation to the sensors and cameras.</p>
<p id="p-0087" num="0086">Prioritization of Cameras</p>
<p id="p-0088" num="0087">Illustrative embodiments of the present invention are also directed to a computer-implemented method for prioritizing video feeds from a plurality of cameras (such as the ones shown in <figref idref="DRAWINGS">FIG. 1</figref>). <figref idref="DRAWINGS">FIG. 9</figref> shows a method <b>900</b> for prioritizing video feeds from a plurality of cameras in accordance with one embodiment of the present invention, while <figref idref="DRAWINGS">FIG. 10</figref> shows a map <b>1000</b> of an outdoor terrain including a plurality of cameras <b>1002</b>, <b>1004</b>, <b>1006</b>, <b>1008</b> in accordance with another embodiment of the present invention.</p>
<p id="p-0089" num="0088">Illustrative embodiments of the method include receiving geospatial coordinates for a point of interest <b>1010</b>, <b>902</b>. In the embodiment shown in <figref idref="DRAWINGS">FIG. 10</figref>, the point of interest <b>1002</b> is selected by a user from the map <b>1000</b> using a display device and an input device. In other illustrative embodiments, the coordinates for the point of interest are representative of a target and the coordinates are received from a sensor. Once the coordinates for the point of interest are received, in various embodiments, one or more cameras <b>1002</b>, <b>1004</b>, <b>1006</b>, <b>1008</b> shift their orientation to capture the point of interest <b>1010</b>. In illustrative embodiments of the present invention, the cameras <b>1002</b>, <b>1004</b>, <b>1006</b>, <b>1008</b> are assigned a prioritization based upon their distance from the point of interest <b>1010</b>.</p>
<p id="p-0090" num="0089">Various exemplary embodiments of the prioritization method include determining whether the point of interest <b>1010</b> is within the viewing range of any of the cameras <b>1002</b>, <b>1004</b>, <b>1006</b>, <b>1008</b>, <b>904</b>. In some exemplary embodiments, the method also includes determining which cameras <b>1002</b>, <b>1004</b>, <b>1006</b>, <b>1008</b> are available for viewing the point of interest (e.g., some cameras may be tracking a different target or the user may not have permission to use certain cameras).</p>
<p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. 11</figref> shows a viewing range <b>1100</b> of a camera <b>1102</b> in accordance with one embodiment of the present invention. As shown in <figref idref="DRAWINGS">FIG. 11</figref>, the viewing range <b>1100</b> of the camera <b>1102</b> can also be defined to account for obstacles <b>1104</b>, <b>1106</b>, <b>1108</b>. For example, in <figref idref="DRAWINGS">FIG. 11</figref>, the viewing range <b>1100</b> of the camera <b>1102</b> is blocked by a wall <b>1108</b> and, accordingly, the area <b>1110</b> that appears behind the wall is omitted from the viewing range. Similarly, a house <b>1106</b> is also in the way of the viewing range <b>1100</b> and thus the area <b>1112</b> behind the house is also omitted from the viewing range.</p>
<p id="p-0092" num="0091">In various exemplary embodiments, the viewing range for each camera is defined by the user. To this end, in illustrative embodiments, the user can use an input device to outline the viewing range <b>1100</b> for the camera <b>1102</b> within a displayed map, as shown in <figref idref="DRAWINGS">FIG. 11</figref>. In illustrative embodiments, the user can outline the viewing range of the camera by generating a plurality of line segments having starting points and end points. The user then closes the line segments to form a polygon that has vertices defined by the starting points and endpoints of the line segments. In this manner, the viewing range of the camera can be defined as relatively complex and irregular area, such as the viewing range <b>1100</b> shown in <figref idref="DRAWINGS">FIG. 11</figref>. In some cases, it may be appropriate to represent the viewing range of the camera using a plurality of polygons. In various exemplary embodiments, the vertices of the polygon are defined in terms of geospatial coordinates, however, in other embodiments, the vertices are defined in terms of a &#x201c;map&#x201d; Cartesian coordinate system or even a &#x201c;camera&#x201d; coordinate system.</p>
<p id="p-0093" num="0092">In exemplary embodiments, to determine whether the point of interest <b>1010</b> is within the viewing range of any of the cameras <b>1002</b>, <b>1004</b>, <b>1006</b>, <b>1008</b>, the method determines whether the point of interest appears within the polygonal viewing range of each of the cameras. To this end, illustrative embodiments of the present invention use various point-in-polygon algorithms known in the art. In one example of such an algorithm, the method assumes a horizontal line starting at the point of interest and continuing across the map. If the assumed horizontal line intersects the line segments of the polygon an even number of times (e.g., 0, 2, 4), then the point of interest is outside the polygon. If the assumed line intersects the polygon an odd number of times (e.g., 1, 3, 5), then the point of interest is within the polygon. In another example, the sum of the angles made between the point of interest and each vertex (or point) making up the polygon are computed. If the sum is 360 degrees (e.g., 2&#x3c0;), then the point of interest is interior to the polygon. If the sum is 0 degrees, then the point of interest is exterior to the polygon and, therefore, also exterior to the viewing range of the camera. Further details of point-in-polygon algorithms can be found in the reference: Paul Bourke, &#x201c;Determining If a Point Lies on the Interior of a Polygon&#x201d; (November 1987) (accessible at http://local.wasp.uwa.edu.au/&#x2dc;pbourke/geometry/insidepoly/), which is hereby incorporated by reference in its entirety.</p>
<p id="p-0094" num="0093">In one exemplary case, if only one camera has the viewing range to capture the point of interest, the processor provides orientation instructions to that camera to capture the point of interest. In another example, if there are at least two cameras with the viewing range to capture the point of interest, the processor determines the distances between the two cameras and the point of interest <b>906</b>. If the camera locations and the point of interest are both defined in terms of a Cartesian coordinate system (e.g., the &#x201c;map&#x201d; Cartesian coordinate system an/or the &#x201c;camera&#x201d; Cartesian coordinate system discussed above), then the distance between the cameras and the point of interest can be calculated according to Equation 17,
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>D</i>=&#x221a;{square root over ((<i>X</i><sub>C</sub><i>&#x2212;X</i><sub>P</sub>)<sup>2</sup>+(<i>Y</i><sub>C</sub><i>&#x2212;Y</i><sub>P</sub>)<sup>2</sup>+(<i>Z</i><sub>C</sub><i>&#x2212;Z</i><sub>P</sub>)<sup>2</sup>)}{square root over ((<i>X</i><sub>C</sub><i>&#x2212;X</i><sub>P</sub>)<sup>2</sup>+(<i>Y</i><sub>C</sub><i>&#x2212;Y</i><sub>P</sub>)<sup>2</sup>+(<i>Z</i><sub>C</sub><i>&#x2212;Z</i><sub>P</sub>)<sup>2</sup>)}{square root over ((<i>X</i><sub>C</sub><i>&#x2212;X</i><sub>P</sub>)<sup>2</sup>+(<i>Y</i><sub>C</sub><i>&#x2212;Y</i><sub>P</sub>)<sup>2</sup>+(<i>Z</i><sub>C</sub><i>&#x2212;Z</i><sub>P</sub>)<sup>2</sup>)}&#x2003;&#x2003;(17)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0095" num="0094">wherein (X<sub>C</sub>, Y<sub>C</sub>, Z<sub>C</sub>) are the Cartesian coordinates for the camera and (X<sub>P</sub>, Y<sub>P</sub>, Z<sub>P</sub>) are the Cartesian coordinates for the point of interest. Equation 17 is particularly accurate for use with surveillance systems that do not need to account for the curvature of the earth (e.g., surveillance systems within buildings). If the camera locations and the point of interest are both defined in terms of a geospatial coordinate system (e.g., latitude, longitude, and altitude), then the Cartesian distance can be calculated using Equations 8, 9, 10, and 11 as described in the &#x201c;Calibration&#x201d; section of the present application. The distance can then be calculated using Equation 17 as described in this section. The benefit of using Equations 8, 9, 10, and 11 is that they account for the curvature of the Earth and, thus, provide better accuracy for surveillance systems that survey, for example, outdoor terrains. Further details of calculating distances between geospatial coordinates can be found in the reference: Ivan S. Ashcroft, &#x201c;Projecting an Arbitrary Latitude and Longitude onto a Tangent Plane&#x201d; Brigham Young University (Jan. 21, 1999), which is hereby incorporated by reference in its entirety.</p>
<p id="p-0096" num="0095">Once the distances between the point of interest and the cameras are determined, the processor determines which of the cameras is the least distance from the point of interest <b>908</b>. In illustrative embodiments of the present invention, the least distance camera is then provided with orientation instruction based upon the coordinates of the point of interest and the video feed from the least distance camera is displayed on a display device <b>910</b>.</p>
<p id="p-0097" num="0096">Illustrative embodiments of the present invention are particularly beneficial when a surveillance system includes a plurality of cameras but only has a limited bandwidth and/or a limited number of display devices to display video feeds. In this case, exemplary embodiments of the present invention allow the system to prioritize the cameras and display the video feed from the camera that is closest to the point of interest and has the best view of the point of interest.</p>
<p id="p-0098" num="0097">Furthermore, illustrative embodiments of the present invention no longer rely on the user's manual control to capture a point of interest or a moving object. For example, if a target is walking in a field and suddenly hides behind a wall, then the target might not be within the viewing range of a camera located in front of the wall, but the target might still be within the viewing range of another camera located behind the wall. In some prior art systems, the user would need to manually toggle between the two cameras in order to capture the target behind the wall. Illustrative embodiments of the present invention, however, automatically prioritize and display video feeds based on the camera's range of view and distance from the target.</p>
<p id="p-0099" num="0098">View Cone</p>
<p id="p-0100" num="0099">Illustrative embodiments of the present invention are also directed to a computer-implemented method for displaying a view cone for a camera. <figref idref="DRAWINGS">FIG. 12</figref> shows a method <b>1200</b> for displaying a view cone for at least one camera in accordance with one embodiment of the present invention. <figref idref="DRAWINGS">FIG. 13</figref> shows a view cone <b>1300</b> and a camera icon <b>1302</b> in accordance with one embodiment of the present invention. In exemplary embodiments of the present invention, the view cone <b>1300</b> is representative of the field of view of the camera. Using the view cone <b>1300</b> is particularly advantageous when it is applied to a map because it helps the user understand the vantage point and perspective of the camera within an area. <figref idref="DRAWINGS">FIG. 14</figref> shows a view cone <b>1400</b> and camera icon <b>1402</b> as applied to a map <b>1404</b> in accordance with one embodiment of the present invention <b>1202</b>.</p>
<p id="p-0101" num="0100">In illustrative embodiments of the present invention, the view cone is created by transposing the effective coordinates of the upper left, upper right, lower left, and lower right corners of a video feed onto a map. <figref idref="DRAWINGS">FIG. 15</figref> shows a video feed <b>1500</b> and field of view cone <b>1502</b> in accordance with another embodiment of the present invention. The video feed <b>1500</b> includes upper left <b>1504</b>, upper right <b>1506</b>, lower left <b>1508</b>, and lower right corners <b>1510</b>. The view cone <b>1502</b> also includes upper left <b>1512</b>, upper right <b>1514</b>, lower left <b>1516</b>, and lower right vertices <b>1518</b>. In illustrative embodiments of the present invention, the view cone <b>1502</b> is created by transposing the effective coordinates of the upper left <b>1504</b>, upper right <b>1506</b>, lower left <b>1508</b>, and lower right corners <b>1510</b> of the video feed <b>1500</b> onto upper left <b>1512</b>, upper right <b>1514</b>, lower left <b>1516</b>, and lower right vertices <b>1518</b> of the view cone <b>1502</b>. The advantage of creating the view cone based upon the effective coordinates of the video feed is that, when the orientation of the camera shifts, the view cone <b>1502</b> on the map also changes to better represent the field of view of the camera.</p>
<p id="p-0102" num="0101">In illustrative embodiments of the present invention, the effective coordinates of the upper left <b>1504</b>, upper right <b>1506</b>, lower left <b>1508</b>, and lower right corners <b>1510</b> of the video feed <b>1500</b> are determined from the orientation of the camera (e.g., the pan, zoom, and tilt coordinates for the camera). For each of the upper left <b>1504</b>, upper right <b>1506</b>, lower left <b>1508</b>, and lower right corners <b>1510</b> of the video feed <b>1500</b>, effective pan and tilt angles are calculated <b>1204</b>. To determine the effective pan and tilt angles for the points, a horizontal angular field of view (H<sub>FOV</sub>) <b>1520</b> and vertical angular field of view (V<sub>FOV</sub>) <b>1522</b> is determined using, for example, Equations 1 and 2, as described in the &#x201c;Calibration&#x201d; section of the present application. Next, the effective pan and tilt angles (&#x3b8;, &#x3c6;) for each of the corners <b>1504</b>, <b>1506</b>, <b>1508</b>, and <b>1510</b> is determined using, for example, Equations 3 and 4 as described above in the &#x201c;Calibration&#x201d; section of the application.</p>
<p id="p-0103" num="0102">In exemplary embodiments of the present invention, the effective pan and tilt angles (&#x3b8;, &#x3c6;) for the corners <b>1504</b>, <b>1506</b>, <b>1508</b>, and <b>1510</b> are converted to Cartesian coordinates defined by a Cartesian coordinate system <b>1206</b>. In various embodiments of the method, the effective pan and tilt angles are converted to unit vectors (u, v, w) on a &#x201c;camera&#x201d; Cartesian coordinate system that has its origin at the camera's location. The effective pan and tilt angles can be converted into Cartesian coordinates using Equations 5, 6, and 7, as described in the &#x201c;Calibration&#x201d; section of the present application.</p>
<p id="p-0104" num="0103">To transpose the corners <b>1504</b>, <b>1506</b>, <b>1508</b>, and <b>1510</b> of the video feed <b>1502</b> to corners <b>1512</b>, <b>1514</b>, <b>1516</b>, and <b>1512</b> of the view cone <b>1500</b> on the map, a mathematical rotation (or its transform) is applied to the &#x201c;camera&#x201d; Cartesian coordinates for the corners <b>1504</b>, <b>1506</b>, <b>1508</b>, and <b>1510</b>, <b>1208</b>. The mathematical rotation (or its transform) determines unit vectors (u, v, w) defined by a &#x201c;map&#x201d; Cartesian coordinate system. The &#x201c;map&#x201d; coordinate system defines the map and the mathematical rotation provides a conversion between the &#x201c;camera&#x201d; coordinate system and the &#x201c;map&#x201d; coordinate system. In illustrative embodiments, the mathematical rotation is developed according to the method described in the &#x201c;Calibration&#x201d; section of the present application. After the mathematical rotation, the unit vectors (u, v, w) are converted to scaled Cartesian coordinates using Equations 18, 19, and 20:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>P</sub><i>=u</i>&#xd7;ScalingFactor&#x2003;&#x2003;(18)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Y</i><sub>P</sub><i>=v</i>&#xd7;ScalingFactor&#x2003;&#x2003;(19)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Z</i><sub>P</sub><i>=w</i>&#xd7;ScalingFactor&#x2003;&#x2003;(20)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0105" num="0104">In various embodiments of the present invention, the scaling factor can be determined using Equation 21,</p>
<p id="p-0106" num="0105">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>ScalingFactor</mi>
        <mo>=</mo>
        <mrow>
          <mo>&#xf603;</mo>
          <mfrac>
            <msub>
              <mi>C</mi>
              <mi>alt</mi>
            </msub>
            <mi>w</mi>
          </mfrac>
          <mo>&#xf604;</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>21</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0107" num="0106">wherein, in various embodiments, C<sub>alt </sub>is the altitude of the camera above the ground. In illustrative embodiments of the present invention, the scaled &#x201c;map&#x201d; Cartesian coordinates (X<sub>P</sub>, Y<sub>P</sub>, Z<sub>P</sub>) for the corners <b>1504</b>, <b>1506</b>, <b>1508</b>, and <b>1510</b> are used to determine the view cone <b>1500</b> and the view cone is then displayed on the map <b>1210</b>, <b>1212</b>. In more specific illustrative embodiments, as shown in <figref idref="DRAWINGS">FIG. 15</figref>, the view cone is a polygon and the &#x201c;map&#x201d; Cartesian coordinates for the corners <b>1504</b>, <b>1506</b>, <b>1508</b>, and <b>1510</b> are used to define the vertices <b>1512</b>, <b>1514</b>, <b>1516</b>, and <b>1512</b> of the view cone <b>1500</b>.</p>
<p id="p-0108" num="0107">In yet another embodiment of the present invention, the scaled &#x201c;map&#x201d; Cartesian coordinates (X<sub>P</sub>, Y<sub>P</sub>, Z<sub>P</sub>) for the corners <b>1504</b>, <b>1506</b>, <b>1508</b>, and <b>1510</b> are converted into geospatial coordinates (P<sub>lat</sub>, P<sub>lon</sub>, P<sub>alt</sub>). In various exemplary embodiments, the &#x201c;map&#x201d; Cartesian coordinates (X<sub>P</sub>, Y<sub>P</sub>, Z<sub>P</sub>) can be converted into geospatial coordinates (P<sub>lat</sub>, P<sub>lon</sub>, P<sub>alt</sub>) by determining deltas for longitude and latitude using Equations 22 and 23,</p>
<p id="p-0109" num="0108">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>&#x394;</mi>
          <mi>lat</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <mi>arcsin</mi>
          <mo>&#x2062;</mo>
          <mfrac>
            <msub>
              <mi>X</mi>
              <mi>P</mi>
            </msub>
            <msub>
              <mi>R</mi>
              <mi>lon</mi>
            </msub>
          </mfrac>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>22</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>&#x394;</mi>
          <mi>lon</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <mi>arcsin</mi>
          <mo>&#x2062;</mo>
          <mfrac>
            <mrow>
              <msub>
                <mi>Y</mi>
                <mi>P</mi>
              </msub>
              <mo>-</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mn>1</mn>
                  <mo>-</mo>
                  <mrow>
                    <mrow>
                      <mi>cos</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <msub>
                          <mi>&#x394;</mi>
                          <mi>lat</mi>
                        </msub>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>&#xd7;</mo>
                    <mrow>
                      <mi>sin</mi>
                      <mo>&#x2061;</mo>
                      <mrow>
                        <mo>(</mo>
                        <msub>
                          <mi>C</mi>
                          <mi>lat</mi>
                        </msub>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>&#xd7;</mo>
                    <msub>
                      <mi>R</mi>
                      <mi>lon</mi>
                    </msub>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
            <msub>
              <mi>R</mi>
              <mi>E</mi>
            </msub>
          </mfrac>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>23</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0110" num="0109">wherein C<sub>lat </sub>is the known latitudinal geospatial coordinate for the camera, R<sub>E </sub>can be calculated from Equation 11 above, and R<sub>lon </sub>is unknown, but can be estimated as approximately equal to the longitudinal coordinate of the camera &#x201c;C<sub>lon</sub>&#x201d;. Further details for converting Cartesian coordinates into geospatial coordinates can be found in the reference: Ivan S. Ashcroft, &#x201c;Projecting an Arbitrary Latitude and Longitude onto a Tangent Plane&#x201d; Brigham Young University (Jan. 21, 1999).</p>
<p id="p-0111" num="0110">In illustrative embodiments of the present invention, once the deltas &#x201c;&#x394;<sub>lat </sub>and &#x394;<sub>lat</sub>&#x201d; are determined, they can be used to determine the geospatial coordinates (P<sub>lat</sub>, P<sub>lon</sub>, P<sub>alt</sub>) for the corners <b>1504</b>, <b>1506</b>, <b>1508</b>, and <b>1510</b> using Equations 24, 25, and 26,
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>lat</sub><i>=C</i><sub>lat</sub>+&#x394;<sub>lat</sub>&#x2003;&#x2003;(24)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>lon</sub><i>=C</i><sub>lon</sub>+&#x394;<sub>lon</sub>&#x2003;&#x2003;(25)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i><sub>alt</sub><i>=C</i><sub>alt</sub>&#x2003;&#x2003;(26)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0112" num="0111">wherein (C<sub>lat</sub>, C<sub>lon</sub>, C<sub>alt</sub>) are the known geospatial coordinates of the camera. Accordingly, in such an embodiment, the geospatial coordinates (P<sub>lat</sub>, P<sub>lon</sub>, P<sub>alt</sub>) for the corners <b>1504</b>, <b>1506</b>, <b>1508</b>, and <b>1510</b> are used to determine the view cone <b>1500</b> and the view cone is then displayed on the map. In more specific illustrative embodiments, the view cone is a polygon and the geospatial coordinates for the corners <b>1504</b>, <b>1506</b>, <b>1508</b>, and <b>1510</b> are used to define the vertices <b>1512</b>, <b>1514</b>, <b>1516</b>, and <b>1512</b> of the view cone <b>1500</b>. <figref idref="DRAWINGS">FIG. 16</figref> shows a video image <b>1600</b> with four corners transposed into a polygonal view cone <b>1602</b> with four vertices in accordance with one embodiment of the present invention.</p>
<p id="p-0113" num="0112">In further illustrative embodiments of the present invention, the upper left <b>1512</b> and upper right vertices <b>1514</b> of the view cone <b>1500</b> are determined in a different manner. In some cases, when the tilt angle for the camera (or the effective tilt angle for at least one of the corners of the video feed) is above the horizon (e.g., &#x3c6; is greater than 0), the transposition of the corners onto the map is not possible because the effective tilt angles for at least some of the corners never intersect the ground.</p>
<p id="p-0114" num="0113">One illustrative method of resolving this issue, is to determine the upper left <b>1512</b> and upper right vertices <b>1514</b> of the view cone <b>1500</b> based upon a resolvable distance of the camera. The resolvable distance can be calculated using the resolution of the camera. Such an approach is advantageous because it better represents the true view field of the camera. In one illustrative embodiment, the view cone <b>1500</b> is displayed so that objects within the view cone can be identified in the video feed (e.g., objects within the resolvable distance), while objects beyond the view cone cannot be distinguished from the background (e.g., objects beyond the resolvable distance). In one illustrative embodiment, the resolvable distance corresponds to a distance at which a 4 meter long object (approximately the size of a vehicle) corresponds to two pixels on the display. In one embodiment, the resolvable distance is determined using Equation 27,</p>
<p id="p-0115" num="0114">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>D</mi>
        <mo>=</mo>
        <mrow>
          <mo>&#xf603;</mo>
          <mfrac>
            <mrow>
              <mfrac>
                <mi>W</mi>
                <mn>2</mn>
              </mfrac>
              <mo>&#xd7;</mo>
              <mi>R</mi>
            </mrow>
            <mrow>
              <mi>tan</mi>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mi>&#x398;</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mfrac>
          <mo>&#xf604;</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>27</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
<br/>
wherein W is the width of the video feed in pixels, R is the resolution of the camera in meters per pixel, and &#x398; is the horizontal angular field of view (H<sub>FOV</sub>) <b>1520</b> divided by two. In some embodiments, the width of the video feed is communicated to a processor by the camera. In other illustrative embodiments, the width of the video feed is a fixed number, such as 320 pixels. <figref idref="DRAWINGS">FIG. 11</figref> shows how the resolvable distance is calculated using a right triangle in accordance with one embodiment of the present invention. The calculated resolvable distance is then used to determine the upper left <b>1512</b> and upper right vertices <b>1514</b> of the view cone <b>1500</b>.
</p>
<p id="p-0116" num="0115">In one illustrative embodiment, the resolvable distance is projected along the effective pan angles (&#x3b8;) for the upper left <b>1504</b> and upper right <b>1506</b> corners of the video feed. The &#x201c;map&#x201d; Cartesian coordinate (X<sub>P</sub>, Y<sub>P</sub>, Z<sub>P</sub>) for the upper left <b>1512</b> and upper right <b>1514</b> vertices can be determined according to Equations 28, 29 and 30,
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>P</sub>=(sin(&#x3b8;)&#xd7;<i>D</i>)+<i>X</i><sub>C</sub>&#x2003;&#x2003;(28)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Y</i><sub>P</sub>=(cos(&#x3b8;)&#xd7;<i>D</i>)+<i>Y</i><sub>C</sub>&#x2003;&#x2003;(29)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Z</i><sub>P</sub><i>=Z</i><sub>C</sub>&#x2003;&#x2003;(30)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
wherein (X<sub>C</sub>, Y<sub>C</sub>, Z<sub>C</sub>) are the &#x201c;map&#x201d; Cartesian coordinates for the camera. In illustrative embodiments of the present invention, the &#x201c;map&#x201d; Cartesian coordinates (X<sub>P</sub>, Y<sub>P</sub>, Z<sub>P</sub>) for upper left <b>1512</b> and upper right <b>1514</b> vertices are then used to determine and display the view cone on the map. In other illustrative embodiments, these &#x201c;map&#x201d; Cartesian coordinates are converted into geospatial coordinates (e.g., using Equations 22-26) and then used to determine and display the view cone on the map.
</p>
<p id="p-0117" num="0116">In another illustrative embodiment, instead of projecting the resolvable distance along the effective pan angles (&#x3b8;) for the corners <b>1504</b> and <b>1506</b>, the resolvable distance is projected along the pan angle (&#x3b8;) of the camera to a resolvable point. In some embodiments, the resolvable point is the center of the most distant edge of the view cone <b>1502</b>. <figref idref="DRAWINGS">FIG. 18</figref> shows a view cone <b>1800</b> with the center <b>1802</b> of its most distant edge as the resolvable point, in accordance with one embodiment of the present invention. The &#x201c;map&#x201d; Cartesian coordinate (X<sub>R</sub>, Y<sub>R</sub>, Z<sub>R</sub>) for the resolvable point can be determined according to Equations 31, 32 and 33:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>X</i><sub>R</sub>=(sin(&#x3b8;)&#xd7;<i>D</i>)+<i>X</i><sub>C</sub>&#x2003;&#x2003;(31)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Y</i><sub>R</sub>=(cos(&#x3b8;)&#xd7;<i>D</i>)+<i>Y</i><sub>C</sub>&#x2003;&#x2003;(32)<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>Z</i><sub>R</sub><i>=Z</i><sub>C</sub>&#x2003;&#x2003;(33)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0118" num="0117">The resolvable point (X<sub>R</sub>, Y<sub>R</sub>, Z<sub>R</sub>) is then rotated left and right by half the horizontal field of view (H<sub>FOV</sub>) <b>1520</b> to determine the &#x201c;map&#x201d; Cartesian coordinates (X<sub>P</sub>, Y<sub>P</sub>, Z<sub>P</sub>) for upper left <b>1804</b> and upper right <b>1806</b> vertices of the view cone. For the sake of computational efficiency in the processor, a matrix can be used to rotate the resolvable point. The &#x201c;map&#x201d; Cartesian coordinates (X<sub>P</sub>, Y<sub>P</sub>, Z<sub>P</sub>) for upper left <b>1804</b> and upper right <b>1806</b> vertices are then used to determine and display the view cone on the map. In other illustrative embodiments, however, the &#x201c;map&#x201d; Cartesian coordinates are converted into geospatial coordinates and then used to determine and display the view cone on the map.</p>
<p id="p-0119" num="0118">In yet further illustrative embodiments, the projections discussed in Equations 28-33 of this section are performed in the &#x201c;camera&#x201d; Cartesian Coordinate system and are then converted into the &#x201c;map&#x201d; Cartesian coordinate system using a mathematical transformation (or its transform).</p>
<p id="p-0120" num="0119">Illustrative embodiments of the present invention are not limited for use with cameras that are characterized by pan, zoom, and tilt coordinates (e.g., pan-zoom-tilt cameras). For example, illustrative embodiments of the present invention may also be used with fixed cameras or other cameras that have a limited range of motion. Indeed, the above described methods of calibration and prioritization apply similarly to fixed cameras. Also, according to illustrative embodiments of the present invention, the view cone of the fixed camera can be modified as its zoom changes. Various embodiments of the present invention also have applicability to other types of cameras, such as cameras that are unable to shift their tilt orientation.</p>
<p id="p-0121" num="0120">It should be noted that terms such as &#x201c;system&#x201d;, &#x201c;processor&#x201d;, &#x201c;server&#x201d;, &#x201c;input device&#x201d;, &#x201c;display device&#x201d;, &#x201c;communications network&#x201d;, and &#x201c;database&#x201d; may be used herein to describe devices that may be used in certain embodiments of the present invention and should not be construed to limit the present invention to any particular device type or system unless the context otherwise requires. Thus, a system may include, without limitation, a client, server, computer, appliance, or other type of device. Such devices typically include one or more network interfaces for communicating over a communication network (e.g., computer network) and a processor (e.g., a microprocessor with memory and other peripherals and/or application-specific hardware) configured accordingly to perform device and/or system functions. Communication networks generally may include public and/or private networks; may include local-area, wide-area, metropolitan-area, storage, and/or other types of networks; and may employ communication technologies including, but in no way limited to, analog technologies, digital technologies, optical technologies, wireless technologies, networking technologies, and internetworking technologies.</p>
<p id="p-0122" num="0121">It should also be noted that devices may use communication protocols and messages (e.g., messages created, transmitted, received, stored, and/or processed by the system), and such messages may be conveyed by a communication network or medium. Unless the context otherwise requires, the present invention should not be construed as being limited to any particular communication message type, communication message format, or communication protocol. Thus, a communication message generally may include, without limitation, a frame, packet, datagram, user datagram, cell, or other type of communication message. Unless the context requires otherwise, references to specific communication protocols are exemplary, and it should be understood that alternative embodiments may, as appropriate, employ variations of such communication protocols (e.g., modifications or extensions of the protocol that may be made from time-to-time) or other protocols either known or developed in the future.</p>
<p id="p-0123" num="0122">It should also be noted that logic flows may be described herein to demonstrate various aspects of the invention, and should not be construed to limit the present invention to any particular logic flow or logic implementation. The described logic may be partitioned into different logic blocks (e.g., programs, modules, interfaces, functions, or subroutines) without changing the overall results or otherwise departing from the true scope of the invention. Often times, logic elements may be added, modified, omitted, performed in a different order, or implemented using different logic constructs (e.g., logic gates, looping primitives, conditional logic, and other logic constructs) without changing the overall results or otherwise departing from the true scope of the invention.</p>
<p id="p-0124" num="0123">The present invention may be embodied in many different forms, including, but in no way limited to, computer program logic for use with a processor (e.g., a microprocessor, microcontroller, digital signal processor, or general purpose computer), programmable logic for use with a programmable logic device (e.g., a Field Programmable Gate Array (FPGA) or other PLD), discrete components, integrated circuitry (e.g., an Application Specific Integrated Circuit (ASIC)), or any other means including any combination thereof. In a typical embodiment of the present invention, predominantly all of the described logic is implemented as a set of computer program instructions that is converted into a computer executable form, stored as such in a computer readable medium, and executed by a microprocessor under the control of an operating system.</p>
<p id="p-0125" num="0124">Computer program logic implementing all or part of the functionality previously described herein may be embodied in various forms, including, but in no way limited to, a source code form, a computer executable form, and various intermediate forms (e.g., forms generated by an assembler, compiler, linker, or locator). Source code may include a series of computer program instructions implemented in any of various programming languages (e.g., an object code, an assembly language, or a high-level language such as Fortran, C, C++, JAVA, or HTML) for use with various operating systems or operating environments. The source code may define and use various data structures and communication messages. The source code may be in a computer executable form (e.g., via an interpreter), or the source code may be converted (e.g., via a translator, assembler, or compiler) into a computer executable form.</p>
<p id="p-0126" num="0125">The computer program may be fixed in any form (e.g., source code form, computer executable form, or an intermediate form) either permanently or transitorily in a tangible storage medium, such as a semiconductor memory device (e.g., a RAM, ROM, PROM, EEPROM, or Flash-Programmable RAM), a magnetic memory device (e.g., a diskette or fixed disk), an optical memory device (e.g., a CD-ROM), a PC card (e.g., PCMCIA card), or other memory device. The computer program may be fixed in any form in a signal that is transmittable to a computer using any of various communication technologies, including, but in no way limited to, analog technologies, digital technologies, optical technologies, wireless technologies, networking technologies, and internetworking technologies. The computer program may be distributed in any form as a removable storage medium with accompanying printed or electronic documentation (e.g., shrink wrapped software), preloaded with a computer system (e.g., on system ROM or fixed disk), or distributed from a server or electronic bulletin board over the communication system (e.g., the Internet or World Wide Web).</p>
<p id="p-0127" num="0126">Hardware logic (including programmable logic for use with a programmable logic device) implementing all or part of the functionality previously described herein may be designed using traditional manual methods, or may be designed, captured, simulated, or documented electronically using various tools, such as Computer Aided Design (CAD), a hardware description language (e.g., VHDL or AHDL), or a PLD programming language (e.g., PALASM, ABEL, or CUPL).</p>
<p id="p-0128" num="0127">Programmable logic may be fixed either permanently or transitorily in at least one tangible storage medium, such as a semiconductor memory device (e.g., a RAM, ROM, PROM, EEPROM, or Flash-Programmable RAM), a magnetic memory device (e.g., a diskette or fixed disk), an optical memory device (e.g., a CD-ROM), or other memory device. The programmable logic may be fixed in a signal that is transmittable to a computer using any of various communication technologies, including, but in no way limited to, analog technologies, digital technologies, optical technologies, wireless technologies (e.g., Bluetooth), networking technologies, and internetworking technologies. The programmable logic may be distributed as a removable storage medium with accompanying printed or electronic documentation (e.g., shrink wrapped software), preloaded with a computer system (e.g., on system ROM or fixed disk), or distributed from a server or electronic bulletin board over the communication system (e.g., the Internet or World Wide Web). Of course, some embodiments of the invention may be implemented as a combination of both software (e.g., a computer program product) and hardware. Still other embodiments of the invention are implemented as entirely hardware, or entirely software.</p>
<p id="p-0129" num="0128">The embodiments of the invention described above are intended to be merely exemplary; numerous variations and modifications will be apparent to those skilled in the art. All such variations and modifications are intended to be within the scope of the present invention as defined in any appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08624709-20140107-M00001.NB">
<img id="EMI-M00001" he="12.02mm" wi="76.20mm" file="US08624709-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08624709-20140107-M00002.NB">
<img id="EMI-M00002" he="7.03mm" wi="76.20mm" file="US08624709-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08624709-20140107-M00003.NB">
<img id="EMI-M00003" he="7.03mm" wi="76.20mm" file="US08624709-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08624709-20140107-M00004.NB">
<img id="EMI-M00004" he="12.70mm" wi="76.20mm" file="US08624709-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08624709-20140107-M00005.NB">
<img id="EMI-M00005" he="6.69mm" wi="76.20mm" file="US08624709-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08624709-20140107-M00006.NB">
<img id="EMI-M00006" he="14.82mm" wi="76.20mm" file="US08624709-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08624709-20140107-M00007.NB">
<img id="EMI-M00007" he="9.57mm" wi="76.20mm" file="US08624709-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method for calibrating at least one camera, the system including at least one camera, the method comprising:
<claim-text>displaying a video feed from the at least one camera, the at least one camera having an orientation characterized by pan, zoom, and tilt coordinates;</claim-text>
<claim-text>displaying a map of an area, the map being characterized by geospatial coordinates;</claim-text>
<claim-text>allowing a user to select at least three pairs of points using at least one input device, a first point of the pair being selected in the map and a second point of the pair being selected from the video feed, the first point and the second point corresponding to the same geographic location;</claim-text>
<claim-text>converting, in a computer process, the at least three points selected in the map from geospatial coordinates into Cartesian coordinates in three dimensions defined by a first three-dimensional coordinate system;</claim-text>
<claim-text>converting, in a computer process, the at least three points selected in the video feed from pan, zoom, and tilt coordinates into Cartesian coordinates in three dimensions defined by a second three-dimensional coordinate system; and</claim-text>
<claim-text>determining, in a computer process, a mathematical rotation between the first three-dimensional coordinate system and the second three-dimensional coordinate system based upon the Cartesian coordinates for the at least three pairs of points.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the mathematical rotation is a matrix.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>displaying the location of the at least one camera on the map.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the geospatial coordinates are latitude, longitude, and altitude coordinates.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the input device is at least one of a mouse, a cursor, a crosshair, a touch screen, and a keyboard.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>allowing the user to select at least one point in the map using the at least one input device;</claim-text>
<claim-text>converting, in a computer process, the geospatial coordinates for the selected point into Cartesian coordinates in three dimensions defined by the first three-dimensional coordinate system;</claim-text>
<claim-text>applying, in a computer process, the rotation to the Cartesian coordinates for the selected point to determine Cartesian coordinates in three dimensions defined by the second three-dimensional coordinate system;</claim-text>
<claim-text>converting, in a computer process, the Cartesian coordinates defined by the second three-dimensional coordinate system into pan and tilt coordinates for the selected point; and</claim-text>
<claim-text>providing orientation instructions to the at least one camera based upon the pan and tilt coordinates for the selected point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>receiving coordinates from a sensor for at least one target;</claim-text>
<claim-text>if the coordinates for the at least one target are not Cartesian coordinates defined by the first three-dimensional coordinate system, converting, in a computer process, the coordinates into Cartesian coordinates in three dimensions defined by the first three-dimensional coordinate system;</claim-text>
<claim-text>applying, in a computer process, the rotation to the Cartesian coordinates defined by the first three-dimensional coordinate system to determine Cartesian coordinates in three dimensions defined by the second three-dimensional coordinate system;</claim-text>
<claim-text>converting, in a computer process, the Cartesian coordinates defined by the second three-dimensional coordinate system into pan and tilt coordinates; and</claim-text>
<claim-text>providing orientation instructions to the at least one camera based upon the pan and tilt coordinates.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. A method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>displaying the location of the at least one sensor on the map.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>displaying the location of the at least one target on the map.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the video feed has upper left, upper right, lower left, and lower right corners, and the method further comprising:
<claim-text>determining, in a computer process, effective pan and tilt angles for at least the lower left and lower right corners of the video feed based upon the pan, zoom, and tilt coordinates for the camera orientation;</claim-text>
<claim-text>converting, in a computer process, the effective pan and tilt angles for at least the lower left and lower right corners of the video feed into Cartesian coordinates in three dimensions defined by the second three-dimensional coordinate system;</claim-text>
<claim-text>applying, in a computer process, the rotation to the Cartesian coordinates defined by the second coordinate system to determine Cartesian coordinates in three dimensions defined by the first coordinate system for at least the lower left and lower right corners of the video feed;</claim-text>
<claim-text>determining, in a computer process, a view cone using the Cartesian coordinates defined by the first coordinate system for at least the lower left and lower right corners of the video feed;</claim-text>
<claim-text>determining, in a computer process, the view cone based upon the upper left and upper right corners of the video feed; and</claim-text>
<claim-text>displaying the view cone on the map.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein, when the tilt coordinate for the camera is below the horizon, determining the view cone based upon the upper left and upper right corners of the video feed comprises:
<claim-text>determining, in a computer process, effective pan for the upper left and upper right corners of the video feed based upon the pan, zoom, and tilt coordinates for the camera orientation; and</claim-text>
<claim-text>converting, in a computer process, the effective pan and tilt angles for the upper left and upper right corners of the video feed into Cartesian coordinates in three dimensions defined by the second three-dimensional coordinate system;</claim-text>
<claim-text>applying, in a computer process, the mathematical rotation to the Cartesian coordinates to determine Cartesian coordinates in three dimensions defined by the first three-dimensional coordinate system for the upper left and upper right corners of the video feed; and</claim-text>
<claim-text>determining, in a computer process, the view cone based upon the Cartesian coordinates, defined by the first three-dimensional coordinate system, for the upper left, upper right, lower left and lower right corners.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. A method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the view cone is a polygon and the Cartesian coordinates, defined by the first three-dimensional coordinate system, for the upper left, upper right, lower left and lower right corners are the vertices of the polygon.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein, when the tilt coordinate for the camera is above the horizon, determining the view cone based upon the upper left and upper right corners of the video feed comprises:
<claim-text>determining, in a computer process, effective tilt angles for the upper left and upper right corners of the video feed based upon the pan, zoom, and tilt coordinates for the camera;</claim-text>
<claim-text>determining, in a computer process, coordinates in three dimensions, defined by the first three-dimensional coordinate system, for the upper left and upper right corners of the video feed based upon a resolvable distance of the camera; and</claim-text>
<claim-text>determining, in a computer process, the view cone based upon the Cartesian coordinates, defined by the first three-dimensional coordinate system, for the upper left, upper right, lower left and lower right corners.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. A method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the view cone is a polygon and the Cartesian coordinates, defined by the first three-dimensional coordinate system, for the upper left, upper right, lower left and lower right corners are the vertices of the polygon.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. Apparatus comprising at least one non-transitory computer readable medium encoded with instructions which when loaded on at least one computer, establish processes for calibrating at least one camera orientation characterized by pan, zoom, and tilt coordinates, the processes including:
<claim-text>allowing a user to select at least three pairs of points using at least one input device, a first point of the pair being selected in a map characterized by geospatial coordinates and a second point of the pair being selected from a video feed from the at least one camera, the first point and the second point corresponding to the same geographic location;</claim-text>
<claim-text>converting the at least three points selected in the map from geospatial coordinates into Cartesian coordinates in three dimensions defined by a first three-dimensional coordinate system;</claim-text>
<claim-text>converting the at least three points selected in the video feed from pan, zoom, and tilt coordinates into Cartesian coordinates in three dimensions defined by a second three-dimensional coordinate system; and</claim-text>
<claim-text>determining a mathematical rotation between the first three-dimensional coordinate system and the second three-dimensional coordinate system based upon the Cartesian coordinates for the at least three pairs of points.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. An apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions establish processes further including:
<claim-text>converting the geospatial coordinates for a selected point into Cartesian coordinates defined by the first three-dimensional coordinate system;</claim-text>
<claim-text>applying the rotation to the Cartesian coordinates for the selected point to determine Cartesian coordinates defined by the second three-dimensional coordinate system;</claim-text>
<claim-text>converting the Cartesian coordinates defined by the second three-dimensional coordinate system into pan and tilt coordinates for the selected point; and</claim-text>
<claim-text>providing orientation instructions to the at least one camera based upon the pan and tilt coordinates for the selected point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. An apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions establish processes further including:
<claim-text>receiving coordinates from a sensor for at least one target;</claim-text>
<claim-text>if the coordinates for the at least one target are not Cartesian coordinates defined by the first three-dimensional coordinate system, converting the coordinates into Cartesian coordinates defined by the first three-dimensional coordinate system;</claim-text>
<claim-text>applying the rotation to the Cartesian coordinates defined by the first three-dimensional coordinate system to determine Cartesian coordinates defined by the second three-dimensional coordinate system;</claim-text>
<claim-text>converting the Cartesian coordinates defined by the second three-dimensional coordinate system into pan and tilt coordinates; and</claim-text>
<claim-text>providing orientation instructions to the at least one camera based upon the pan and tilt coordinates.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. An apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions establish processes further including:
<claim-text>determining effective pan and tilt angles for at least the lower left and lower right corners of the video feed based upon the pan, zoom, and tilt coordinates for the camera orientation;</claim-text>
<claim-text>converting the effective pan and tilt angles for at least the lower left and lower right corners of the video feed into Cartesian coordinates defined by the second three-dimensional coordinate system;</claim-text>
<claim-text>applying the rotation to the Cartesian coordinates defined by the second three-dimensional coordinate system to determine Cartesian coordinates defined by the first three-dimensional coordinate system for at least the lower left and lower right corners of the video feed;</claim-text>
<claim-text>determining a view cone using the Cartesian coordinates defined by the first three-dimensional coordinate system for at least the lower left and lower right corners of the video feed;</claim-text>
<claim-text>determining the view cone based upon the upper left and upper right corners of the video feed; and</claim-text>
<claim-text>displaying the view cone on the map.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. A surveillance system comprising:
<claim-text>at least one camera having an orientation characterized by pan, zoom and tilt coordinates;</claim-text>
<claim-text>a processor in communication with the at least one camera;</claim-text>
<claim-text>at least one display in communication with the processor, the at least one display displaying a video feed from the at least one camera and a map of an area, the map being characterized by geospatial coordinates;</claim-text>
<claim-text>at least one input device in communication with the processor allowing a user to select points on the video feed and the map;</claim-text>
<claim-text>a memory storing instructions executable by the processor to perform processes that include:
<claim-text>converting points selected in the map from the geospatial coordinates into Cartesian coordinates in three dimensions defined by a first three-dimensional coordinate system;</claim-text>
<claim-text>converting points selected in the video feed from pan, zoom, and tilt coordinates into Cartesian coordinates in three dimensions defined by a second three-dimensional coordinate system; and</claim-text>
<claim-text>determining a mathematical rotation between the first three-dimensional coordinate system and the second three-dimensional coordinate system based upon the Cartesian coordinates for at least three pairs of points, a first point of the pair having been selected in the map and a second point of the pair having been selected from the video feed.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The surveillance system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the mathematical rotation is a matrix.</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The surveillance system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the geospatial coordinates are latitude, longitude, and altitude coordinates.</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The surveillance system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the at least one input device is at least one of a mouse, a cursor, a crosshair, a touch screen, and a keyboard.</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The surveillance system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the memory further stores instructions executable by the processor to perform processes that include:
<claim-text>applying the mathematical rotation to the Cartesian coordinates defined by the first three-dimensional coordinate system for a point selected by the user in the map to determine Cartesian coordinates defined by the second three-dimensional coordinate system;</claim-text>
<claim-text>converting the Cartesian coordinates defined by the second three-dimensional coordinate system into pan and tilt coordinates for the selected point; and</claim-text>
<claim-text>providing orientation instructions to the at least one camera based upon the pan and tilt coordinates for the selected point.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. The surveillance system of <claim-ref idref="CLM-00019">claim 19</claim-ref> further comprising a sensor connected to provide coordinates to the processor. </claim-text>
</claim>
</claims>
</us-patent-grant>
