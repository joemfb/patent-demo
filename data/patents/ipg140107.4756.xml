<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625849-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625849</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13364077</doc-number>
<date>20120201</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>36</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
<further-classification>382154</further-classification>
</classification-national>
<invention-title id="d2e51">Multiple camera control system</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4144449</doc-number>
<kind>A</kind>
<name>Funk et al.</name>
<date>19790300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4247767</doc-number>
<kind>A</kind>
<name>OBrien et al.</name>
<date>19810100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>4507557</doc-number>
<kind>A</kind>
<name>Tsikos</name>
<date>19850300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>4742221</doc-number>
<kind>A</kind>
<name>Sasaki et al.</name>
<date>19880500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>4746770</doc-number>
<kind>A</kind>
<name>McAvinney</name>
<date>19880500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>4782328</doc-number>
<kind>A</kind>
<name>Denlinger</name>
<date>19881100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>4818826</doc-number>
<kind>A</kind>
<name>Kimura</name>
<date>19890400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5097516</doc-number>
<kind>A</kind>
<name>Amir</name>
<date>19920300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5109435</doc-number>
<kind>A</kind>
<name>Lo et al.</name>
<date>19920400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5317140</doc-number>
<kind>A</kind>
<name>Dunthorn</name>
<date>19940500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>5483603</doc-number>
<kind>A</kind>
<name>Luke et al.</name>
<date>19960100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>5502568</doc-number>
<kind>A</kind>
<name>Ogawa et al.</name>
<date>19960300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>5534917</doc-number>
<kind>A</kind>
<name>MacDougall</name>
<date>19960700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>5581276</doc-number>
<kind>A</kind>
<name>Cipolla et al.</name>
<date>19961200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>5638092</doc-number>
<kind>A</kind>
<name>Eng et al.</name>
<date>19970600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>5769640</doc-number>
<kind>A</kind>
<name>Jacobus et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>5844392</doc-number>
<kind>A</kind>
<name>Peurach et al.</name>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>5907328</doc-number>
<kind>A</kind>
<name>Brush, II et al.</name>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>5911004</doc-number>
<kind>A</kind>
<name>Ohuchi et al.</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>5936615</doc-number>
<kind>A</kind>
<name>Waters</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6002808</doc-number>
<kind>A</kind>
<name>Freeman</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6008798</doc-number>
<kind>A</kind>
<name>Mato, Jr. et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6072494</doc-number>
<kind>A</kind>
<name>Nguyen</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>6075895</doc-number>
<kind>A</kind>
<name>Qiao et al.</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>6100538</doc-number>
<kind>A</kind>
<name>Ogawa</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>6173066</doc-number>
<kind>B1</kind>
<name>Peurach et al.</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>6191773</doc-number>
<kind>B1</kind>
<name>Maruno et al.</name>
<date>20010200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>6208330</doc-number>
<kind>B1</kind>
<name>Hasegawa et al.</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>6215890</doc-number>
<kind>B1</kind>
<name>Matsuo et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>6222465</doc-number>
<kind>B1</kind>
<name>Kumar et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>6256033</doc-number>
<kind>B1</kind>
<name>Nguyen</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>6256400</doc-number>
<kind>B1</kind>
<name>Takata et al.</name>
<date>20010700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>6335724</doc-number>
<kind>B1</kind>
<name>Takekawa et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>6414671</doc-number>
<kind>B1</kind>
<name>Gillespie et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>6421042</doc-number>
<kind>B1</kind>
<name>Omura et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>6421048</doc-number>
<kind>B1</kind>
<name>Shih et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>6429856</doc-number>
<kind>B1</kind>
<name>Omura et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>6531999</doc-number>
<kind>B1</kind>
<name>Trajkovic</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>6563491</doc-number>
<kind>B1</kind>
<name>Omura</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>6594023</doc-number>
<kind>B1</kind>
<name>Omura et al.</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>6614422</doc-number>
<kind>B1</kind>
<name>Rafii et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>6993179</doc-number>
<kind>B1</kind>
<name>Weinshall et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2001/0019325</doc-number>
<kind>A1</kind>
<name>Takekawa</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2001/0020933</doc-number>
<kind>A1</kind>
<name>Maggioni</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2001/0022579</doc-number>
<kind>A1</kind>
<name>Hirabayashi</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2001/0026268</doc-number>
<kind>A1</kind>
<name>Ito</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2002/0015024</doc-number>
<kind>A1</kind>
<name>Westerman et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2002/0015064</doc-number>
<kind>A1</kind>
<name>Robotham et al.</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2003/0112228</doc-number>
<kind>A1</kind>
<name>Gillespie et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2004/0012573</doc-number>
<kind>A1</kind>
<name>Morrison et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>DE</country>
<doc-number>19810452</doc-number>
<kind>A1</kind>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>EP</country>
<doc-number>0905644</doc-number>
<kind>A2</kind>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>JP</country>
<doc-number>57211637</doc-number>
<kind>A</kind>
<date>19821200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>JP</country>
<doc-number>8240407</doc-number>
<kind>A</kind>
<date>19960900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>JP</country>
<doc-number>9091094</doc-number>
<kind>A</kind>
<date>19970400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>JP</country>
<doc-number>9319501</doc-number>
<kind>A</kind>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>WO</country>
<doc-number>WO9935633</doc-number>
<kind>A2</kind>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>WO</country>
<doc-number>WO9940562</doc-number>
<kind>A1</kind>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00059">
<othercit>Bud K. Funk,&#x201c; CCDs in Optical Touch Panels Deliver High Resolution&#x201d;, Electronic Design, Sep. 1980.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00060">
<othercit>Christopher eveland et al., &#x201c;Background Modeling for Segmentation of Video-Rate Stereo Sequences&#x201d;, Computer Science Department, University of Rochester, Rochester, NY, 1998, pp. 266-271, XP002229566.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00061">
<othercit>International Search Report&#x2014;PCT/US2001/030840&#x2014;ISA/EPO&#x2014;Feb. 24, 2003.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00062">
<othercit>Jennings, C. &#x201c;Robust Finger Tracking with Multiple Cameras&#x201d;, Proceedings, International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems, Jul. 20, 1999, (http://www.cs.ubc.ca/spider/jennings/ratfg-rts99/cj99.html).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00063">
<othercit>Morrison, et al,. &#x201c;Machine Vision Passive Touch Technology for Interactive Displays,&#x201d; SID 01 Digest, Jun. 2001, pp. 74-77.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00064">
<othercit>Nebojsa Jojie et al .,&#x201c;Detection and Estimation of Pointing Gestures in Dense Disparity Maps&#x201d;, Fourth IEEE International Conference on Automatic Face and Gesture Recognition, Mar. 28-30, 2000, ECE Dept. &#x26; Beckman Institute; University of Illinois, Urbana, pp. 468-475, XP10378301.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00065">
<othercit>Okutomi, et al., &#x201c;A Multiple-baseline Stereo,&#x201d; IEEE Transactions of Pattern Analysis and machine Intelligence, vol. 14, No. 4, Apr. 1993, pp. 355-363.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00066">
<othercit>Scharstein D. et al., &#x201c;Stereo Matching with Non-Linear Diffusion&#x201d; which was published in Proceedings of the 1996 Conference on Computer Vision and Pattern Recognition (CVPR '96) middlebury.edu/.about.schar/papers/diffusion.ps.gz, 1996.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00067">
<othercit>Toyama et al., &#x201c;Wallflower: Principles and Practice of Background Maintenance&#x201d;, The Proceedings of the Seventh IEEE International Conference on Kerkyra, Computer Vision 1999, vol. 1, pp. 255-261. XP-002229565.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00068">
<othercit>Urmson, &#x201c;A Comparison of Pt. Grey Research's Digiclops and Videre Design's Small Vision System for Sensing on the Hypeion Robot,&#x201d; (http://www.videredesign.com), Nov. 2000.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00069">
<othercit>Zhang, &#x201c;A Flexible New Technique for Camera Calibration&#x201d;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, No. 11, pp. 1330-1334, 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00070">
<othercit>Zhang, &#x201c;Determining the Epipolar Geometry and its Uncertainty&#x201d;, International Journal of Computer Vision, vol. 27, No. 2, pp. 161-198, 1998.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00071">
<othercit>Small Vision Systems Stereo Developer Kit, http://www.videredesign.com., Oct. 19, 2011, 4 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>24</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382103</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382106</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382107</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382154</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382286</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382291</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382292</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 42</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 47</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348 48</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348143</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348153</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>348159</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345863</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>16</number-of-drawing-sheets>
<number-of-figures>26</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12493958</doc-number>
<date>20090629</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8131015</doc-number>
<date>20120206</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13364077</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11932869</doc-number>
<date>20071031</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7555142</doc-number>
<date>20090630</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12493958</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11304000</doc-number>
<date>20051219</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7421093</doc-number>
<date>20080902</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11932869</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>09962612</doc-number>
<date>20010926</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7058204</doc-number>
<date>20060606</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11304000</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60237187</doc-number>
<date>20001003</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120206337</doc-number>
<kind>A1</kind>
<date>20120816</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hildreth</last-name>
<first-name>Evan</first-name>
<address>
<city>Ottawa</city>
<country>CA</country>
</address>
</addressbook>
<residence>
<country>CA</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Macdougall</last-name>
<first-name>Francis</first-name>
<address>
<city>Ottawa</city>
<country>CA</country>
</address>
</addressbook>
<residence>
<country>CA</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Hildreth</last-name>
<first-name>Evan</first-name>
<address>
<city>Ottawa</city>
<country>CA</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Macdougall</last-name>
<first-name>Francis</first-name>
<address>
<city>Ottawa</city>
<country>CA</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Kilpatrick Townsend &#x26; Stockton LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Qualcomm Incorporated</orgname>
<role>02</role>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Couso</last-name>
<first-name>Yon</first-name>
<department>2665</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A multiple camera tracking system for interfacing with an application program is provided. The tracking system includes multiple cameras arranged to provide different viewpoints of a region of interest, and are operable to produce a series of video images. A processor is operable to receive the series of video images and detect objects appearing in the region of interest. The processor executes a process to generate a background data set from the video images, generate an image data set for each received video image, compare each image data set to the background data set to produce a difference map for each image data set, detect a relative position of an object of interest within each difference map, and produce an absolute position of the object of interest from the relative positions of the object of interest and map the absolute position to a position indicator associated with the application program.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="235.63mm" wi="166.88mm" file="US08625849-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="224.28mm" wi="186.10mm" file="US08625849-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="215.31mm" wi="183.30mm" file="US08625849-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="206.33mm" wi="189.06mm" file="US08625849-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="235.20mm" wi="168.57mm" file="US08625849-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="237.74mm" wi="129.46mm" file="US08625849-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="213.36mm" wi="174.33mm" file="US08625849-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="231.31mm" wi="177.55mm" file="US08625849-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="207.60mm" wi="166.62mm" file="US08625849-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="212.09mm" wi="171.70mm" file="US08625849-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="226.23mm" wi="177.55mm" file="US08625849-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="132.00mm" wi="136.48mm" file="US08625849-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="181.27mm" wi="186.77mm" file="US08625849-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="110.24mm" wi="134.54mm" file="US08625849-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="250.19mm" wi="173.14mm" file="US08625849-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="241.30mm" wi="181.27mm" file="US08625849-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="217.85mm" wi="179.41mm" file="US08625849-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">&#x201c;This application is a continuation of U.S. application Ser. No. 12/493,958, filed Jun. 29, 2009, entitled &#x201c;MULTIPLE CAMERA CONTROL SYSTEM,&#x201d; which is a continuation of U.S. application Ser. No. 11/932,869, filed Oct. 31, 2007, entitled &#x201c;MULTIPLE CAMERA CONTROL SYSTEM,&#x201d; now U.S. Pat. No. 7,555,142, which is a continuation of U.S. application Ser. No. 11/304,000, filed Dec. 19, 2005, entitled &#x201c;MULTIPLE CAMERA CONTROL SYSTEM,&#x201d; now U.S. Pat. No. 7,421,093, which is a continuation of U.S. application Ser. No. 09/962,612, filed Sep. 26, 2001, entitled &#x201c;MULTIPLE CAMERA CONTROL SYSTEM,&#x201d; now U.S. Pat. No. 7,058,204, which claims the benefit of U.S. Provisional Application No. 60/237,187, filed Oct. 3, 2000, entitled &#x201c;DUAL CAMERA CONTROL SYSTEM.&#x201d; All of these applications are incorporated by reference in their entirety.&#x201d;</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">This invention relates to an object tracking system, and more particularly to a video camera based object tracking and interface control system.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">A variety of operating systems are currently available for interacting with and controlling a computer system. Many of these operating systems use standardized interface functions based on commonly accepted graphical user interface (GUI) functions and control techniques. As a result, different computer platforms and user applications can be easily controlled by a user who is relatively unfamiliar with the platform and/or application, as the functions and control techniques are generally common from one GUI to another.</p>
<p id="p-0005" num="0004">One commonly accepted control technique is the use of a mouse or trackball style pointing device to move a cursor over screen objects. An action, such as clicking (single or double) on the object, executes a GUI function. However, for someone who is unfamiliar with operating a computer mouse, selecting GUI functions may present a challenge that prevents them from interfacing with the computer system. There also exist situations where it becomes impractical to provide access to a computer mouse or trackball, such as in front of a department store display window on a city street, or while standing in front of a large presentation screen to lecture before a group of people.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0006" num="0005">In one general aspect, a method of tracking an object of interest is disclosed. The method includes acquiring a first image and a second image representing different viewpoints of the object of interest, and processing the first image into a first image data set and the second image into a second image data set. The method further includes processing the first image data set and the second image data set to generate a background data set associated with a background, and generating a first difference map by determining differences between the first image data set and the background data set, and a second difference map by determining differences between the second image data set and the background data set. The method also includes detecting a first relative position of the object of interest in the first difference map and a second relative position of the object of interest in the second difference map, and producing an absolute position of the object of interest from the first and second relative positions of the object of interest.</p>
<p id="p-0007" num="0006">The step of processing the first image into the first image data set and the second image into the second image data set may include determining an active image region for each of the first and second images, and extracting an active image data set from the first and second images contained within the active image region. The step of extracting the active image data set may include one or more techniques of cropping the first and second images, rotating the first and second images, or shearing the first and second images.</p>
<p id="p-0008" num="0007">In one implementation, the step of extracting the active image data set may include arranging the active image data set into an image pixel array having rows and columns. The step of extracting further may include identifying the maximum pixel value within each column of the image pixel array, and generating data sets having one row wherein the identified maximum pixel value for each column represents that column.</p>
<p id="p-0009" num="0008">Processing the first image into a first image data set and the second image into a second image data set also may include filtering the first and second images. Filtering may include extracting the edges in the first and second images. Filtering further may include processing the first image data set and the second image data set to emphasize differences between the first image data set and the background data set, and to emphasize differences between the second image data set and the background data set.</p>
<p id="p-0010" num="0009">Processing the first image data set and the second image data set to generate the background data set may include generating a first set of one or more background data sets associated with the first image data set, and generating a second set of one or more background data sets associated with the second image data set.</p>
<p id="p-0011" num="0010">Generating the first set of one or more background data sets may include generating a first background set representing a maximum value of data within the first image data set representative of the background, and generating the second set of one or more background data sets includes generating a second background set representing a maximum value of data within the second image data set representative of the background. Generating further may include, for the first and second background sets representing the maximum value of data representative of the background, increasing the values contained within the first and second background sets by a predetermined value.</p>
<p id="p-0012" num="0011">Generating the first set of one or more background data sets may include generating a first background set representing a minimum value of data within the first image data set representative of the background, and generating the second set of one or more background data sets may include generating a second background set representing a minimum value of data within the second image data set representative of the background. Generating further may include, for the first and second background sets representing the minimum value of data representative of the background, decreasing the values contained within the first and second background sets by a predetermined value.</p>
<p id="p-0013" num="0012">Generating the first set of background data sets may include sampling the first image data set, and generating the second set of background data sets may include sampling the second image data set. Sampling may occur automatically at predefined time intervals, where each sample may include data that is not associated with the background.</p>
<p id="p-0014" num="0013">Generating the first set of one or more background data sets may include maintaining multiple samples of the first image data set within each background data set, and generating the second set of one or more background data sets may include maintaining multiple samples of the second image data set within each background data set.</p>
<p id="p-0015" num="0014">Generating each first background data set may include selecting from the multiple samples one value that is representative of the background for each element within the first image data set, and generating each second background data set may include selecting from the multiple samples one value that is representative of the background for each element within the second image data set. Selecting may include selecting the median value from all sample values in each of the background data sets.</p>
<p id="p-0016" num="0015">In other implementations, generating may include comparing the first image data set to a subset of the background data set, and comparing the second image data set to a subset of the background data set.</p>
<p id="p-0017" num="0016">In other implementations generating a first difference map further may include representing each element in the first image data set as one of two states, and generating a second difference map further may include representing each element in the second image data set as one of two states, where the two states represent whether the value is consistent with the background.</p>
<p id="p-0018" num="0017">In still other implementations, detecting may include identifying a cluster in each of the first and second difference maps, where each cluster has elements whose state within its associated difference map indicates that the elements are inconsistent with the background.</p>
<p id="p-0019" num="0018">Identifying the cluster further may include reducing the difference map to one row by counting the elements within a column that are inconsistent with the background. Identifying the cluster further may include identifying the column as being within the cluster and classifying nearby columns as being within the cluster. Identifying the column as being within the cluster also may include identifying the median column.</p>
<p id="p-0020" num="0019">Identifying the cluster further may include identifying a position associated with the cluster. Identifying the position associated with the cluster may include calculating the weighted mean of elements within the cluster.</p>
<p id="p-0021" num="0020">Detecting further may include classifying the cluster as the object of interest. Classifying the cluster further may include counting the elements within the cluster and classifying the cluster as the object of interest only if that count exceeds a predefined threshold. Classifying the cluster further may include counting the elements within the cluster and counting a total number of elements classified as inconsistent within the background within the difference map, and classifying the cluster as the object of interest only if the ratio of the count of elements within the cluster over the total number of elements exceeds a predefined threshold.</p>
<p id="p-0022" num="0021">The step of detecting further may include identifying a sub-cluster within the cluster that represents a pointing end of the object of interest and identifying a position of the sub-cluster.</p>
<p id="p-0023" num="0022">In the above implementations, the object of interest may be a user's hand, and the method may include controlling an application program using the absolute position of the object of interest.</p>
<p id="p-0024" num="0023">The above implementations further may include acquiring a third image and a fourth image representing different viewpoints of the object of interest, processing the third image into a third image data set and the fourth image into a fourth image data set, and processing the third image data set and the fourth image data set to generate the background data set associated with the background. The method also may include generating a third difference map by determining differences between the third image data set and the background data set, and a fourth difference map by determining differences between the fourth image data set and the background data set, and detecting a third relative position of the object of interest in the third difference map and a fourth relative position of the object of interest in the fourth difference map. The absolute position of the object of interest may be produced from the first, second, third and fourth relative positions of the object of interest.</p>
<p id="p-0025" num="0024">As part of this implementation, the object of interest may be a user's hand, and also may include controlling an application program using the absolute position of the object of interest.</p>
<p id="p-0026" num="0025">In another aspect, a method of tracking an object of interest controlled by a user to interface with a computer is disclosed. The method includes acquiring images from at least two viewpoints, processing the acquired images to produce an image data set for each acquired image, and comparing each image data set to one or more background data sets to produce a difference map for each acquired image. The method also includes detecting a relative position of an object of interest within each difference map, producing an absolute position of the object of interest from the relative positions of the object of interest, and using the absolute position to allow the user to interact with a computer application.</p>
<p id="p-0027" num="0026">Additionally, this method may include mapping the absolute position of the object of interest to screen coordinates associated with the computer application, and using the mapped position to interface with the computer application. This method also may include recognizing a gesture associated with the object of interest by analyzing changes in the absolute position of the object of interest, and combining the absolute position and the gesture to interface with the computer application.</p>
<p id="p-0028" num="0027">In another aspect, a multiple camera tracking system for interfacing with an application program running on a computer is disclosed. The multiple camera tracking system includes two or more video cameras arranged to provide different viewpoints of a region of interest and are operable to produce a series of video images. A processor is operable to receive the series of video images and detect objects appearing in the region of interest. The processor executes a process to generate a background data set from the video images, generate an image data set for each received video image and compare each image data set to the background data set to produce a difference map for each image data set, detect a relative position of an object of interest within each difference map, and produce an absolute position of the object of interest from the relative positions of the object of interest and map the absolute position to a position indicator associated with the application program.</p>
<p id="p-0029" num="0028">In the above implementation, the object of interest may be a human hand. Additionally, the region of interest may be defined to be in front of a video display associated with the computer. The processor may be operable to map the absolute position of the object of interest to the position indicator such that the location of the position indicator on the video display is aligned with the object of interest.</p>
<p id="p-0030" num="0029">The region of interest may be defined to be any distance in front of a video display associated with the computer, and the processor may be operable to map the absolute position of the object of interest to the position indicator such that the location of the position indicator on the video display is aligned to a position pointed to by the object of interest. Alternatively, the region of interest may be defined to be any distance in front of a video display associated with the computer, and the processor may be operable to map the absolute position of the object of interest to the position indicator such that movements of the object of interest are scaled to larger movements of the location of the position indicator on the video display.</p>
<p id="p-0031" num="0030">The processor may be configured to emulate a computer mouse function. This may include configuring the processor to emulate controlling buttons of a computer mouse using gestures derived from the motion of the object of interest. A sustained position of the object of interest for a predetermined time period may trigger a selection action within the application program.</p>
<p id="p-0032" num="0031">The processor may be configured to emulate controlling buttons of a computer mouse based on a sustained position of the object of interest for a predetermined time period. Sustaining a position of the object of interest within the bounds of an interactive display region for a predetermined time period may trigger a selection action within the application program.</p>
<p id="p-0033" num="0032">The processor may be configured to emulate controlling buttons of a computer mouse based on a sustained position of the position indicator within the bounds of an interactive display region for a predetermined time period.</p>
<p id="p-0034" num="0033">In the above aspects, the background data set may include data points representing at least a portion of a stationary structure. In this implementation, at least a portion of the stationary structure may include a patterned surface that is visible to the video cameras. The stationary structure may be a window frame. Alternatively, the stationary structure may include a strip of light.</p>
<p id="p-0035" num="0034">In another aspect, a multiple camera tracking system for interfacing with an application program running on a computer is disclosed. The system includes two or more video cameras arranged to provide different viewpoints of a region of interest and are operable to produce a series of video images. A processor is operable to receive the series of video images and detect objects appearing in the region of interest. The processor executes a process to generate a background data set from the video images, generate an image data set for each received video image, compare each image data set to the background data set to produce a difference map for each image data set, detect a relative position of an object of interest within each difference map, produce an absolute position of the object of interest from the relative positions of the object of interest, define sub regions within the region of interest, identify a sub region occupied by the object of interest, associate an action with the identified sub region that is activated when the object of interest occupies the identified sub region, and apply the action to interface with the application program.</p>
<p id="p-0036" num="0035">In the above implementation, the object of interest may be a human hand. Additionally, the action associated with the identified sub region may emulate the activation of keys of a keyboard associated with the application program. In a related implementation, sustaining a position of the object of interest in any sub region for a predetermined time period may trigger the action.</p>
<p id="p-0037" num="0036">The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features and advantages will be apparent from the description and drawings, and from the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">DESCRIPTION OF DRAWINGS</heading>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 1</figref> shows the hardware components of a typical implementation of the multicamera control system, and their typical physical layout.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 2A</figref> shows the typical geometric relationship between the cameras and various image regions of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 2B</figref> shows a typical image captured by one of the cameras of <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 3</figref> is a flow diagram showing the processes that are performed, typically within a microcomputer program associated with the multicamera control system.</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 4</figref> is a flow diagram showing a portion of the process shown in <figref idref="DRAWINGS">FIG. 3</figref> in greater detail, and in particular, the processes involved in detecting an object and extracting its position from the image signals captured by the cameras.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 5A</figref> shows sample image data, presented as a gray-scale bitmap image, acquired by a camera and generated by part of the process shown in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 5B</figref> shows sample image data, presented as a gray-scale bitmap image, generated by part of the process shown in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 5C</figref> shows sample image data, presented as a gray-scale bitmap image, generated by part of the process shown in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. 5D</figref> shows sample image data, presented as a gray-scale bitmap image, generated by part of the process shown in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. 5E</figref> shows sample data, presented as a binary bitmap image, identifying those pixels that likely belong to the object that is being tracked in the sample, generated by part of the process shown in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 6</figref> is a flow diagram showing a portion of the process described in <figref idref="DRAWINGS">FIG. 4</figref> in greater detail, and in particular, the processes involved in classifying and identifying the object given a map of pixels that have been identified as likely to belong to the object that is being tracked, for example given the data shown in <figref idref="DRAWINGS">FIG. 5E</figref>.</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 7A</figref> shows the sample data presented in <figref idref="DRAWINGS">FIG. 5E</figref>, presented as a binary bitmap image, with the identification of those data samples that the processes shown in <figref idref="DRAWINGS">FIG. 6</figref> have selected as belonging to the object in this sample.</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 7B</figref> shows the sample data presented in <figref idref="DRAWINGS">FIG. 5E</figref>, presented as a bar graph, with the identification of those data samples that the processes outlined in <figref idref="DRAWINGS">FIG. 6</figref> have selected as belonging to the object, with specific points in the graph being identified.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 7C</figref> shows a difference set of sample data, presented as a binary bitmap image, with the identification of those data samples that the processes shown in <figref idref="DRAWINGS">FIG. 6</figref> have selected as belonging to the object and key parts of the object in this sample.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. 8</figref> is a flow diagram that shows a part of the process shown in <figref idref="DRAWINGS">FIG. 4</figref> in greater detail, and in particular, the processes involved in generating and maintaining a description of the background region over which the object occludes.</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. 9A</figref> shows the geometry on which Eq. 3 is based, that is, an angle defining the position of the object within the camera's field of view, given the location on the image plane where the object has been sensed.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 9B</figref> shows the geometry on which Eq. 4, 5 and 6 are based, that is, the relationship between the positions of the cameras and the object that is being tracked.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 10</figref> is a graph illustrating Eq. 8, that is, the amount of dampening that may be applied to coordinates given the change in position of the object to refine the positions.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. 11A</figref> is an example of an application program that is controlled by the system, where the object of interest controls a screen pointer in two dimensions.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 11B</figref> shows the mapping between real-world coordinates and screen coordinates used by the application program in <figref idref="DRAWINGS">FIG. 11A</figref>.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIGS. 12A and 12B</figref> are examples of an application program that is controlled by the multicamera control system, where the object of interest controls a screen pointer in a three dimensional virtual reality environment.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. 13A</figref> shows the division of the region of interest into detection planes used by a gesture detection method to identify a gesture that may be associated with the intention to activate.</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. 13B</figref> shows the division of the region of interest into detection boxes used by a gesture detection method to identify a gesture that may be associated with selecting a cursor direction.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 13C</figref> shows an alternate division of the region of interest into direction detection boxes used by a gesture detection method to identify a gesture that may be associated with selecting a cursor direction.</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. 13D</figref> illustrates in greater detail the relationship of neighboring divisions of <figref idref="DRAWINGS">FIG. 13C</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0063" num="0062">Like reference symbols in the various drawings indicate like elements.</p>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 1</figref> shows a multicamera motion tracking and control system <b>100</b> interfaced with an image viewing system. In this implementation two cameras <b>101</b> and <b>102</b> scan a region of interest <b>103</b>. A controlled or known background <b>104</b> surrounds the region of interest <b>103</b>. An object of interest <b>105</b> is tracked by the system when it enters the region of interest <b>103</b>. The object of interest <b>105</b> may be any generic object inserted into the region of interest <b>103</b>, and is typically a hand or finger of a system user. The object of interest <b>105</b> also may be a selection device such as a pointer.</p>
<p id="p-0065" num="0064">The series of video images acquired from the cameras <b>101</b> and <b>102</b> are conveyed to a computing device or image processor <b>106</b>. In this implementation, the computing device is a general-purpose computer that runs additional software that provides feedback to the user on a video display <b>107</b>.</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. 2A</figref> illustrates a typical implementation of the multicamera control system <b>100</b>. The two cameras <b>101</b> and <b>102</b> are positioned outside of the region of interest <b>103</b>. The cameras are oriented so that the intersection <b>204</b> of their field of views (<b>205</b> for camera <b>101</b>, <b>206</b> for camera <b>102</b>) completely encompasses the region of interest <b>103</b>. The orientation is such that the cameras <b>101</b>, <b>102</b> are rotated on axes that are approximately parallel. In this example, a floor or window ledge and sidewalls provide a controlled background <b>104</b> having distinct edges. The corresponding view captured by camera <b>101</b> is shown in <figref idref="DRAWINGS">FIG. 2B</figref>. While not shown, it should be understood that the view captured by camera <b>102</b> is a mirror image of the view captured by camera <b>101</b>. The controlled background <b>104</b> may not cover the camera's entire field of view <b>205</b>. For each camera, an active image region <b>208</b> is found that is entirely contained within the controlled background <b>104</b>, and also contains the entire region of interest <b>103</b>. The background <b>104</b> is controlled so that a characteristic of the background can be modeled, and the object of interest <b>105</b>, either in part or in whole, differs from the background <b>104</b> in that characteristic. When the object of interest <b>105</b> appears within the region of interest <b>103</b>, the object <b>105</b> will occlude a portion of the controlled background <b>104</b> within the active image region <b>208</b> of each camera <b>101</b>, <b>102</b>. In the location of the occlusion, either as a whole or in parts, the captured images will, in terms of the selected characteristic, be inconsistent with the model of the controlled background <b>104</b>.</p>
<p id="p-0067" num="0066">In summary, the object of interest <b>105</b> is identified and, if found, its position within the active image region <b>208</b> of both cameras is calculated. Using the position data of each camera <b>101</b>, <b>102</b>, as well as the positions of the cameras relative to the region of interest <b>103</b>, and parameters describing the cameras, the position of the object of interest <b>105</b> within the region of interest <b>103</b> is calculated.</p>
<p id="p-0068" num="0067">The processes performed by the image processor <b>106</b> (<figref idref="DRAWINGS">FIG. 1</figref>), which may be implemented through a software process, or alternatively through hardware, are generally shown in <figref idref="DRAWINGS">FIG. 3</figref>. The camera images are simultaneously conveyed from the cameras <b>101</b>, <b>102</b> and captured by image acquisition modules <b>304</b>, <b>305</b> (respectively) into image buffers <b>306</b>, <b>307</b> (respectively) within the image processor <b>106</b>. Image detection modules <b>308</b>, <b>309</b> independently detect the object of interest <b>105</b> in each image, and determine its position relative to the camera view. The relative position information <b>310</b>, <b>311</b> from both camera views is combined by a combination module <b>312</b> and optionally refined by a position refinement module <b>313</b>, to determine at block <b>314</b>, the global presence and position of the object of interest <b>105</b> within the region of interest <b>103</b>. Optionally, specific gestures performed by the user may be detected in a gesture detection module <b>315</b>. The results of the gesture detection process are then conveyed to another process or application <b>316</b>, either on the same image processor <b>106</b> or to another processing device. The process of gesture detection is described in greater detail below.</p>
<p id="p-0069" num="0068">Image detection modules <b>308</b> and <b>309</b> are identical in the processes that they execute. An implementation of these image detection modules <b>308</b>, <b>309</b> is shown in <figref idref="DRAWINGS">FIG. 4</figref>. In block <b>402</b>, the image processor <b>106</b> extracts, from the captured image data stored in the image buffers <b>306</b> or <b>307</b>, the image data that corresponds to the active image region <b>208</b> (of <figref idref="DRAWINGS">FIG. 2B</figref>). The image may be filtered in a filtering process <b>403</b> to emphasize or extract the aspects or characteristics of the image where the background <b>104</b> and object of interest <b>105</b> differ, but are otherwise invariant within the background <b>104</b> over time. In some implementations, the data representing the active image region may also be reduced by a scaling module <b>404</b> in order to reduce the amount of computations required in later processing steps. Using the resulting data, the background <b>104</b> is modeled by one or more instances of a background model process at block <b>405</b> to produce one or more descriptions represented as background model data <b>406</b> of the controlled background <b>104</b>. Therefore the background <b>104</b> is modeled in terms of the desired aspects or characteristics of the image. The background model(s) <b>406</b> are converted into a set of criteria in process <b>407</b>. In a comparison process <b>408</b>, the filtered (from process <b>403</b>) and/or reduced (from module <b>404</b>) image data is compared to those criteria (from process <b>407</b>), and the locations where the current data is inconsistent with the background model data <b>406</b>, that is where the criteria is not satisfied, are stored in an image or difference map <b>409</b>. In detection module <b>410</b>, the difference map <b>409</b> is analyzed to determine if any such inconsistencies qualify as a possible indication of an object of interest <b>105</b> and, if these criteria are satisfied, its position within the camera view (<b>205</b> or <b>206</b>) is determined. The position of the object <b>105</b> may be further refined (optionally) at block <b>411</b>, which produces a camera-relative presence and position output <b>310</b> or <b>311</b> associated with the object of interest <b>105</b> (as described above with respect to <figref idref="DRAWINGS">FIG. 3</figref>).</p>
<p id="p-0070" num="0069">In block <b>402</b> of <figref idref="DRAWINGS">FIG. 4</figref>, image processor <b>106</b> extracts the image data that corresponds to the active image region <b>208</b> (of <figref idref="DRAWINGS">FIG. 2B</figref>). The image data may be extracted by cropping, shearing, rotating, or otherwise transforming the captured image data. Cropping extracts only the portion of the overall image that is within the active image region <b>208</b>. Bounds are defined, and any pixels inside the bounds are copied, unmodified, to a new buffer, while pixels outside of the bounds are ignored. The active image region <b>208</b> may be of arbitrary shape. Shearing and rotation reorder the data into an order that is more convenient for further processing, such as a rectangular shape so that it may be addressed in terms of rows and columns of pixels.</p>
<p id="p-0071" num="0070">Rotation causes the contents of an image to appear as if the image has been rotated. Rotation reorders the position of pixels from (x,y) to (x&#x2032;,y&#x2032;) according to the following equation:</p>
<p id="p-0072" num="0071">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
  <mrow>
    <mo>[</mo>
    <mtable>
      <mtr>
        <mtd>
          <msup>
            <mi>x</mi>
            <mi>&#x2032;</mi>
          </msup>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <msup>
            <mi>y</mi>
            <mi>&#x2032;</mi>
          </msup>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>1</mn>
        </mtd>
      </mtr>
    </mtable>
    <mo>]</mo>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mrow>
      <mo>[</mo>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mi>cos</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>&#x3b8;</mi>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mrow>
                <mo>-</mo>
                <mi>sin</mi>
              </mrow>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>&#x3b8;</mi>
            </mrow>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mi>sin</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>&#x3b8;</mi>
            </mrow>
          </mtd>
          <mtd>
            <mrow>
              <mi>cos</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mi>&#x3b8;</mi>
            </mrow>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>0</mn>
          </mtd>
          <mtd>
            <mn>1</mn>
          </mtd>
        </mtr>
      </mtable>
      <mo>]</mo>
    </mrow>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>[</mo>
      <mtable>
        <mtr>
          <mtd>
            <mi>x</mi>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mi>y</mi>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>1</mn>
          </mtd>
        </mtr>
      </mtable>
      <mo>]</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0072">where &#x3b8; is the angle that the image is to be rotated.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0073" num="0073">If the cameras <b>101</b> and <b>102</b> are correctly mounted with respect to the region of interest <b>103</b>, the desired angle of rotation will typically be small. If the desired angle of rotation is small, shearing may be used to provide an approximation that is computationally simpler than rotation. Shearing distorts the shape of an image such that the transformed shape appears as if the rows and columns have been caused to slide over and under each other. Shearing reorders the position of pixels according to the following equations:</p>
<p id="p-0074" num="0074">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mrow>
    <mo>[</mo>
    <mtable>
      <mtr>
        <mtd>
          <msup>
            <mi>x</mi>
            <mi>&#x2032;</mi>
          </msup>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <msup>
            <mi>y</mi>
            <mi>&#x2032;</mi>
          </msup>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>1</mn>
        </mtd>
      </mtr>
    </mtable>
    <mo>]</mo>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mrow>
      <mrow>
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <mn>1</mn>
              </mtd>
              <mtd>
                <msub>
                  <mi>sh</mi>
                  <mi>x</mi>
                </msub>
              </mtd>
              <mtd>
                <mn>0</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>0</mn>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
              <mtd>
                <mn>0</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>0</mn>
              </mtd>
              <mtd>
                <mn>0</mn>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <mi>x</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>y</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
      </mrow>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.8em" height="0.8ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mrow>
        <mi>and</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.8em" height="0.8ex"/>
        </mstyle>
        <mo>[</mo>
        <mtable>
          <mtr>
            <mtd>
              <msup>
                <mi>x</mi>
                <mi>&#x2032;</mi>
              </msup>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <msup>
                <mi>y</mi>
                <mi>&#x2032;</mi>
              </msup>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mn>1</mn>
            </mtd>
          </mtr>
        </mtable>
        <mo>]</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mo>[</mo>
        <mtable>
          <mtr>
            <mtd>
              <mn>1</mn>
            </mtd>
            <mtd>
              <mn>0</mn>
            </mtd>
            <mtd>
              <mn>0</mn>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <msub>
                <mi>sh</mi>
                <mi>y</mi>
              </msub>
            </mtd>
            <mtd>
              <mn>1</mn>
            </mtd>
            <mtd>
              <mn>0</mn>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mn>0</mn>
            </mtd>
            <mtd>
              <mn>0</mn>
            </mtd>
            <mtd>
              <mn>1</mn>
            </mtd>
          </mtr>
        </mtable>
        <mo>]</mo>
      </mrow>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>[</mo>
        <mtable>
          <mtr>
            <mtd>
              <mi>x</mi>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mi>y</mi>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mn>1</mn>
            </mtd>
          </mtr>
        </mtable>
        <mo>]</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
where sh<sub>x </sub>represents the amount of horizontal shear within the image, and sh<sub>y </sub>represents the amount of vertical shear within the image.
</p>
<p id="p-0075" num="0075">An implementation of the multicamera control system <b>100</b> applies in scenarios where the object of interest <b>105</b>, either in whole or in part, is likely to have either higher or lower luminance than the controlled background <b>104</b>. For example, the background <b>104</b> may be illuminated to create this scenario. A filtering block <b>403</b> passes through the luminance information associated with the image data. A single background model <b>406</b> represents the expected luminance of the background <b>104</b>. In practice, the luminance of the controlled background <b>104</b> may vary within the active image region <b>208</b>, therefore the background model <b>406</b> may store the value of the expected luminance for every pixel within the active image region <b>208</b>. The comparison criteria generation process <b>407</b> accounts for signal noise (above that which may be accounted for within the background model) and minor variability of the luminance of the controlled background <b>104</b> by modifying each luminance value from the background model <b>406</b>, thus producing the minimal luminance value that may be classified as being consistent with the background model <b>406</b>. For example, if the luminance of the controlled background <b>104</b> is higher than the luminance of the object of interest <b>105</b>, then processes block <b>407</b> decreases the luminance value of each pixel by an amount greater than the expected magnitude of signal noise and variability of luminance.</p>
<p id="p-0076" num="0076">In some implementations of system <b>100</b>, the region of interest <b>103</b> is sufficiently narrow such that it may to be modeled as a region of a plane. The orientation of that plane is parallel to the front and rear faces of the dotted cube that represents the region of interest <b>103</b> in <figref idref="DRAWINGS">FIG. 1</figref>. The active image region <b>208</b> may be reduced to a single row of pixels in the optional scaling module <b>404</b> if two conditions are satisfied: 1) the object of interest <b>105</b>, when it is to be detected, will occlude the background <b>104</b> in all rows of some columns of the active image region <b>208</b>, and 2) a single set of values in the background model <b>406</b> sufficiently characterizes an entire column of pixels in the active image region <b>208</b>. The first condition is usually satisfied if the active image region <b>208</b> is thinner than the object of interest <b>105</b>. The second condition is satisfied by the implementation of blocks <b>403</b>, <b>405</b>, <b>406</b> and <b>407</b> described above. Application of the scaling module <b>404</b> reduces the complexity of processing that is required to be performed in later processes, as well as reducing the storage requirements of the background model(s) <b>406</b>.</p>
<p id="p-0077" num="0077">The particular implementation of the scaling module <b>404</b> depends on the specifics of processing blocks <b>403</b>, <b>405</b>, <b>406</b> and <b>407</b>. If the luminance of the controlled background <b>104</b> is expected to be higher than that of the object of interest <b>105</b>, as described above, one implementation of the scaling module <b>404</b> is to represent each column by the luminance of greatest magnitude within that column. That is to say, for each column, the highest value in that column is copied to a new array. This process has the added benefit that the high-luminance part of the controlled background <b>104</b> need not fill the entire controlled background <b>104</b>.</p>
<p id="p-0078" num="0078">An alternative implementation applies in scenarios where the controlled background <b>104</b> is static, that is, contains no motion, but is not otherwise limited in luminance. A sample source image is included in <figref idref="DRAWINGS">FIG. 5A</figref> as an example. In this case, the object of interest, as sensed by the camera, may contain, or be close in magnitude to, the luminance values that are also found within the controlled background <b>104</b>. In practice, the variability of luminance of the controlled background <b>104</b> (for example, caused by a user moving in front of the apparatus thereby blocking some ambient light) may be significant in magnitude relative to the difference between the controlled background <b>104</b> and the object of interest <b>105</b>. Therefore, a specific type of filter may be applied in the filtering process <b>403</b> that produces results that are invariant to or de-emphasize variability in global luminance, while emphasizing parts of the object of interest <b>105</b>. A 3&#xd7;3 Prewitt filter is typically used in the filtering process <b>403</b>. <figref idref="DRAWINGS">FIG. 5B</figref> shows the result of this 3&#xd7;3 Prewitt filter on the image in <figref idref="DRAWINGS">FIG. 5A</figref>. In this implementation, two background models <b>406</b> may be maintained, one representing each of the high and low values, and together representing the range of values expected for each filtered pixel. The comparison criteria generation process <b>407</b> then decreases the low-value and increases the high-value by an amount greater than the expected magnitude of signal noise and variability of luminance. The result is a set of criterion, an example of which, for the low-value, is shown in <figref idref="DRAWINGS">FIG. 5C</figref>, and an example of which, for the high-value, is shown in <figref idref="DRAWINGS">FIG. 5D</figref>. These modified images are passed to the comparison process <b>408</b>, which classifies pixels as being inconsistent to the controlled background <b>104</b> if their value is either lower than the low-value criterion (<figref idref="DRAWINGS">FIG. 5C</figref>) or higher than the high-value criterion (<figref idref="DRAWINGS">FIG. 5D</figref>). The result is a binary difference map <b>409</b>, of which example corresponding to <figref idref="DRAWINGS">FIG. 5B</figref> is shown in <figref idref="DRAWINGS">FIG. 5E</figref>.</p>
<p id="p-0079" num="0079">The preceding implementation allows the use of many existing surfaces, walls or window frames, for example, as the controlled background <b>104</b> where those surfaces may have arbitrary luminance, textures, edges, or even a light strip secured to the surface of the controlled background <b>104</b>. The above implementation also allows the use of a controlled background <b>104</b> that contains a predetermined pattern or texture, a stripe for example, where the above processes detect the lack of the pattern in the area where the object of interest <b>105</b> occludes the controlled background <b>104</b>.</p>
<p id="p-0080" num="0080">The difference map <b>409</b> stores the positions of all pixels that are found to be inconsistent with the background <b>104</b> by the above methods. In this implementation, the difference map <b>409</b> may be represented as a binary image, where each pixel may be in one of two states. Those pixels that are inconsistent with the background <b>104</b> are identified or &#x201c;tagged&#x201d; by setting the pixel in the corresponding row and column of the difference map to one of those states. Otherwise, the corresponding pixel is set to the other state.</p>
<p id="p-0081" num="0081">An implementation of the detection module <b>410</b>, which detects an object of interest <b>105</b> in the difference map <b>409</b>, shown in <figref idref="DRAWINGS">FIG. 6</figref>. Another scaling module at block <b>603</b> provides an additional opportunity to reduce the data to a single dimensional array of data, and may optionally be applied to scenarios where the orientation of the object of interest <b>105</b> does not have a significant effect on the overall bounds of the object of interest <b>105</b> within the difference map <b>409</b>. In practice, this applies to many scenarios where the number of rows is less than or similar to the typical number of columns that the object of interest <b>105</b> occupies. When applied, the scaling module at block <b>603</b> reduces the difference map <b>409</b> into a map of one row, that is, a single dimensional array of values. In this implementation, the scaling module <b>603</b> may count the number of tagged pixels in each column of the difference map <b>409</b>. As an example, the difference map <b>409</b> of <figref idref="DRAWINGS">FIG. 7A</figref> is reduced in this manner and depicted as a graph <b>709</b> in <figref idref="DRAWINGS">FIG. 7B</figref>. Applying this optional processing step reduces the processing requirements and simplifies some of the calculations that follow.</p>
<p id="p-0082" num="0082">Continuing with this implementation of the detection module <b>410</b>, it is observed that the pixels tagged in the difference map (<b>409</b> in example <figref idref="DRAWINGS">FIG. 7A</figref>) that are associated with the object of interest <b>105</b> will generally form a cluster <b>701</b>, however the cluster is not necessarily connected. A cluster identification process <b>604</b> classifies pixels (or, if the scaling module <b>603</b> has been applied, classifies columns) as to whether they are members of the cluster <b>701</b>. A variety of methods of finding clusters of samples exist and may be applied, and the following methods have been selected on the basis of processing simplicity. It is noted that, when the object of interest <b>105</b> is present, it is likely that the count of correctly tagged pixels will exceed the number of false-positives. Therefore the median position is expected to fall somewhere within the object of interest <b>105</b>. Part of this implementation of the cluster identification process <b>604</b>, when applied to a map of one row (for example, where the scaling module at block <b>603</b> or <b>404</b> has been applied), is to calculate the median column <b>702</b> and tag columns as part of the cluster <b>701</b> (<figref idref="DRAWINGS">FIG. 7B</figref>) if they are within a predetermined distance <b>703</b> that corresponds to the maximum number of columns expected to be occupied. Part of this implementation of the cluster identification process <b>604</b>, when applied to a map of multiple rows, is to add tagged pixels to the cluster <b>703</b> if they meet a neighbor-distance criterion.</p>
<p id="p-0083" num="0083">In this implementation, a set of criteria is received by a cluster classification process <b>605</b> and is then imposed onto the cluster <b>701</b> to verify that the cluster has qualities consistent with those expected of the object of interest <b>105</b>. Thus, process <b>605</b> determines whether the cluster <b>701</b> should be classified as belonging to the object of interest <b>105</b>. Part of this implementation of the cluster classification process <b>605</b> is to calculate a count of the tagged pixels within the cluster <b>701</b> and to calculate a count of all tagged pixels. The count within the cluster <b>701</b> is compared to a threshold, eliminating false matches in clusters having too few tagged pixels to be considered as an object of interest <b>105</b>. Also, the ratio of the count of pixels within the cluster <b>701</b> relative to the total count is compared to a threshold, further reducing false matches.</p>
<p id="p-0084" num="0084">If the cluster <b>701</b> passes these criteria, a description of the cluster is refined in process block <b>606</b> by calculating the center of gravity associated with the cluster <b>701</b> in process <b>607</b>. Although the median position found by the scaling module <b>603</b> is likely to be within the bounds defining the object of interest <b>105</b>, it is not necessarily at the object's center. The weighted mean <b>710</b>, or center of gravity, provides a better measure of the cluster's position and is optionally calculated within process <b>606</b>, as sub-process <b>607</b>. The weighted mean <b>710</b> is calculated by the following equation:</p>
<p id="p-0085" num="0085">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <mover>
    <mi>x</mi>
    <mi>_</mi>
  </mover>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <munderover>
        <mo>&#x2211;</mo>
        <mrow>
          <mi>x</mi>
          <mo>=</mo>
          <mn>0</mn>
        </mrow>
        <mrow>
          <mi>c</mi>
          <mo>-</mo>
          <mn>1</mn>
        </mrow>
      </munderover>
      <mo>&#x2062;</mo>
      <mrow>
        <mi>x</mi>
        <mo>&#xb7;</mo>
        <mrow>
          <mi>C</mi>
          <mo>&#x2061;</mo>
          <mrow>
            <mo>[</mo>
            <mi>x</mi>
            <mo>]</mo>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
    <mrow>
      <munderover>
        <mo>&#x2211;</mo>
        <mrow>
          <mi>x</mi>
          <mo>=</mo>
          <mn>0</mn>
        </mrow>
        <mrow>
          <mi>c</mi>
          <mo>-</mo>
          <mn>1</mn>
        </mrow>
      </munderover>
      <mo>&#x2062;</mo>
      <mrow>
        <mi>C</mi>
        <mo>&#x2061;</mo>
        <mrow>
          <mo>[</mo>
          <mi>x</mi>
          <mo>]</mo>
        </mrow>
      </mrow>
    </mrow>
  </mfrac>
</mrow>
</math>
</maths>
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0086">where:
        <ul id="ul0005" list-style="none">
            <li id="ul0005-0001" num="0087"> <o ostyle="single">x</o> is the mean</li>
            <li id="ul0005-0002" num="0088">c is the number of columns</li>
            <li id="ul0005-0003" num="0089">C[x] is the count of tagged pixels in column x.</li>
        </ul>
        </li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0086" num="0090">The cluster's bounds <b>704</b> may also be optionally calculated within process <b>606</b>, shown as process <b>608</b>. The cluster <b>703</b> may include some false-positive outliers, so as part of this implementation, the bounds may be defined as those that encompass a predetermined percentile of the tagged pixels, or, in scenarios where relatively few pixels are expected to be tagged, encompasses those tagged pixels (or columns, if scaling module <b>603</b> is applied) that form tight sub-clusters, that is those tagged pixels (or columns) that have neighbors that are also tagged.</p>
<p id="p-0087" num="0091">In addition to the middle and bound coordinates, the orientation of the object of interest <b>105</b> may optionally be inferred by calculation of the moments of the cluster. This calculation is represented by a cluster orientation calculation process at sub-process <b>609</b> within process <b>606</b>.</p>
<p id="p-0088" num="0092">In some applications of the system <b>100</b>, the object of interest <b>105</b> is used as a pointer. In this case, the &#x201c;pointing end&#x201d; of the object <b>105</b> is desired and may also be determined by a pointing end calculation sub-process within process <b>606</b> if the region of interest <b>103</b> contains a sufficient number of rows and the number of rows has not been reduced. An example is depicted in <figref idref="DRAWINGS">FIG. 7C</figref>. The object of interest <b>105</b> will typically enter, or be constrained to enter, the active image region <b>208</b> from a known border of that region. The pointing end <b>705</b> (for example the user's fingertip) of the object of interest <b>105</b> is likely to be the portion of the cluster <b>701</b> that is furthest from the region of entry <b>706</b> into the active image region <b>208</b>. The cluster <b>701</b> may include some false-positive outliers. As such, the pointing end <b>705</b> may be defined as the region <b>707</b> within the cluster <b>701</b> that encompasses multiple tagged pixels near the furthest bounding side of the cluster <b>701</b>, or, in scenarios where relatively few pixels are expected to be tagged, encompasses the furthest tagged pixels that form a tight sub-cluster; that is those tagged pixels that have neighbors that are also tagged. This sub-cluster is identified by a sub-cluster pointing end process <b>610</b>, and the position of the sub-cluster is found in process <b>611</b>.</p>
<p id="p-0089" num="0093">Continuing with this implementation, a process implemented by a smoothing module <b>612</b> may optionally be applied to any or all of the positions found in process <b>606</b>. Smoothing is a process of combining the results with those solved previously so they move in a steady manner from frame to frame. The weighted mean coordinate <b>710</b>, found by the center of gravity determination process <b>607</b>, is dependent on many samples and therefore is inherently steady. The bound <b>704</b>, found by the cluster bounding dimension determination process <b>608</b>, and pointing end <b>705</b>, found by <b>611</b>, coordinates are dependent on relatively fewer members of the cluster, and the state of a single pixel may have a significant effect. Since the size of the region occupied by the object of interest <b>105</b> is expected to remain relatively steady, smoothing may be applied to the distance between the bounds <b>704</b> measured relative to the cluster's weighted mean coordinate <b>710</b>. Since the shape and orientation of the object of interest <b>105</b> is expected to change less rapidly than the overall position object of interest <b>105</b>, smoothing may be applied to the distance of the pointing end <b>705</b> measured relative to the cluster's weighted mean coordinate <b>710</b>.</p>
<p id="p-0090" num="0094">A process used in the center of gravity process <b>607</b> is Eq. 1 as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>s</i>(<i>t</i>)=(<i>a&#xd7;r</i>(<i>t</i>))+((1<i>&#x2212;a</i>)&#xd7;<i>s</i>(<i>t&#x2212;</i>1))<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
In Eq. 1, the smoothed value at time t (s(t)) is equal to one minus the scalar value (a) multiplied by the smoothed value at time minus one (t&#x2212;1). This amount is added to the raw value at time t (r(t)) multiplied by a scalar (a) that is between zero and one.
</p>
<p id="p-0091" num="0095">Referring to <figref idref="DRAWINGS">FIG. 8</figref>, implementations of system <b>100</b> make use of, as described above, one or more background models <b>406</b> (<figref idref="DRAWINGS">FIG. 4</figref>). An implementation of the background model process or component <b>405</b> that generates the background model data <b>406</b> is shown in <figref idref="DRAWINGS">FIG. 8</figref>. This implementation of the background model component <b>405</b> automatically generates and dynamically updates the background model, allowing unattended operation of the system.</p>
<p id="p-0092" num="0096">Input data <b>802</b> is provided by the output of scaling module <b>404</b> for this implementation of the background model component <b>405</b>. Input is available every frame, and is sampled in a sampling process <b>803</b>. The sample may contain the object of interest <b>105</b> occluding part of the controlled background <b>104</b>. For each pixel, a range of values may be a better representative of the background <b>104</b> than a single value. By including the effects of this range in the background model, the expansion in process <b>407</b> may be made tighter. Contributing multiple frames of data to the sample allows this range to be observed, but also increases the portion of the background <b>104</b> that is occluded by the object of interest <b>105</b> if the object of interest <b>105</b> is in motion while the frames are being sampled. The optimal number of frames to use is dependent on the expected motion of the object of interest <b>105</b> in the particular application of the system. In practice, for systems that are tracking a hand, 10 frames, representing approximately 0.33 seconds, is sufficient to observe the majority of that range without allowing motion of the object of interest to occlude an undue portion of the background. If the particular background model is to be compared in comparison process <b>408</b> as the upper bound on values that are considered to be consistent with the background <b>104</b>, then the maximum value of each pixel observed in the multiple frames may be recorded as the sample value. If the particular background model <b>406</b> is to be compared in process <b>408</b> as the lower bound on values that are considered to be consistent with the background <b>104</b>, then the minimum value of each pixel observed in the multiple frames may be recorded as the sample value.</p>
<p id="p-0093" num="0097">In this implementation of the background model component <b>405</b>, samples from the sampling process <b>803</b> are added to a buffer <b>804</b> having storage locations to store n samples, where the oldest sample in the history is replaced. The history therefore contains n sampled values for each pixel. The span of time, d, represented in the buffer is dependent on the rate that new samples are acquired and added to the history, r, by Eq. 2, described as follows:</p>
<p id="p-0094" num="0098">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mrow>
  <mi>d</mi>
  <mo>=</mo>
  <mfrac>
    <mi>n</mi>
    <mi>r</mi>
  </mfrac>
</mrow>
</math>
</maths>
</p>
<p id="p-0095" num="0099">In this implementation, a median process block <b>805</b> selects, for each pixel, a value that it determines is representative of the controlled background <b>104</b> at the location represented by that pixel. One method of selecting a value representative of the controlled background <b>104</b> within process block <b>805</b> is to select the median value of the n samples of each pixel. For any pixel, a number of the n sampled values in the buffer <b>804</b> may represent the object of interest <b>105</b>. Duration d is selected so that it is unlikely that the object of interest <b>105</b> will occlude any one pixel of the controlled background <b>104</b> for an accumulated duration of d/2 or longer within any time-span of d. Therefore, for any pixel, the majority of the sampled values will be representative of the background <b>104</b>, and therefore the median of the sampled values will be a value representative of the background <b>104</b>.</p>
<p id="p-0096" num="0100">The background model component <b>405</b> is adaptive, and any changes to the background <b>104</b> will be reflected in the output of median process block <b>805</b> once they have been observed for time of d/2. This system does not require that the entire controlled background <b>104</b> be visible when initialized, the object of interest <b>105</b> may be present when initialized, however it does require that samples be observed for time of d before providing output. Optionally, the constraint may be applied that the object of interest <b>105</b> must be absent when the system is initialized, in which case the first observed sample values may be copied into all n samples of the buffer <b>804</b>, allowing the system to produce an output sooner.</p>
<p id="p-0097" num="0101">The duration that any one pixel of the controlled background <b>104</b> will be occluded by the object of interest <b>105</b>, and therefore the duration d, is dependent on the particular application of the system. The number of samples, n, can be scaled for the memory buffer and processing power available.</p>
<p id="p-0098" num="0102">The preceding discussion presents one implementation of obtaining the position of the object of interest <b>105</b> within and relative to the images acquired by the cameras <b>101</b> and <b>102</b>. If the object of interest <b>105</b> was successfully detected and its coordinates found in both cameras views <b>205</b> and <b>206</b> by detection modules <b>308</b> and <b>309</b> of <figref idref="DRAWINGS">FIG. 3</figref>, then the combination of these coordinates is sufficient to recover the position of the object of interest <b>105</b> within the region of interest <b>103</b>. In the implementation outlined in <figref idref="DRAWINGS">FIG. 3</figref>, the position of the object of interest <b>105</b> is calculated in combination module <b>312</b>.</p>
<p id="p-0099" num="0103">Turning to <figref idref="DRAWINGS">FIGS. 9A and 9B</figref>, an implementation of the combination module <b>312</b> is shown. For each camera <b>101</b> and <b>102</b>, the position p <b>902</b> of the object of interest <b>105</b> on the camera's image plane <b>904</b> is converted to an angle <b>905</b>, which is referred in this description as beta (&#x3b2;), and is measured on the reference plane whose normal is defined by the axes of the rotations of the cameras <b>101</b>, <b>102</b>. (In practice, the axes are not precisely parallel and do not exactly define a single plane, however the process described herein is tolerant of that error). By approximating the camera <b>101</b>, <b>102</b> as an ideal pinhole model of the camera, that angle (&#x3b2;), relative to the vector <b>906</b> defining the orientation of the camera, is approximated.</p>
<p id="p-0100" num="0104">Eq. 3, as shown in <figref idref="DRAWINGS">FIG. 9A</figref>, illustrates an approximation calculation as follows:</p>
<p id="p-0101" num="0105">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
<mrow>
  <mi>&#x3b2;</mi>
  <mo>=</mo>
  <mrow>
    <msup>
      <mi>tan</mi>
      <mrow>
        <mo>-</mo>
        <mn>1</mn>
      </mrow>
    </msup>
    <mo>&#x2061;</mo>
    <mrow>
      <mo>(</mo>
      <mfrac>
        <mi>f</mi>
        <mi>p</mi>
      </mfrac>
      <mo>)</mo>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
To approximate the angle beta (&#x3b2;), the inverse tangent is applied to the quantity of the focal length (f) divided by the position p on the image plane projected onto the intersection of the reference plane and the image plane.
</p>
<p id="p-0102" num="0106">For maximum precision, the intrinsic camera parameters (location of the principal point and scale of image) and radial distortion caused by the lens should be corrected for by converting the distorted position (as represented by the relative position information <b>310</b>, <b>311</b>) to the ideal position. More specifically, the ideal position is the position on the image plane <b>904</b> that the object <b>105</b> would be projected if the camera <b>101</b>, <b>102</b> had the properties of an ideal pinhole camera, whereby Eq. 3 will produce the exact angle. One set of correction equations are presented in Z. Zhang, A Flexible New Technique for Camera Calibration, Microsoft Research, http://research.microsoft.com/&#x2dc;zhang, which is incorporated by reference. For many applications of the system, the approximation has been found to provide sufficient precision without this correction noted above.</p>
<p id="p-0103" num="0107">Continuing with the description of combination module <b>312</b>, a reference vector <b>907</b>, as illustrated in <figref idref="DRAWINGS">FIG. 9B</figref>, is defined such that it passes through the positions of both cameras <b>101</b> and <b>102</b> on the reference plane where the reference plane is defined such that the axis of rotation of the cameras define the normal of the reference plane. The angles <b>908</b> that the cameras are rotated are measured relative to the reference vector <b>907</b>.</p>
<p id="p-0104" num="0108">A formula for measurement of the angles is shown in Eq. 4:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b1;=&#x3b2;<sub>0</sub>+&#x3b2;<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
Measurement of the angle alpha (&#x3b1;) is equal to the angle beta_not (&#x3b2;<sub>0</sub>) and the angle beta (&#x3b2;).
</p>
<p id="p-0105" num="0109">Eq. 4 is applied to measure the angles <b>909</b> of the object of interest <b>105</b> relative to the reference vector <b>907</b>. That angle is referred to by the alpha (&#x3b1;) symbol herein. The angle alpha <b>909</b> for each camera <b>101</b> and <b>102</b>, and the length of the reference vector <b>907</b>, are sufficient to find the position of the object of interest <b>105</b> on the reference plane, by Eq. 5 and Eq. 6.</p>
<p id="p-0106" num="0110">Eq. 5 calculates the offset of the object of interest (y) by the formula:</p>
<p id="p-0107" num="0111">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
<mrow>
  <mi>y</mi>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>w</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <mi>tan</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <msub>
        <mi>&#x3b1;</mi>
        <mi>A</mi>
      </msub>
      <mo>&#x2062;</mo>
      <mi>tan</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <msub>
        <mi>&#x3b1;</mi>
        <mi>B</mi>
      </msub>
    </mrow>
    <mrow>
      <mrow>
        <mi>tan</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <msub>
          <mi>&#x3b1;</mi>
          <mi>A</mi>
        </msub>
      </mrow>
      <mo>+</mo>
      <mrow>
        <mi>tan</mi>
        <mo>&#x2062;</mo>
        <mstyle>
          <mspace width="0.3em" height="0.3ex"/>
        </mstyle>
        <mo>&#x2062;</mo>
        <msub>
          <mi>&#x3b1;</mi>
          <mi>B</mi>
        </msub>
      </mrow>
    </mrow>
  </mfrac>
</mrow>
</math>
</maths>
<br/>
The offset (y) is equal to the reciprocal of the tangent of the angle (a<smallcaps>A</smallcaps>) for camera A <b>101</b> and the tangent of the angle (a<smallcaps>B</smallcaps>) for camera B <b>102</b> multiplied by the vector length <b>907</b> (w), the tangent of the angle (a<smallcaps>A</smallcaps>) for camera A <b>101</b> and the tangent of the angle (a<smallcaps>B</smallcaps>) for camera B <b>102</b>.
</p>
<p id="p-0108" num="0112">Eq. 6 calculates the offset of the object of interest (x<smallcaps>A</smallcaps>) as follows:</p>
<p id="p-0109" num="0113">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>x</mi>
    <mi>A</mi>
  </msub>
  <mo>=</mo>
  <mfrac>
    <mi>y</mi>
    <mrow>
      <mi>tan</mi>
      <mo>&#x2062;</mo>
      <mstyle>
        <mspace width="0.3em" height="0.3ex"/>
      </mstyle>
      <mo>&#x2062;</mo>
      <msub>
        <mi>&#x3b1;</mi>
        <mi>A</mi>
      </msub>
    </mrow>
  </mfrac>
</mrow>
</math>
</maths>
<br/>
In Eq. 6, the offset (x<smallcaps>A</smallcaps>) is measured by the offset from Eq. 5 (y) divided by the tangent of the angle (a<smallcaps>A</smallcaps>) for camera A <b>101</b>.
</p>
<p id="p-0110" num="0114">The position of the object <b>105</b> on the axis perpendicular to the reference plane may be found by Eq. 7, which is applied to the position in each image, using the distance of the object of interest <b>105</b> from the camera.</p>
<p id="p-0111" num="0115">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
<mrow>
  <mi>z</mi>
  <mo>=</mo>
  <mrow>
    <mi>l</mi>
    <mo>&#x2062;</mo>
    <mfrac>
      <mi>p</mi>
      <mi>f</mi>
    </mfrac>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0112" num="0116">In Eq. 7, the position (z) is calculated as the position (p) on the image plane projected onto the vector of the image plane perpendicular to that use in Eq. 3 divided by the focal length (f) multiplied by the distance of the object of interest <b>105</b> from the camera (l).</p>
<p id="p-0113" num="0117">These relations provide a coordinate of the object of interest <b>105</b> relative to Camera A <b>101</b>. Knowing the position and size of the region of interest <b>103</b> relative to Camera A <b>101</b>, the coordinate may be converted so that it is relative to the region of interest <b>103</b>, <b>312</b> of <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0114" num="0118">Smoothing may optionally be applied to these coordinates in refinement module <b>313</b> of the implementation of this system shown in <figref idref="DRAWINGS">FIG. 3</figref>. Smoothing is a process of combining the results with those solved previously so that motion is steady from frame to frame. One method of smoothing for these particular coordinate values (x<sub>A</sub>, y, z found by combination module <b>312</b>) is described herein. Each of the components of the coordinate values associated with the object of interest <b>105</b>, that is x, y, and z, are smoothed independently and dynamically. The degree of dampening S is calculated by Eq. 8, where S is dynamically and automatically adjusted in response to the change in position is calculated as follows:</p>
<p id="p-0115" num="0119">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
<mrow>
  <mi>S</mi>
  <mo>=</mo>
  <mrow>
    <mo>{</mo>
    <mrow>
      <mrow>
        <mtable>
          <mtr>
            <mtd>
              <msub>
                <mi>S</mi>
                <mi>A</mi>
              </msub>
            </mtd>
            <mtd>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
            </mtd>
            <mtd>
              <mrow>
                <mi>if</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>D</mi>
                    <mo>&#x2264;</mo>
                    <msub>
                      <mi>D</mi>
                      <mi>A</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mrow>
                <mrow>
                  <mi>&#x3b1;</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.3em" height="0.3ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <msub>
                    <mi>S</mi>
                    <mi>B</mi>
                  </msub>
                </mrow>
                <mo>+</mo>
                <mrow>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mn>1</mn>
                      <mo>-</mo>
                      <mi>&#x3b1;</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                  <mo>&#x2062;</mo>
                  <msub>
                    <mi>S</mi>
                    <mi>A</mi>
                  </msub>
                </mrow>
              </mrow>
            </mtd>
            <mtd>
              <mrow>
                <mrow>
                  <mi>where</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>&#x2062;</mo>
                  <mi>&#x3b1;</mi>
                </mrow>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>D</mi>
                    <mo>-</mo>
                    <msub>
                      <mi>D</mi>
                      <mi>A</mi>
                    </msub>
                  </mrow>
                  <mrow>
                    <msub>
                      <mi>D</mi>
                      <mi>B</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>D</mi>
                      <mi>A</mi>
                    </msub>
                  </mrow>
                </mfrac>
              </mrow>
            </mtd>
            <mtd>
              <mrow>
                <mi>if</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>D</mi>
                      <mi>A</mi>
                    </msub>
                    <mo>&#x3c;</mo>
                    <mi>D</mi>
                    <mo>&#x3c;</mo>
                    <msub>
                      <mi>D</mi>
                      <mi>B</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <msub>
                <mi>S</mi>
                <mi>B</mi>
              </msub>
            </mtd>
            <mtd>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
            </mtd>
            <mtd>
              <mrow>
                <mi>if</mi>
                <mo>&#x2062;</mo>
                <mstyle>
                  <mspace width="0.8em" height="0.8ex"/>
                </mstyle>
                <mo>&#x2062;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>D</mi>
                    <mo>&#x2265;</mo>
                    <msub>
                      <mi>D</mi>
                      <mi>B</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
        <mo>&#x2062;</mo>
        <mstyle>
          <mtext>
</mtext>
        </mstyle>
        <mo>&#x2062;</mo>
        <mi>D</mi>
      </mrow>
      <mo>=</mo>
      <mrow>
        <mo>&#xf603;</mo>
        <mrow>
          <mrow>
            <mi>r</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mi>s</mi>
            <mo>&#x2061;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mi>t</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>&#xf604;</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
</maths>
<br/>
In Eq. 8, s(t) is the smoothed value at time t, r(t) is the raw value at time t, D<sub>A </sub>and D<sub>B </sub>are thresholds, and S<sub>A </sub>and S<sub>B </sub>define degrees of dampening.
</p>
<p id="p-0116" num="0120">Two distance thresholds, D<sub>A </sub>and D<sub>B</sub>, as shown in <figref idref="DRAWINGS">FIG. 10</figref>, define three ranges of motion. A change in position that is less than D<sub>A</sub>, motion is heavily dampened <b>1001</b> by S<sub>A</sub>, thereby reducing the tendency of a value to switch back and forth between two nearby values (a side effect of the discrete sampling of the images). A change in position greater than D<sub>B </sub>is lightly dampened <b>1002</b> by S<sub>B</sub>, or not dampened. This reduces or eliminates lag and vagueness that is introduced in some other smoothing procedures. The degree of dampening is varied for motion between D<sub>A </sub>and D<sub>B</sub>, the region marked as <b>1003</b>, so that the transition between light and heavy dampening is less noticeable. The scalar a, which is applied to Eq. 1, is found by Eq. 9 as follows:</p>
<p id="p-0117" num="0121">
<maths id="MATH-US-00010" num="00010">
<math overflow="scroll">
<mrow>
  <mi>a</mi>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>e</mi>
      <mo>&#x2061;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mn>1</mn>
          <mo>-</mo>
          <mi>S</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mi>S</mi>
  </mfrac>
</mrow>
</math>
</maths>
<br/>
In Eq. 9, scalar (a) is bound such that equal to or greater than zero, and less than or equal to one, the dampening value of S is found by Eq. 8, and e is the elapsed time since the previous frame.
</p>
<p id="p-0118" num="0122">These coordinates <b>314</b> of the object of interest <b>105</b>, if found, are typically conveyed to another process such as a user application program <b>316</b> for use. They may be conveyed to another process executing on the same image processor <b>106</b> as the above calculations where performed, or to another computing device. The method in which the data are conveyed to the application program <b>316</b> may include emulation of a traditional user input device (including mouse and keyboard), allowing the system to provide control of existing control functions within the application program <b>316</b>. The coordinates <b>314</b> of the object of interest <b>105</b> may be calculated for every video frame captured by the cameras, where one video frame is typically captured 30 times or more every second. This results in little latency between the user's actions and the application's reactions.</p>
<p id="p-0119" num="0123">In a typical implementation of the system, the application program <b>316</b> provides user feedback by displaying to the video display <b>107</b> a visual representation of an indicator. The indicator is caused to move such that its position and motion mimics the motion of the object of interest <b>105</b> (typically the user's hand).</p>
<p id="p-0120" num="0124">In one variation of this form of user interface, the indicator, such as a mouse pointer, is shown in front of other graphics, and its movements are mapped to the two dimensional space defined by the surface of the screen. This form of control is analogous to that provided by a computer mouse, such as that used with the Microsoft&#xae; Windows&#xae; operating system. An example feedback image of an application that uses this style of control is shown as <b>1102</b> in <figref idref="DRAWINGS">FIG. 11A</figref>.</p>
<p id="p-0121" num="0125">Referring to <figref idref="DRAWINGS">FIG. 11A</figref> (and briefly to <figref idref="DRAWINGS">FIG. 3</figref>), the image processor <b>106</b> also includes an optional coordinate re-mapping process <b>317</b> (<figref idref="DRAWINGS">FIG. 3</figref>). The coordinate re-mapping process <b>317</b> is operable to remap the global presence and position coordinates <b>314</b> (associated with the object of interest <b>105</b>) into the position where the indicator <b>1101</b> (such as a cursor or mouse pointer) is overlaid onto the image <b>1102</b> by way of Eq. 10 for the x coordinate, and the equivalent of this equation for the y coordinate, as follows:</p>
<p id="p-0122" num="0126">
<maths id="MATH-US-00011" num="00011">
<math overflow="scroll">
<mrow>
  <msub>
    <mi>x</mi>
    <mi>c</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <mo>{</mo>
    <mtable>
      <mtr>
        <mtd>
          <mn>0</mn>
        </mtd>
        <mtd>
          <mrow>
            <mrow>
              <mi>if</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <msub>
                <mi>x</mi>
                <mi>h</mi>
              </msub>
            </mrow>
            <mo>&#x3c;</mo>
            <msub>
              <mi>b</mi>
              <mi>l</mi>
            </msub>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mfrac>
            <mrow>
              <msub>
                <mi>x</mi>
                <mi>h</mi>
              </msub>
              <mo>-</mo>
              <msub>
                <mi>b</mi>
                <mi>l</mi>
              </msub>
            </mrow>
            <mrow>
              <msub>
                <mi>b</mi>
                <mi>r</mi>
              </msub>
              <mo>-</mo>
              <msub>
                <mi>b</mi>
                <mi>l</mi>
              </msub>
            </mrow>
          </mfrac>
        </mtd>
        <mtd>
          <mrow>
            <mrow>
              <mi>if</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <msub>
                <mi>b</mi>
                <mi>l</mi>
              </msub>
            </mrow>
            <mo>&#x2264;</mo>
            <msub>
              <mi>x</mi>
              <mi>h</mi>
            </msub>
            <mo>&#x2264;</mo>
            <msub>
              <mi>b</mi>
              <mi>r</mi>
            </msub>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>1</mn>
        </mtd>
        <mtd>
          <mrow>
            <mrow>
              <mi>if</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.8em" height="0.8ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <msub>
                <mi>x</mi>
                <mi>h</mi>
              </msub>
            </mrow>
            <mo>&#x3e;</mo>
            <msub>
              <mi>b</mi>
              <mi>r</mi>
            </msub>
          </mrow>
        </mtd>
      </mtr>
    </mtable>
    <mo>}</mo>
  </mrow>
</mrow>
</math>
</maths>
</p>
<p id="p-0123" num="0127">In Eq. 10, x<sub>h </sub>is the coordinate position <b>314</b> associated with the object <b>105</b>, x<sub>c </sub>is the cursor position on the screen, mapped 0-1, and b<sub>l </sub>and b<sub>r </sub>are the positions of the left and right bounds of a sub-region within the region of interest <b>103</b>. As illustrated in <figref idref="DRAWINGS">FIG. 11B</figref>, the entire region of the display <b>1102</b> is represented by a sub-region <b>1103</b> contained entirely within the region of interest <b>103</b>. Positions (for example, position A <b>1105</b>) within the sub-region <b>1103</b> are linearly mapped to positions (for example, <b>1106</b>) within the display <b>1102</b>. Positions (for example, position B <b>1107</b>) outside the sub-region <b>1103</b> but still within the region of interest <b>103</b> are mapped to the nearest position (for example, <b>1108</b>) on the border of the display region <b>1102</b>. This reduces the likelihood of the user unintentionally removing the object of interest <b>105</b> (usually the user's hand or pointing finger) from the sub-region while attempting to move the indicator <b>1101</b> to a position near a border of the display.</p>
<p id="p-0124" num="0128">In scenarios where the region of interest <b>103</b> is immediately in front of the video display <b>107</b>, the sub-region <b>1103</b> may be defined to be aligned to the video display <b>107</b>, so that the indicator <b>1101</b> will appear to be aligned with the object of interest <b>105</b>. If the region of interest <b>103</b> is relatively thin, for example less that 5 cm, and the sub-region <b>1103</b> is defined in this way, then the system approximates, in terms of user-interaction, a &#x201c;touch-screen&#x201d; without limitations on the size of the video display <b>107</b>, and without requiring direct contact between the user and video display's <b>107</b> surface (for example, the video display and user may be on opposite sides of a window). As will be appreciated, the system <b>100</b> can be used with a variety of video display sizes, and may include not only computer monitors (whether CRT or LCD type displays), but also may include rear projection style television monitors, large flat screen LCD monitors, and forward projection style presentation systems.</p>
<p id="p-0125" num="0129">In scenarios where the region of interest <b>103</b> is not immediately in front of a large video display <b>107</b>, and the active image region <b>208</b> is sufficiently deep that the orientation of the object of interest is found in the orientation calculation process <b>609</b>, a vector may be extended from the object of interest's position to the video display <b>107</b> using the angle of orientation to detect the position on the video display that the user is &#x201c;pointing to.&#x201d;</p>
<p id="p-0126" num="0130">Most often, however, the active image region <b>208</b> is not sufficiently deep to accurately calculate the orientation in process block <b>609</b>. In these scenarios, where the region of interest <b>103</b> is not immediately in front of a large video display <b>107</b> and the orientation is not calculated, Eq. 10 may be applied where the sub-region <b>1103</b> is smaller than the video display. The processor then maps the absolute position of the object of interest <b>105</b> to the position indicator such that movements of the object of interest <b>105</b> are scaled to larger movements of the location of the position indicator on the video display, which allows the entire area of the video display to be easily reached by the user (for example the sub region <b>1103</b> may be defined to be at most 750 mm in width and proportional in height, a size that is easily reached by most users). When setup in this way, the system still provides the user the feeling of &#x201c;pointing to the screen.&#x201d;</p>
<p id="p-0127" num="0131">In another variation of this form of user interface, the user causes a representation of an indicator to move within a representation of a three dimensional virtual environment (examples are presented in <figref idref="DRAWINGS">FIG. 12A</figref> and <figref idref="DRAWINGS">FIG. 12B</figref>). The virtual environment may be rendered using projective transforms, so that the depths of the virtual environment are implied by the image presented on the video display <b>107</b>. Techniques for rending this sort of virtual environment include OpenGL. Eq. 10 is used to remap the x, y, and z coordinates (the sub-region <b>1103</b> becomes, for example, a cube).</p>
<p id="p-0128" num="0132">Applications that are controlled by a movable on screen indicator (for example, <figref idref="DRAWINGS">FIGS. 11A</figref>, <b>12</b>A, and <b>12</b>B), whose control has been discussed, typically present graphic representations of data or interactive elements (for example, a button <b>1109</b> or an object representation <b>1202</b>). The user is expected to cause the indicator <b>1101</b> to be positioned over one of these objects, or if a three-dimensional virtual environment is presented, touches or interacts with the object. For a two-dimensional interface, this condition may be detected by comparing the remapped indicator position <b>1106</b> to the bounds (for example, <b>1110</b>) of the graphic representation of the object, where this condition is true if the indicator position is within the object bounds. For the three-dimensional interface, this condition may be detected by comparing the bounds <b>1203</b> of either the entire indicator <b>1101</b>, or if finer control is required, a part of the indicator, with the bounds <b>1204</b> of the object <b>1202</b>. The user optionally receives feedback indicating that the cursor is positioned over an object. Feedback may be of a variety of forms, including an audio cue and/or a change in the graphical representation of either or both the cursor and object. The user may then activate, manipulate, or move the object that is under the cursor. The user is expected to indicate his intention to activate, manipulate, or move the object by performing a gesture.</p>
<p id="p-0129" num="0133">The motion of the object of interest <b>105</b> may optionally be interpreted and classified by the gesture detection module <b>315</b> as described above with respect to <figref idref="DRAWINGS">FIG. 3</figref>. The gesture detection process <b>315</b> may utilize the data produced from any component of the system. The final coordinates <b>314</b>, image coordinates <b>310</b> and <b>311</b>, or a combination of <b>310</b>, <b>311</b>, and <b>314</b>, may be sampled over time and provided as input to the gesture detection process <b>315</b>. A variety of gestures (for example, &#x201c;hovering&#x201d; and &#x201c;poking&#x201d;) have been successfully detected using this data as input to a gesture detection process <b>315</b>.</p>
<p id="p-0130" num="0134">In scenarios where the application's state (that is, whether of not the indicator <b>1101</b> is over a button <b>1109</b>) is known and is conveyed to the gesture detection module <b>315</b>. One gesture that the user performs to indicate the intention to activate the object (for example screen objects <b>1109</b>, <b>1202</b>) that is under the cursor <b>1101</b> is to cause the cursor to hover over the object (examples <b>1109</b>, <b>1202</b>) for longer than a predefined duration. This gesture performed by the user is detected by monitoring the application's state and triggering the gesture when the application state remains unchanged for the predetermined duration. The application need not be created specifically for the multicamera control system <b>100</b>, as techniques exist that can unobtrusively monitor an application's state (in the Windows operating system by setting a &#x201c;hook&#x201d; using the Windows SDK function &#x201c;SetWindowsHookEx&#x201d;) and emulating a mouse &#x201c;click&#x201d; (in the Windows operating system by using the Windows SDK function &#x201c;SendInput&#x201d;).</p>
<p id="p-0131" num="0135">In some scenarios, the application state may not be available and may not be monitored. In this case, some exemplary gestures that indicate the intention to active the object (for example screen objects <b>1109</b>, <b>1202</b>) under the cursor <b>1101</b> are holding the hand stationary (&#x201c;hovering&#x201d;), or poking the hand quickly forward and back.</p>
<p id="p-0132" num="0136">A method by which &#x201c;hovering&#x201d; has been detected is by keeping a history of the position of the object of interest <b>105</b>, where that history contains all records of the position and state for a predefined duration of time, ending with the most recent sample. That duration represents the minimum duration that the user must hold the hand stationary. The minimum and maximum position, separately in each of the three (x,y,z) dimensions, is found within the history. If the object of interest <b>105</b> was present within the region of interest <b>103</b> in all samples of the history, and the distance between the minimum and maximum is within a predefined threshold for each of the three dimensions, then the &#x201c;hovering&#x201d; gesture is reported. Those distance thresholds represent the maximum amount that the object of interest <b>105</b> is allowed to move, plus the maximum amount of variation (or &#x201c;jitter&#x201d;) expected to be introduced into the hand position by the various components of the system. The typical method in which this gesture is reported, where the system is emulating a mouse as described above, is to emulate a mouse &#x201c;click.&#x201d; Gestures representing additional operations of the mouse, &#x201c;double clicks&#x201d; and &#x201c;dragging,&#x201d; have also been detected and those operations have been emulated.</p>
<p id="p-0133" num="0137">In addition, gestures that are independent of the position of the indicator relative to an object may optionally be detected and given meaning by the application that may or may not be dependent on the application's state. An application that uses this style of interaction typically does not explicitly use or display the object of interest's position <b>317</b> or other positions. These applications can be wholly or primarily controlled with only the interpretations of the positions made by this system. These applications also need not be created specifically for this system because the interpretations made by this system can be used to simulate an action that would be performed on a traditional user input device, such as a keyboard or joystick.</p>
<p id="p-0134" num="0138">Many useful interpretations depend directly on the absolute position of the object of interest <b>105</b> within the region of interest <b>103</b>. (Alternately, the indicator position <b>1105</b> within the sub-region <b>1103</b> may be used in an equivalent manner). One method of making these interpretations is to define boxes, planes, or other shapes. A state is triggered on if the position (for example the position defined by block <b>314</b>, or alternately by the remapped coordinates from remapping process <b>317</b>) of the object of interest <b>105</b> is found to be within a first box (or beyond the border defined by the first plane), and had not been in the immediately preceding observation (either because it was elsewhere within the region of interest <b>103</b>, or was not detected). This state is maintained until the hand position is not found to be within a second box (or beyond the border defined by the second plane), at which time the state is triggered off. The second box must contain the entire first box, and is typically larger. The use of a larger box reduces occurrences of the state unintentionally triggering on and off when the object of interest <b>105</b> is detected to be near the border of the boxes, where a very small motion or minor noise in the image signals would otherwise cause the position <b>317</b> to otherwise drift in and out of the box. Typically one of three methods of interpreting this state is used, depending on the intended use of the gesture. In one method, the gesture directly reflects the state with an on and off trigger. When emulating a keyboard key or joystick fire button, it is &#x201c;pressed&#x201d; when the state is triggered on, and &#x201c;released&#x201d; when the state is triggered off. In another method, the gesture is only triggered by the transition of the state from off to on. When emulating a keyboard key or joystick button, the key is &#x201c;clicked.&#x201d; Although the duration and off state are not reported to the application, they are maintained so that the gesture will not be repeated until after the state is triggered off, so that each instance of the gesture requires a clearly defined intent by the user. A third method is to trigger the gesture when by the transition of the state from off to on, and to periodically re-trigger the gesture at predefined intervals so long as the state remains on. This emulates that way in which, holding a key down on a keyboard, causes the character to repeat in some applications.</p>
<p id="p-0135" num="0139">One way in which boxes or planes, for the above techniques, may be defined within the region of interest <b>103</b> is as follows. By defining a first plane (<b>1501</b> in <figref idref="DRAWINGS">FIG. 13A</figref>) and second plane <b>1502</b> that divides the region of interest into &#x201c;fire&#x201d; <b>1503</b> and &#x201c;neutral&#x201d; <b>1504</b> regions (the gesture reported when the object of interest <b>105</b> is in the region <b>1505</b> between the planes depends on the previous positions of the object, as described above), the above technique can detect the object of interest <b>105</b> (typically a hand) &#x201c;pushing&#x201d; forward, which is one gesture for emulating a fire button on a joystick, or causing the application to respond in a way that is commonly associated with the pressing of a joystick button (for example, the firing of a weapon in a video game).</p>
<p id="p-0136" num="0140">Another technique in which boxes or planes, for the above techniques, may be defined within the region of interest <b>103</b> is as follows. Planes of the first type <b>1506</b>, <b>1507</b>, <b>1508</b>, <b>1509</b> are defined that separate each of the left, right, top and bottom portions of the region of interest <b>103</b>, overlapping in the corner regions as illustrated in <figref idref="DRAWINGS">FIG. 13B</figref>. Planes of the second type are labeled as <b>1510</b>, <b>1511</b>, <b>1512</b>, <b>1513</b>. Each pair of first and second planes is processed independently. This combination of planes emulates the four directional cursor keys, where a hand in a corner triggers two keys, commonly interpreted by many applications as the four secondary 45 degree (diagonal) directions. Emulating the keyboard cursor in this method allows a variety of existing applications to be controlled by system <b>100</b>, including, for example, Microsoft&#xae; PowerPoint&#xae; which responds to the emulated cursor keys (e.g. the up and down arrow keys) by advancing to the next or previous slide in a presentation sequence.</p>
<p id="p-0137" num="0141">Another method of emulating control of discreet directions applies for applications that expect the four 45 degree direction states to be explicitly represented. Boxes <b>1514</b>, <b>1515</b>, <b>1516</b>, <b>1517</b> are defined for each of the four primary (horizontal and vertical) directions, with additional boxes <b>1518</b>, <b>1519</b>, <b>1520</b>, <b>1521</b> defined for each of the secondary 45 degree (diagonal) directions as illustrated <figref idref="DRAWINGS">FIG. 13C</figref>. For clarity, only boxes of the first type are illustrated. A gap is placed between these boxes. <figref idref="DRAWINGS">FIG. 13D</figref> illustrates how neighboring boxes are defined. The gap between boxes of the first type <b>1522</b>, <b>1523</b> assures that the user intentionally causes the object of interest <b>105</b> to enter the box, while the gap <b>1524</b> is filled by overlapping boxes of the second type <b>1525</b>, <b>1526</b>, so that the system will report the previous gesture until the user was clearly intended to move the object of interest <b>105</b> into either a neighboring box or the central neutral region. This combination of buttons can be used to emulate an eight-directional joystick pad.</p>
<p id="p-0138" num="0142">A wider class of gestures depend on motion instead of or in addition to position. An example is the gesture of &#x201c;swiping the hand to the left.&#x201d; This is a one gesture to convey to an application that it is to return to a previous page or state. Through emulation of a keyboard and mouse, this gesture may be used to control information presentation software, in particular Microsoft&#xae; PowerPoint&#xae;, to go to the previous slide of a presentation sequence. Through emulation of a keyboard and mouse, this gesture causes a web browser to perform the action associated with its &#x201c;back&#x201d; button. Similarly, the gesture of &#x201c;swiping the hand to the right&#x201d; is one gesture to convey to an application that the user desires to go to the next page or state. For example, this gesture causes presentation software to go to the next slide of a presentation sequence, and causes browser software to go to the next page.</p>
<p id="p-0139" num="0143">One method for detecting &#x201c;swiping the hand to the left&#x201d; is as follows. A thin stripe along the leftmost part of the region of interest <b>103</b> is defined as the left-edge region. The position (for example the position defined by block <b>314</b>, or alternately by the remapped coordinates from remapping process <b>317</b>) of the object of interest <b>105</b> is represented as the following three states:</p>
<p id="p-0140" num="0144">1. Object of interest is present and not inside the left-edge region</p>
<p id="p-0141" num="0145">2. Object of interest is present and inside the left-edge region</p>
<p id="p-0142" num="0146">3. Object of interest is not present within the hand detection region.</p>
<p id="p-0143" num="0147">A transition from state <b>1</b> to state <b>2</b> above causes the gesture detection module <b>315</b> to enter a state whereby it starts a timer and waits for the next transition. If a transition to state <b>3</b> is observed within a predetermined duration of time, the &#x201c;swiping the hand off to the left&#x201d; gesture is reported to have occurred. This technique is typically duplicated for the right, upper, and lower edges, and, because the hand position is found in three dimensions, also duplicated to detect &#x201c;pulling the hand back.&#x201d;</p>
<p id="p-0144" num="0148">A variety of gesture detection techniques have been discussed. Still other gesture detection techniques (for example, Hidden Markov Layers) are described in research literature, and may be applied in the various implementations of the system <b>100</b> described herein.</p>
<p id="p-0145" num="0149">Referring back to <figref idref="DRAWINGS">FIGS. 1 and 3</figref>, another implementation of the multicamera control system <b>100</b> is described in further detail. While <figref idref="DRAWINGS">FIG. 1</figref> shows a two camera system, it should be understood that the image processor <b>106</b> can be configured to receive input from more than two cameras, and may for particular applications include four (4) or more video cameras. In the four camera implementation, components <b>304</b>-<b>311</b> of <figref idref="DRAWINGS">FIG. 3</figref> are duplicated to support the two additional cameras. Additionally, the combination module <b>312</b> is configured to receive four sets of camera-relative presence and position data (similar to data <b>310</b> and <b>311</b>) associated with the object of interest <b>105</b> being tracked. The techniques and equations (in particular, Eq. 5 and Eq. 6) previously described can be applied to the additional pair(s) of cameras, where the output of the combination module <b>312</b> is the average of all the position from each of the camera pairs. The gesture detection module <b>315</b> is similarly reconfigured to receive four sets of cameral-relative presence and position data <b>310</b>, <b>311</b> from the two additional detection modules (similar to <b>308</b>, <b>309</b>) which are substantially similar to detection modules <b>310</b> and <b>311</b>.</p>
<p id="p-0146" num="0150">The output from the image processor <b>106</b>, which now includes processed object position coordinates and gesture information associated with four cameras, can be used by another process or user application program <b>316</b>. The formulas and geometry (described above) used to calculate coordinate information associated with the object of interest <b>105</b> from the two additional cameras are also used.</p>
<p id="p-0147" num="0151">In one implementation using four cameras, the two additional cameras are positioned at the bottom two corners within the controlled background <b>104</b> and are oriented such that to the region of interest <b>103</b> is within the field of view <b>205</b> of each camera. The advantage of a four camera system is that the position of the object of interest <b>105</b> can be tracked with greater accuracy. Thus, the application program may include more screen objects with increased density on the video display <b>107</b> because the increased tracking accuracy allows objects that are close in proximity to be correctly selected by small movements with the object of interest <b>105</b>. Moreover, the two additional cameras reduce errors in tracking the object of interest <b>105</b> when a portion of the object of interest <b>105</b> is occluded within the field of view <b>205</b> associated with one or more of the other cameras.</p>
<p id="p-0148" num="0152">While a number of implementations have been described, it will be understood that various modifications may be made. Accordingly, other implementations are within the scope of the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625849-20140107-M00001.NB">
<img id="EMI-M00001" he="12.02mm" wi="76.20mm" file="US08625849-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625849-20140107-M00002.NB">
<img id="EMI-M00002" he="12.02mm" wi="76.20mm" file="US08625849-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08625849-20140107-M00003.NB">
<img id="EMI-M00003" he="15.92mm" wi="76.20mm" file="US08625849-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08625849-20140107-M00004.NB">
<img id="EMI-M00004" he="5.67mm" wi="76.20mm" file="US08625849-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US08625849-20140107-M00005.NB">
<img id="EMI-M00005" he="6.69mm" wi="76.20mm" file="US08625849-20140107-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US08625849-20140107-M00006.NB">
<img id="EMI-M00006" he="6.35mm" wi="76.20mm" file="US08625849-20140107-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US08625849-20140107-M00007.NB">
<img id="EMI-M00007" he="6.01mm" wi="76.20mm" file="US08625849-20140107-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US08625849-20140107-M00008.NB">
<img id="EMI-M00008" he="6.35mm" wi="76.20mm" file="US08625849-20140107-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US08625849-20140107-M00009.NB">
<img id="EMI-M00009" he="19.39mm" wi="76.20mm" file="US08625849-20140107-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00010" nb-file="US08625849-20140107-M00010.NB">
<img id="EMI-M00010" he="6.01mm" wi="76.20mm" file="US08625849-20140107-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00011" nb-file="US08625849-20140107-M00011.NB">
<img id="EMI-M00011" he="14.48mm" wi="76.20mm" file="US08625849-20140107-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method comprising:
<claim-text>accessing a first image and a second image of at least a portion of a scene;</claim-text>
<claim-text>analyzing, by a processor, the first image and the second image to detect an object of interest in at least one of the first image and the second image;</claim-text>
<claim-text>generating a history of the object of interest;</claim-text>
<claim-text>detecting an occurrence of a gesture made by the object of interest by analyzing the history; and</claim-text>
<claim-text>providing an output based on the detected occurrence of the gesture.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>determining coordinates representing a location of the detected object of interest, wherein:
<claim-text>the coordinates representing the location of the detected object of interest are used in generating the history of the object of interest.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein analyzing the first image and the second image to detect the object of interest comprises:
<claim-text>generating a background model, wherein the background model comprises a luminance value for each pixel in a region of interest and the object of interest is detected within the region of interest.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein analyzing the first image and the second image to detect the object of interest further comprises:
<claim-text>creating a difference map, wherein the difference map comprises positions of pixels, from at least one of the first image and the second image, having a luminance inconsistent with the background model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein analyzing the first image and the second image to detect the object of interest further comprises:
<claim-text>performing a cluster identification process on the difference map to detect the object of interest.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the region of interest is substantially planar.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein providing the output based on the detected occurrence of the gesture comprises controlling an application based on the detected occurrence of the gesture.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the gesture comprises a poking gesture.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object of interest comprises a human hand.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. A system, comprising:
<claim-text>one or more processors; and</claim-text>
<claim-text>a memory communicatively coupled with and readable by the one or more processors and having stored therein processor-readable instructions which, when executed by the one or more processors, cause the one or more processors to:
<claim-text>access a first image and a second image of at least a portion of a scene;</claim-text>
<claim-text>analyze the first image and the second image to detect an object of interest in at least one of the first image and the second image;</claim-text>
<claim-text>generate a history of the object of interest;</claim-text>
<claim-text>detect an occurrence of a gesture made by the object of interest by analyzing the history; and</claim-text>
<claim-text>provide an output based on the detected occurrence of the gesture.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processor-readable instructions further comprise processor-readable instructions which, when executed by the one or more processors, cause the one or more processors to:
<claim-text>determine coordinates representing a location of the detected object of interest, wherein:
<claim-text>the coordinates representing the location of the detected object of interest are used in generating the history of the object of interest.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processor-readable instructions that cause the one or more processors to analyze the first image and the second image to detect the object of interest further comprise processor-readable instructions which, when executed by the one or more processors, cause the one or more processors to:
<claim-text>generate a background model, wherein the background model comprises a luminance value for each pixel in a region of interest and the object of interest is detected within the region of interest.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the processor-readable instructions that cause the one or more processors to analyze the first image and the second image to detect the object of interest further comprise processor-readable instructions which, when executed by the one or more processors, cause the one or more processors to:
<claim-text>create a difference map, wherein the difference map comprises positions of pixels, from at least one of the first image and the second image, having a luminance inconsistent with the background model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor-readable instructions that cause the one or more processors to analyze the first image and the second image to detect the object of interest further comprise processor-readable instructions which, when executed by the one or more processors, cause the one or more processors to:
<claim-text>perform a cluster identification process on the difference map to detect the object of interest.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the region of interest is substantially planar.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processor-readable instructions that cause the one or more processors to provide the output based on the detected occurrence of the gesture further comprises processor-readable instructions configured to cause the one or more processors to control an application based on the detected occurrence of the gesture.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the gesture comprises a poking gesture.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the object of interest comprises a human hand.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. An apparatus, the apparatus comprising:
<claim-text>means for accessing a first image and a second image of at least a portion of a scene;</claim-text>
<claim-text>means for analyzing the first image and the second image to detect an object of interest in at least one of the first image and the second image;</claim-text>
<claim-text>means for generating a history of the object of interest;</claim-text>
<claim-text>means for detecting an occurrence of a gesture made by the object of interest by analyzing the history; and</claim-text>
<claim-text>means for providing an output based on the detected occurrence of the gesture.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. The apparatus of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising:
<claim-text>means for determining coordinates representing a location of the detected object of interest, wherein:
<claim-text>the coordinates representing the location of the detected object of interest are used in generating the history of the object of interest.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text>21. The apparatus of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the means for analyzing the first image and the second image to detect the object of interest comprises:
<claim-text>means for generating a background model, wherein the background model comprises a luminance value for each pixel in a region of interest and the object of interest is detected within the region of interest.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text>22. The apparatus of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the means for analyzing the first image and the second image to detect the object of interest further comprises:
<claim-text>means for creating a difference map, wherein the difference map comprises positions of pixels, from at least one of the first image and the second image, having a luminance inconsistent with the background model.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text>23. The apparatus of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the means for analyzing the first image and the second image to detect the object of interest further comprises:
<claim-text>means for performing a cluster identification process on the difference map to detect the object of interest.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text>24. A computer program product residing on a non-transitory processor-readable medium, the computer program product comprising processor-readable instructions configured to cause one or more processors to:
<claim-text>access a first image and a second image of at least a portion of a scene;</claim-text>
<claim-text>analyze the first image and the second image to detect an object of interest in at least one of the first image and the second image;</claim-text>
<claim-text>generate a history of the object of interest;</claim-text>
<claim-text>detect an occurrence of a gesture made by the object of interest by analyzing the history; and</claim-text>
<claim-text>provide an output based on the detected occurrence of the gesture.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
