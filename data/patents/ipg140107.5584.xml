<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626684-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626684</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13325321</doc-number>
<date>20111214</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>196</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>706 16</main-classification>
<further-classification>706 45</further-classification>
</classification-national>
<invention-title id="d2e53">Multi-modal neural network for universal, online learning</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5764860</doc-number>
<kind>A</kind>
<name>Yatsuzuka</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5768476</doc-number>
<kind>A</kind>
<name>Sugaya et al.</name>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>7430546</doc-number>
<kind>B1</kind>
<name>Suri</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>7953683</doc-number>
<kind>B2</kind>
<name>Minamino et al.</name>
<date>20110500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>7996342</doc-number>
<kind>B2</kind>
<name>Grabarnik et al.</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2008/0071712</doc-number>
<kind>A1</kind>
<name>Cecchi et al.</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2008/0091628</doc-number>
<kind>A1</kind>
<name>Srinivasa et al.</name>
<date>20080400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2008/0162391</doc-number>
<kind>A1</kind>
<name>Izhikevich</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2010/0049677</doc-number>
<kind>A1</kind>
<name>Jaros et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2010/0179935</doc-number>
<kind>A1</kind>
<name>Srinivasa et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>WO</country>
<doc-number>2008112921</doc-number>
<kind>A1</kind>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>WO</country>
<doc-number>2009006735</doc-number>
<kind>A1</kind>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00013">
<othercit>Pham, et al., Controlling Multi-Class Error Rates for MLP Classifier by Bias Adjustment based on Penalty Matrix, ICUIMC'12, Feb. 20-22, 2012, pp. 1-9.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00014">
<othercit>Hahnloser, R., &#x201c;Learning Algorithms Based on Linearization,&#x201d; Network: Computation in Neural System, Aug. 1998, pp. 363-380, vol. 9, No. 3, Informa Healthcare, United Kingdom.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00015">
<othercit>Cios, K.J. et al., &#x201c;Advances in Applications of Spiking Neuron Networks,&#x201d; Proceedings of the SPIE Applications and Science of Computational Intelligence III Conference, 2000, pp. 324-336, vol. 4055, SPIE, United States.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00016">
<othercit>Buonomano, D.V. et al., &#x201c;State-Dependent Computations: Spatiotemporal Processing in Cortical Networks,&#x201d; Nature Reviews Neuroscience, 2009, pp. 113-125, vol. 10, No. 2, Macmillan Publishers Limited, United States.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00017">
<othercit>Huerta, R. et al., &#x201c;Fast and Robust Learning by Reinforcement Signals: Explorations in the Insect Brain,&#x201d; Letter in Neural Computation, Aug. 2009, pp. 2123-2151, vol. 21, No. 8, Massachusetts Institute of Technology Press, United States.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00018">
<othercit>Seeger, M.W. et al., &#x201c;Bayesian Inference and Optimal Design for the Sparse Linear Model,&#x201d; Journal of Machine Learning Research (JMLR), Jun. 1, 2008, pp. 759-813, vol. 9, Massachusetts Institute of Technology Press and Microtome Publishing, United States.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00019">
<othercit>Upegui, A. et al., &#x201c;An FPGA Platform for On-line Topology Exploration of Spiking Neural Networks,&#x201d; Microprocessors and Microsystems, 2005, pp. 211-223, vol. 29, No. 5, Elsevier B.V., The Netherlands.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Schaal, S. et al., &#x201c;Computational Approaches to Motor Learning by Imitation,&#x201d; Philosophical Transactions of the Royal Society B: Biological Sciences, Mar. 29, 2003, pp. 537-547, vol. 358, No. 1431, The Royal Society, London, United Kingdom.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Swinehart, C.D. et al, &#x201c;Dimensional Reduction for Reward-based Learning&#x201d;, Network: Computation in Neural Systems, Sep. 2006, pp. 235-252, vol. 17, Issue 3, Informa Healthcare, United Kingdom.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>17</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>706 16</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>706 45</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>25</number-of-drawing-sheets>
<number-of-figures>27</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130159229</doc-number>
<kind>A1</kind>
<date>20130620</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Modha</last-name>
<first-name>Dharmendra S.</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Modha</last-name>
<first-name>Dharmendra S.</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Sherman, Esq.</last-name>
<first-name>Kenneth L.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="02" rep-type="attorney">
<addressbook>
<last-name>Zarrabian, Esq.</last-name>
<first-name>Michael</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
<agent sequence="03" rep-type="attorney">
<addressbook>
<orgname>Sherman &#x26; Zarrabian LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>International Business Machines Corporation</orgname>
<role>02</role>
<address>
<city>Armonk</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Starks</last-name>
<first-name>Wilbert L</first-name>
<department>2122</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">In one embodiment, the present invention provides a neural network comprising multiple modalities. Each modality comprises multiple neurons. The neural network further comprises an interconnection lattice for cross-associating signaling between the neurons in different modalities. The interconnection lattice includes a plurality of perception neuron populations along a number of bottom-up signaling pathways, and a plurality of action neuron populations along a number of top-down signaling pathways. Each perception neuron along a bottom-up signaling pathway has a corresponding action neuron along a reciprocal top-down signaling pathway. An input neuron population configured to receive sensory input drives perception neurons along a number of bottom-up signaling pathways. A first set of perception neurons along bottom-up signaling pathways drive a first set of action neurons along top-down signaling pathways. Action neurons along a number of top-down signaling pathways drive an output neuron population configured to generate motor output.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="168.40mm" wi="202.78mm" file="US08626684-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="108.29mm" wi="93.56mm" file="US08626684-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="205.06mm" wi="187.79mm" orientation="landscape" file="US08626684-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="153.16mm" wi="95.50mm" file="US08626684-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="173.65mm" wi="148.67mm" file="US08626684-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="194.82mm" wi="158.24mm" file="US08626684-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="256.96mm" wi="173.65mm" orientation="landscape" file="US08626684-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="222.33mm" wi="167.89mm" orientation="landscape" file="US08626684-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="219.79mm" wi="173.65mm" orientation="landscape" file="US08626684-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="165.35mm" wi="156.97mm" file="US08626684-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="210.82mm" wi="171.70mm" file="US08626684-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="249.26mm" wi="68.58mm" orientation="landscape" file="US08626684-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="254.42mm" wi="130.73mm" orientation="landscape" file="US08626684-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="237.07mm" wi="186.52mm" orientation="landscape" file="US08626684-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="227.50mm" wi="185.17mm" orientation="landscape" file="US08626684-20140107-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="223.01mm" wi="185.84mm" orientation="landscape" file="US08626684-20140107-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="128.19mm" wi="110.24mm" file="US08626684-20140107-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="146.73mm" wi="105.75mm" file="US08626684-20140107-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="145.46mm" wi="105.07mm" file="US08626684-20140107-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="90.34mm" wi="110.24mm" file="US08626684-20140107-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="83.99mm" wi="107.70mm" file="US08626684-20140107-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="96.77mm" wi="103.21mm" file="US08626684-20140107-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="98.72mm" wi="102.53mm" file="US08626684-20140107-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="217.25mm" wi="142.24mm" file="US08626684-20140107-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="221.06mm" wi="136.48mm" file="US08626684-20140107-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="233.26mm" wi="189.06mm" file="US08626684-20140107-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?GOVINT description="Government Interest" end="lead"?>
<p id="p-0002" num="0001">This invention was made with Government support under HR0011-09-C-0002 awarded by Defense Advanced Research Projects Agency (DARPA). The Government has certain rights in this invention.</p>
<?GOVINT description="Government Interest" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">The present invention relates to neuromorphic and synaptronic computation, and in particular, a multi-modal neural network for universal, online learning.</p>
<p id="p-0004" num="0003">Neuromorphic and synaptronic computation, also referred to as artificial neural networks, are computational systems that permit electronic systems to essentially function in a manner analogous to that of biological brains. Neuromorphic and synaptronic computation do not generally utilize the traditional digital model of manipulating 0 s and 1 s. Instead, neuromorphic and synaptronic computation create connections between processing elements that are roughly functionally equivalent to neurons of a biological brain. Neuromorphic and synaptronic computation may comprise various electronic circuits that are modeled on biological neurons.</p>
<p id="p-0005" num="0004">In biological systems, the point of contact between an axon of a neuron and a dendrite on another neuron is called a synapse, and with respect to the synapse, the two neurons are respectively called pre-synaptic and post-synaptic. The essence of our individual experiences is stored in conductance of the synapses. The synaptic conductance changes with time as a function of the relative spike times of pre-synaptic and post-synaptic neurons, as per spike-timing dependent plasticity (STDP). The STDP rule increases the conductance of a synapse if its post-synaptic neuron fires after its pre-synaptic neuron fires, and decreases the conductance of a synapse if the order of the two firings is reversed.</p>
<heading id="h-0002" level="1">BRIEF SUMMARY</heading>
<p id="p-0006" num="0005">In one embodiment, the present invention provides a neural network comprising multiple modalities. Each modality comprises multiple neurons. The neural network further comprises an interconnection lattice for cross-associating signaling between the neurons in different modalities. Each neuron generates a signal in response to input signals from one or more other neurons via the interconnection lattice. The interconnection lattice comprises a plurality of reciprocal signaling pathways for directed information flow between different modalities. The plurality of reciprocal signaling pathways comprises top-down signaling pathways and bottom-up signaling pathways configured for information flow in a first direction and a second direction opposite to the first direction, respectively. Each bottom-up signaling pathway has a reciprocal top-down signaling pathway, such that bottom-up signaling pathways for a first set of modalities influence top-down signaling pathways for a second set of modalities via learning rules.</p>
<p id="p-0007" num="0006">In another embodiment, the present invention provides a neural network comprising a first set and a second set of neural nodes. Each node of the first set comprises multiple neuron populations including multiple neurons. Each node of the second set is a union of at least two nodes of the first set. The neural network further comprises an interconnect network comprising multiple directed edges that connect neuron in nodes of the first set with neurons in nodes of the second set. Nodes of the first and second set are arranged in a lattice. A connected node of the second set exchanges signals with at least two nodes of the first set via the interconnect network. Each neuron generates a firing signal in response to input signals from one or more other neurons via the interconnect network.</p>
<p id="p-0008" num="0007">In yet another embodiment, the present invention provides a method comprising interconnecting a plurality of neural nodes via an interconnect network of multiple signaling pathways arranged in a lattice. Interconnecting said plurality of neural nodes includes connecting a plurality of first nodes in a first set of nodes with a plurality of second nodes in a second set of nodes. Each node generates a signal in response to input signals received from one or more other nodes via the interconnect network. A connected node in the second set exchanging signals with at least two nodes in the first set via the interconnect network.</p>
<p id="p-0009" num="0008">In yet another embodiment, the present invention provides a computer program product on a computer-readable medium for cross-associating signaling in a neural network comprising a plurality of neural nodes connected via an interconnect network. The interconnect network comprises bottom-up signaling pathways and top-down signaling pathways arranged in a lattice. Each node has a sensory-motor modality and generates a signal in response to input signals received from one or more other nodes via the interconnect network.</p>
<p id="p-0010" num="0009">These and other features, aspects and advantages of the present invention will become understood with reference to the following description, appended claims and accompanying figures.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS</heading>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an example structure of a neural module, in accordance with an embodiment of the invention;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a neural network circuit, in accordance with an embodiment of the invention;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 3</figref> illustrates nodes arranged in an interconnection lattice, in accordance with an embodiment of the invention;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 4</figref> illustrates acyclic digraphs for each node in a lattice, in accordance with an embodiment of the invention;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a bottom-up digraph of the lattice, in accordance with an embodiment of the invention;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a lattice including a top-down digraph corresponding to a bottom-up digraph of the lattice, in accordance with an embodiment of the invention;</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a lattice including a combined perception-action graph, in accordance with an embodiment of the invention;</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 8</figref> illustrates a lattice including a combined perception-action graph, in accordance with another embodiment of the invention;</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 9</figref> illustrates a bijection between vertices, in accordance with an embodiment of the invention;</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 10</figref> illustrates neuron populations of vertices, in accordance with an embodiment of the invention;</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 11</figref> illustrates an example of synaptic connections between neuron populations, in accordance with an embodiment of the invention;</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 12</figref> illustrates another example of synaptic connections between neuron populations, in accordance with an embodiment of the invention;</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 13</figref> illustrates another example of neuron populations, in accordance with an embodiment of the invention;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 14</figref> illustrates yet another example of synaptic connections between neuron populations;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 15</figref> illustrates an example neural network, in accordance with an embodiment of the invention;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 16</figref> illustrates a neural network with an evaluation module, in accordance with an embodiment of the invention;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. 17</figref> illustrates a neural network with an evaluation module, in accordance with an embodiment of the invention;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 18</figref> illustrates an evaluation module <b>70</b> of a neural network, in accordance with an embodiment of the invention;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 19A</figref> illustrates an example of bijection between vertices, in accordance with an embodiment of the invention;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 19B</figref> illustrates another example of bijection between vertices, in accordance with an embodiment of the invention;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 20</figref> illustrates an example of weights of the synaptic connections between neuron populations, in accordance with an embodiment of the invention;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 21</figref> illustrates another example of weights of the synaptic connections between neuron populations, in accordance with an embodiment of the invention;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 22A</figref> illustrates an example Hebbian learning rule, in accordance with the present invention;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 22B</figref> illustrates an example anti-Hebbian learning rule, in accordance with the present invention;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 23</figref> illustrates a flowchart of an example process <b>800</b> for a lattice, in accordance with an embodiment of the invention;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 24</figref> illustrates a flowchart of an example process <b>900</b> for a neural network, in accordance with an embodiment of the invention; and</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 25</figref> is a high level block diagram showing an information processing circuit <b>300</b> useful for implementing one embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0038" num="0037">The present invention relates to a multi-modal neural network for universal, online learning. In one embodiment, the present invention provides a neural network comprising multiple modalities, wherein each modality comprises multiple neurons. The neural network further comprises an interconnection lattice for cross-associating signaling between the neurons in the different modalities. Each neuron generates a signal in response to input signals from one or more other neurons via the interconnection lattice.</p>
<p id="p-0039" num="0038">The interconnection lattice comprises a plurality of reciprocal signaling pathways for directed information flow between different modalities. The plurality of reciprocal signaling pathways comprises top-down signaling pathways and bottom-up signaling pathways configured for information flow in a first direction and a second direction opposite to the first direction, respectively.</p>
<p id="p-0040" num="0039">Each bottom-up signaling pathway has a reciprocal top-down signaling pathway, such that bottom-up signaling pathways for a first set of modalities influence top-down signaling pathways for a second set of modalities via learning rules. The bottom-up signaling pathways are arranged in an acyclic bottom-up digraph. The top-down signaling pathways are arranged in an acyclic top-down digraph, such that each top-down signaling pathway in the top-down digraph has a reciprocal bottom-up signaling pathway in the bottom-up digraph, and each action neuron population along a top-down signaling pathway in the top-down digraph corresponds to a perception neuron population along a reciprocal bottom-down pathway in the bottom-up digraph.</p>
<p id="p-0041" num="0040">Each modality further includes a perception neuron population and an action neuron population, such that each perception neuron has a corresponding action neuron. A perception neuron population at an input periphery of the neural network is designated as an input neuron population configured to receive sensory input, and an action neuron population at an output periphery of the neural network is designated as an output neuron population configured to generate motor output.</p>
<p id="p-0042" num="0041">The interconnection lattice further includes a plurality of perception neuron populations along a number of bottom-up signaling pathways, and a plurality of action neuron populations along a number of top-down signaling pathways. The input neuron population drives perception neurons along a number of bottom-up signaling pathways. A first set of perception neurons along bottom-up signaling pathways drive a first set of action neurons along top-down signaling pathways. Action neurons along a number of top-down signaling pathways drive the output neuron population.</p>
<p id="p-0043" num="0042">Each perception neuron along a bottom-up signaling pathway is trained using a learning rule based on the firing events of said perception neuron and the firing events of the corresponding action neuron along a reciprocal top-down signaling pathway. Each action neuron along a top-down signaling pathway is trained using a learning rule based on the firing events of said action neuron and the firing events of the corresponding perception neuron along a reciprocal bottom-up signaling pathway.</p>
<p id="p-0044" num="0043">In another embodiment, the present invention provides a neural network comprising a first set and a second set of neural nodes. Each node of the first set comprises multiple neuron populations including multiple neurons. Each node of the second set is a union of at least two nodes of the first set. The neural network further comprises an interconnect network comprising multiple directed edges that connect neuron in nodes of the first set with neurons in nodes of the second set. Nodes of the first and second set are arranged in a lattice. A connected node of the second set exchanges signals with at least two nodes of the first set via the interconnect network. Each neuron generates a firing signal in response to input signals from one or more other neurons via the interconnect network.</p>
<p id="p-0045" num="0044">Neuron populations in a node are interconnected via multiple directed edges arranged in an acyclic digraph, each edge comprising a signaling pathway in the interconnect network. The interconnect network interconnects said nodes via bottom-up signaling pathways arranged in an acyclic bottom-up digraph in the interconnect network, each bottom-up signaling pathway including one or more neuron populations and directed edges. A first neuron population in a first node is interconnected to a second neuron population in a second node only if the second node is a superset of the first node. The interconnect network further interconnects said nodes via top-down signaling pathways arranged in an acyclic top-down digraph in the interconnect network, each top-down signaling pathway including one or more neuron populations and directed edges.</p>
<p id="p-0046" num="0045">Each neuron population in the top-down digraph corresponds to a neuron population in the bottom-up digraph. Each top-down signaling pathway in the top-down digraph has a reciprocal bottom-up signaling pathway in the bottom-up digraph, wherein information flows along said top-down signaling pathway in a first direction, and information flows along said reciprocal bottom-up signaling pathway in a direction opposite of the first direction.</p>
<p id="p-0047" num="0046">A neuron population at an input periphery of the neural network is designated as an input neuron population configured to receive sensory input, wherein the input neuron population drives neurons along a number of bottom-up signaling pathways. A first set of neurons along bottom-up signaling pathways drive a first set of neurons along top-down signaling pathways. A neuron population at an output periphery of the neural network is designated as an output neuron population configured to generate motor output, wherein neurons along a number of top-down signaling pathways drive the output neuron population.</p>
<p id="p-0048" num="0047">Each neuron along a bottom-up signaling pathway is trained using a learning rule based on the firing events of said neuron and the firing events of the corresponding neuron along a reciprocal top-down signaling pathway. Each neuron along a top-down signaling pathway is trained using a learning rule based on the firing events of said neuron and the firing events of the corresponding neuron along a reciprocal bottom-up signaling pathway.</p>
<p id="p-0049" num="0048">In yet another embodiment, the present invention provides a method comprising interconnecting a plurality of neural nodes via an interconnect network of multiple signaling pathways arranged in a lattice. Interconnecting said plurality of neural nodes includes connecting a plurality of first nodes in a first set of nodes with a plurality of second nodes in a second set of nodes. Each node generates a signal in response to input signals received from one or more other nodes via the interconnect network. A connected node in the second set exchanging signals with at least two nodes in the first set via the interconnect network.</p>
<p id="p-0050" num="0049">A node comprises one or more neuron populations interconnected via multiple directed edges arranged in an acyclic digraph, wherein each neuron population comprises one or more neurons, and each edge includes a signaling pathway in the interconnect network. Interconnecting said plurality of neural nodes further includes interconnecting said plurality of neural nodes via bottom-up signaling pathways arranged in an acyclic bottom-up digraph in the interconnect network, each bottom-up signaling pathway including one or more neuron populations and directed edges. Interconnecting said plurality of neural nodes further includes interconnecting said plurality of neural nodes via top-down signaling pathways arranged in an acyclic top-down digraph in the interconnect network, each top-down signaling pathway including one or more neuron populations and directed edges.</p>
<p id="p-0051" num="0050">In yet another embodiment, the present invention provides a computer program product on a computer-readable medium for cross-associating signaling in a neural network comprising a plurality of neural nodes connected via an interconnect network. The interconnect network comprises bottom-up signaling pathways and top-down signaling pathways arranged in a lattice. Each node has a sensory-motor modality and generates a signal in response to input signals received from one or more other nodes via the interconnect network.</p>
<p id="p-0052" num="0051">Embodiments of the present invention provide a computational architecture representing a number of sensory and motor (&#x201c;sensory-motor&#x201d;) modalities of a neural network, wherein each modality comprises an input and an output neuron population such that each input neuron corresponds to exactly one output neuron. The input population of each modality drives perception neurons along a number of bottom-up pathways and the output population is driven by action neurons along a number of top-down pathways, where each bottom-up pathway has a reciprocal top-down pathway and each perception neuron has a corresponding action neuron. Bottom-up pathways from several modalities can interact along an underlying lattice and can influence top-down pathways of other modalities. Each perception neuron is trained via the output of the corresponding action neuron in the reciprocal top-down pathway, and, conversely, each action neuron is trained via the output of the corresponding perception neuron. The entire system maintains stable activity levels via self-tuning and gain control. The resulting computational architecture is online, local, parallel, distributed, can deal with massive influx of data, can be adapted to work with spiking neurons, can extract deep features, and can solve problems of unsupervised learning, supervised learning, and reinforcement learning within a single universal substrate of adaptation.</p>
<p id="p-0053" num="0052">The term digital neuron as used herein represents an architecture configured to simulate a biological neuron. A digital neuron creates connections between processing elements that are roughly functionally equivalent to neurons of a biological brain. As such, a neuromorphic and synaptronic computation comprising digital neurons according to embodiments of the invention may include various electronic circuits that are modeled on biological neurons. Further, a neuromorphic and synaptronic computation comprising digital neurons according to embodiments of the invention may include various processing elements (including computer simulations) that are modeled on biological neurons. Although certain illustrative embodiments of the invention are described herein using digital neurons comprising electronic circuits, the present invention is not limited to electronic circuits. A neuromorphic and synaptronic computation according to embodiments of the invention can be implemented as a neuromorphic and synaptronic architecture comprising circuitry, and additionally as a computer simulation. Indeed, embodiments of the invention can take the form of an entirely hardware embodiment, an entirely software embodiment or an embodiment containing both hardware and software elements.</p>
<p id="p-0054" num="0053">An external two-way communication environment may supply sensory inputs and consume motor outputs. Digital neurons implemented using complementary metal-oxide-semiconductor (CMOS) logic gates receive spike inputs and integrate them. The neurons include comparator circuits that generate spikes when the integrated input exceeds a threshold. In one embodiment, weighted synaptic connections are implemented using transposable 1-bit static random-access memory (SRAM) cells, wherein each neuron can be an excitatory or inhibitory neuron. Each learning rule on each neuron axon and dendrite are reconfigurable.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an example structure of a neural module <b>20</b>, in accordance with an embodiment of the invention. Each neural module <b>20</b> comprises multiple neurons <b>1</b>. For instance, the neural module <b>20</b> comprises four neurons, neurons v<sup>g</sup>&#x2191;, v<sup>g</sup>&#x2193;, v<sup>b</sup>&#x2191;, and v<sup>b</sup>&#x2193;. Each neuron <b>1</b> of a neural module <b>20</b> is classified as one of the following four types of neurons: a perception neuron in a learning, bottom-up pathway; an action neuron in a learning, top-down pathway; a perception neuron in an unlearning, bottom-up pathway; and, an action neuron in an unlearning, top-down pathway. In this specification, a neuron in a learning pathway is generally referred to as a good neuron. A neuron in an unlearning pathway is generally referred to as a bad neuron.</p>
<p id="p-0056" num="0055">In <figref idref="DRAWINGS">FIG. 1</figref>, the neuron v<sup>g</sup>&#x2191; is a good perception neuron in a learning, bottom-up pathway, the neuron v<sup>g</sup>&#x2193;, is a good action neuron in a learning, top-down pathway, the neuron v<sup>b</sup>&#x2191; is a bad perception neuron in an unlearning, bottom-up pathway, and the neuron v<sup>b</sup>&#x2193; is a bad neuron in an unlearning, top-down pathway.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a neural network circuit <b>30</b>, in accordance with an embodiment of the invention. The neural network circuit <b>30</b> comprises a plurality of neural modules <b>20</b>, such as neural modules N<b>1</b>, N<b>2</b>, N<b>3</b>, N<b>4</b>, N<b>5</b>, and N<b>6</b>. Each neural module <b>20</b> comprises multiple neurons <b>1</b> (<figref idref="DRAWINGS">FIG. 1</figref>). The neural network circuit <b>30</b> further comprises a plurality of synapses (i.e., synaptic connections) <b>5</b> interconnecting the neural modules <b>20</b>. Each synapse <b>5</b> interconnects a first neural module <b>20</b> to a second neural module <b>20</b>, thereby allowing bi-directional information flow between the neurons <b>1</b> in the first neural module <b>20</b> and the neurons <b>1</b> in the second neural module <b>20</b>.</p>
<p id="p-0058" num="0057">The neural network circuit <b>30</b> may be used to implement a neural network circuit combining multiple sensory and motor modalities into one computational architecture. Sensory and motor modalities represent biological sensors and actuators (e.g., eyes, ears, hands), as well as non-biological sensors and actuators (e.g., thermal sensors).</p>
<p id="p-0059" num="0058">For instance, the neural modules <b>20</b> of the neural network circuit <b>30</b> may be organized into multiple neural module sets <b>31</b>, such as a first neural module set <b>31</b> comprising the neural modules N<b>1</b> and N<b>2</b>, a second neural module set <b>31</b> comprising the neural modules N<b>3</b> and N<b>4</b>, and a third neural module set <b>31</b> comprising the neural modules N<b>5</b> and N<b>6</b>. Each neural module set <b>31</b> represents a different sensory or motor modality (e.g., vision, auditory, etc.).</p>
<p id="p-0060" num="0059">Each neural module set <b>31</b> may cross-associate with other neural module sets <b>31</b>. Specifically, a neural module <b>20</b> in one neural module set <b>31</b> may be interconnected to another neural module <b>20</b> in another neural module set <b>31</b>. For instance, a synapse <b>5</b> interconnects the neural module N<b>2</b> in the first neural module set <b>31</b> to the neural module N<b>4</b> in the second neural module set <b>31</b>. Another synapse <b>5</b> interconnects the neural module N<b>2</b> in the first neural module set <b>31</b> to the neural module N<b>6</b> in the third neural module set <b>31</b>. As such, the first neural module set <b>31</b> cross-associates with both the second and third neural module sets <b>31</b>.</p>
<p id="p-0061" num="0060">A neural network may be represented as an acyclic directed graph (&#x201c;digraph&#x201d;) comprising a set of vertices (i.e., nodes) and a set of directed edges. Each directed edge interconnects a sending vertex to a receiving vertex. In this specification, let G&#x2032;=(V&#x2032;, E&#x2032;) generally denote an acyclic digraph comprising a set of vertices V&#x2032; and a set of directed edges E&#x2032;. Let sink (G&#x2032;) denote a subset of vertices in V&#x2032; that have no outgoing directed edges (i.e., do not send out outgoing connections). Let source (G&#x2032;) denote a subset of vertices in V&#x2032; that have no incoming directed edges (i.e., do not receive incoming connections).</p>
<p id="p-0062" num="0061">Neural modules from several sensory and motor modalities can interact along an underlying interconnection lattice. Let L generally denote an interconnection lattice that cross-associates multiple sensory and motor modalities. Let S<sub>1</sub>, S<sub>2</sub>, . . . , S<sub>m </sub>generally denote the sensory and motor modalities, where m is the total number of sensory or motor modalities that the lattice L cross-associates. Let S generally denote a modality of the lattice L.</p>
<p id="p-0063" num="0062">Further, let S&#x2032; generally denote a set. For any set S&#x2032;, let |S&#x2032;| denote its cardinality. L comprises non-empty subsets of F&#x2261;{S<sub>1</sub>, S<sub>2</sub>, . . . , S<sub>m</sub>}. Sets {S<sub>1</sub>}, {S<sub>2</sub>}, . . . , {S<sub>m</sub>} denote atomic sets of the modalities S<sub>1</sub>, S<sub>2</sub>, . . . , S<sub>m</sub>, respectively. Let A={{S<sub>1</sub>}, {S<sub>2</sub>}, . . . , {S<sub>m</sub>}}<sub>5 </sub>where A denotes the set of all atomic sets.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIGS. 3-6</figref> illustrate the stages of setting up an example interconnection lattice <b>100</b>, in accordance with an embodiment of the invention.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 3</figref> illustrates nodes arranged in an interconnection lattice <b>100</b>, in accordance with an embodiment of the invention. The lattice <b>100</b> cross-associates multiple sensory and motor modalities, such as modalities S<sub>1</sub>, S<sub>2</sub>, and S<sub>3</sub>. The lattice <b>100</b> comprises a first set of nodes comprising multiple nodes <b>2</b>, and a second set of nodes comprising multiple nodes <b>3</b>. Each node <b>2</b> represents an atomic set that has a sensory or motor modality. Specifically, a first node <b>2</b> represents an atomic set {S<sub>1</sub>} having the modality S<sub>1</sub>, a second node <b>2</b> represents an atomic set {S<sub>2</sub>} having the modality S<sub>2</sub>, and a third node <b>2</b> represents an atomic set {S<sub>3</sub>} having the modality S<sub>3</sub>.</p>
<p id="p-0066" num="0065">Each node <b>3</b> represents a union of two or more nodes <b>2</b> (i.e., two or more atomic sets). Specifically, a first node <b>3</b> represents a superset {S<sub>1</sub>, S<sub>2</sub>} that is the union of atomic sets {S<sub>1</sub>} and {S<sub>2</sub>}. A second node <b>3</b> represents a superset {S<sub>1</sub>, S<sub>3</sub>} that is the union of atomic sets {S<sub>1</sub>} and {S<sub>3</sub>}. A third node <b>3</b> represents a superset {S<sub>2</sub>, S<sub>3</sub>} that is the union of atomic sets {S<sub>2</sub>} and {S<sub>3</sub>}. Finally, a fourth node <b>3</b> represents a superset {S<sub>1</sub>, S<sub>2</sub>, S<sub>3</sub>} that is the union of atomic sets {S<sub>1</sub>}, {S<sub>2</sub>}, and {S<sub>3</sub>}.</p>
<p id="p-0067" num="0066">Each modality S of the lattice L may be represented by an acyclic digraph G<sup>S</sup>. Let acyclic digraph G<sup>S</sup>=(V<sup>S</sup>, E<sup>S</sup>) where V<sup>S </sup>denotes the set of vertices in G<sup>S</sup>, and E<sup>S </sup>denotes the set of directed edges in G<sup>S</sup>. Let sink (G<sup>S</sup>) denote a subset of vertices in V<sup>S </sup>that have no outgoing edges (i.e., outgoing connections). Let source (G<sup>S</sup>) denote a subset of vertices in V<sup>S </sup>that have no incoming edges (i.e., incoming connections).</p>
<p id="p-0068" num="0067">If S&#x2208;A (i.e., set {S} is an atomic set), digraph G<sup>S </sup>is non-empty and |source (G<sup>S</sup>)|=1. One of the vertices in V<sup>S </sup>must be a source, and only one of the vertices in V<sup>S </sup>can be a source. If S&#x2209;A (i.e., set {S} is not an atomic set), digraph G<sup>S </sup>may be empty.</p>
<p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. 4</figref> illustrates acyclic digraphs <b>4</b> for each node <b>2</b>, <b>3</b> in the lattice <b>100</b>, in accordance with an embodiment of the invention. Each node <b>2</b>, <b>3</b> comprises multiple vertices interconnected via multiple directed edges <b>10</b>. The vertices and edges <b>10</b> of each node <b>2</b>, <b>3</b> are arranged in an acyclic digraph <b>4</b>.</p>
<p id="p-0070" num="0069">Specifically, the first node <b>2</b> representing the atomic set {S<sub>1</sub>} (<figref idref="DRAWINGS">FIG. 3</figref>) provides an acyclic digraph G<sup>{S</sup><sub>1</sub><sup>}</sup> in <figref idref="DRAWINGS">FIG. 4</figref>. The digraph G<sup>{S</sup><sub>1</sub><sup>}</sup> is non-empty, comprising vertices V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup> and V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup>. The digraph G<sup>{S</sup><sub>1</sub><sup>}</sup> further comprises a directed edge <b>10</b> interconnecting the vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup> to the vertex V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup>. The vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>1</sub><sup>}</sup> is a source vertex.</p>
<p id="p-0071" num="0070">The second node <b>2</b> representing the atomic set {S<sub>2</sub>} (<figref idref="DRAWINGS">FIG. 3</figref>) provides an acyclic digraph G<sup>{S</sup><sub>2</sub><sup>}</sup> in <figref idref="DRAWINGS">FIG. 4</figref>. The digraph G<sup>{S</sup><sub>2</sub><sup>}</sup> is non-empty, comprising vertices V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, V<sub>2</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, V<sub>3</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, and V<sub>4</sub><sup>{S</sup><sub>2</sub><sup>}</sup>. The digraph G<sup>{S</sup><sub>2</sub><sup>}</sup> further comprises multiple directed edges <b>10</b> interconnecting the vertices in the digraph G<sup>{S</sup><sub>2</sub><sup>}</sup>. Specifically, the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup> is interconnected to the vertex V<sub>2</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup> is interconnected to the vertex V<sub>3</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, the vertex V<sub>2</sub><sup>{S</sup><sub>2</sub><sup>}</sup> is interconnected to the vertex V<sub>3</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, and the vertex V<sub>3</sub><sup>{S</sup><sub>2</sub><sup>}</sup> is interconnected to the vertex V<sub>4</sub><sup>{S</sup><sub>2</sub><sup>}</sup>. The vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>2</sub><sup>}</sup> is a source vertex.</p>
<p id="p-0072" num="0071">The third node <b>2</b> representing the atomic set {S<sub>3</sub>} (<figref idref="DRAWINGS">FIG. 3</figref>) provides an acyclic digraph G<sup>{S</sup><sub>3</sub><sup>}</sup> in <figref idref="DRAWINGS">FIG. 4</figref>. The digraph G<sup>{S</sup><sub>3</sub><sup>}</sup> is non-empty, comprising vertices V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, V<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, and V<sub>3</sub><sup>{S</sup><sub>3</sub><sup>}</sup>. The digraph G<sup>{S</sup><sub>3</sub><sup>}</sup> further comprises multiple directed edges <b>10</b> interconnecting the vertices in the digraph G<sup>{S</sup><sub>3</sub><sup>}</sup>. Specifically, the vertex V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> is interconnected to the vertex V<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, and the vertex V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> is interconnected to the vertex V<sub>3</sub><sup>{S</sup><sub>3</sub><sup>}</sup>. The vertex V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>3</sub><sup>}</sup> is a source vertex.</p>
<p id="p-0073" num="0072">The first node <b>3</b> representing the atomic set {S<sub>1</sub>, S<sub>2</sub>} (<figref idref="DRAWINGS">FIG. 3</figref>) provides an acyclic digraph G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> in <figref idref="DRAWINGS">FIG. 4</figref>. The digraph G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> is a non-empty, comprising vertices V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, and V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>. The digraph G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> further comprises multiple directed edges <b>10</b> interconnecting the vertices in the digraph G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>. Specifically, the vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> is interconnected to the vertex V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, and the vertex V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> is interconnected to the vertex V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>.</p>
<p id="p-0074" num="0073">The second node <b>3</b> representing the atomic set {S<sub>1</sub>, S<sub>3</sub>} (<figref idref="DRAWINGS">FIG. 3</figref>) provides an acyclic digraph G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>3</sub><sup>}</sup> in <figref idref="DRAWINGS">FIG. 4</figref>. The digraph G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>3</sub><sup>}</sup> is empty.</p>
<p id="p-0075" num="0074">The third node <b>3</b> representing the atomic set {S<sub>2</sub>, S<sub>3</sub>} (<figref idref="DRAWINGS">FIG. 3</figref>) provides an acyclic digraph G<sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> in <figref idref="DRAWINGS">FIG. 4</figref>. The digraph G<sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> is a non-empty, comprising only the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>.</p>
<p id="p-0076" num="0075">Each digraph G<sup>S </sup>may be arranged in an acyclic bottom-up digraph, wherein the bottom-up digraph comprises the vertices and edges of each digraph G<sup>S</sup>. Let G<sup>&#x2191;</sup> generally denote a bottom-up digraph. G<sup>&#x2191;</sup>=(V<sup>&#x2191;</sup>, E<sup>&#x2191;</sup>), where V<sup>&#x2191;</sup> comprises all vertices &#x222a;<sub>S&#x2209;L</sub>V<sup>S</sup>, and E<sup>&#x2191;</sup> comprises all edges &#x222a;<sub>S&#x2209;L</sub>E<sup>S</sup>. Source vertices in G<sup>&#x2191;</sup> comprise only the source vertices in the acyclic digraphs corresponding to the atomic sets, that is source (G<sup>&#x2191;</sup>)=&#x222a;<sub>S&#x2209;A </sub>source (G<sup>S</sup>).</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a bottom-up digraph <b>200</b> of the lattice <b>100</b>, in accordance with an embodiment of the invention. The lattice <b>100</b> further provides said acyclic bottom-up digraph <b>200</b> including all vertices and directed edges <b>10</b> of the digraphs G<sup>{S</sup><sub>1</sub><sup>}</sup>, G<sup>{S</sup><sub>2</sub><sup>}</sup>, G<sup>{S</sup><sub>3</sub><sup>}</sup>, G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, G<sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>. Specifically, the bottom-up digraph <b>200</b> includes the vertices V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup>, V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup>, V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, V<sub>2</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, V<sub>3</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, V<sub>4</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, V<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, V<sub>3</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, and V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, and all directed edges <b>10</b> of the digraphs G<sup>{S</sup><sub>1</sub><sup>}</sup>, G<sup>{S</sup><sub>2</sub><sup>}</sup>, G<sup>{S</sup><sub>3</sub><sup>}</sup>, G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, G<sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, and G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>.</p>
<p id="p-0078" num="0077">The set of directed edges E<sup>&#x2191;</sup> for the bottom-up digraph G<sup>&#x2191;</sup> may contain additional directed edges <b>10</b>. Three constraints are provided below to ensure that G<sup>&#x2191;</sup> is an acyclic digraph.</p>
<p id="p-0079" num="0078">The first constraint is as follows: For S, T&#x2209;L, a directed edge from a vertex V<sub>i</sub><sup>S </sup>in G<sup>S </sup>to another vertex V<sub>j</sub><sup>T </sup>in G<sup>T </sup>can exist only if the set {S} is a strict subset of the set {T}.</p>
<p id="p-0080" num="0079">Referring back to <figref idref="DRAWINGS">FIG. 5</figref>, the bottom-up digraph <b>200</b> further comprises additional directed edges <b>10</b>. Each directed edge <b>10</b> interconnects a vertex of a first node with a vertex of a second node, wherein the set represented by the first node is a subset of the set represented by the second node.</p>
<p id="p-0081" num="0080">For example, the sets {S<sub>1</sub>} and {S<sub>2</sub>} are subsets of the set {S<sub>1</sub>, S<sub>2</sub>}. A first directed edge <b>10</b> interconnects the vertex V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup> to the vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, a second directed edge <b>10</b> interconnects the vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup> to the vertex V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, a third directed edge <b>10</b> interconnects the vertex V<sub>3</sub><sup>{S</sup><sub>2</sub><sup>}</sup> to the vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, and a fourth directed edge <b>10</b> interconnects the vertex V<sub>4</sub><sup>{S</sup><sub>2</sub><sup>}</sup> to the vertex V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>. Similarly, the sets {S<sub>2</sub>} and {S<sub>3</sub>} are subsets of the set {S<sub>2</sub>, S<sub>3</sub>}. A fifth directed edge <b>10</b> interconnects the vertex V<sub>3</sub><sup>{S</sup><sub>2</sub><sup>}</sup> to the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> a sixth directed edge <b>10</b> interconnects the vertex V<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup> to the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> and a seventh directed edge <b>10</b> interconnects the vertex V<sub>3</sub><sup>{S</sup><sub>3</sub><sup>}</sup> to the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>.</p>
<p id="p-0082" num="0081">The second constraint is as follows: The only source vertices in G<sup>&#x2191;</sup> are the source vertices in the acyclic digraphs corresponding to the atomic sets, that is source (G<sup>&#x2191;</sup>)=&#x222a;<sub>S&#x2208;A </sub>source (G<sup>S</sup>). For some S&#x2208;L, let V<sub>s</sub><sup>S </sup>denote a vertex in V<sup>S</sup>. Let S<sub>j </sub>denote an atomic set that is a subset of S, and let S<sub>1 </sub>denote an atomic set that is not a subset of S. For every vertex V<sub>s</sub><sup>S </sup>that is not a source vertex, a path from a source vertex to V<sub>s</sub><sup>S </sup>exists. There, however, can be no path from a source vertex in every atomic set S<sub>i </sub>to V<sub>s</sub><sup>S</sup>.</p>
<p id="p-0083" num="0082">Referring back to <figref idref="DRAWINGS">FIG. 5</figref>, each set {S<sub>1</sub>}, {S<sub>2</sub>}, and {S<sub>3</sub>} is an atomic set with the source vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup>, V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, and V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, respectively. Vertices V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup>, V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, and V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> are the only vertices in the bottom-up digraph <b>200</b> that do not have incoming directed edges. All other vertices in the bottom-up digraph <b>200</b> have incoming directed edges. As such, the source vertices for the bottom-up digraph <b>200</b> comprise only the vertices V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup>, V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, and V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>. As illustrated in <figref idref="DRAWINGS">FIG. 5</figref>, a path exists from each source vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup>, V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, and V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> to the non-source vertices. For example, the first directed edge <b>10</b> in G<sup>{S</sup><sub>1</sub><sup>}</sup> forms a path between the source vertex V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> and the vertex V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup>.</p>
<p id="p-0084" num="0083">The third constraint is as follows: A vertex can be a sink in G<sup>&#x2191;</sup> only if it is in G<sup>F</sup>. Alternatively, a vertex can be a sink in G<sup>&#x2191;</sup> only if it is in G<sup>S </sup>such that for every strict superset T of set S (i.e., T<u style="single">&#x2283;</u>S) where sets S, T&#x2208;L, G<sup>T </sup>is an empty acyclic digraph. Further, there must be at least one outgoing directed edge from every vertex in G<sup>S </sup>that is not a sink to some vertex in G<sup>T</sup>, where T<u style="single">&#x2283;</u>S.</p>
<p id="p-0085" num="0084">Referring back to <figref idref="DRAWINGS">FIG. 5</figref>, vertices V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> and V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> are the only vertices in the bottom-up digraph <b>200</b> that do not have outgoing directed edges. All the other vertices in the bottom-up digraph <b>200</b> have outgoing directed edges. As such, the sink vertices for the bottom-up digraph <b>200</b> comprise only the vertices V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> and V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>. As illustrated in <figref idref="DRAWINGS">FIG. 5</figref>, a path exists from each non-sink vertex in S to a sink vertex in T, where T<u style="single">&#x2283;</u>S. For example, the fifth directed edge <b>10</b> forms a path between the non-sink vertex V<sub>3</sub><sup>{S</sup><sub>2</sub><sup>}</sup> and the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, where {S<sub>2</sub>, S<sub>3</sub>} <u style="single">&#x2283;</u>{S<sub>2</sub>}.</p>
<p id="p-0086" num="0085">For every S&#x2208;L, let U<sup>S </sup>be a set such that every element u&#x2208;U<sup>S </sup>corresponds to one and exactly one element in V<sup>S</sup>, and vice versa. Thus, for every S&#x2208;L, there is a bijection between U<sup>S </sup>and V<sup>S</sup>. Let u&#x2261;P<sup>S</sup>(v) generally denote the bijection.</p>
<p id="p-0087" num="0086">For every G<sup>S</sup>, there is a corresponding acyclic digraph H<sup>S</sup>=(U<sup>S</sup>, D<sup>S</sup>) where, for u<sub>1</sub>=P<sup>S</sup>(v<sub>1</sub>) and u<sub>2</sub>=P<sup>S</sup>(v<sub>2</sub>), there is an edge from u<sub>1 </sub>to u<sub>2 </sub>in D<sup>S </sup>if and only if there is an edge from v<sub>2 </sub>to v<sub>1 </sub>in E<sup>S</sup>. Further, the bottom-up digraph G<sup>&#x2191;</sup> has a corresponding top-down digraph H<sup>&#x2193;</sup>=(U<sup>&#x2193;</sup>, E<sup>&#x2193;</sup>), where U<sup>&#x2193;</sup> comprises all vertices &#x222a;<sub>S&#x2208;L</sub>U<sup>S</sup>. As stated above, there is a bijection between U<sup>S </sup>and V<sup>S </sup>for every S&#x2208;L. Accordingly, there is a natural one-to-one correspondence between elements of U<sup>&#x2193;</sup> and V<sup>&#x2191;</sup>. For every v&#x2208;V<sup>&#x2191;</sup>, let P(v) denote its corresponding element in U<sup>&#x2193;</sup>. For u<sub>1</sub>=P(v<sub>1</sub>) and u<sub>2</sub>=P(v<sub>2</sub>), there is an edge from u<sub>1 </sub>to u<sub>2 </sub>in E<sup>&#x2193;</sup> if and only if there is an edge from v<sub>2 </sub>to v<sub>1 </sub>in E<sup>&#x2191;</sup>. For every v&#x2208;G<sup>&#x2191;</sup> that is a source, the bijection u=P(v) in H<sup>&#x2193;</sup> is a sink. For every v&#x2208;G<sup>&#x2191;</sup> that is a sink, the bijection u=P(v) in H<sup>&#x2193;</sup> is a source.</p>
<p id="p-0088" num="0087">Therefore, each vertex in the top-down digraph H<sup>&#x2193;</sup> corresponds to a vertex in the bottom-up digraph G<sup>&#x2191;</sup>, and each directed edge <b>10</b> in the top-down digraph H<sup>&#x2193;</sup> corresponds to a directed edge <b>10</b> in the bottom-up digraph G<sup>&#x2191;</sup>. Information flows along a directed edge <b>10</b> in the top-down digraph H<sup>&#x2193;</sup> in a first direction, and information flows along a corresponding directed edge <b>10</b> in the bottom-up digraph in a direction opposite of the first direction.</p>
<p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. 6</figref> illustrates the lattice <b>100</b> including a top-down digraph <b>400</b> corresponding to the bottom-up digraph <b>200</b>, in accordance with an embodiment of the invention. The top-down digraph <b>400</b> comprises acyclic digraphs H<sup>{S</sup><sub>1</sub><sup>}</sup>, H<sup>{S</sup><sub>2</sub><sup>}</sup>, H<sup>{S</sup><sub>3</sub><sup>}</sup>, H<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, H<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>3</sub><sup>}</sup> and H<sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> corresponding to the acyclic digraphs G<sup>{S</sup><sub>1</sub><sup>}</sup>, G<sup>{S</sup><sub>2</sub><sup>}</sup>, G<sup>{S</sup><sub>3</sub><sup>}</sup>, G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, and G<sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> in the bottom-up digraph <b>200</b>, respectively.</p>
<p id="p-0090" num="0089">Specifically, the digraph H<sup>{S</sup><sub>1</sub><sup>}</sup> comprises vertices U<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup> and U<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup> corresponding to the vertices V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup> and V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>1</sub><sup>}</sup>, respectively. The vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup> is a source, whereas the vertex U<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup>, the bijection of V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup>, is a sink. For every directed edge <b>10</b> in the digraph G<sup>{S</sup><sub>1</sub><sup>}</sup>, there is a reciprocal directed edge <b>10</b> in the digraph H<sup>{S</sup><sub>1</sub><sup>}</sup>. For example, a directed edge <b>10</b> interconnecting the vertex U<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup> to the vertex U<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup> in the digraph H<sup>{S</sup><sub>1</sub><sup>}</sup> corresponds to the directed edge <b>10</b> interconnecting the vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup> to in the vertex V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>1</sub><sup>}</sup>.</p>
<p id="p-0091" num="0090">The digraph H<sup>{S</sup><sub>2</sub><sup>}</sup> comprises vertices U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, U<sub>2</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, U<sub>3</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, and U<sub>4</sub><sup>{S</sup><sub>2</sub><sup>}</sup> corresponding to the vertices V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, V<sub>2</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, V<sub>3</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, and V<sub>4</sub><sup>{S</sup><sub>2</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>2</sub><sup>}</sup>, respectively. The vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup> is a source, whereas the vertex U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, the bijection of V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, is a sink. For every directed edge <b>10</b> in the digraph G<sup>{S</sup><sub>2</sub><sup>}</sup>, there is a reciprocal directed edge <b>10</b> in the digraph H<sup>{S</sup><sub>2</sub><sup>}</sup>. For example, a directed edge <b>10</b> interconnecting the vertex U<sub>2</sub><sup>{S</sup><sub>2</sub><sup>}</sup> to the vertex U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup> in the digraph H<sup>{S</sup><sub>2</sub><sup>}</sup> corresponds to the directed edge <b>10</b> interconnecting the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup> to the vertex V<sub>2</sub><sup>{S</sup><sub>2</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>2</sub><sup>}</sup>.</p>
<p id="p-0092" num="0091">The digraph H<sup>{S</sup><sub>3</sub><sup>}</sup> comprises vertices U<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, U<sub>2</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, and U<sub>3</sub><sup>{S</sup><sub>3</sub><sup>}</sup> corresponding to the vertices V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, V<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, and V<sub>3</sub><sup>{S</sup><sub>3</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>3</sub><sup>}</sup>, respectively. The vertex V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> is a source, whereas the vertex U<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, the bijection of V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, is a sink. For every directed edge <b>10</b> in the digraph G<sup>{S</sup><sub>3</sub><sup>}</sup>, there is a reciprocal directed edge <b>10</b> in the digraph H<sup>{S</sup><sub>3</sub><sup>}</sup>. For example, a directed edge <b>10</b> interconnecting the vertex U<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup> to the vertex U<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> in the digraph H<sup>{S</sup><sub>3</sub><sup>}</sup> corresponds to the directed edge <b>10</b> interconnecting the vertex V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> to the vertex V<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>3</sub><sup>}</sup>.</p>
<p id="p-0093" num="0092">The digraph H<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> comprises vertices U<sub>1</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, U<sub>2</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, and U<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> corresponding to the vertices V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, and V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, respectively. The vertex V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> is a sink, whereas the vertex U<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, the bijection of V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, is a source. For every directed edge <b>10</b> in the digraph G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, there is a reciprocal directed edge <b>10</b> in the digraph H<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>. For example, a directed edge <b>10</b> interconnecting the vertex U<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> to the vertex U<sub>1</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> in the digraph H<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> corresponds to the directed edge <b>10</b> interconnecting the vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> to the vertex V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>.</p>
<p id="p-0094" num="0093">Like the digraph G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, the digraph H<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>3</sub><sup>}</sup> is an empty digraph.</p>
<p id="p-0095" num="0094">The digraph H<sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> comprises a vertex U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> corresponding to the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, respectively. The vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> is a sink, whereas the vertex U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, the bijection of V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, is a source.</p>
<p id="p-0096" num="0095">The top-down digraph <b>400</b> further comprises additional directed edges <b>10</b>. Specifically, for every directed edge <b>10</b> in the bottom-up digraph <b>200</b>, there is a reciprocal directed edge <b>10</b> in the top-down digraph <b>400</b>. For example, a directed edge <b>10</b> interconnecting the vertex U<sub>1</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> in the digraph H<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> to the vertex U<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup> in the digraph H<sup>{S</sup><sub>1</sub><sup>}</sup> corresponds to the directed edge <b>10</b> interconnecting the vertex V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>1</sub><sup>}</sup> to the vertex V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> in the digraph G<sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>.</p>
<p id="p-0097" num="0096">Vertices in the bottom-up digraph G<sup>&#x2191;</sup> are interconnected via the directed edges <b>10</b> to form bottom-up signaling pathways along which information flows in a first direction. Vertices in the top-down digraph H<sup>&#x2193;</sup> are interconnected via the directed edges <b>10</b> to form top-down signaling pathways along which information flows in a direction opposite to the first direction. The bottom-up digraph G<sup>&#x2191;</sup> and the top-down digraph H<sup>&#x2193;</sup> may be connectable to provide a combined acyclic perception-action G=(V, E), where V=V<sup>&#x2191;</sup> U<sup>&#x2193;</sup> and E contains all the directed edges E<sup>&#x2191;</sup> E<sup>&#x2193;</sup>.</p>
<p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. 7</figref> illustrates the lattice <b>100</b> including a combined perception-action graph <b>500</b>, in accordance with an embodiment of the invention. The perception-action graph <b>500</b> comprises all vertices in the bottom-up digraph <b>200</b> and the top-down digraph <b>400</b>. The perception-action graph <b>500</b> further comprises all directed edges <b>10</b> in the bottom-up digraph <b>200</b> and the top-down digraph <b>400</b>.</p>
<p id="p-0099" num="0098">The set of edges E of the perception-action graph G may contain additional directed edges <b>10</b>. Three constraints are provided below to ensure that G is an acyclic digraph, and that for every directed edge <b>10</b> interconnecting a vertex v to a vertex u, there is a reciprocal directed edge interconnecting a vertex P(u) to a vertex P(v), where the vertices v, u, P(v), P(u)&#x2208;G.</p>
<p id="p-0100" num="0099">The first constraint is as follows: Every vertex in sink (G<sup>&#x2191;</sup>) must have an outgoing directed edge, and every vertex in source (H<sup>&#x2193;</sup>) must have an incoming directed edge. Let source (G)=source (G<sup>&#x2191;</sup>), and let sink (G)=sink (H<sup>&#x2193;</sup>).</p>
<p id="p-0101" num="0100">As described above, the sink vertices in the bottom-up digraph <b>200</b> are the vertices V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> and V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, and the source vertices in the top-down digraph <b>400</b> are the vertices U<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> and U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>. Referring back to <figref idref="DRAWINGS">FIG. 7</figref>, each vertex V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> and V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> in the bottom-up digraph <b>200</b> now has an outgoing directed edge <b>10</b>. Specifically, a directed edge <b>10</b> interconnects the vertex V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> to the vertex U<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, and a directed edge <b>10</b> interconnects the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> to the vertex U<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup>. Each vertex U<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> and U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> in the top-down digraph <b>400</b> now has an incoming directed edge <b>10</b>. Specifically, a directed edge <b>10</b> interconnects the vertex V<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup> to the vertex U<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>, and a directed edge <b>10</b> interconnects the vertex V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup> to the vertex U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>.</p>
<p id="p-0102" num="0101">The source vertices in the perception-action graph <b>500</b> are the vertices V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup>, V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, and V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>. The sink vertices in the perception-action graph <b>500</b> are the vertices U<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup>, U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup>, and U<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>.</p>
<p id="p-0103" num="0102">The second constraint is as follows: For a vertex v&#x2208;V<sup>S</sup>, where V<sup>S </sup>V<sup>&#x2191;</sup>, and for a vertex u&#x2208;U<sup>T</sup>, where U<sup>T </sup>V<sup>&#x2193;</sup>, v and u are connectable if S&#x2229;T=0. If v and u are connectable, then E may contain a pair of directed edges <b>10</b> from v to u and from P(v) to P<sup>&#x2212;1</sup>(u). For example, referring back to <figref idref="DRAWINGS">FIG. 7</figref>, P(V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup>) and P<sup>&#x2212;1</sup>(U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>) are the vertices U<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup> and V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>, respectively. The directed edge <b>10</b> interconnecting the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> to the vertex U<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup> has a reciprocal directed edge <b>10</b> interconnecting the vertex V<sub>2</sub><sup>{S</sup><sub>1</sub><sup>}</sup> to the vertex U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>. Similarly, P(V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>) and P<sup>&#x2212;1</sup>(U<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup>) are the vertices U<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> and V<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup>, respectively. The directed edge <b>10</b> interconnecting the vertex V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> to the vertex U<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup> has a reciprocal directed edge <b>10</b> interconnecting the vertex V<sub>2</sub><sup>{S</sup><sub>3</sub><sup>}</sup> to the vertex U<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>. This second constraint ensures that information arising from a source vertex never feedbacks into itself. Thus, for prediction purposes, the prediction of each modality can be based other modalities but not itself.</p>
<p id="p-0104" num="0103">In one embodiment, the third constraint is as follows: To enable estimation (i.e., auto-association), a vertex v&#x2208;V<sup>S </sup>is equated to a vertex P(V)&#x2208;U<sup>S</sup>, where V<sup>S </sup>V<sup>&#x2191;</sup>, and U<sup>S </sup>U<sup>&#x2193;</sup>.</p>
<p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. 8</figref> illustrates the lattice <b>100</b> including the combined perception-action graph <b>500</b>, in accordance with another embodiment of the invention. To equate the vertex V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> to the vertex U<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> (i.e., P(V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>)), output of the vertex V<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup> is copied to the vertex U<sub>3</sub><sup>{S</sup><sub>1</sub><sup>,S</sup><sub>2</sub><sup>}</sup>. Similarly, to equate the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> to the vertex U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> (i.e., P(V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>)), output of the vertex V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup> is copied to the vertex U<sub>1</sub><sup>{S</sup><sub>2</sub><sup>,S</sup><sub>3</sub><sup>}</sup>.</p>
<p id="p-0106" num="0105">In another embodiment, the third constraint is as follows: A vertex v&#x2208;V<sup>S </sup>is connectable to vertex u&#x2208;U<sup>T</sup>, where V<sup>S </sup>V<sup>&#x2191;</sup>, U<sup>T </sup>V<sup>&#x2193;</sup>, and S=T. If v and u are connectable, then E may contain a pair of directed edges from v to u and from P(v) to P<sup>&#x2212;1</sup>(u).</p>
<p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. 9</figref> illustrates the bijection between vertices, in accordance with an embodiment of the invention. As described above, for every directed edge interconnecting a vertex v to a vertex u, there is a reciprocal directed edge interconnecting a vertex P(u) to a vertex P(v), where the vertices v, u, P(v), P(u)&#x2208;G.</p>
<p id="p-0108" num="0107">Using the perception-action graph G described above, embodiments of a neural network substrate (&#x201c;neural network&#x201d;) suitable for supervised and unsupervised learning are now disclosed herein below. Every vertex v&#x2208;V<sup>&#x2191;</sup> (i.e., a vertex in the bottom-up digraph G<sup>&#x2191;</sup>) comprises a neuron population of perception neurons generally denoted as N(v). Every vertex P(v)&#x2208;V<sup>&#x2193;</sup> (i.e., a vertex in the top-down digraph H<sup>&#x2193;</sup>) comprises another neuron population of action neurons generally denoted as N(P(v)). Each directed edge <b>10</b> interconnecting a sending vertex to a receiving vertex comprises multiple synaptic connections <b>5</b> such that a neuron in a neuron population corresponding to the receiving vertex receives synaptic connections from a subset of neurons in a neuron population corresponding to the second vertex.</p>
<p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. 10</figref> illustrates neuron populations of vertices, in accordance with an embodiment of the invention. For any vertex v&#x2208;V<sup>&#x2191;</sup>, the vertex v comprises a neuron population including multiple neurons <b>1</b>, wherein the neurons <b>1</b> are perception neurons. The bijection of v, that is P(v)&#x2208;V<sup>&#x2193;</sup>, similarly comprises another neuron population including multiple neurons <b>1</b>, wherein the neurons <b>1</b> are action neurons. The total number of neurons <b>1</b> in the vertex v is the same as the total number of neurons in the vertex P(v). Each neuron in the vertex v corresponds to exactly one neuron in the vertex P(v).</p>
<p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. 11</figref> illustrates an example of synaptic connections <b>5</b> between neuron populations, in accordance with an embodiment of the invention. For a directed edge <b>10</b> (<figref idref="DRAWINGS">FIG. 9</figref>) interconnecting a vertex v in G to a vertex u in G, every neuron <b>1</b> in the vertex u receives some synaptic connections <b>5</b> from a set of neurons <b>1</b> in the vertex v. Each synaptic connection <b>5</b> has a plastic, adaptive weight.</p>
<p id="p-0111" num="0110">In one embodiment, every neuron <b>1</b> in the vertex u receives synaptic connections <b>5</b> from every neuron <b>1</b> in the vertex v. In another embodiment, every neuron in the vertex u receives synaptic connections <b>5</b> from a subset of neurons in the vertex v. In yet another embodiment, every neuron in the vertex u receives connections from a random subset of neurons in the vertex v.</p>
<p id="p-0112" num="0111">Similarly, for the reciprocal directed edge <b>10</b> (<figref idref="DRAWINGS">FIG. 9</figref>) interconnecting a vertex P(u) to a vertex P(v), every neuron in the vertex P(v) receives some synaptic connections <b>5</b> from a set of neurons <b>1</b> in the vertex P(u). The synaptic connections <b>5</b> between the vertex v and the vertex u need not be symmetric with the synaptic connections <b>5</b> between the vertex P(u) and the vertex P(v).</p>
<p id="p-0113" num="0112">In a neural network suitable for supervised and unsupervised learning, weights of the synaptic connections <b>5</b> are first initialized. Activation then propagates in the neural network as follows: In every epoch (e.g., time step), training patterns are presented to neurons <b>1</b> in all or some source vertices. The neurons <b>1</b> in the source vertices are perception neurons. As source vertices are vertices at an input periphery of the neural network, the neurons <b>1</b> in the source vertices are designated as input neurons.</p>
<p id="p-0114" num="0113">Other neurons <b>1</b> corresponding to non-source vertices then determine whether to emit a firing signal only after all the input neurons have determined whether to emit a firing signal. Activation is propagated through the neural network until all neurons <b>1</b> have fired. Since the neural network is acyclic, there can be no deadlock, and every neuron <b>1</b> will eventually fire.</p>
<p id="p-0115" num="0114">In one embodiment, every neuron <b>1</b> is trained according to perceptron learning rules. Specifically, every neuron <b>1</b> has a target output. For every perception neuron <b>1</b> in a vertex v, the target output is simply the output of the corresponding action neuron <b>1</b> in the vertex P(v), and vice versa. In another embodiment, Winnow learning rules or its variants may be used for training the neurons <b>1</b>.</p>
<p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. 12</figref> illustrates another example of synaptic connections <b>5</b> between neuron populations, in accordance with an embodiment of the invention. As described above, if there is a directed edge <b>10</b> (<figref idref="DRAWINGS">FIG. 9</figref>) interconnecting a vertex v in G to a vertex u in G, then every neuron in the vertex u receives some synaptic connections <b>5</b> from a set of neurons <b>1</b> in the vertex v. Unlike <figref idref="DRAWINGS">FIG. 10</figref>, the synaptic connections <b>5</b> between the vertex v and the vertex u are symmetric with the synaptic connections <b>5</b> between the vertex P(u) and the vertex P(v). For a synaptic connection <b>5</b> from a first neuron <b>1</b> in the vertex v to a second neuron <b>1</b> in the vertex u, there is a reciprocal synaptic connection <b>5</b> from a third neuron <b>1</b> in the vertex P(u) to a fourth neuron <b>1</b> in the vertex P(v), where the fourth neuron corresponds to the first neuron and the third neuron corresponds to the second neuron.</p>
<p id="p-0117" num="0116">In one embodiment, each synaptic connection <b>5</b> has an adaptive, plastic weight. In another embodiment, the synaptic connection <b>5</b> interconnecting the first neuron to the second neuron has the same weight as the reciprocal synaptic connection <b>5</b> interconnecting the third neuron to the fourth neuron.</p>
<p id="p-0118" num="0117">In one embodiment, the neurons <b>1</b> are spiking neurons. For spiking neurons, an example learning rule is described as follows: If a perception neuron n along a bottom-up pathway fires, a spike-timing dependent Hebbian learning function may be applied between the last firing time of the corresponding action neuron P(n) along a corresponding top-down pathway and the last firing times of pre-synaptic neurons to the neuron n (i.e., the neurons that send out outgoing synaptic connections <b>5</b> to the neuron n). Similarly, if an action neuron m along a top-down pathway fires, a spike-timing dependent Hebbian learning function may be applied between the last firing time of the corresponding perception neuron P<sup>&#x2212;1</sup>(m) along the corresponding bottom-up pathway and the respective last firing times of the pre-synaptic neurons to the neuron m (i.e., the neurons that send out outgoing synaptic connections <b>5</b> to the neuron m).</p>
<p id="p-0119" num="0118">Other learning rules for the spiking neurons may also be used. In one embodiment, the weight of each synaptic connection <b>5</b> cannot be negative. In another embodiment, the weight of each synaptic connection <b>5</b> may be negative.</p>
<p id="p-0120" num="0119"><figref idref="DRAWINGS">FIG. 13</figref> illustrates another example of neuron populations, in accordance with an embodiment of the invention. In one embodiment, for every vertex v&#x2208;V in the perception-action graph G, vertices v<sup>g </sup>and v<sup>b </sup>are provided. Each vertex v<sup>g </sup>comprises a neuron population comprising multiple neurons <b>1</b>, wherein each neuron <b>1</b> is a good perception neuron. Each vertex v<sup>b </sup>comprises another neuron population comprising multiple neurons <b>1</b>, wherein each neuron <b>1</b> is a bad perception neuron. The total number of neurons <b>1</b> in the vertex v<sup>g </sup>is the same as the total number of neurons in the vertex v<sup>b </sup>such that each neuron in the vertex v<sup>g </sup>corresponds to exactly one neuron in the vertex v<sup>g</sup>.</p>
<p id="p-0121" num="0120">The bijection of v<sup>g</sup>, that is the vertex P(v<sup>g</sup>)&#x2208;V, comprises another neuron population comprising multiple neurons <b>1</b>, wherein each neuron is a good action neuron. The bijection of v<sup>b</sup>, that is the vertex P(v<sup>b</sup>)&#x2208;V, comprises another neuron population comprising multiple neurons <b>1</b>, wherein each neuron is a bad action neuron. The total number of neurons <b>1</b> in the vertex P(v<sup>g</sup>) is the same as the total number of neurons in the vertex P(v<sup>b</sup>) such that each neuron in the vertex P(v<sup>g</sup>) corresponds to exactly one neuron in the vertex P(v<sup>g</sup>). As such, there will be equal number of neurons <b>1</b> in each vertex v<sup>g</sup>, v<sup>b</sup>, P(v<sup>g</sup>), and P(v<sup>b</sup>).</p>
<p id="p-0122" num="0121"><figref idref="DRAWINGS">FIG. 14</figref> illustrates yet another example of synaptic connections <b>5</b> between neuron populations, in accordance with an embodiment of the invention. Let n and m generally denote a neuron <b>1</b> in the vertex v<sup>g </sup>and a neuron <b>1</b> in the vertex u<sup>g</sup>, respectively. For every synaptic connection <b>5</b> with a weight w interconnecting a neuron n of the vertex v<sup>g </sup>to a neuron m of the vertex u<sup>g</sup>, there is a synaptic connection <b>5</b> with the same weight w interconnecting a neuron corresponding to n in the vertex v<sup>b </sup>to a neuron corresponding to m in the vertex u<sup>b</sup>. Further, there are reciprocal synaptic connections between the vertex P(u<sup>g</sup>) and the vertex P(v<sup>g</sup>) and the vertex P(u<sup>b</sup>) and the vertex P(v<sup>b</sup>). The reciprocal synaptic connections <b>5</b> have the same weight x.</p>
<p id="p-0123" num="0122"><figref idref="DRAWINGS">FIG. 15</figref> illustrates an example neural network <b>600</b>, in accordance with an embodiment of the invention. There must be at least one sensory modality in a neural network. The neural network <b>600</b> utilizes the lattice <b>100</b> (<figref idref="DRAWINGS">FIG. 7</figref>) to interconnect the motor modality S<sub>3 </sub>and the sensory modalities S<sub>1 </sub>and S<sub>2</sub>. Each vertex provides vertices v<sup>g </sup>and v<sup>b </sup>(<figref idref="DRAWINGS">FIG. 13</figref>) comprising a population of good neurons (G) and a population of bad neurons (B), respectively. For each source vertex in an acyclic digraph representing a sensory modality, good neurons in said source vertex receive sensory input for the sensory modality. As illustrated in <figref idref="DRAWINGS">FIG. 15</figref>, the good neurons in the source vertices V<sub>1</sub><sup>{S</sup><sub>1</sub><sup>}</sup> and V<sub>1</sub><sup>{S</sup><sub>2</sub><sup>}</sup> are configured to receive sensory input. For each sink vertex in an acyclic digraph representing a motor modality, good neurons in said sink vertex are configured to generate motor output. As illustrated in <figref idref="DRAWINGS">FIG. 15</figref>, the good neurons in the sink vertex U<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> are configured to generate motor output.</p>
<p id="p-0124" num="0123"><figref idref="DRAWINGS">FIG. 16</figref> illustrates the neural network <b>600</b> with an evaluation module <b>70</b>, in accordance with an embodiment of the invention. In another embodiment, the neural network <b>600</b> further comprises said evaluation module <b>70</b>. For each sink vertex in an acyclic digraph representing a motor modality, motor output of the good neurons in said sink vertex are forwarded to the evaluation module <b>70</b>. As illustrated in <figref idref="DRAWINGS">FIG. 16</figref>, motor output of the good neurons in the sink vertex U<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> are forwarded to the evaluation module <b>70</b>. In one example implementation, each motor modality has its own corresponding evaluation module <b>70</b>. In another example implementation, the evaluation module <b>70</b> receives input from multiple motor modalities.</p>
<p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. 17</figref> illustrates the neural network <b>600</b> with the evaluation module <b>70</b>, in accordance with an embodiment of the invention. The evaluation module <b>70</b> determines if the motor output received from the good neurons in the sink vertex is good or bad. If the motor output is good, the evaluation module <b>70</b> forwards the good motor output to the good neurons in the corresponding source vertex of the motor modality. If the motor output is bad, the evaluation module <b>70</b> forwards the bad motor output to the bad neurons in the corresponding source vertex of the motor modality. As illustrated in <figref idref="DRAWINGS">FIG. 17</figref>, good motor output of the good neurons in the sink vertex U<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> is forwarded to the good neurons in the corresponding source vertex V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>. Bad motor output of the good neurons in the sink vertex U<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup> is forwarded to the bad neurons in the corresponding source vertex V<sub>1</sub><sup>{S</sup><sub>3</sub><sup>}</sup>.</p>
<p id="p-0126" num="0125"><figref idref="DRAWINGS">FIG. 18</figref> illustrates the evaluation module <b>70</b> of the neural network <b>600</b> (<figref idref="DRAWINGS">FIG. 17</figref>), in accordance with an embodiment of the invention. If the motor output of the neurons in the vertex P(v<sup>g</sup>) is good, the evaluation module <b>70</b> feeds the good motor output to the neurons in the corresponding source vertex v<sup>g</sup>, and a zero input to the neurons in the corresponding source vertex v<sup>b</sup>. If the motor output of the neurons in the vertex P(v<sup>g</sup>) is bad, the evaluation module <b>70</b> feeds the bad motor output to the neurons in the corresponding source vertex v<sup>b</sup>, and a zero input to the neurons in the corresponding source vertex v<sup>g</sup>. Deadlock in activation propagation is thus prevented.</p>
<p id="p-0127" num="0126"><figref idref="DRAWINGS">FIG. 19A</figref> illustrates an example of bijection between vertices, in accordance with an embodiment of the invention. In one embodiment, every good neuron is trained according to a perceptron learning rule. Specifically, every good neuron has a target output. For every good perception neuron in the bottom-up digraph, the target output is simply the output of the corresponding good action neuron in the top-down digraph, and also the complement of the output of the corresponding bad action neuron in the top-down digraph.</p>
<p id="p-0128" num="0127">Similarly, for every good action neuron in the top-down digraph, the target output is simply the output of the corresponding bad perception neuron in the bottom-up digraph, and also the complement of the output of the corresponding bad perception neuron in the bottom-up digraph. In another embodiment, a Winnow learning rule or its variants may also be used.</p>
<p id="p-0129" num="0128">For bad every neuron, there is no associated learning. Bad neurons are simply used to train good neurons in counter path.</p>
<p id="p-0130" num="0129"><figref idref="DRAWINGS">FIG. 19B</figref> illustrates another example of bijection between vertices, in accordance with an embodiment of the invention.</p>
<p id="p-0131" num="0130">In both <figref idref="DRAWINGS">FIGS. 19A and 19B</figref>, good perception neurons in the u<sup>g </sup>vertex are trained to approximate good action neurons in the P(u<sup>g</sup>) vertex and not to approximate bad action neurons in the P(u<sup>b</sup>) vertex. If a neuron in the u<sup>g </sup>vertex fires but a corresponding neuron in the P(u<sup>g</sup>) vertex does not, this is a false positive. The neuron in the u<sup>g </sup>vertex is trained to unlearn the false positive. If a neuron in the u<sup>g </sup>vertex does not fire but a corresponding neuron in the P(u<sup>g</sup>) vertex does fire, this is a false negative. The neuron in the u<sup>g </sup>vertex is trained to learn the false negative. Similarly, good perception neurons in the v<sup>g </sup>vertex are trained to approximate good action neurons in the P(v<sup>g</sup>) vertex and not to approximate bad action neurons in the P(v<sup>b</sup>) vertex.</p>
<p id="p-0132" num="0131"><figref idref="DRAWINGS">FIG. 20</figref> illustrates an example of weights of the synaptic connections between neuron populations, in accordance with an embodiment of the invention. The weight of the synaptic connections interconnecting the neurons in the v<sup>g </sup>vertex to the neurons in the u<sup>g </sup>vertex must be the same as the weight of the synaptic connections interconnecting the neurons in the v<sup>b </sup>vertex to the neurons in the u<sup>b </sup>vertex. Similarly, the weight of the synaptic connections interconnecting the neurons in the P(u<sup>g</sup>) vertex to the neurons in the P(v<sup>g</sup>) vertex must be the same as the weight of the synaptic connections interconnecting the neurons in the P(u<sup>b</sup>) vertex to the neurons in the P(v<sup>b</sup>) vertex.</p>
<p id="p-0133" num="0132">In one embodiment, the weight of the synaptic connections interconnecting the neurons in the v<sup>g </sup>vertex to the neurons in the u<sup>g </sup>vertex is asymmetric to the weight of the synaptic connections interconnecting the neurons in the P(u<sup>g</sup>) vertex to the neurons in the P(v<sup>g</sup>) vertex. Likewise, the weight of the synaptic connections interconnecting the neurons in the v<sup>b </sup>vertex to the neurons in the u<sup>b </sup>vertex is asymmetric to the weight of the synaptic connections interconnecting the neurons in the P(u<sup>b</sup>) vertex to the neurons in the P(v<sup>b</sup>) vertex.</p>
<p id="p-0134" num="0133"><figref idref="DRAWINGS">FIG. 21</figref> illustrates another example of weights of the synaptic connections between neuron populations, in accordance with an embodiment of the invention. In another embodiment, the weight of the synaptic connections interconnecting the neurons in the v<sup>g </sup>vertex to the neurons in the u<sup>g </sup>vertex is symmetric to the weight of the synaptic connections interconnecting the neurons in the P(u<sup>g</sup>) vertex to the neurons in the P(v<sup>g</sup>) vertex. Likewise, the weight of the synaptic connections interconnecting the neurons in the v<sup>b </sup>vertex to the neurons in the u<sup>b </sup>vertex is symmetric to the weight of the synaptic connections interconnecting the neurons in the P(u<sup>b</sup>) vertex to the neurons in the P(v<sup>b</sup>) vertex.</p>
<p id="p-0135" num="0134">Symmetric weights are the preferred embodiment for the example provided in <figref idref="DRAWINGS">FIG. 19A</figref>. Symmetric or asymmetric weights may be used for the example provided in <figref idref="DRAWINGS">FIG. 19B</figref>.</p>
<p id="p-0136" num="0135"><figref idref="DRAWINGS">FIG. 22A</figref> illustrates an example Hebbian learning rule, in accordance with the present invention. In one embodiment, the neurons <b>1</b> in the neural network <b>600</b> are spiking neurons. The Hebbian learning rule is applied when a good neuron fires.</p>
<p id="p-0137" num="0136"><figref idref="DRAWINGS">FIG. 22B</figref> illustrates an example anti-Hebbian learning rule, in accordance with the present invention. The anti-Hebbian learning rule is applied when a bad neuron fires.</p>
<p id="p-0138" num="0137"><figref idref="DRAWINGS">FIG. 23</figref> illustrates a flowchart of an example process <b>800</b> for a lattice, in accordance with an embodiment of the invention. In process block <b>801</b>, establish first nodes and second nodes arranged in a lattice, wherein each second node is a union of two or more first nodes. In process block <b>802</b>, for each node, specify an acyclic digraph comprising vertices interconnected via directed edges. In process block <b>803</b>, interconnect nodes via bottom-up signaling pathways arranged in an acyclic bottom-up digraph, wherein bottom-up signaling pathways include one or more vertices and directed edges. In process block <b>804</b>, interconnect nodes via top-down signaling pathways arranged in an acyclic top-down digraph, wherein top-down signaling pathways include one or more vertices and directed edges. In process block <b>805</b>, provide directed edges that interconnect the bottom-up digraph to the top-down digraph.</p>
<p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. 24</figref> illustrates a flowchart of an example process <b>900</b> for a neural network, in accordance with an embodiment of the invention. In process block <b>901</b>, establish a neuron population for each vertex, wherein each neuron population in the bottom-up digraph and top-down digraph comprises perception neurons and action neurons, respectively. In process block <b>902</b>, designate perception neurons at an input periphery as input neurons configured to receive sensory input, and designate action neurons at an output periphery as output neurons configured to generate motor output. In process block <b>903</b>, the input neurons drive the perception neurons along bottom-up pathways, and the action neurons along the top-down pathways drive the output neurons. In process block <b>904</b>, each perception neuron along a bottom-up pathway is trained using a learning rule, based on a firing event of a corresponding action neuron along a reciprocal top-down pathway, and each action neuron along a top-down pathway is trained using a learning rule, based on a firing event of a corresponding perception neuron along a reciprocal bottom-up pathway.</p>
<p id="p-0140" num="0139"><figref idref="DRAWINGS">FIG. 25</figref> is a high level block diagram showing an information processing circuit <b>300</b> useful for implementing one embodiment of the present invention. The computer system includes one or more processors, such as processor <b>302</b>. The processor <b>302</b> is connected to a communication infrastructure <b>304</b> (e.g., a communications bus, cross-over bar, or network).</p>
<p id="p-0141" num="0140">The computer system can include a display interface <b>306</b> that forwards graphics, text, and other data from the communication infrastructure <b>304</b> (or from a frame buffer not shown) for display on a display unit <b>308</b>. The computer system also includes a main memory <b>310</b>, preferably random access memory (RAM), and may also include a secondary memory <b>312</b>. The secondary memory <b>312</b> may include, for example, a hard disk drive <b>314</b> and/or a removable storage drive <b>316</b>, representing, for example, a floppy disk drive, a magnetic tape drive, or an optical disk drive. The removable storage drive <b>316</b> reads from and/or writes to a removable storage unit <b>318</b> in a manner well known to those having ordinary skill in the art. Removable storage unit <b>318</b> represents, for example, a floppy disk, a compact disc, a magnetic tape, or an optical disk, etc. which is read by and written to by removable storage drive <b>316</b>. As will be appreciated, the removable storage unit <b>318</b> includes a computer readable medium having stored therein computer software and/or data.</p>
<p id="p-0142" num="0141">In alternative embodiments, the secondary memory <b>312</b> may include other similar means for allowing computer programs or other instructions to be loaded into the computer system. Such means may include, for example, a removable storage unit <b>320</b> and an interface <b>322</b>. Examples of such means may include a program package and package interface (such as that found in video game devices), a removable memory chip (such as an EPROM, or PROM) and associated socket, and other removable storage units <b>320</b> and interfaces <b>322</b> which allow software and data to be transferred from the removable storage unit <b>320</b> to the computer system.</p>
<p id="p-0143" num="0142">The computer system may also include a communication interface <b>324</b>. Communication interface <b>324</b> allows software and data to be transferred between the computer system and external devices. Examples of communication interface <b>324</b> may include a modem, a network interface (such as an Ethernet card), a communication port, or a PCMCIA slot and card, etc. Software and data transferred via communication interface <b>324</b> are in the form of signals which may be, for example, electronic, electromagnetic, optical, or other signals capable of being received by communication interface <b>324</b>. These signals are provided to communication interface <b>324</b> via a communication path (i.e., channel) <b>326</b>. This communication path <b>326</b> carries signals and may be implemented using wire or cable, fiber optics, a phone line, a cellular phone link, an RF link, and/or other communication channels.</p>
<p id="p-0144" num="0143">In this document, the terms &#x201c;computer program medium,&#x201d; &#x201c;computer usable medium,&#x201d; and &#x201c;computer readable medium&#x201d; are used to generally refer to media such as main memory <b>310</b> and secondary memory <b>312</b>, removable storage drive <b>316</b>, and a hard disk installed in hard disk drive <b>314</b>.</p>
<p id="p-0145" num="0144">Computer programs (also called computer control logic) are stored in main memory <b>310</b> and/or secondary memory <b>312</b>. Computer programs may also be received via communication interface <b>324</b>. Such computer programs, when run, enable the computer system to perform the features of the present invention as discussed herein. In particular, the computer programs, when run, enable the processor <b>302</b> to perform the features of the computer system. Accordingly, such computer programs represent controllers of the computer system.</p>
<p id="p-0146" num="0145">From the above description, it can be seen that the present invention provides a system, computer program product, and method for implementing the embodiments of the invention. References in the claims to an element in the singular is not intended to mean &#x201c;one and only&#x201d; unless explicitly so stated, but rather &#x201c;one or more.&#x201d; All structural and functional equivalents to the elements of the above-described exemplary embodiment that are currently known or later come to be known to those of ordinary skill in the art are intended to be encompassed by the present claims. No claim element herein is to be construed under the provisions of 35 U.S.C. section 112, sixth paragraph, unless the element is expressly recited using the phrase &#x201c;means for&#x201d; or &#x201c;step for.&#x201d;</p>
<p id="p-0147" num="0146">The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein, the singular forms &#x201c;a&#x201d;, &#x201c;an&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms &#x201c;comprises&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.</p>
<p id="p-0148" num="0147">The corresponding structures, materials, acts, and equivalents of all means or step plus function elements in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A neural network, comprising:
<claim-text>multiple modalities, wherein each modality comprises multiple neurons; and</claim-text>
<claim-text>an interconnection lattice for cross-associating signaling between the neurons in the different modalities.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The neural network of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interconnection lattice comprises:
<claim-text>a plurality of reciprocal signaling pathways for directed information flow between different modalities, said plurality of reciprocal signaling pathways comprising top-down signaling pathways and bottom-up signaling pathways configured for information flow in a first direction and a second direction opposite to the first direction, respectively;</claim-text>
<claim-text>wherein each bottom-up signaling pathway has a reciprocal top-down signaling pathway, such that bottom-up signaling pathways for a first set of modalities influence top-down signaling pathways for a second set of modalities via learning rules.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The neural network of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein:
<claim-text>each neuron generates a signal in response to input signals from one or more other neurons via the interconnection lattice; and</claim-text>
<claim-text>each modality further includes a perception neuron population and an action neuron population, such that each perception neuron has a corresponding action neuron.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The neural network of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein:
<claim-text>a perception neuron population at an input periphery of the neural network is designated as an input neuron population configured to receive sensory input; and</claim-text>
<claim-text>an action neuron population at an output periphery of the neural network is designated as an output neuron population configured to generate motor output.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The neural network of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein:
<claim-text>the interconnection lattice further includes a plurality of perception neuron populations along a number of bottom-up signaling pathways, and a plurality of action neuron populations along a number of top-down signaling pathways;</claim-text>
<claim-text>wherein the input neuron population drives perception neurons along a number of bottom-up signaling pathways;</claim-text>
<claim-text>wherein a first set of perception neurons along bottom-up signaling pathways drive a first set of action neurons along top-down signaling pathways; and</claim-text>
<claim-text>wherein action neurons along a number of top-down signaling pathways drive the output neuron population.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The neural network of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein:
<claim-text>each perception neuron along a bottom-up signaling pathway is trained using a learning rule based on the firing events of said perception neuron and the firing events of the corresponding action neuron along a reciprocal top-down signaling pathway; and</claim-text>
<claim-text>each action neuron along a top-down signaling pathway is trained using a learning rule based on the firing events of said action neuron and the firing events of the corresponding perception neuron along a reciprocal bottom-up signaling pathway.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The neural network of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the bottom-up signaling pathways are arranged in an acyclic bottom-up digraph.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The neural network of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the top-down signaling pathways are arranged in an acyclic top-down digraph, such that each top-down signaling pathway in the top-down digraph has a reciprocal bottom-up signaling pathway in the bottom-up digraph, and each action neuron population along a top-down signaling pathway in the top-down digraph corresponds to a perception neuron population along a reciprocal bottom-down pathway in the bottom-up digraph.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A neural network, comprising:
<claim-text>a first set of neural nodes, wherein each node comprises multiple neuron populations including multiple neurons;</claim-text>
<claim-text>a second set of neural nodes, wherein each node of the second set is a union of at least two nodes of the first set; and</claim-text>
<claim-text>an interconnect network comprising multiple directed edges that connect neuron in nodes of the first set with neurons in nodes of the second set, such that a connected node of the second set exchanges signals with at least two nodes of the first set via the interconnect network;</claim-text>
<claim-text>wherein nodes of the first and second set are arranged in a lattice; and</claim-text>
<claim-text>wherein each neuron generates a firing signal in response to input signals from one or more other neurons via the interconnect network.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The neural network of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein neuron populations in a node are interconnected via multiple directed edges arranged in an acyclic digraph, each edge comprising a signaling pathway in the interconnect network.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The neural network of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the interconnect network interconnects said nodes via bottom-up signaling pathways arranged in an acyclic bottom-up digraph in the interconnect network, each bottom-up signaling pathway including one or more neuron populations and directed edges.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The neural network of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein a first neuron population in a first node is interconnected to a second neuron population in a second node only if the second node is a superset of the first node.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The neural network of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the interconnect network further interconnects said nodes via top-down signaling pathways arranged in an acyclic top-down digraph in the interconnect network, each top-down signaling pathway including one or more neuron populations and directed edges.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The neural network of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein:
<claim-text>each neuron population in the top-down digraph corresponds to a neuron population in the bottom-up digraph; and</claim-text>
<claim-text>each top-down signaling pathway in the top-down digraph has a reciprocal bottom-up signaling pathway in the bottom-up digraph, wherein information flows along said top-down signaling pathway in a first direction, and information flows along said reciprocal bottom-up signaling pathway in a direction opposite of the first direction.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The neural network of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein:
<claim-text>a neuron population at an input periphery of the neural network is designated as an input neuron population configured to receive sensory input, wherein the input neuron population drives neurons along a number of bottom-up signaling pathways;</claim-text>
<claim-text>a first set of neurons along bottom-up signaling pathways drive a first set of neurons along top-down signaling pathways; and</claim-text>
<claim-text>a neuron population at an output periphery of the neural network is designated as an output neuron population configured to generate motor output, wherein neurons along a number of top-down signaling pathways drive the output neuron population.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The neural network of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein:
<claim-text>each neuron along a bottom-up signaling pathway is trained using a learning rule based on the firing events of said neuron and the firing events of the corresponding neuron along a reciprocal top-down signaling pathway; and</claim-text>
<claim-text>each neuron along a top-down signaling pathway is trained using a learning rule based on the firing events of said neuron and the firing events of the corresponding neuron along a reciprocal bottom-up signaling pathway.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. A computer program product on a computer-readable medium for cross-associating signaling in a neural network comprising a plurality of neural nodes connected via an interconnect network comprising bottom-up signaling pathways and top-down signaling pathways arranged in a lattice, wherein each node has a sensory-motor modality and generates a signal in response to input signals received from one or more other nodes via the interconnect network, the computer program product comprising instructions which when executed on a computer cause the computer to perform operations including:
<claim-text>connecting a plurality of first nodes in a first set of nodes with a plurality of second nodes in a second set of nodes, such that a connected node in the second set exchanges signals with at least two nodes in the first set via the interconnect network;</claim-text>
<claim-text>for a node comprising one or more neuron populations, interconnecting the neuron populations within said node via multiple directed edges arranged in an acyclic digraph, wherein each neuron population comprises one or more neurons, and each edge includes a signaling pathway in the interconnect network;</claim-text>
<claim-text>interconnecting said plurality of neural nodes via bottom-up signaling pathways arranged in an acyclic bottom-up digraph in the interconnect network, each bottom-up signaling pathway including one or more neuron populations and directed edges;</claim-text>
<claim-text>interconnecting said plurality of neural nodes via top-down signaling pathways arranged in an acyclic top-down digraph in the interconnect network, each top-down signaling pathway including one or more neuron populations and directed edges, wherein each neuron population in the top-down digraph corresponds to a neuron population in the bottom-up digraph, wherein each top-down signaling pathway in the top-down digraph has a reciprocal bottom-up signaling pathway in the bottom-up digraph, such that information flows along said top-down signaling pathway in a first direction, and information flows along said reciprocal bottom-up signaling pathway in a direction opposite of the first direction;</claim-text>
<claim-text>designating a neuron population at an input periphery of the neural network as an input neuron population configured to receive sensory input, wherein the input neuron population drives neurons along a number of bottom-up signaling pathways;</claim-text>
<claim-text>a first set of neurons along bottom-up signaling pathways driving a first set of neurons along top-down signaling pathways;</claim-text>
<claim-text>designating a neuron population at an output periphery of the neural network as an output neuron population configured to generate motor output, wherein neurons along a number of top-down signaling pathways drive the output neuron population;</claim-text>
<claim-text>training each neuron along a bottom-up signaling pathway using a learning rule based on the firing events of said neuron and the firing events of the corresponding neuron along a reciprocal top-down signaling pathway;</claim-text>
</claim-text>
<claim-text>training each neuron along a top-down signaling pathway using a learning rule based on the firing events of said neuron and the firing events of the corresponding neuron along a reciprocal bottom-up signaling pathway; and
<claim-text>interconnecting neuron populations in the bottom-up digraph to neuron populations in the top-down digraph using additional directed edges. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
