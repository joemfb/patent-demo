<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624901-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624901</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12699229</doc-number>
<date>20100203</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>KR</country>
<doc-number>10-2009-0030794</doc-number>
<date>20090409</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>939</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20110101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>T</subclass>
<main-group>13</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345473</main-classification>
<further-classification>345419</further-classification>
<further-classification>345581</further-classification>
</classification-national>
<invention-title id="d2e71">Apparatus and method for generating facial animation</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5933151</doc-number>
<kind>A</kind>
<name>Jayant et al.</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6043827</doc-number>
<kind>A</kind>
<name>Christian et al.</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345474</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6097381</doc-number>
<kind>A</kind>
<name>Scott et al.</name>
<date>20000800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715203</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6633294</doc-number>
<kind>B1</kind>
<name>Rosenthal et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345474</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6654018</doc-number>
<kind>B1</kind>
<name>Cosatto et al.</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6919892</doc-number>
<kind>B1</kind>
<name>Cheiky et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7050655</doc-number>
<kind>B2</kind>
<name>Ho et al.</name>
<date>20060500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2002/0080139</doc-number>
<kind>A1</kind>
<name>Koo et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2004/0056857</doc-number>
<kind>A1</kind>
<name>Zhang et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2006/0164440</doc-number>
<kind>A1</kind>
<name>Sullivan et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345639</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2006/0176301</doc-number>
<kind>A1</kind>
<name>Sohn et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2008/0136814</doc-number>
<kind>A1</kind>
<name>Chu et al.</name>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>KR</country>
<doc-number>10-2002-0054243</doc-number>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>KR</country>
<doc-number>10-2002-0085669</doc-number>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>KR</country>
<doc-number>10-0530812</doc-number>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>KR</country>
<doc-number>10-2006-0082984</doc-number>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>KR</country>
<doc-number>10-0601989</doc-number>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>KR</country>
<doc-number>10-2008-0051007</doc-number>
<date>20080600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>WO</country>
<doc-number>WO 99/53443</doc-number>
<date>19991000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Pyun, Hye-won, &#x201c;An On-line Performance-driven Approach to Real-time Facial Motion Capture and Generation,&#x201d; Apr. 20, 2004, Department of Electrical Engineering &#x26; Computer Science Division of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon, South Korea, pp. i-79.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Park, Bongcheol, et al., &#x201c;A Feature-Based Approach to Facial Expression Cloning,&#x201d; Feb. 3, 2004, Department of Computer Science, Korea Advanced Institute of Science and Technology, South Korea, pp. 1-17.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Brox, Thomas, et al., &#x201c;High Accuracy Optical Flow Estimation Based on a Theory for Warping,&#x201d; Mathematical Image Analysis Group of Saarland University, Saarbrucken, Germany, In Proc. 8th European Conference on Computer Vision, Springer LNCS 3024, T. Pajdla and J. Matas (Eds.), vol. 4, pp. 25-36, Prague, Czech Republic, May 2004.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Park Doyoung, et al., &#x201c;Motions Syntheses in 3D Facial Model Using Features and Motion Parameters Estimated Through Optical Flow,&#x201d; Department of Computer Science, Yonsei University, 1998, pp. 408-410.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Sloan, Peter-Pike J. et al., &#x201c;Shape by Example,&#x201d; Proceedings of the 2001 Symposium on Interactive 3D Graphics, 2001, pp. 135-143.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>20</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>9</number-of-drawing-sheets>
<number-of-figures>13</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100259538</doc-number>
<kind>A1</kind>
<date>20101014</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Bong-cheol</first-name>
<address>
<city>Hwaseong-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Jung</last-name>
<first-name>Sung-gun</first-name>
<address>
<city>Anyang-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Hyo-won</first-name>
<address>
<city>Hwaseong-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="004" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Moon-ho</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="005" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Oh</last-name>
<first-name>Du-na</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Bong-cheol</first-name>
<address>
<city>Hwaseong-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Jung</last-name>
<first-name>Sung-gun</first-name>
<address>
<city>Anyang-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Kim</last-name>
<first-name>Hyo-won</first-name>
<address>
<city>Hwaseong-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="004" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Moon-ho</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="005" designation="us-only">
<addressbook>
<last-name>Oh</last-name>
<first-name>Du-na</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>NSIP Law</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Samsung Electronics Co., Ltd.</orgname>
<role>03</role>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Broome</last-name>
<first-name>Said</first-name>
<department>2679</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An apparatus and method for generating an animation of a virtual character using video data and a limited number of three-dimensional (3D) key models of a virtual character, are provided. The video data includes a sequence of two-dimensional image frames. The key models are blended together using the calculated weights, and a 3D facial expression most similar to the 2D facial expression included in each image frame may be generated. When the above operations are performed for input video data including a sequence of image frames, a facial animation, which makes facial expressions according to facial expressions included in the video data may be created.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="93.98mm" wi="197.95mm" file="US08624901-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="166.88mm" wi="119.89mm" orientation="landscape" file="US08624901-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="201.68mm" wi="124.71mm" orientation="landscape" file="US08624901-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="173.06mm" wi="157.40mm" file="US08624901-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="200.24mm" wi="85.85mm" file="US08624901-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="140.97mm" wi="138.26mm" file="US08624901-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="207.09mm" wi="132.16mm" file="US08624901-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="118.53mm" wi="156.04mm" file="US08624901-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="178.48mm" wi="168.91mm" file="US08624901-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="158.07mm" wi="141.73mm" file="US08624901-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application claims the benefit under 35 U.S.C. &#xa7;119(a) of Korean Patent Application No. 10-2009-0030794, filed on Apr. 9, 2009, in the Korean Intellectual Property Office, the entire disclosure of which is incorporated herein by reference for all purposes.</p>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">1. Field</p>
<p id="p-0004" num="0003">The following description relates to facial animation generation technology, and more particularly, to an apparatus and method for generating an animation of a facial character with various facial expressions using a plurality of key models.</p>
<p id="p-0005" num="0004">2. Description of the Related Art</p>
<p id="p-0006" num="0005">Facial animation of virtual characters is becoming important not only in the fields of animation, movies, and games, but also in products closely related to daily lives, for example, billboards, electronic devices, information desks, and the like. For example, human-like conversation systems may be used in electronic goods that have user interactions, such as televisions, cameras, phones, and camcorders.</p>
<p id="p-0007" num="0006">Producing facial animation of virtual characters is labor-intensive work that involves a lot of time and effort. Data-based facial animation is drawing a lot of attention, because in data-based facial animation, desired facial animation may be generated more efficiently by reusing existing data. In data-based facial animation, obtaining a large number of high-quality facial animation data samples is desirable.</p>
<p id="p-0008" num="0007">According to another method of facial animation, a database of facial expressions and mouth shapes of people may be scanned as three-dimensional (3D) information and may be employed. The facial animation may be generated from a single image by blending the 3D information.</p>
<p id="p-0009" num="0008">The above facial animation data may be obtained by capturing facial expressions of real people using motion capture or stereo imaging technology. Facial expressions using this technology generate facial animation data that is natural. However, there are many restrictions in obtaining the facial animation data. That is, expensive specialized equipment is needed to obtain the facial animation data. In addition, this equipment cannot be installed in a home or office environment. It must be installed in a special environment with special lighting and backgrounds to obtain the facial animation data. Furthermore, markers must be attached to the face of a person making a facial expression, or the face of the person must be colored. That is, a considerable amount of time is needed to prepare for the acquisition of the facial animation data.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0010" num="0009">In one general aspect, there is provided an apparatus for generating facial animation, the apparatus including: a pre-processing unit for selecting image frame having a facial region that is most similar to a facial region of one of a plurality of key models of facial characters from a sequence of image frames, each image frame being divided into a plurality of facial regions; and a facial animation generation unit for determining a blending weight for each facial region of each of the key models using the selected image frame and for synthesizing facial character shapes on a facial region-by-facial region basis using the determined blending weight for each facial region of each of the key models.</p>
<p id="p-0011" num="0010">The pre-processing unit may process each of the plurality of key models and determine a plurality of facial regions for the key models, and set patches corresponding individually to the facial regions of each of the key models, in each of the image frames.</p>
<p id="p-0012" num="0011">The pre-processing unit may divide each of the key models into a left eye region, a right eye region, and a mouth region, and set patches, which correspond to the left eye region, the right eye region, and the mouth region, in each of the image frames.</p>
<p id="p-0013" num="0012">The pre-processing unit may compare directions of respective two-dimensional (2D) displacement vectors of vertices included in each key model with directions of respective optical flow vectors of pixels included in each image frame and calculate a similarity between the directions, wherein the 2D displacement vectors are determined based on vertices included in one of the key models, and the optical flow vectors of the pixels are generated by calculating optical flows of the pixels based on pixels included in one of the image frames.</p>
<p id="p-0014" num="0013">The pre-processing unit may determine respective three-dimensional (3D) displacement vectors of the vertices included in each key model, based on one of the key models, and project the 3D displacement vectors onto a 2D plane to generate the 2D displacement vectors of the vertices.</p>
<p id="p-0015" num="0014">The pre-processing unit may divide each facial region of each key model into a plurality of sub-facial regions and divide each facial region of each image frame into a plurality of sub-facial regions and calculate a similarity between a direction of an average vector representing the average of 2D displacement vectors in each sub-facial region of each key model and a direction of an average vector representing the average of optical flow vectors in a corresponding sub-facial region of each image frame.</p>
<p id="p-0016" num="0015">The pre-processing unit may divide each facial region of each key model and of each image frame into quadrants.</p>
<p id="p-0017" num="0016">The pre-processing unit may set an optical flow vector of the facial region of the image frame, which is most similar to a facial region of a key model, as a seed, cluster optical flow vectors of the facial regions of the other image frames using the optical flow vector set as the seed, and determine an image frame having an optical flow vector, which corresponds to the average of all optical vectors in each cluster as an image frame having a facial region most similar to a facial region of a key model in the cluster.</p>
<p id="p-0018" num="0017">The pre-processing unit may group the optical flow vectors of the image frames into a number of clusters equal to the number of key models.</p>
<p id="p-0019" num="0018">The facial animation generation unit may use a four-dimensional (4D) vector, which indicates a direction of an optical flow vector in each quadrant of each facial region, as a parameter to determine the blending weight for each facial region of each key model.</p>
<p id="p-0020" num="0019">The facial animation generation unit may determine the blending weight for each facial region of each key model such that the determined blending weight is inversely proportional to a distance of a representative parameter value corresponding to each key model from the parameter of the image frame.</p>
<p id="p-0021" num="0020">The facial animation generation unit may generate a whole facial character by generating a facial character for each facial region using the blending weight for each facial region of each key model and by synthesizing the generated facial characters.</p>
<p id="p-0022" num="0021">When video data about the same person as the one in a facial image included in the image frames of the pre-processed video is input, the facial animation generation unit may generate facial animation using representative parameter information of facial regions of key models, corresponding to the facial region of the image frame, which is most similar to a facial region of a key model.</p>
<p id="p-0023" num="0022">In another general aspect, there is provided s method of generating facial animation based on a video, the method including selecting an image frame having a facial region, which is most similar to a facial region of one of a plurality of key models of facial characters, from a sequence of image frames, determining a blending weight for each facial region of each of the key models using the selected image frame; and generating facial animation by synthesizing facial characters on a facial region-by-facial region basis using the determined blending weight for each facial region of each key model, wherein each of the key models is divided into a plurality of facial regions, and each of the image frames is divided into a plurality of facial regions.</p>
<p id="p-0024" num="0023">The method may further include setting patches in each image frame, which are regions corresponding to the facial regions of each of the key models, in video data comprised of the image frames.</p>
<p id="p-0025" num="0024">The selecting of the image frame having a facial region most similar to a facial region of one of a key model may further include comparing directions of respective 2D displacement vectors of vertices included in each key model with directions of respective optical flow vectors of pixels included in each image frame and calculating a similarity between the directions, wherein the 2D displacement vectors are determined based on vertices included in one of the key models, and the optical flow vectors of the pixels are generated by calculating optical flows of the pixels based on pixels included in one of the image frames.</p>
<p id="p-0026" num="0025">Each of the key models may be a 3D shape, and the 2D displacement vectors of the vertices may be generated by determining respective 3D displacement vectors of the vertices included in each key model based on one of the key models and projecting the 3D displacement vectors onto a 2D plane.</p>
<p id="p-0027" num="0026">The comparing of the directions and calculating of the similarity may include dividing each facial region of each key model into a plurality of sub-facial regions and dividing each facial region of each image frame into a plurality of sub-facial regions; and calculating a similarity between a direction of an average vector representing the average of 2D displacement vectors in each sub-facial region of each key model and a direction of an average vector representing the average of optical flow vectors in a corresponding sub-facial region of each image frame.</p>
<p id="p-0028" num="0027">A parameter used to determine the blending weight for each facial region of each key model may be a 4D vector.</p>
<p id="p-0029" num="0028">The method may further include generating a complete facial character by generating the facial character for each facial region using the blending weight for each facial region of each key model and synthesizing the generated facial characters. Other features and aspects will be apparent from the following detailed description, the drawings, and the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 1</figref> is a diagram illustrating an example of an apparatus for generating facial animation.</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 2</figref> is a diagram illustrating an example of a pre-processing unit that may be included in the apparatus illustrated in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> are examples of animated key models that are based on pronunciation and emotion.</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 4</figref> is an animated key model illustrating an example of extracted feature points.</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an example of a three-dimensional (3D) displacement vector projected onto a two-dimensional (2D) plane.</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 6</figref> is a diagram illustrating an example of patches set in an image frame.</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 7A</figref> is a diagram illustrating an example of optical flow vectors in a facial region of an image frame, and <figref idref="DRAWINGS">FIG. 7B</figref> is a diagram illustrating an example of displacement vectors in a facial region of a key model.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 8</figref> is a diagram illustrating an example of parameter space.</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram illustrating an example of a facial animation generation unit that may be included in the apparatus illustrated in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 10</figref> is a diagram illustrating an example of another facial animation generation unit that may be included in the apparatus illustrated in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 11</figref> is a flowchart illustrating an example of a method for generating facial animation.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0041" num="0040">Throughout the drawings and the detailed description, unless otherwise described, the same drawing reference numerals will be understood to refer to the same elements, features, and structures. The relative size and depiction of these elements may be exaggerated for clarity, illustration, and convenience.</p>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0042" num="0041">The following description is provided to assist the reader in gaining a comprehensive understanding of the methods, apparatuses, and/or systems described herein. Accordingly, various changes, modifications, and equivalents of the methods, apparatuses, and or systems described herein will be suggested to those of ordinary skill in the art. Also, descriptions of well-known functions and constructions may be omitted for increased clarity and conciseness.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. 1</figref> illustrates an example of an apparatus for generating facial animation. Referring to <figref idref="DRAWINGS">FIG. 1</figref>, the apparatus <b>100</b> generates facial animation by creating a facial character based on facial expressions included in a sequence of input image frames. The apparatus <b>100</b> includes a pre-processing unit <b>110</b>, a storage unit <b>120</b>, and a facial animation generation unit <b>130</b>.</p>
<p id="p-0044" num="0043">The pre-processing unit <b>110</b> receives image information, for example, a video, which includes a sequence of image frames. The pre-processing unit <b>110</b> also receives a plurality of key models of a virtual character. The number of key models may be less than the number of image frames included in a piece of video content. To generate numerous facial expressions of a facial image included in a sequence of image frames using a limited number of key models, the pre-processing unit <b>110</b> may divide and process the facial areas of the image frames and the facial areas of the key models.</p>
<p id="p-0045" num="0044">In some embodiments, the pre-processing unit <b>110</b> may divide each of a plurality of key models of a facial character into a plurality of facial regions and set a plurality of patches. The patches are regions corresponding respectively to the facial regions of each of the key models, in each of a sequence of image frames. For example, the pre-processing unit <b>110</b> may process the key models and determine to divide each key model of a virtual character into three facial regions, for example, a left eye, a right eye, and a mouth, and extract patches from the image frames, which correspond respectively to the facial regions of the key model, from each image frame of a video. The three facial regions may later be synthesized together and the key models may be blended on a facial region-by-facial region basis so as to generate the whole virtual facial character.</p>
<p id="p-0046" num="0045">The pre-processing unit <b>110</b> respectively selects an image frame having a facial region that is most similar to a facial region of each key model of a facial character, from a plurality of input image frames, for each facial region.</p>
<p id="p-0047" num="0046">In some embodiments, the pre-processing unit <b>110</b> may process the key models and determine a plurality of facial regions for a virtual facial character, based on the key models. For example, the pre-processing unit <b>110</b> may determine the virtual facial character has two facial regions, three facial regions, four facial regions, or more. The pre-processing unit <b>110</b> may divide the facial area of the image frames into a plurality of facial regions, or patches, according to the determined facial regions for the virtual character. Throughout the specification, a facial region of an image frame denotes a patch region or a patch.</p>
<p id="p-0048" num="0047">The storage unit <b>120</b> stores data processing results of the pre-processing unit <b>110</b>. The data processing results include information about an image frame having a facial region that is most similar to a facial region of a key model. For example, when identification information of an image frame having a facial region most similar to a first facial region of key model #1 is #003 and when identification information of an image having a facial region most similar to a second facial region of key model #1 is #050, this information may be stored in the storage unit <b>120</b>.</p>
<p id="p-0049" num="0048">The facial animation generation unit <b>130</b> determines a blending weight for each facial region of each key model of a facial character, using a parameter that is determined based on an image frame having a facial region most similar to a facial region of a key model. The facial animation generation unit <b>130</b> blends the key models together using the determined blending weights and generates a facial character for a facial region. The facial animation generation unit <b>130</b> may blend the plurality of key models with respect to a facial region. This process may be repeated for any desired facial region. For example, the facial animation generation unit <b>130</b> may blend key models on a facial region-by-facial region basis for an image frame of a video and produce a three-dimensional (3D) output for each facial region of the image frame using the key models. Thus, the facial animation generation unit <b>130</b> may generate a complete facial expression by combining the 3D outputs.</p>
<p id="p-0050" num="0049">An apparatus for generating facial animation may easily generate a facial animation at a low cost by using a piece of video data and a number of key models of a virtual character. Therefore, the difficulty of obtaining facial animation data using motion capture or stereo imaging technology or 3D scanning technology may be removed.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. 2</figref> illustrates an example of a pre-processing unit that may be included in the apparatus <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref>. Referring to <figref idref="DRAWINGS">FIG. 2</figref>, the pre-processing unit <b>110</b> includes a key model pre-processing unit <b>210</b>, an image frame pre-processing unit <b>220</b>, and a similarity calculation unit <b>230</b>.</p>
<p id="p-0052" num="0051">The key model pre-processing unit <b>210</b> includes a key model region divider <b>212</b> and a two-dimensional (2D) displacement vector generator <b>214</b>.</p>
<p id="p-0053" num="0052">The key model region divider <b>212</b> receives a plurality of key models. Key models may be a plurality of key models used in facial animation and may include, for example, key models pronouncing vowels and consonants, and/or key models expressing emotions. Examples of key models are shown in <figref idref="DRAWINGS">FIGS. 3A and 3B</figref>.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIGS. 3A and 3B</figref> illustrate animated examples of key models based on pronunciation and emotion. For example, <figref idref="DRAWINGS">FIG. 3A</figref> illustrates animated key models expressing different emotions such as neutral, joy, surprise, anger, sadness, disgust, and sleepiness. <figref idref="DRAWINGS">FIG. 3B</figref> illustrates animated key models pronouncing consonants, such as &#x201c;m,&#x201d; &#x201c;sh,&#x201d; &#x201c;f&#x201d; and &#x201c;th,&#x201d; and vowels such as &#x201c;a,&#x201d; &#x201c;e,&#x201d; and &#x201c;o.&#x201d; Various key models other than those shown in <figref idref="DRAWINGS">FIGS. 3A and 3B</figref> may also be generated according to pronunciation and emotion.</p>
<p id="p-0055" num="0054">Referring back to <figref idref="DRAWINGS">FIG. 2</figref>, the key model region divider <b>212</b> divides each key model into a plurality of facial regions. The dividing a key model into a plurality of facial regions may include, for example, extracting feature points, grouping the extracted feature points, and dividing vertices.</p>
<p id="p-0056" num="0055">A key model may be represented as a spring-mass network composed of triangular meshes. When a key model is represented as a spring-mass network, vertices that form a face may be regarded as mass points, and each edge of each triangle formed by connecting the vertices may be regarded as a spring. In addition, each vertex (or mass point) may be indexed. Thus, a key model of the virtual facial character may include vertices, which have, for example, 600 indices, 800 indices, 1500 indices, or other desired amount of indices. The key model of facial character may also include edges (or springs).</p>
<p id="p-0057" num="0056">The number of springs and the number of mass points may not vary from one key model to another. However, the position of mass points may vary according to a facial expression of a key model. Accordingly, a length of each spring connecting two of the mass points varies. Thus, for each key model expressing a different emotion, a displacement &#x394;x of a length &#x201c;x&#x201d; of each spring connecting two mass points and an energy variation (E=&#x394;x<sup>2</sup>/2) of each mass point may be measured based on a key model of a neutral facial expression.</p>
<p id="p-0058" num="0057">When feature points are extracted from mass points of a key model in order to divide the key model into a plurality of facial regions, the change in the length of a spring connected to a mass point of each key model, which corresponds to each mass point of the key model of the neutral facial expression, may be measured. When the measured change in the length of a spring connected to a mass point is greater than the measured changes in the lengths of springs connected to its neighboring mass points, the mass point may be selected (extracted) as a feature point. When three springs are connected to one mass point, the average of changes in the lengths of the three springs may be used.</p>
<p id="p-0059" num="0058">When a facial character image is represented by a spring-mass network, the key model region divider <b>212</b> may extract each of mass points connected by a spring as feature points. The key model region divider <b>212</b> may extract mass points connected by a spring when the change in the length of the spring is greater than a predetermined threshold value, based on a reference key model (the key model of the neutral expression). An example of extracted feature points is shown in the animated key model of <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0060" num="0059">The key model region divider <b>212</b> may measure coherence in the motion of extracted feature points and group the extracted feature points according to the measured coherence. The extracted feature points may be divided into a plurality of groups according to the measured coherence in the motion of the extracted feature points. To measure the coherence in the motion of the extracted feature points, displacements of the extracted feature points may be measured. The similarities in the magnitudes and directions of the displacements of the extracted feature points and the geometric proximity of the extracted feature points to their counterparts in the key model of the neutral facial expression may be measured as the coherence in the motion of the extracted feature points.</p>
<p id="p-0061" num="0060">Once the coherence in the motion of the extracted feature points is quantified, a graph may be created based on the quantified coherence, for example, an undirected graph. The nodes of the graph may represent a feature point, and the edges of the graph may represent motion coherence.</p>
<p id="p-0062" num="0061">When a value representing motion coherence is lower than a predetermined threshold value, the key model region divider <b>212</b> may determine that there is no coherence in motion. Accordingly, a corresponding edge may be removed. According to an embodiment, the nodes of the graph may be grouped using a connected component analysis technique. As a result, the extracted feature points may be automatically divided into a plurality of groups.</p>
<p id="p-0063" num="0062">The key model region divider <b>212</b> may further include the mass points (vertexes) unselected as a feature point in a corresponding group of feature points. The key model region divider <b>212</b> may measure the coherence between the motion of each mass point unselected as a feature point and the motion of each group of feature points.</p>
<p id="p-0064" num="0063">The coherence between the motion of each unselected mass point and the motion of each group of feature points may be measured in the same way as feature points are grouped. The coherence in the motion of each group of feature points and the motion of each mass point unselected as a feature point may be determined to be the average of values, each representing the coherence between the motion of each feature point in each group and the motion of each mass point unselected as a feature point. When a value representing the coherence between the motion of a feature point group and a mass point unselected as a feature point exceeds a predetermined threshold value, the mass point may be included in the feature point group. Thus, one mass point may be included in several feature point groups.</p>
<p id="p-0065" num="0064">When mass points (vertices) for modeling a facial character shape are divided into a plurality of groups, the facial character shape may be divided into the groups. As a result, indices may be generated for vertices in each facial region of a key model. In some embodiments, a key model of a facial character may be divided into a first facial region containing a left eye, a second facial region containing a right eye, and a third facial region containing an area around a mouth. For example, vertices in the area around the mouth may be indexed with a list of vertex identification information, for example, in the form of {1, 4, 122, 233, . . . 599}. Each facial region of the facial character shape and data on each facial region may be applied to each key model and used when the key models are synthesized on a facial region-by-facial region basis.</p>
<p id="p-0066" num="0065">The 2D displacement vector generator <b>214</b> determines respective 3D displacement vectors of vertices included in each key model, of a facial character based on a key model selected from the key models. The 2D displacement vector generator <b>214</b> projects the 3D displacement vectors onto a 2D plane to generate 2D displacement vectors of the vertices. For example, the selected key model may be the key model of the neutral facial expression. The 3D displacement vector may be projected onto a 2D plane to compare the 3D displacement vector with a 2D optical flow vector that indicates the displacement of a facial expression included in a 2D video image.</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. 5</figref> illustrates an example of a 3D displacement vector projected onto a 2D plane. Referring to <figref idref="DRAWINGS">FIG. 5</figref>, the 3D displacement vector <b>11</b> is projected onto the 2D plane <b>10</b> and is approximated to a 2D displacement vector <b>13</b>.</p>
<p id="p-0068" num="0067">The configuration of the image frame pre-processing unit <b>220</b> will be described with reference to <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0069" num="0068">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, an image frame region divider <b>222</b> sets patches in each of a sequence of image frames. The patches correspond to a plurality of facial regions of a key model. For example, when the key model region divider <b>212</b> divides each of a plurality of key models into a first facial region containing a left eye, a second facial region containing a right eye, and a third facial region containing an area around a mouth, the image frame divider <b>222</b> may set patches, which correspond to the first through third facial regions, in each of a plurality of image frames.</p>
<p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. 6</figref> illustrates an example of patches set in an image frame. Referring to <figref idref="DRAWINGS">FIG. 6</figref>, the image frame region divider <b>222</b> may decide that three regions, for example, a left eye region, a right eye region, and a mouth region, include features determining a facial expression in all image frames of a video. Accordingly, the image frame region divider <b>222</b> may set patches corresponding respectively to a left eye region <b>601</b>, a right eye region <b>602</b>, and a mouth region <b>603</b> in each image frame of the video. Each patch may include motions of feature points of a corresponding facial region.</p>
<p id="p-0071" num="0070">Referring back to <figref idref="DRAWINGS">FIG. 2</figref>, an optical flow calculator <b>224</b> obtains a 2D displacement vector of each pixel in each image frame of a video by calculating the optical flow of each pixel based on a reference image frame, for example, an image frame of the neutral facial expression. Optical flow calculation may be used to obtain 2D displacement vectors of pixels to estimate the motion of an object in one of two 2D images relative to the other 2D image.</p>
<p id="p-0072" num="0071">The similarity calculation unit <b>230</b> compares 2D displacements of vertices generated by the 2D displacement vector generator <b>214</b> with displacements of image pixels calculated by the optical flow calculator <b>224</b>, and matches the 2D displacements of the vertices respectively with the displacements of the image pixels which are most similar to the 2D displacements of the vertices. As a result, the similarity calculation unit <b>230</b> may select an image frame having a facial region, which is most similar to a facial region of one of a plurality of key models of facial character, from a plurality of input image frames. The similarity calculation unit <b>230</b> may perform this calculation for any desired amount of image frames. In some embodiments, the similarity calculation unit <b>230</b> may perform the calculation for each facial region of each image frame.</p>
<p id="p-0073" num="0072">The similarity calculation unit <b>230</b> compares directions of respective 2D displacement vectors of vertices included in each key model with directions of respective optical flow vectors of pixels included in each image frame and calculates the similarity between the directions. The 2D displacement vectors of the vertices included in each key model may be determined based on vertices included in one or more of the key models, for example, the neutral facial expression, the joy facial expression, the surprise facial expression, the anger facial expression, the sadness facial expression, the disgust facial expression, the sleepiness facial expression, and the like. The optical flow vectors of the pixels included in each image frame may be generated by calculating optical flows of the pixels based on pixels included in the image frame of one or more of the different facial expression.</p>
<p id="p-0074" num="0073">The similarity calculation unit <b>230</b> may divide the plurality of facial regions of each key model of a facial character into a plurality of sub-facial regions and divide each of a plurality of facial regions of the image frames into a plurality of sub-facial regions. The similarity calculation unit <b>230</b> may calculate a similarity between the direction of an average vector representing the average of 2D displacement vectors in each sub-facial region of the key models and the direction of an average vector representing the average of optical flow vectors in a corresponding sub-facial region of the image frames. The similarity calculation unit <b>230</b> may divide each facial region of each key model and each facial region of each image frame into quadrants as illustrated in <figref idref="DRAWINGS">FIGS. 7A and 7B</figref>.</p>
<p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. 7A</figref> illustrates an example of average optical flow vectors in a facial region of an image frame. <figref idref="DRAWINGS">FIG. 7B</figref> illustrates an example of average displacement vectors in a facial region of a key model of a facial character.</p>
<p id="p-0076" num="0075">Referring to <figref idref="DRAWINGS">FIG. 7A</figref>, when the average of optical flow vectors of pixels included in each quadrant of a mouth region patch <b>710</b> is calculated, one or more vectors may be obtained, for example, four vectors <b>712</b>, <b>714</b>, <b>716</b>, and <b>718</b>. The vector <b>712</b> is an average vector representing the average of optical flow vectors of pixels included in a first quadrant of the mouth region patch <b>710</b>. The vector <b>714</b> is an average vector representing the average of optical flow vectors of pixels included in a second quadrant of the mouth region patch <b>710</b>. The vector <b>716</b> is an average vector representing the average of optical flow vectors of pixels included in a third quadrant of the mouth region patch <b>710</b>. The vector <b>718</b> is an average vector representing the average of optical flow vectors of pixels included in a fourth quadrant of the mouth region patch <b>710</b>.</p>
<p id="p-0077" num="0076">Referring to <figref idref="DRAWINGS">FIG. 7B</figref>, when the average of displacement vectors of vertices included in each quadrant of a mouth region patch <b>720</b> of a key model is calculated, one or more vectors may be obtained, for example, four vectors <b>722</b>, <b>724</b>, <b>726</b>, and <b>728</b>. The vector <b>722</b> is an average vector representing the average of displacement vectors of vertices included in a first quadrant of the mouth region <b>720</b>. The vector <b>724</b> is an average vector representing the average of displacement vectors of vertices included in a second quadrant of the mouth region <b>720</b>. The vector <b>726</b> is an average vector representing the average of displacement vectors of vertices included in a third quadrant of the mouth region <b>720</b>. The vector <b>728</b> is an average vector representing the average of displacement vectors of vertices included in a fourth quadrant of the mouth region <b>720</b>.</p>
<p id="p-0078" num="0077">The similarity calculation unit <b>230</b> may compare directions of 2D displacement vectors of vertices included in the key models with directions of optical flow vectors of pixels included in the image frames and calculate the similarity between the directions. As described above, optical flow vectors of pixels included in each image frame may be generated by calculating optical flows of the pixels based on pixels included in one of the image frames.</p>
<p id="p-0079" num="0078">The similarity calculation unit <b>230</b> may calculate an angle difference between a vector of a quadrant of a key model and a vector of a corresponding quadrant of an image frame. For example, the similarity calculation unit <b>230</b> may calculate an angle difference &#x3b8;<sub>1 </sub>between the vectors <b>712</b> and <b>722</b>, an angle difference &#x3b8;<sub>2 </sub>between the vectors <b>714</b> and <b>724</b>, an angle difference &#x3b8;<sub>3 </sub>between the vectors <b>716</b> and <b>726</b>, and an angle difference &#x3b8;<sub>4 </sub>between the vectors <b>718</b> and <b>728</b>. The similarity calculation unit <b>230</b> may calculate the average of the angle differences between &#x3b8;<sub>1 </sub>through &#x3b8;<sub>4</sub>. When the calculated average of the angle differences &#x3b8;<sub>1 </sub>through &#x3b8;<sub>4 </sub>approaches zero, the similarity calculation unit <b>230</b> may determine that there is a high similarity between the mouth region patch <b>710</b> of the image frame shown in <figref idref="DRAWINGS">FIG. 7A</figref> and the mouth region <b>720</b> of the key model shown in <figref idref="DRAWINGS">FIG. 7B</figref>.</p>
<p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. 8</figref> illustrates an example of parameter space. The parameter space may be set independently for each facial region. For example, a separate parameter space may be set for each of a left-eye region, a right-eye region, and a mouth region, to represent displacement vectors of image frames with respect to a reference image frame. In addition, a four-dimensional (4D) real number vector may be generated using angles ranging from 0 to 360 degrees of the vectors <b>712</b> through <b>718</b> of the quadrants shown in <figref idref="DRAWINGS">FIG. 7A</figref>, and the generated 4D real number vector may be used as a parameter.</p>
<p id="p-0081" num="0080">The parameter space illustrated in <figref idref="DRAWINGS">FIG. 8</figref> may be a parameter space of, for example, a mouth region, and parameters for optical flow vectors of a plurality of image frames, which are calculated based on a reference image frame, may be set in the parameter space. While only two clusters are illustrated in <figref idref="DRAWINGS">FIG. 8</figref>, the similarity calculation unit <b>230</b> may group 4D parameters in the parameter space into a number of clusters equal to the number of key models.</p>
<p id="p-0082" num="0081">Referring to the parameter space of <figref idref="DRAWINGS">FIG. 8</figref>, in this example, a parameter of a seed image frame <b>812</b> included in a k<sup>th </sup>cluster <b>810</b> is determined to be most similar to a 2D displacement vector of a k<sup>th </sup>key model. Therefore, to synthesize an image character corresponding to another image frame in the k<sup>th </sup>cluster <b>810</b>, for example, a j<sup>th </sup>image frame <b>811</b>, a blending weight of each of a plurality of key models may be calculated. The blending weight may be based on the distance between the parameter of the j<sup>th </sup>image frame <b>811</b> and the parameter of the seed image frame <b>812</b> in the k<sup>th </sup>cluster <b>810</b>, and the distances between the parameter of the j<sup>th </sup>image frame <b>811</b> and parameters of seed image frames in the other clusters (not shown).</p>
<p id="p-0083" num="0082">After clustering parameters of optical vectors of a plurality of image frames by setting a seed image frame, it may be found that an optical flow vector corresponding to the average of optical flow vectors included in a corresponding cluster is different from an optical flow vector of the set seed image frame. When this occurs, an optical flow vector corresponding to the average of all vectors included in each cluster may be selected as a representative value and used as a new reference value to calculate a blending weight of each key model.</p>
<p id="p-0084" num="0083">In summary, the similarity calculation unit <b>230</b> may select an image frame having a facial region, which is most similar to a facial region of one of a plurality of key models of a facial character, from a plurality of image frames, based on the similarity between 2D displacement vectors and optical flow vectors. In addition, the similarity calculation unit <b>230</b> may set an optical flow vector of a facial region of an image frame, which is most similar to a facial region of a key model, as a seed and cluster optical flow vectors of the facial regions of the other image frames using the optical flow vector set as the seed. Then, the similarity calculation unit <b>230</b> may determine an image frame having an optical flow vector, which is most similar to the average of all vectors in each cluster, as an image frame having a facial region most similar to a facial region of a key model in the cluster.</p>
<p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. 9</figref> illustrates an example of a facial animation generation unit that may be included in the apparatus <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref>. Referring to <figref idref="DRAWINGS">FIG. 9</figref>, the facial animation generation unit <b>130</b> includes a weight calculator <b>310</b> and an image synthesizer <b>320</b>.</p>
<p id="p-0086" num="0085">The weight calculator <b>310</b> may use a 4D vector as a parameter to determine a blending weight for each facial region of each of a plurality of key models of a facial character. The 4D vector indicates the direction of an optical flow vector in each quadrant of each facial region of an image frame. The weight calculator <b>310</b> may determine a blending weight for each facial region of the key model such that the blending weight is inversely proportional to the distance of a representative parameter value corresponding to each key model from the parameter of the image frame. To determine a blending weight, a distributed data interpolation technique may be used. Examples of interpolation techniques are described in &#x201c;Shape by Example,&#x201d; by Peter-Pike J. Sloan et al.</p>
<p id="p-0087" num="0086">The image synthesizer <b>320</b> may synthesize key models on a facial region-by-facial region basis using a blending weight of each facial region of each of the key models and thus generate a facial character for each facial region. In addition, the image synthesizer <b>320</b> may synthesize the respective facial characters of the facial regions to generate the whole facial character.</p>
<p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. 10</figref> illustrates another example of a facial animation generation unit that may be included in the apparatus <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0089" num="0088">Referring to <figref idref="DRAWINGS">FIG. 10</figref>, the facial animation generation unit <b>130</b> includes an image frame region divider <b>410</b>, an optical flow calculator <b>420</b>, a weight calculator <b>430</b>, and an image synthesizer <b>440</b>. The image frame region divider <b>410</b> and the optical flow calculator <b>420</b> perform the same respective functions as the image frame region divider <b>222</b> and the optical flow calculator <b>224</b> included in the image frame pre-processing unit <b>220</b> illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, and the weight calculator <b>430</b> and the image synthesizer <b>440</b> perform the same respective functions as the weight calculator <b>310</b> and the image synthesizer <b>320</b> of <figref idref="DRAWINGS">FIG. 9</figref>.</p>
<p id="p-0090" num="0089">When video data about the same person as the one in a facial image included in image frames of a pre-processed video is input, the image frame region divider <b>410</b> may set patches in each image frame, and the optical flow calculator <b>420</b> may calculate the optical flow of each patch in each image frame.</p>
<p id="p-0091" num="0090">The weight calculator <b>430</b> may calculate blending weights of key models for each image frame using previously calculated representative parameter information corresponding to a facial region of an image frame which is most similar to a facial region of a key model. The image synthesizer <b>440</b> may generate a facial animation by synthesizing respective facial character shapes of the facial regions using the calculated blending weights.</p>
<p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. 11</figref> illustrates an example of a method for generating facial animation.</p>
<p id="p-0093" num="0092">Referring to <figref idref="DRAWINGS">FIG. 11</figref>, the apparatus <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. 1</figref> selects an image frame having a facial region, which is most similar to a facial region of one of a plurality of key models of a face character, from a plurality of input image frames, in <b>1110</b>. The apparatus <b>100</b> may set patches, which are regions corresponding respectively to the facial regions of each key model, in video data composed of a sequence of the image frames.</p>
<p id="p-0094" num="0093">In <b>1120</b>, the apparatus <b>100</b> determines a blending weight for each facial region of each key model by using the image frame having the facial region most similar to a facial region of a key model. The respective 2D displacement vectors of vertices included in each key model may be determined based on vertices included in one of the key models, and respective optical flow vectors of pixels included in each image frame may be generated by calculating optical flows of the pixels based on pixels included in one of the image frames. Then, the directions of the 2D displacement vectors may be compared with the directions of the optical flow vectors to determine the similarity between them.</p>
<p id="p-0095" num="0094">In addition, each of the facial regions of each key model may be divided into a plurality of sub-facial regions, and each of the facial regions of each image frame may be divided into a plurality of sub-facial regions. Then, a similarity between the direction of an average vector representing the average of 2D displacement vectors in each sub-facial region of each key model and the direction of an average vector representing the average of optical flow vectors in a corresponding sub-facial region of each image frame may be calculated.</p>
<p id="p-0096" num="0095">In <b>1130</b>, the apparatus <b>100</b> synthesizes facial character shapes on a facial region-by-facial region basis using the determined blending weight for each facial region of each key model. The apparatus <b>100</b> generates the whole facial character by synthesizing respective facial characters of the facial regions which are generated using the blending weight for each facial region of each key model, in <b>1140</b>. When operations <b>1130</b> and <b>1140</b> are performed for each sequence of image frames, a facial animation, which makes facial expressions similar to facial expressions of a person included in the image frames, can be generated and output.</p>
<p id="p-0097" num="0096">According to example(s) described above, provided herein are an apparatus and a method for receiving a video input by a user, and the video may be broken into image frames. For each image frame an image region of a key model may be matched/selected to an image range of the image frame. This matching/selecting may be performed for each image region of each image frame. Once each image frame has been processed, the image frames can be used for animation. The animation is done by synthesizing each image region using the key models selected for each specific image region. Furthermore, to complete the animated facial character, the image regions are combined with a weighting algorithm. The result is a complete animated facial character that represents the video that was input.</p>
<p id="p-0098" num="0097">According to the examples described above, provided herein are an apparatus and method for generating facial animation of a facial character, which makes facial expressions according to two-dimensional (2D) facial expressions included in image frames of an input video, by using video data of the input video and a limited number of three-dimensional (3D) key models of a virtual character.</p>
<p id="p-0099" num="0098">The processes, functions, methods and software described above may be recorded, stored, or fixed in one or more computer-readable storage media that includes program instructions to be implemented by a computer to cause a processor to execute or perform the program instructions. The media may also include, alone or in combination with the program instructions, data files, data structures, and the like. Examples of computer-readable media include magnetic media, such as hard disks, floppy disks, and magnetic tape; optical media such as CD ROM disks and DVDs; magneto-optical media, such as optical disks; and hardware devices that are specially configured to store and perform program instructions, such as read-only memory (ROM), random access memory (RAM), flash memory, and the like. Examples of program instructions include machine code, such as produced by a compiler, and files containing higher level code that may be executed by the computer using an interpreter. The described hardware devices may be configured to act as one or more software modules in order to perform the operations and methods described above, or vice versa. In addition, a computer-readable storage medium may be distributed among computer systems connected through a network and computer-readable codes or program instructions may be stored and executed in a decentralized manner.</p>
<p id="p-0100" num="0099">A number of examples have been described above. Nevertheless, it will be understood that various modifications may be made. For example, suitable results may be achieved if the described techniques are performed in a different order and/or if components in a described system, architecture, device, or circuit are combined in a different manner and/or replaced or supplemented by other components or their equivalents. Accordingly, other implementations are within the scope of the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An apparatus for generating facial animation, the apparatus comprising:
<claim-text>a pre-processing unit for selecting an image frame having a facial region that is most similar to a facial region of one of a plurality of key models of facial characters, from a sequence of image frames, each image frame being divided into a plurality of facial regions;</claim-text>
<claim-text>a facial animation generation unit for determining a blending weight for each selected facial region of the key models using the selected facial region and for synthesizing facial character shapes on a facial region-by-facial region basis using the determined blending weight for each facial region of each of the key models; and</claim-text>
<claim-text>a display device for displaying facial animation,</claim-text>
<claim-text>wherein the pre-processing unit compares directions of respective two-dimensional (2D) displacement vectors of vertices included in each key model with directions of respective optical flow vectors of pixels included in each image frame and calculates a similarity between the directions, wherein the 2D displacement vectors are determined based on vertices included in one of the key models, and the optical flow vectors of the pixels are generated by calculating optical flows of the pixels based on pixels included in one of the image frames.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pre-processing unit processes each of the plurality of key models and determines a plurality of facial regions for the key models, and sets patches corresponding individually to the facial regions of each of the key models, in each of the image frames.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the pre-processing unit divides each of the key models into a left eye region, a right eye region, and a mouth region, and sets patches, which correspond to the left eye region, the right eye region, and the mouth region, in each of the image frames.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pre-processing unit determines respective three-dimensional (3D) displacement vectors of the vertices included in each key model, based on one of the key models, and projects the 3D displacement vectors onto a 2D plane to generate the 2D displacement vectors of the vertices.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pre-processing unit divides each facial region of each key model into a plurality of sub-facial regions and divides each facial region of each image frame into a plurality of sub-facial regions and calculates a similarity between a direction of an average vector representing the average of 2D displacement vectors in each sub-facial region of each key model and a direction of an average vector representing the average of optical flow vectors in a corresponding sub-facial region of each image frame.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The apparatus of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the pre-processing unit divides each facial region of each key model and of each image frame into quadrants.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pre-processing unit sets an optical flow vector of the facial region of the image frame which is most similar to a facial region of a key model, as a seed, clusters optical flow vectors of the facial regions of the other image frames using the optical flow vector set as the seed, and determines an image frame having an optical flow vector, which corresponds to the average of all optical vectors in each cluster, as an image frame having a facial region most similar to a facial region of a key model in the cluster.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the pre-processing unit groups the optical flow vectors of the image frames into a number of clusters equal to the number of key models.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the facial animation generation unit uses a four-dimensional (4D) vector, which indicates a direction of an optical flow vector in each quadrant of each facial region, as a parameter to determine the blending weight for each facial region of each key model.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. An apparatus for generating facial animation, the apparatus comprising:
<claim-text>a pre-processing unit for selecting an image frame having a facial region that is most similar to a facial region of one of a plurality of key models of facial characters, from a sequence of image frames, each image frame being divided into a plurality of facial regions;</claim-text>
<claim-text>a facial animation generation unit for determining a blending weight for each selected facial region of the key models using the selected facial region and for synthesizing facial character shapes on a facial region-by-facial region basis using the determined blending weight for each facial region of each of the key models; and</claim-text>
<claim-text>a display device for displaying facial animation,</claim-text>
<claim-text>wherein the facial animation generation unit determines the blending weight for each facial region of each key model such that the determined blending weight is inversely proportional to a distance of a representative parameter value corresponding to each key model from the parameter of the image frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the facial animation generation unit generates a whole facial character by generating a facial character for each facial region using the blending weight for each facial region of each key model and by synthesizing the generated facial characters.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein when video data about the same person as the one in a facial image included in the image frames of the pre-processed video is input, the facial animation generation unit generates facial animation using representative parameter information corresponding to the facial region of the image frame, which is most similar to a facial region of a key model.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A method of generating facial animation based on a video, the method comprising:
<claim-text>selecting an image frame having a facial region that is most similar to a facial region of one of a plurality of key models of facial characters, from a sequence of image frames;</claim-text>
<claim-text>determining a blending weight for each facial region of each of the key model using the selected image frame;</claim-text>
<claim-text>generating facial animation by synthesizing facial characters on a facial region-by-facial region basis using the determined blending weight for each facial region of each key model; and</claim-text>
<claim-text>displaying the facial animation on a display device,</claim-text>
<claim-text>wherein each of the key models is divided into a plurality of facial regions, and each of the image frames is divided into a plurality of facial regions, wherein the selecting of the image frame having a facial region most similar to a facial region of one of a key model further comprises comparing directions of respective 2D displacement vectors of vertices included in each key model with directions of respective optical flow vectors of pixels included in each image frame and calculating a similarity between the directions, wherein the 2D displacement vectors are determined based on vertices included in one of the key models, and the optical flow vectors of the pixels are generated by calculating optical flows of the pixels based on pixels included in one of the image frames.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising setting patches in each image frame, which are regions corresponding to the facial regions of each of the key models, in video data comprised of the image frames.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein each of the key models is a 3D shape, and the 2D displacement vectors of the vertices are generated by determining respective 3D displacement vectors of the vertices included in each key model based on one of the key models and projecting the 3D displacement vectors onto a 2D plane.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the comparing of the directions and calculating of the similarity comprises:
<claim-text>dividing each facial region of each key model into a plurality of sub-facial regions and dividing each facial region of each image frame into a plurality of sub-facial regions; and</claim-text>
<claim-text>calculating a similarity between a direction of an average vector representing the average of 2D displacement vectors in each sub-facial region of each key model and a direction of an average vector representing the average of optical flow vectors in a corresponding sub-facial region of each image frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein a parameter used to determine the blending weight for each facial region of each key model is a 4D vector.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text>18. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising generating a complete facial character by generating the facial character for each facial region using the blending weight for each facial region of each key model and synthesizing the generated facial characters.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text>19. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the key models are represented as spring-mass networks and the position of mass points may vary according to a facial expression of the key models.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text>20. An apparatus for generating facial animation, the apparatus comprising:
<claim-text>a pre-processing unit for selecting an image frame having a facial region that is most similar to a facial region of one of a plurality of key models of facial characters, from a sequence of image frames, each image frame being divided into a plurality of facial regions;</claim-text>
<claim-text>a facial animation generation unit for determining a blending weight for each selected facial region of the key models using the selected facial region and for synthesizing facial character shapes on a facial region-by-facial region basis using the determined blending weight for each facial region of each of the key models; and</claim-text>
<claim-text>a display device for displaying facial animation,</claim-text>
<claim-text>wherein a key model region divider may extract mass points of each of the key models when a change in length of a spring is greater than a predetermined threshold value compared with a reference key model, and may divide the extracted mass points into a plurality of groups according to coherence in motion. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
