<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625960-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625960</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>11325490</doc-number>
<date>20060105</date>
</document-id>
</application-reference>
<us-application-series-code>11</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>KR</country>
<doc-number>10-2005-0001749</doc-number>
<date>20050107</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>1959</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>9</main-group>
<subgroup>80</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>84</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>89</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>386241</main-classification>
<further-classification>386332</further-classification>
</classification-national>
<invention-title id="d2e71">Apparatus and method for reproducing storage medium that stores metadata for providing enhanced search function</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5694381</doc-number>
<kind>A</kind>
<name>Sako</name>
<date>19971200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5999696</doc-number>
<kind>A</kind>
<name>Tsuga et al.</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6181872</doc-number>
<kind>B1</kind>
<name>Yamane et al.</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6289165</doc-number>
<kind>B1</kind>
<name>Abecassis</name>
<date>20010900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6360057</doc-number>
<kind>B1</kind>
<name>Tsumagari et al.</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>6360234</doc-number>
<kind>B2</kind>
<name>Jain et al.</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>6553086</doc-number>
<kind>B1</kind>
<name>Yoo et al.</name>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>375354</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6633903</doc-number>
<kind>B1</kind>
<name>Gould</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6760721</doc-number>
<kind>B1</kind>
<name>Chasen et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6772125</doc-number>
<kind>B2</kind>
<name>Harradine et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6799180</doc-number>
<kind>B1</kind>
<name>McGrath et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7031595</doc-number>
<kind>B2</kind>
<name>Yamaguchi</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386346</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>7565060</doc-number>
<kind>B2</kind>
<name>Hamada et al.</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386 98</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>7764866</doc-number>
<kind>B2</kind>
<name>Seo et al.</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386241</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>7787755</doc-number>
<kind>B2</kind>
<name>Seo et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386125</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>8041189</doc-number>
<kind>B2</kind>
<name>Shinkai et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386281</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2002/0018422</doc-number>
<kind>A1</kind>
<name>Tonami</name>
<date>20020200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2002/0040360</doc-number>
<kind>A1</kind>
<name>Sohma et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2002/0044757</doc-number>
<kind>A1</kind>
<name>Kawamura et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2002/0069218</doc-number>
<kind>A1</kind>
<name>Sull et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2002/0076201</doc-number>
<kind>A1</kind>
<name>Tsumagari et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2002/0164152</doc-number>
<kind>A1</kind>
<name>Kato et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2002/0174430</doc-number>
<kind>A1</kind>
<name>Ellis et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>725 46</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2002/0198864</doc-number>
<kind>A1</kind>
<name>Ostermann et al.</name>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2003/0061610</doc-number>
<kind>A1</kind>
<name>Errico</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2003/0103604</doc-number>
<kind>A1</kind>
<name>Kato et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2003/0113096</doc-number>
<kind>A1</kind>
<name>Taira et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386 46</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2003/0156504</doc-number>
<kind>A1</kind>
<name>Kanagae et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2003/0167264</doc-number>
<kind>A1</kind>
<name>Ogura et al.</name>
<date>20030900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707  3</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2003/0229894</doc-number>
<kind>A1</kind>
<name>Okada et al.</name>
<date>20031200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>725 41</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2004/0001700</doc-number>
<kind>A1</kind>
<name>Seo et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2004/0012621</doc-number>
<kind>A1</kind>
<name>Kaneko et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2004/0047588</doc-number>
<kind>A1</kind>
<name>Okada et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2004/0086264</doc-number>
<kind>A1</kind>
<name>Okada et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386 69</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2004/0139047</doc-number>
<kind>A1</kind>
<name>Rechsteiner et al.</name>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2004/0170391</doc-number>
<kind>A1</kind>
<name>Tsumagari et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2004/0175146</doc-number>
<kind>A1</kind>
<name>Tsumagari et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386 95</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2004/0210932</doc-number>
<kind>A1</kind>
<name>Mori et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2004/0215643</doc-number>
<kind>A1</kind>
<name>Brechner et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2004/0220791</doc-number>
<kind>A1</kind>
<name>Lamkin et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2004/0247284</doc-number>
<kind>A1</kind>
<name>Yamasaki</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386 52</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2004/0267742</doc-number>
<kind>A1</kind>
<name>Polson et al.</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2005/0141869</doc-number>
<kind>A1</kind>
<name>Kanegae et al.</name>
<date>20050600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2005/0163480</doc-number>
<kind>A1</kind>
<name>Takemoto</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>386 69</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2005/0165844</doc-number>
<kind>A1</kind>
<name>Yanagita et al.</name>
<date>20050700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2005/0198006</doc-number>
<kind>A1</kind>
<name>Boicey et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707  2</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2005/0244137</doc-number>
<kind>A1</kind>
<name>Takashima et al.</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2005/0254363</doc-number>
<kind>A1</kind>
<name>Hamada et al.</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2006/0045473</doc-number>
<kind>A1</kind>
<name>Alterman</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2006/0153542</doc-number>
<kind>A1</kind>
<name>Chun et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2007/0140653</doc-number>
<kind>A1</kind>
<name>Kozuka et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2007/0146792</doc-number>
<kind>A1</kind>
<name>Shiraiwa</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2009/0182719</doc-number>
<kind>A1</kind>
<name>Chun et al.</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2010/0202753</doc-number>
<kind>A1</kind>
<name>Chun et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2010/0217775</doc-number>
<kind>A1</kind>
<name>Chun et al.</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>CN</country>
<doc-number>1142112</doc-number>
<kind>A</kind>
<date>19970200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>CN</country>
<doc-number>1199908</doc-number>
<kind>A</kind>
<date>19981100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>CN</country>
<doc-number>1338743</doc-number>
<kind>A</kind>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>EP</country>
<doc-number>0 685 845</doc-number>
<date>19950400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>EP</country>
<doc-number>0 847 196</doc-number>
<date>19980600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>EP</country>
<doc-number>0 929 072</doc-number>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00062">
<document-id>
<country>EP</country>
<doc-number>1 102 271</doc-number>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00063">
<document-id>
<country>EP</country>
<doc-number>1198133</doc-number>
<kind>A1</kind>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00064">
<document-id>
<country>EP</country>
<doc-number>1280347</doc-number>
<kind>A1</kind>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00065">
<document-id>
<country>EP</country>
<doc-number>1 376 587</doc-number>
<kind>A2</kind>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00066">
<document-id>
<country>EP</country>
<doc-number>1521267</doc-number>
<kind>A1</kind>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00067">
<document-id>
<country>GB</country>
<doc-number>2 354 105</doc-number>
<kind>A</kind>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00068">
<document-id>
<country>JP</country>
<doc-number>11-025654</doc-number>
<kind>A</kind>
<date>19990100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00069">
<document-id>
<country>JP</country>
<doc-number>2000-322875</doc-number>
<date>20001100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00070">
<document-id>
<country>JP</country>
<doc-number>2001-155036</doc-number>
<kind>A</kind>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00071">
<document-id>
<country>JP</country>
<doc-number>2001-216726</doc-number>
<date>20010800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00072">
<document-id>
<country>JP</country>
<doc-number>2001-292425</doc-number>
<kind>A</kind>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00073">
<document-id>
<country>JP</country>
<doc-number>2002-25224</doc-number>
<kind>A</kind>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00074">
<document-id>
<country>JP</country>
<doc-number>2002-108892</doc-number>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00075">
<document-id>
<country>JP</country>
<doc-number>2002-158971</doc-number>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00076">
<document-id>
<country>JP</country>
<doc-number>2002-373481</doc-number>
<date>20021200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00077">
<document-id>
<country>JP</country>
<doc-number>2003-122761</doc-number>
<kind>A</kind>
<date>20030400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00078">
<document-id>
<country>JP</country>
<doc-number>2003-186885</doc-number>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00079">
<document-id>
<country>JP</country>
<doc-number>2003-122761</doc-number>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00080">
<document-id>
<country>KR</country>
<doc-number>10-2003-0033852</doc-number>
<kind>A</kind>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00081">
<document-id>
<country>KR</country>
<doc-number>10-2004-0066222</doc-number>
<kind>A</kind>
<date>20040700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00082">
<document-id>
<country>KR</country>
<doc-number>10-2004-0094408</doc-number>
<kind>A</kind>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00083">
<document-id>
<country>KR</country>
<doc-number>2006-11779</doc-number>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00084">
<document-id>
<country>RU</country>
<doc-number>2228546</doc-number>
<date>19990400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00085">
<document-id>
<country>RU</country>
<doc-number>2 228 546</doc-number>
<kind>C2</kind>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00086">
<document-id>
<country>WO</country>
<doc-number>WO 9222983</doc-number>
<kind>A2</kind>
<date>19921200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00087">
<document-id>
<country>WO</country>
<doc-number>WO 97/22201</doc-number>
<date>19970600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00088">
<document-id>
<country>WO</country>
<doc-number>WO 01/82624</doc-number>
<date>20011100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00089">
<document-id>
<country>WO</country>
<doc-number>WO02/08948</doc-number>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00090">
<document-id>
<country>WO</country>
<doc-number>WO 2004/074976</doc-number>
<kind>A2</kind>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00091">
<document-id>
<country>WO</country>
<doc-number>WO 2004-084214</doc-number>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00092">
<document-id>
<country>WO</country>
<doc-number>WO 2004-086371</doc-number>
<kind>A1</kind>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00093">
<document-id>
<country>WO</country>
<doc-number>WO 2004/095834</doc-number>
<kind>A1</kind>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00094">
<document-id>
<country>WO</country>
<doc-number>WO-2004/098183</doc-number>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00095">
<document-id>
<country>WO</country>
<doc-number>WO-2006/073275</doc-number>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00096">
<othercit>Office Action issued in Korean Patent Application No. 2005-1749 on Dec. 1, 2006.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00097">
<othercit>PCT International Search Report and Written Opinion issued Mar. 30, 2006 re: International Application No. PCT/KR2006/000051 (10 pp).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00098">
<othercit>Extended European Search Report issued on Nov. 26, 2009, in couterpart European Application No. 06702009.9 (10 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00099">
<othercit><i>White paper&#x2014;Blu-ray Disc Format&#x2014;2.A Logical and Audio Visual Application Format Specifications for BD-RE</i>, Blu-ray Disc Founders, Aug. 2004, pp. 1-26 (complete document).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00100">
<othercit><i>White paper&#x2014;Blu-ray Disc Format&#x2014;2.B Audio Visual Application Format Specifications for BD-ROM</i>, Blu-ray Disc Association, Mar. 2005, pp. 1-35 (complete document).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00101">
<othercit><i>Application Definition&#x2014;Blu-ray Disc Format&#x2014;BD-J Baseline Application and Logical Model Definition for BD-ROM</i>, Blu-ray Disc Association, Mar. 2005, pp. 1-45 (complete document).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00102">
<othercit>Extended European Search Report issued on Nov. 24, 2010, in counterpart European Application No. 10169530.2 (17 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00103">
<othercit>Japanese Office Action issued on Jul. 27, 2010, in corresponding Japanese Patent Application No. 2007-550293 (4 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00104">
<othercit>Japanese Office Action issued on Aug. 16, 2011, in counterpart Japanese Application No. 2009-228034 (3 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00105">
<othercit>Chinese Office Action issued Sep. 26, 2011, in counterpart Chinese Application NO. 201010003912.6 (9 pages, including English translation).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00106">
<othercit>Japanese Office Action issued Nov. 29, 2011, in counterpart Japanese Application No. 2007-550292 (10 pages, including English translation).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00107">
<othercit>Japanese Office Action issued Nov. 29, 2011, in couterpart Japanese Application No. 2009-066978 (8 pages, including English translation).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00108">
<othercit>Indian Office Action issued on Apr. 19, 2011, in counterpart Indian Application No. 0348/CHENP/2007 92 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00109">
<othercit>Chinese Notification of Granting of Patent Right to Invention mailed Aug. 6, 2012, issued in counterpart Chinese Patent Application No. 201010003913.0; 8 pages including English translation.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00110">
<othercit>Korean Office Action mailed May 23, 2007, issued in counterpart Korean Patent Application No. 10-2005-0001749; 3 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00111">
<othercit>Korean Office Action mailed Oct. 30, 2007 issued in counterpart Korean Patent Application No. 10-2005-0001749; 2 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00112">
<othercit>Korean Office Action mailed Sep. 14, 2010, issued in counterpart Korean Patent Application No. 10-2005-0108532; 6 pages.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00113">
<othercit>European Search Report issued on Dec. 18, 2009, in corresponding European Application No. 06702008.1 (11 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00114">
<othercit>U.S. Appl. No. 60/634,546 (available to public on Jun. 15, 2006) (8 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00115">
<othercit>International Search Report and Written Opinion issued Mar. 30, 2006, issued in counterpart International Patent Application No. PCT/KR2006/000050 (10 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00116">
<othercit>Malaysian Office Action issued in corresponding Malaysian Patent Application No. PI20060054 dated May 22, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00117">
<othercit>Taiwanese Preliminary Notice of First Office Action issued in Taiwan Patent Application No. 095100437 on Nov. 26, 2008 (with English translation).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00118">
<othercit>Russian Office Action issued Dec. 24, 2012 in counterpart Russian Application No. 2008151865/28(068181); (11 pages including English translation).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00119">
<othercit>Russian Decision on Grant issued on Feb. 27, 2009, in counterpart Russian Application No. 2007125643/28(027937) (14 pages, including English translation).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00120">
<othercit>Taiwanese Preliminary Notice of First Office Action issued on Nov. 26, 2008 in counterpart Taiwanese Patent Application No. 095100437 (16 pages, including complete English translation).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00121">
<othercit>Taiwanese Preliminary Notice of First Office Action issued on Feb. 12, 2009 in counterpart Taiwanese Patent Application No. 095100436 (21 pages, including complete English translation).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00122">
<othercit>Examination Report dated Mar. 7, 2012, in counterpart European Patent Application No. 10 169 530.2 (in English, 10 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00123">
<othercit>Summons to Oral Proceedings dated Mar. 23, 2012, in counterpart European Patent Application No. 06702008.1 (in English, 13 pages).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00124">
<othercit>Decision on Grant issued by the Federal Service on Industrial Property, Patents &#x26; Trademarks of Russia in Russian Patent Application No. 2007124568/28(026752) on May 28, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00125">
<othercit>Office Action issued by Japanese Patent Office in Japanese Patent Application No. 2007-550293 on Jun. 30, 2009.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>2</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>386  1</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386 45- 46</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386 69</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386 95- 96</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386125-126</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386241</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386248</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386262</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>386332-336</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>13</number-of-drawing-sheets>
<number-of-figures>15</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20060153535</doc-number>
<kind>A1</kind>
<date>20060713</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Chun</last-name>
<first-name>Hye-jeong</first-name>
<address>
<city>Yongin-si</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Sung-wook</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
<residence>
<country>KR</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Chun</last-name>
<first-name>Hye-jeong</first-name>
<address>
<city>Yongin-si</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Park</last-name>
<first-name>Sung-wook</first-name>
<address>
<city>Seoul</city>
<country>KR</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>NSIP Law</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Samsung Electronics Co., Ltd.</orgname>
<role>03</role>
<address>
<city>Suwon-si</city>
<country>KR</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Tran</last-name>
<first-name>Thai</first-name>
<department>2484</department>
</primary-examiner>
<assistant-examiner>
<last-name>Chowdhury</last-name>
<first-name>Nigar</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An apparatus and method for reproducing a storage medium that stores metadata are provided for an enhanced search function using various search keywords of audio-visual (AV) data. The apparatus includes: a search unit for searching for scenes matching a search keyword by conducting an enhanced search function on the AV data with reference to metadata which contains information regarding at least one search keyword for each of the scenes of the AV data; and a reproducing unit for reproducing the AV data corresponding to at least one scene found by the search unit. The metadata may include information regarding an entry point and/or duration, angles, etc. of each scene. Hence, the enhanced search can be conducted using various search keywords. Further, search results can be reproduced according to diverse scenarios, and the enhanced search function can be provided for movie titles that support multiple angles or multiple paths. Moreover, metadata can be created in multiple languages, thereby enabling the enhanced search function to support multiple languages.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="117.43mm" wi="156.63mm" file="US08625960-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="192.87mm" wi="165.44mm" file="US08625960-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="224.37mm" wi="155.53mm" file="US08625960-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="143.34mm" wi="193.04mm" file="US08625960-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="226.48mm" wi="152.23mm" orientation="landscape" file="US08625960-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="216.66mm" wi="157.06mm" file="US08625960-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="217.76mm" wi="151.47mm" file="US08625960-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="222.84mm" wi="161.46mm" file="US08625960-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="208.62mm" wi="152.32mm" orientation="landscape" file="US08625960-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="183.30mm" wi="97.03mm" orientation="landscape" file="US08625960-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="182.63mm" wi="140.97mm" orientation="landscape" file="US08625960-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="177.12mm" wi="124.46mm" orientation="landscape" file="US08625960-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="208.20mm" wi="131.74mm" orientation="landscape" file="US08625960-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="188.47mm" wi="128.52mm" orientation="landscape" file="US08625960-20140107-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims the benefit of Korean Patent Application No. 2005-1749 filed on Jan. 7, 2005, in the Korean Intellectual Property Office, the disclosure of which is incorporated herein by reference.</p>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates to reproducing audio-visual (AV) data recorded on a storage medium, and more particularly, to a storage medium including meta data to provide an enhanced search function, an apparatus and method for reproducing AV data from a storage medium that stores metadata for providing an enhanced search function.</p>
<p id="p-0005" num="0004">2. Related Art</p>
<p id="p-0006" num="0005">Storage media, such as DVDs and Blu-ray discs (BDs), store audio-visual (AV) data composed of video, audio, and/or subtitles that are compression-encoded according to standards for digital video and audio compression, such as a MPEG (Motion Picture Experts Group) standard. Storage media also store additional information such as encoding properties of AV data or the order in which the AV data is to be reproduced. In general, moving pictures recorded on a storage medium are sequentially reproduced in a predetermined order. However, the moving pictures can be reproduced in units of chapters while AV data is being reproduced.</p>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a structure of AV data recorded on a typical storage medium. As shown in <figref idref="DRAWINGS">FIG. 1</figref>, a storage medium (such as the medium <b>250</b> shown, for example, in <figref idref="DRAWINGS">FIG. 2</figref>) is typically formed with multiple layers in order to manage a structure of AV data recorded thereon. The data structure <b>100</b> includes one or more clips <b>110</b> that are recording units of a multimedia image (AV data); one or more playlists <b>120</b> that are reproducing units of multimedia image (AV data); move objects <b>130</b> including navigation commands that are used to reproduce a multimedia image (AV data); and an index table <b>140</b> that is used to specify a movie object to be first reproduced and titles of movie objects <b>130</b>.</p>
<p id="p-0008" num="0007">The clips <b>110</b> are implemented as one object which includes a clip AV stream <b>112</b> for an AV data stream for a high picture quality movie and clip information <b>114</b> for attributes corresponding to the AV data stream. For example, the AV data stream may be compressed according to a standard, such as the motion picture experts group (MPEG). However, such clips <b>110</b> need not require the AV data stream <b>112</b> to be compressed in all aspects of the present invention. In addition, the clip information <b>114</b> may include audio/video properties of the AV data stream <b>112</b>, an entry point map in which information regarding a location of a randomly accessible entry point is recorded in units of a predetermined section and the like.</p>
<p id="p-0009" num="0008">Each playlist <b>120</b> includes a playlist mark composed of marks which indicate the positions of clips <b>110</b> corresponding to the playlist <b>120</b>. Each playlist <b>120</b> also includes a set of reproduction intervals of these clips <b>110</b>, and each reproduction interval is referred to as a play item <b>122</b>. Hence, AV data can be reproduced in units of playlists <b>120</b> and in an order of playitems <b>122</b> listed in a playlist <b>120</b>.</p>
<p id="p-0010" num="0009">The movie object <b>130</b> is formed with navigation command programs, and these navigation commands start reproduction of a playlist <b>120</b>, switch between movie objects <b>130</b>, or manage reproduction of a playlist <b>120</b> according to preference of a user.</p>
<p id="p-0011" num="0010">The index table <b>140</b> is a table at the top layer of the storage medium to define a plurality of titles and menus, and includes start location information of all titles and menus such that a title or menu selected by a user operation, such as title search or menu call, can be reproduced. The index table <b>140</b> also includes start location information of a title or menu that is automatically reproduced first when a storage medium is placed on a reproducing apparatus.</p>
<p id="p-0012" num="0011">However, in such a storage medium, there is no method for jumping to an arbitrary scene according to a search condition (e.g., scene, character, location, sound, or item) desired by a user and reproducing the scene. In other words, a typical storage medium does not provide a function for moving to a portion of the AV data according to a search condition (e.g., scene, character, location, sound, or item) set by the user and reproducing the portion. Therefore, the storage medium cannot offer diverse search functions.</p>
<p id="p-0013" num="0012">Since AV data is compression-encoded and recorded on a conventional storage medium according to an MPEG 2 standard and multiplexed, it is difficult to manufacture a storage medium that contains metadata needed to search for a moving picture. In addition, once a storage medium is manufactured, it is almost impossible to edit or reuse AV data or metadata stored on the storage medium.</p>
<p id="p-0014" num="0013">Further, a currently defined playlist mark cannot distinguish multiple angles or multiple paths. Therefore, even when AV data supports multiple angles or multiple paths, it is difficult to provide diverse enhanced search functions on the AV data.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0015" num="0014">Various aspects and example embodiments of the present invention provide an apparatus and method for reproducing a storage medium that stores metadata for providing an enhanced search function using various search keywords of audio-visual (AV) data. In addition, the present invention also provides an apparatus and method for reproducing a storage medium that stores metadata for actively providing an enhanced search function in connection with AV data in various formats.</p>
<p id="p-0016" num="0015">Additional aspects and/or advantages of the invention will be set forth in part in the description which follows and, in part, will be obvious from the description, or may be learned by practice of the invention.</p>
<p id="p-0017" num="0016">In accordance with an aspect of the present invention, there is provided a reproducing apparatus which reproduces audio-visual (AV) data stored in an information storage medium. The reproducing apparatus comprises a search unit arranged to search for scenes matching a search keyword by conducting an enhanced search function on the AV data with reference to metadata which contains information regarding at least one search keyword for each of the scenes of the AV data; and a reproducing unit arranged to reproduce the AV data corresponding to at least one scene found by the search unit.</p>
<p id="p-0018" num="0017">The apparatus may further include a user interface receiving the search keyword input by a user and displaying search results for the search keyword.</p>
<p id="p-0019" num="0018">The enhanced search function may be enabled when the AV data is reproduced along a main playback path defined by an author and may be disabled when the AV data is reproduced along a side playback path defined by a user.</p>
<p id="p-0020" num="0019">When there are found scenes, the reproducing unit may display the found scenes on the user interface, receive information regarding the selection of one of the found scenes by the user, and reproduce the AV data corresponding to the selected scene.</p>
<p id="p-0021" num="0020">The reproducing unit may reproduce the AV data corresponding to the scene immediately before or after the selected scene based on the user's input.</p>
<p id="p-0022" num="0021">When there are found scenes, the reproducing unit may sequentially reproduce AV data corresponding to the found scenes.</p>
<p id="p-0023" num="0022">When the AV data supports multiple angles, the reproducing apparatus may reproduce AV data corresponding to a predetermined angle or an angle input by the user using information regarding angles included in the metadata.</p>
<p id="p-0024" num="0023">The metadata may be defined for each of the scenes. The reproducing unit may find a start position of the at least one found scene using an entry point indicating the start position of the at least one found scene. The search results may be displayed together with respective thumbnails.</p>
<p id="p-0025" num="0024">In accordance with another aspect of the present invention, there is provided a method of reproducing AV data stored in an information storage medium. Such a method comprises searching for scenes matching a search keyword by conducting an enhanced search function on the AV data with reference to metadata which contains information regarding at least one search keyword for each of the scenes of the AV data; and reproducing the AV data corresponding to a scene found.</p>
<p id="p-0026" num="0025">In addition to the example embodiments and aspects as described above, further aspects and embodiments of the present invention will be apparent by reference to the drawings and by study of the following descriptions.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0027" num="0026">A better understanding of the present invention will become apparent from the following detailed description of example embodiments and the claims when read in connection with the accompanying drawings, all forming a part of the disclosure of this invention. While the following written and illustrated disclosure focuses on disclosing example embodiments of the invention, it should be clearly understood that the same is by way of illustration and example only and that the invention is not limited thereto. The spirit and scope of the present invention are limited only by the terms of the appended claims. The following represents brief descriptions of the drawings, wherein:</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a structure of AV data recorded on a typical storage medium;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of an example reproducing apparatus which reproduces a storage medium storing meta data for providing an enhanced search function according to an embodiment of the present invention;</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart illustrating a method of reproducing a recording medium storing metadata for providing the enhanced search function according to an embodiment of the present invention;</p>
<p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. 4</figref> illustrates example screens displayed an example of searching for a desired scene using metadata for a title scene search;</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 5</figref> illustrates the relationship between metadata for a title scene search and audio-visual (AV) data according to an embodiment of the present invention;</p>
<p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a directory of metadata according to an embodiment of the present invention;</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a naming rule of an example metadata file according to an embodiment of the present invention;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. 8</figref> illustrates the structure of metadata according to an embodiment of the present invention;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. 9</figref> illustrates a detailed structure of metadata shown in <figref idref="DRAWINGS">FIG. 8</figref>;</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 10</figref> illustrates the application scope of a title which provides the enhanced search function;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. 11</figref> illustrates an application of metadata according to an embodiment of the present invention;</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. 12</figref> illustrates an application of metadata according to another embodiment of the present invention;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. 13</figref> illustrates an example of a highlight playback using metadata according to an embodiment of the present invention;</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. 14</figref> illustrates a multi-angle title that provides the enhanced search function using metadata according to an embodiment of the present invention; and</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 15</figref> illustrates a reproducing process of an example reproducing apparatus according to an embodiment of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading>
<p id="p-0043" num="0042">Reference will now be made in detail to the present embodiments of the present invention, examples of which are illustrated in the accompanying drawings, wherein like reference numerals refer to the like elements throughout. The embodiments are described below in order to explain the present invention by referring to the figures.</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of an example reproducing apparatus which reproduces a storage medium storing metadata for providing an enhanced search function according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 2</figref>, the reproducing apparatus <b>200</b> includes a reading unit <b>210</b>, a reproducing unit <b>220</b>, a search unit <b>230</b>, and a user interface <b>240</b>.</p>
<p id="p-0045" num="0044">The reading unit <b>210</b> reads audio-visual (AV) data and metadata for providing the enhanced search function from a storage medium <b>250</b> such as a Blu-ray disc (BD). The reproducing unit <b>220</b> decodes and reproduces the AV data. In particular, when a user inputs a search keyword, the reproducing unit <b>220</b> receives from the search unit <b>230</b> information regarding a scene matching the search keyword and reproduces the scene. When there are multiple scenes matching the search keyword, the reproducing unit <b>220</b> displays all scenes matching the search keyword on the user interface <b>240</b> and reproduces one or more scenes selected by the user or sequentially reproduces all of the scenes. The reproducing unit <b>220</b> may also be called a playback control engine.</p>
<p id="p-0046" num="0045">The search unit <b>230</b> receives a search keyword from the user interface <b>240</b> and searches for scenes matching the search keyword. Then, the search unit <b>230</b> transmits the search results to the user interface <b>240</b> to display the search results in the form of a list or to the reproducing unit <b>220</b> to reproduce the same. As illustrated in <figref idref="DRAWINGS">FIG. 2</figref>, search results may be presented as a list of scenes matching a search keyword.</p>
<p id="p-0047" num="0046">The user interface <b>240</b> receives a search keyword input by a user or displays search results. Also, when a user selects a scene from search results, i.e., a list of scenes found, displayed on the user interface <b>240</b>, the user interface <b>240</b> receives information regarding the selection.</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart illustrating a method of reproducing a recording medium storing the metadata for providing the enhanced search function according to an embodiment of the present invention. Referring to the reproducing method <b>300</b> shown in <figref idref="DRAWINGS">FIG. 3</figref>, a user inputs a search keyword using the user interface <b>240</b>, as shown in <figref idref="DRAWINGS">FIG. 2</figref>, at block <b>310</b>. The search keyword may be a scene type, a character, an actor, an item, a location, a sound, or any word defined by an author. For example, when the movie &#x201c;The Matrix&#x201d; is reproduced, all scenes in which the character &#x201c;Neo&#x201d; appears can be searched for. Also, all scenes in which an item &#x201c;mobile phone&#x201d; appears can be searched for.</p>
<p id="p-0049" num="0048">Next, all scenes matching the input search keyword are searched for with reference to a metadata file at block <b>320</b>. The metadata file defines a plurality of scenes, and includes information regarding search keywords associated with each scene and an entry point of each scene. The structure of the metadata file will be described in detail later. Portions of AV data which correspond to found scenes are searched for using entry points of the found scenes and are reproduced at block <b>330</b>. In this way, an enhanced search can be conducted on AV data using various search keywords. Hereinafter, the enhanced search function will also be referred to as a &#x201c;title scene search function.&#x201d;</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. 4</figref> illustrates example screens <b>400</b> displayed in an example of searching for a desired scene using the metadata for the title scene search. The metadata for the title scene search includes search information for each scene in AV data recorded on a storage medium <b>250</b>, such as a Blu-ray disc (BD) as shown in <figref idref="DRAWINGS">FIG. 2</figref>. Referring to <figref idref="DRAWINGS">FIG. 4</figref>, while a movie title such as &#x201c;The Matrix&#x201d; or &#x201c;The Lord of the Rings&#x201d; is being reproduced at stage #<b>1</b>, a user selects the title scene search function using the user interface <b>240</b>, as shown in <figref idref="DRAWINGS">FIG. 2</figref>, such as a remote controller, to search for scenes that are associated with a desired search keyword.</p>
<p id="p-0051" num="0050">The user selects one of a plurality of search keyword categories displayed on the user interface <b>240</b> at stage #<b>2</b>, and selects a search keyword from the selected search keyword category at stage #<b>3</b>. For example, when the user selects &#x201c;item&#x201d; as a search keyword category and selects &#x201c;tower&#x201d; as a search keyword corresponding to &#x201c;item,&#x201d; the movie title is searched for scenes in which &#x201c;tower&#x201d; appears, and search results are displayed together with respective thumbnails at stage #<b>4</b>. When the user selects one of the search results, i.e., found scenes, the selected scene is reproduced at stage #<b>5</b>. Using a command such as &#x201c;skip to next search result&#x201d; or &#x201c;skip to previous search result&#x201d; on the user interface <b>240</b>, a previous or next scene can be searched for and reproduced at stage #<b>6</b>.</p>
<p id="p-0052" num="0051">A &#x201c;highlight playback&#x201d; function for sequentially reproducing all scenes found can also be provided. In the highlight playback, all search results are sequentially reproduced. As a result, there is no need to wait until a user selects one of the search results. When a user selects a search keyword associated with contents, search results for the selected search keyword are obtained. The search results form the highlights of the contents associated with the selected search keyword.</p>
<p id="p-0053" num="0052">The structure of the metadata for the title scene search will now be described in detail herein below.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. 5</figref> illustrates the relationship between metadata <b>500</b> for the title scene search and AV data on a storage medium according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 5</figref>, the storage medium according to an embodiment of the present invention (such as medium <b>250</b>, shown in <figref idref="DRAWINGS">FIG. 2</figref>) stores the metadata <b>500</b> in addition to the AV data shown in <figref idref="DRAWINGS">FIG. 1</figref>. The metadata <b>500</b> may be stored in files separately from movie playlists, which are reproducing units. A metadata file <b>510</b> is created for each playlist <b>520</b>, and includes a plurality of scenes <b>512</b>, which are author-defined sections of each playlist <b>520</b>. Each scene <b>512</b> includes an entry point indicating a start position thereof. In example embodiments of the present invention, each scene <b>512</b> may further include the duration thereof.</p>
<p id="p-0055" num="0054">Using an entry point (EP) map included in clip information <b>114</b>, each entry point is converted into an address of a scene in a clip AV stream <b>112</b> included in each clip <b>110</b>. Therefore, the start position of each scene included in a clip AV stream <b>112</b>, which is real AV data, can be found using an entry point. Each scene <b>512</b> also includes information regarding search keywords associated therewith (hereinafter referred to as search keyword information). For example, the search keyword information may include the following:</p>
<p id="p-0056" num="0055">Scene 1 is a battle scene,</p>
<p id="p-0057" num="0056">Characters are A, B and C,</p>
<p id="p-0058" num="0057">Actors are a, b and c, and</p>
<p id="p-0059" num="0058">Location is x.</p>
<p id="p-0060" num="0059">Accordingly, a user can search for scenes matching a desired search keyword based on the search keyword information of each scene <b>512</b>. In addition, the start positions of found scenes in a clip AV stream <b>112</b> can be determined using the entry points of the found scenes, and then the found scenes can be reproduced.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a directory of metadata <b>500</b> according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 6</figref>, metadata <b>500</b> related to the AV data shown in <figref idref="DRAWINGS">FIG. 5</figref>, is stored in files in respective directories. Specifically, an index table is stored in an index.bdmv file, a movie object is stored in a MovieObject.bdmv file, and playlists are stored in xxxxx.mpis files in a PLAYLIST directory. In addition, clip information is stored in xxxxx.clpi files in a CLIPINF directory, clip AV streams are stored in xxxxx.m2ts files in a STREAM directory, and other data is stored in files in an AUXDATA directory.</p>
<p id="p-0062" num="0061">The metadata <b>500</b> for the title scene search is stored in files in a META directory separately from the AV data. A metadata file for a disc library is dlmt_xxx.xml, and a metadata file for the title scene search is esmt_xxx_yyyyy.xml. According to an embodiment of the present invention, the meta data <b>100</b> is recorded in an XML format and in a markup language for easy editing and reuse. Hence, after the storage medium is manufactured, data recorded thereon can be edited and reused.</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. 7</figref> illustrates a naming rule of an example metadata file <b>510</b> according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 7</figref>, the name of the metadata file <b>510</b> starts with a prefix esmt_indicating metadata <b>500</b>. The next three characters indicate a language code according to an ISO 639-2 standard, and the next five characters indicate a corresponding playlist number. As described above, a metadata file <b>510</b> is created for each playlist <b>520</b>, as shown in <figref idref="DRAWINGS">FIG. 5</figref>. In addition, a menu displayed during the title scene search can support multiple languages using the language code according to an ISO 639-2 standard.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. 8</figref> illustrates the structure of an example metadata file <b>510</b> according to an embodiment of the present invention. As described in connection with <figref idref="DRAWINGS">FIG. 5</figref>, each metadata file <b>510</b> includes a plurality of scenes <b>512</b>. Referring to <figref idref="DRAWINGS">FIG. 8</figref>, each scene <b>512</b> corresponds to search keywords such as a scene type, a character, actor, etc. A value of each search keyword may be expressed using a sub-element or an attribute of the search keyword according to an XML rule.</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. 9</figref> illustrates a detailed structure of an example metadata file <b>510</b> shown in <figref idref="DRAWINGS">FIG. 8</figref>. Referring to <figref idref="DRAWINGS">FIG. 9</figref>, each scene <b>512</b> for the title scene search includes a scene type element, a character element, an actor element, or an &#x201c;authordef&#x201d; element which is an author-defined search keyword. In addition, each scene <b>512</b> includes &#x201c;entry_point&#x201d; indicating the start position of each scene and &#x201c;duration&#x201d; indicating a period of time during which each scene is reproduced. When multiple angles are supported, each scene <b>512</b> also includes &#x201c;angle_num&#x201d; indicating a particular angle. Whether to include &#x201c;duration&#x201d; and &#x201c;angle_num&#x201d; in each scene <b>512</b> is optional.</p>
<p id="p-0066" num="0065">An example of conducting the title scene search using metadata <b>500</b> will now be described as follows.</p>
<p id="p-0067" num="0066">Specifically, <figref idref="DRAWINGS">FIG. 10</figref> illustrates the application scope of a title which provides the enhanced search function according to an embodiment of the present invention. As previously shown in <figref idref="DRAWINGS">FIG. 5</figref>, a storage medium <b>250</b>, such as a Blu-ray disc (BD), may store a movie title for reproducing a moving picture such as a movie and an interactive title including programs for providing interactive functions to users. The metadata <b>500</b> for the title scene search provides the enhanced search function while a moving picture is being reproduced. Thus, the metadata <b>500</b> is used only for movie titles. The type of title can be identified by a &#x201c;Title_playback_type&#x201d; field. If the &#x201c;Title_playback_type&#x201d; field of a title is 0b, the title is a movie title. If the &#x201c;Title_playback_type&#x201d; field of a title is 1b, the title is an interactive title. Therefore, the title scene search according to an embodiment of the present invention can be conducted only when the &#x201c;Title_playback_type&#x201d; field is 0b.</p>
<p id="p-0068" num="0067">Referring to <figref idref="DRAWINGS">FIG. 10</figref>, when a storage medium <b>250</b>, such as a Blu-ray disc (BD), is loaded into an example reproducing apparatus <b>200</b>, as shown in <figref idref="DRAWINGS">FIG. 2</figref>, title #<b>1</b> is accessed using an index table. When a navigation command &#x201c;Play playlist #<b>1</b>&#x201d; included in movie object #<b>1</b> of title #<b>1</b> is executed, playlist #<b>1</b> is reproduced. As shown in <figref idref="DRAWINGS">FIG. 10</figref>, playlist #<b>1</b> is composed of at least one play item. An author may arbitrarily define a chapter or a scene, regardless of a play item.</p>
<p id="p-0069" num="0068">A playlist which is automatically reproduced according to the index table when a storage medium <b>250</b> is loaded into an example reproducing apparatus <b>200</b>, shown in <figref idref="DRAWINGS">FIG. 2</figref>, is called a main playback path playlist, and a playlist which is reproduced by another movie object that a user calls using a button object while the main playback path playlist is being reproduced is called a side playback path playlist. The side playback path playlist is not within the scope of a chapter or a scene defined by an author. Therefore, according to an embodiment of the present invention, the title scene search function is enabled for the main playback path playlist and disabled for the side playback path playlist.</p>
<p id="p-0070" num="0069">In summary, the application scope of the title that provides the enhanced search function has the following constraints.
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0070">1) The title scene search is applied to movie titles.</li>
        <li id="ul0002-0002" num="0071">2) Metadata for the title scene search is defined in units of playlists. Since a movie title may include one or more playlists, one or more metadata may be defined for a playlist.</li>
        <li id="ul0002-0003" num="0072">3) The title scene search is applied to the main playback path playlist, but not to the side playback path playlist.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0071" num="0073"><figref idref="DRAWINGS">FIG. 11</figref> illustrates an application of metadata <b>500</b> according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 11</figref>, scenes used in the metadata <b>500</b> are defined. The scenes are basic units used in the metadata <b>500</b> for the title scene search and basic units of contents included in a playlist. An author may designate entry points in a playlist on a global time axis. Content between two neighboring entry points is a scene.</p>
<p id="p-0072" num="0074">When a user searches for contents using a search keyword, search results are represented as a group of entry points included in scenes having metadata whose search keyword information matches the search keyword. Such entry points are sequentially arranged temporally and transmitted to the playback control engine, i.e., as the reproducing unit <b>200</b> as shown in <figref idref="DRAWINGS">FIG. 2</figref>. The playback control engine can search for a plurality of scenes associated with identical search keywords and reproduce the scenes.</p>
<p id="p-0073" num="0075">Referring to <figref idref="DRAWINGS">FIG. 11</figref>, entry points for each search keyword are expressed as circles. For example, when a user selects scenetype #<b>1</b> as a search keyword, the search results include scene #<b>1</b>, scene #<b>3</b>, and scene #n. Then, the user may select some of scene #<b>1</b>, scene #<b>3</b>, and scene #n for reproduction. In addition, the user may navigate and reproduce previous or next search results using a user operation (UO) such as &#x201c;Skip to next scene( )&#x201d; or &#x201c;Skip to previous scene( )&#x201d; through the user interface <b>240</b>, shown in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0074" num="0076"><figref idref="DRAWINGS">FIG. 12</figref> illustrates an application of metadata <b>500</b> according to another embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 12</figref>, scenes are defined using duration in addition to entry points described above. An interval between an entry point and a point at the end of the duration is defined as a scene. When a user selects a scene, search results can be reproduced according to three scenarios.</p>
<p id="p-0075" num="0077">1) Scenario 1: Simple Playback</p>
<p id="p-0076" num="0078">Regardless of duration, a playlist is reproduced from an entry point of a scene selected by a user from search results to the end of the playlist unless there is a user input. For example, when a user selects scenetype #<b>1</b>, playlist #<b>1</b> is reproduced from an entry point of scene #<b>1</b> to the end of playlist #<b>1</b>.</p>
<p id="p-0077" num="0079">2) Scenario 2: Highlight Playback</p>
<p id="p-0078" num="0080">A playlist is reproduced from an entry point of a scene selected by a user from search results until the end of the duration of the selected scene. Then, the reproducing unit <b>20</b> jumps to a next scene and reproduces the next scene. For example, when a user selects scenetype #<b>2</b>, only scene #<b>1</b> and scene #<b>3</b>, which are search results, are reproduced. In other words, only the highlights of playlist #<b>1</b> which are associated with the search keyword scenetype #<b>2</b> are reproduced. Another example of the highlight playback is illustrated in <figref idref="DRAWINGS">FIG. 13</figref>. Referring to <figref idref="DRAWINGS">FIG. 13</figref>, search results are sequentially reproduced. Therefore, there is no need to stop and wait for a user input after a found scene is reproduced. In other words, after one of a plurality of search results for actor &#x201c;a&#x201d; is reproduced, a next search result is subsequently reproduced. In this way, only the highlights of actor &#x201c;a&#x201d; are reproduced. For the highlight playback, each search result is expressed using a duration and an entry point. The search results can be linked and sequentially reproduced using the entry points and the duration information.</p>
<p id="p-0079" num="0081">3) Scenario 3: Scene-Based Playback</p>
<p id="p-0080" num="0082">Search results are reproduced by scene. In other words, a scene selected by a user from search results is reproduced from an entry point of the scene for the duration of the scene. After the duration, reproduction is stopped until a user input is received. Scenario 3 is similar to scenario 2 except that the reproduction is stopped at the end of the scene.</p>
<p id="p-0081" num="0083"><figref idref="DRAWINGS">FIG. 14</figref> illustrates an example multi-angle title that provides the enhanced search function using metadata <b>500</b> according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 14</figref>, an example of a multi-path title composed of multiple angles is illustrated. The multi-path title is composed of five (5) play items. Of the five play items, a second (2nd) play item is composed of three (3) angles, and a fourth (4th) play item is composed of four (4) angles. In a playlist that supports multiple angles, scene #<b>1</b> and scene #<b>2</b> matching the search keyword scenetype #<b>1</b> and scene #<b>3</b> and scene #<b>4</b> matching the search keyword scenetype #<b>2</b> are found. Each scene is defined by an entry point and duration.</p>
<p id="p-0082" num="0084">Found scenes can be overlapped each other because overlapping entry points can be distinguished by &#x201c;angle_num&#x201d; shown in <figref idref="DRAWINGS">FIG. 5</figref>. However, when entry points do not overlap each other, scenes found as a result of the enhanced search cannot overlap each other. When a user desires to reproduce search results according to scenario 2, the reproducing apparatus sequentially reproduces scenes along a dotted arrow in <figref idref="DRAWINGS">FIG. 14</figref>.</p>
<p id="p-0083" num="0085">Referring to <figref idref="DRAWINGS">FIG. 14</figref>, scenes which cover a portion of a play item or a plurality of play items are illustrated. In each scene, the metadata <b>500</b> of AV data thereof is defined.</p>
<p id="p-0084" num="0086">In the case of play items which support multiple angles (for example, the second and fourth play items), the metadata <b>500</b> is applied to AV data corresponding to one of the supported multiple angles. For example, in the case of scene #<b>1</b>, parts of first and second play items are defined as a reproduction section, and a value of angle_num is three. The value of angle_num is applied only to play items that support multiple angles. Therefore, play items that do not support multiple angles are reproduced at a default angle. A player status register (PSR), <b>3</b> which is a state register of the reproducing apparatus <b>200</b>, as shown, for example, in <figref idref="DRAWINGS">FIG. 2</figref>, is designated as a default angle. Accordingly, when scene #<b>1</b> is reproduced, play item #<b>1</b> which does not support multiple angles is reproduced at the default angle, and play item #<b>2</b> which supports multiple angles is reproduced at angle <b>3</b> according to the value designated as the attribute of angle_num. In this case, search keywords defined for scene #<b>1</b> for the title scene search are applied to angle <b>3</b> for play item <b>2</b> that supports multiple angles. As described above, when metadata <b>500</b> including angle_num is used, a title which supports multiple angles can also provide various enhanced search functions according to a designated search keyword.</p>
<p id="p-0085" num="0087"><figref idref="DRAWINGS">FIG. 15</figref> illustrates a reproducing process of an example reproducing apparatus according to an embodiment of the present invention. Referring to <figref idref="DRAWINGS">FIG. 15</figref>, the reproducing apparatus <b>200</b>, shown in <figref idref="DRAWINGS">FIG. 2</figref>, provides the title scene search function while reproducing a movie title. When a storage medium <b>250</b>, such as a Blu-ray disc (BD), is loaded into the reproducing apparatus <b>200</b> and the reproduction of a movie title starts (operation <b>1510</b>), the title scene search function is activated to be in a valid state (operation <b>1520</b>). As described with reference to <figref idref="DRAWINGS">FIG. 14</figref>, when a movie title that supports multiple angles is reproduced, the title scene search can be conducted by changing an angle (operation <b>1530</b>). In addition, if a multi-path playlist is supported (operation <b>1522</b>), when a playlist is changed to a main playback path playlist, the title scene search function is activated to be in the valid state (operation <b>1534</b>). However, when the playlist is changed to a side playback path playlist, the title scene search function becomes invalid (operation <b>1532</b>). Further, when a title is changed to an interactive title, not a movie title, the title scene search function becomes invalid (operation <b>1538</b>).</p>
<p id="p-0086" num="0088">As described above, the present invention provides a storage medium storing metadata for providing an enhanced search function using various search keywords for AV data, an apparatus and method for reproducing the storage medium. The present invention can also provide the enhanced search function in connection with AV data in various formats.</p>
<p id="p-0087" num="0089">In other words, the metadata for providing the enhanced search function is defined by scene by an author, and each scene includes information regarding at least one search keyword. In addition, each scene includes information regarding an entry point and/or duration, angles, and so on. Hence, the enhanced search function can be conducted using various search keywords.</p>
<p id="p-0088" num="0090">Further, search results can be reproduced according to diverse scenarios, and the enhanced search function can be provided for movie titles that support multiple angles or multiple paths. Moreover, metadata can be created in multiple languages, thereby enabling the provision of the enhanced search function that supports multiple languages.</p>
<p id="p-0089" num="0091">Example embodiments of the enhanced search method according to the present invention can also be written as a computer program and can be implemented in a general digital computer that executes the computer program recorded on a computer-readable medium. Codes and code segments constructing the computer program can be easily induced by computer programmers in the art. The computer-readable medium can be any data storage device that can store data which can be thereafter read by a computer system. Examples of the computer-readable recording medium include read-only memory (ROM), random-access memory (RAM), CD-ROMs, magnetic tapes, floppy disks, optical data storage devices, and carrier waves (such as data transmission through the Internet). The computer-readable medium can also be distributed over network coupled computer systems so that the computer readable code is stored and executed in a distributed fashion.</p>
<p id="p-0090" num="0092">While the present invention has been particularly shown and described with reference to exemplary embodiments thereof, it will be understood by those of ordinary skill in the art that various changes in form and details may be made therein without departing from the spirit and scope of the present invention. For example, any computer readable media or data storage devices may be utilized, as long as metadata is included in the playlist in the manner shown in <figref idref="DRAWINGS">FIG. 5</figref> through <figref idref="DRAWINGS">FIG. 15</figref>. In addition, metadata can also be configured differently as shown in <figref idref="DRAWINGS">FIG. 5</figref>. Moreover, a reproducing apparatus as shown in <figref idref="DRAWINGS">FIG. 2</figref> can be implemented as part of a recording apparatus, or alternatively a single apparatus for performing recording and/or reproducing functions with respect to a storage medium. Similarly, the CPU can be implemented as a chipset having firmware, or alternatively, a general or special purposed computer programmed to perform the methods as described, for example, with reference to <figref idref="DRAWINGS">FIG. 3</figref>, and <figref idref="DRAWINGS">FIGS. 10-15</figref>. Accordingly, it is intended, therefore, that the present invention not be limited to the various example embodiments disclosed, but that the present invention includes all embodiments falling within the scope of the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A reproducing apparatus configured to reproduce audio-visual (AV) data stored on an optical information storage medium, the reproducing apparatus comprising:
<claim-text>a search unit configured to search for scenes of at least one moving picture matching a search keyword by conducting an enhanced search function on the AV data with reference to metadata that contains information regarding at least one search keyword for each of the scenes of the AV data, the AV data and the metadata being stored on the optical information storage medium, the metadata being stored in at least one file separately from the AV data; and</claim-text>
<claim-text>a reproducing unit configured to reproduce the AV data, from the optical information storage medium, corresponding to at least one scene found by the search unit,</claim-text>
<claim-text>wherein the enhanced search function is enabled for a movie title indicated by an index table, and is disabled for an interactive title indicated by the index table.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. A non-transitory information storage medium formed with multiple layers to manage a data structure of audio-visual (AV) data recorded thereon, the information storage medium comprising:
<claim-text>one or more clips that are recording units of AV data, each clip being implemented as one object including a clip AV stream for the AV data and clip information for attributes corresponding to the AV data;</claim-text>
<claim-text>one or more playlists that are reproducing units of AV data, each playlist including a set of playitems corresponding to reproduction intervals of the clips; and</claim-text>
<claim-text>one or more metadata files created for each playlist for providing an enhanced search function on the AV data, the one or more metadata files comprising metadata defined scene by scene and comprising information regarding at least one search keyword to be applied to a corresponding scene, in response to a search keyword being input by a user, and the AV data corresponding to one or more scenes matching the search keyword being reproduced,</claim-text>
<claim-text>wherein the enhanced search function is enabled for a movie title indicated by an index table, and is disabled for an interactive title indicated by the index table. </claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
