<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08627035-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08627035</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13184939</doc-number>
<date>20110718</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>246</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>12</main-group>
<subgroup>08</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>711165</main-classification>
<further-classification>711117</further-classification>
<further-classification>711E12019</further-classification>
</classification-national>
<invention-title id="d2e53">Dynamic storage tiering</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>8359444</doc-number>
<kind>B2</kind>
<name>Arakawa</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711165</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>8370597</doc-number>
<kind>B1</kind>
<name>Chatterjee et al.</name>
<date>20130200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711170</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2003/0110357</doc-number>
<kind>A1</kind>
<name>Nguyen et al.</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711136</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2005/0246503</doc-number>
<kind>A1</kind>
<name>Fair</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711147</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2006/0004957</doc-number>
<kind>A1</kind>
<name>Hand et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711113</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2008/0244239</doc-number>
<kind>A1</kind>
<name>DeWitt et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>712220</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2009/0150593</doc-number>
<kind>A1</kind>
<name>Hamilton et al.</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711101</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2010/0077168</doc-number>
<kind>A1</kind>
<name>Arakawa</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711165</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2010/0115204</doc-number>
<kind>A1</kind>
<name>Li et al.</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711130</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2011/0197046</doc-number>
<kind>A1</kind>
<name>Chiu et al.</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711171</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2012/0023300</doc-number>
<kind>A1</kind>
<name>Tremaine et al.</name>
<date>20120100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711162</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2012/0059994</doc-number>
<kind>A1</kind>
<name>Montgomery et al.</name>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711119</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2012/0089782</doc-number>
<kind>A1</kind>
<name>McKean et al.</name>
<date>20120400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711122</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2012/0117349</doc-number>
<kind>A1</kind>
<name>Lau et al.</name>
<date>20120500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711165</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2012/0246403</doc-number>
<kind>A1</kind>
<name>McHale et al.</name>
<date>20120900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711114</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2012/0278511</doc-number>
<kind>A1</kind>
<name>Alatorre et al.</name>
<date>20121100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>710 33</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2012/0317338</doc-number>
<kind>A1</kind>
<name>Yi et al.</name>
<date>20121200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711103</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2013/0013850</doc-number>
<kind>A1</kind>
<name>Baderdinni</name>
<date>20130100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>711103</main-classification></classification-national>
</us-citation>
</us-references-cited>
<number-of-claims>16</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>711117</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711126</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711165</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711E12016</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>711E12019</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>1</number-of-drawing-sheets>
<number-of-figures>1</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130024650</doc-number>
<kind>A1</kind>
<date>20130124</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Ambat</last-name>
<first-name>Gopakumar</first-name>
<address>
<city>Bangalore</city>
<country>IN</country>
</address>
</addressbook>
<residence>
<country>IN</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Hawargi</last-name>
<first-name>Vishwanath Nagalingappa</first-name>
<address>
<city>Bangalore</city>
<country>IN</country>
</address>
</addressbook>
<residence>
<country>IN</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sharma</last-name>
<first-name>Yask</first-name>
<address>
<city>Bangalore</city>
<country>IN</country>
</address>
</addressbook>
<residence>
<country>IN</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Ambat</last-name>
<first-name>Gopakumar</first-name>
<address>
<city>Bangalore</city>
<country>IN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Hawargi</last-name>
<first-name>Vishwanath Nagalingappa</first-name>
<address>
<city>Bangalore</city>
<country>IN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Sharma</last-name>
<first-name>Yask</first-name>
<address>
<city>Bangalore</city>
<country>IN</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Suiter Swantz pc llo</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>LSI Corporation</orgname>
<role>02</role>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Schnee</last-name>
<first-name>Hal</first-name>
<department>2182</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for dynamic storage tiering may include, but is not limited to: receiving an input/output (I/O) request from a host device; determining whether the I/O request results in a cache hit; and relocating data associated with the I/O request between a higher-performance storage device and lower-performance storage device according to the determination whether the data associated with the I/O request is stored in a cache.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="156.80mm" wi="185.76mm" file="US08627035-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="188.30mm" wi="161.80mm" orientation="landscape" file="US08627035-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND</heading>
<p id="p-0002" num="0001">Data tiering is a hierarchical storage management technique wherein data is moved between high-cost/high-performance storage to low-cost/low performance storage based on the data usage pattern. Dynamic Storage Tiering (DST) is a concept of grouping storage devices into tiers based on their performance characteristics and relocating data dynamically across the devices to leverage their specific capabilities. A DST system performs this relocation while data remains online and accessible.</p>
<p id="p-0003" num="0002">For performance management, data that has a high activity or load level may be relocated to high performing storage tiers. Alternately, data with a low activity level may be relocated to lower performing storage tiers in order to provide increased capacity in high-performance storage tiers. Such a data movement may be achieved by specialized software that is application/file system aware and resides on host systems (e.g. IBM's Tivoli Storage Manager, Veritas Enterprise Vault etc.) and data movement here is usually at a file level.</p>
<heading id="h-0002" level="1">SUMMARY</heading>
<p id="p-0004" num="0003">The present disclosure describes systems and methods for dynamic storage tiering.</p>
<p id="p-0005" num="0004">A method for dynamic storage tiering may include, but is not limited to: receiving an input/output (I/O) request from a host device; determining whether the I/O request results in a cache hit; and relocating data associated with the I/O request between a higher-performance storage device and lower-performance storage device according to the determination whether the data associated with the I/O request is stored in a cache.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0006" num="0005">The numerous advantages of the disclosure may be better understood by those skilled in the art by reference to the accompanying figures in which:</p>
<p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. 1</figref> shows system for dynamic storage tiering.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0008" num="0007">The present invention is directed to an automated storage array-based mechanism wherein a quantum of cache hits for logical block addresses (LBAs) on a virtual volume is used to deduce data access activity.</p>
<p id="p-0009" num="0008">This present invention proposes systems and methods for tracking input/output (I/O) cache hits for logical volumes and using those cache hits as an indicator of the data access activity associated with those blocks and moving data between high performance/cost storage to low performance/cost storage in response to that access activity. For example, logical block addresses (LBAs) within a virtual volume that are being frequently accessed within a volume may be treated as potential candidates for relocation to storage having alternate performance characteristics). Cache hits may be ranked based on how frequently the hits occur and how many hits were recorded and these metrics may be provided to the intelligent system within controller firmware that, in turn, may carry out data tiering operations.</p>
<p id="p-0010" num="0009">In the following detailed description, reference may be made to the accompanying drawings, which form a part hereof. In the drawings, similar symbols typically identify similar components, unless context dictates otherwise. The illustrative embodiments described in the detailed description, drawings, and claims may be not meant to be limiting. Other embodiments may be utilized, and other changes may be made, without departing from the spirit or scope of the subject matter presented here.</p>
<p id="p-0011" num="0010">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, an illustrative representation of a mass storage system <b>100</b> comprising a host <b>101</b> computing device and a storage array <b>102</b> including a storage array controller <b>103</b>, is shown. The storage array controller <b>103</b> may include volume management circuitry/software whereby the storage array controller <b>103</b> can process read/write requests of the host <b>101</b> accessing various logical volumes configured on the storage array <b>102</b>. The storage array <b>102</b> may include n physical drives <b>104</b>. The physical drives <b>104</b> may be grouped into storage pools <b>105</b> (e.g. storage pool <b>105</b>A, storage pool <b>105</b>B, etc.) according to their relative performance characteristics. A storage pool <b>105</b> may be defined as one or more physical drives <b>104</b> (or one or more logical partitions of the one or more physical drives <b>104</b>) which have similar performance characteristics. For example, storage pool <b>105</b>A (i.e. drive <b>1</b>) may include high-performance Solid State Drives (SSDs) whereas storage pool <b>105</b>B (i.e. drive <b>2</b> and drive <b>3</b>) may include lower performance devices such as Serial ATA (SATA) Hard Disk Drives (HDDs). Factors that may distinguish higher-performance storage pools from lower-performance storage pools may include numbers of I/O operations processed per unit time, number of bytes read or written per unit time, and/or average response time for an I/O request.</p>
<p id="p-0012" num="0011">As described above, in order to enhance overall system performance, it may be desirable to allocate data having a high activity level (e.g. high numbers of I/O requests are addressed to the data) to high performance storage pools <b>105</b> (hereinafter referred to as &#x201c;promotion&#x201d;) while also allocating data with a low activity levels to low performance storage pools <b>105</b> (hereinafter referred to as &#x201c;demotion.&#x201d;)</p>
<p id="p-0013" num="0012">To affect an efficient dynamic storage tiering solution, the data portions moved between storage pools may be defined by a range of LBAs. Such block-based tiering may overcome problems with file-based tiering solutions. Specifically, if only a portion of a file is accessed regularly, a file system based tiering solution may fail to relegate the un-accessed portion of the file to a lower performing storage pool <b>105</b>. The use of block-based tiering may allow for relegation of such un-accessed parts of the file despite the regular access remaining portions of the file.</p>
<p id="p-0014" num="0013">An LBA range characterized by a significant activity load compared to the rest of the LBAs of a virtual volume may be referred to as a hot-spot. Alternately, data that is accessed infrequently may be referred to as a cold-spot and may be moved to a lower-performance storage pool utilizing the same systems and methods as described herein with respect to hot-spot movement.</p>
<p id="p-0015" num="0014">As described below, a hot-spot may be identified by the storage array controller <b>103</b> by monitoring the presence or absence of data associated with an I/O request received from the host <b>101</b> within an array cache <b>106</b>.</p>
<p id="p-0016" num="0015">For example, during operation of the mass storage system <b>100</b>, the storage array controller <b>103</b> may receive one or more I/O requests <b>107</b> from a host <b>101</b>. Upon receipt of an I/O request <b>107</b> from a host <b>101</b>, the storage array controller <b>103</b> may determine whether data associated with the I/O request <b>107</b> is presently maintained in the array cache <b>106</b>. The storage array controller <b>103</b> may include an LBA translator module <b>108</b> configured to allocate data across the various drives <b>104</b> of the storage array <b>102</b>. Upon receipt of a write request <b>107</b>, the LBA translator module <b>108</b> may store the data associated with the write request <b>107</b> across the LBAs of various drives <b>104</b> according to a designated allocation procedure and record the respective mapping of that data in an LBA mapping database <b>109</b>. Upon, receipt of a read request <b>107</b>, the LBA translator module <b>108</b> may determine the LBAs of the various drives <b>104</b> containing data associated with the read request <b>107</b> from the LBA mapping database <b>109</b> and return that data to the host <b>101</b>.</p>
<p id="p-0017" num="0016">The storage array controller <b>103</b> may further include a cache management module <b>110</b> configured for directing the storage of recently accessed data to the array cache <b>106</b> for rapid retrieval by the host <b>101</b>. For example, assuming that an I/O request <b>107</b> was determined by the cache management module <b>110</b> to be the first read request for a given LBA range, the I/O request <b>107</b> would not result in a cache hit and the data associated with the LBA range is read out from disk and copied to the array cache <b>106</b>. Subsequently, when another read request <b>107</b> to the same LBA range is received, a cache hit may be recognized by the cache management module <b>110</b> and I/O request <b>107</b> is redirected to the array cache <b>106</b> where the data associated with the I/O request <b>107</b> is retrieved.</p>
<p id="p-0018" num="0017">As described above, such cache hits may serve as a metric for identifying the frequency of data block access and reallocating data blocks accordingly across storage devices having various performance characteristics. Towards this end, a mechanism to track blocks that are frequently requested is proposed. Referring again to <figref idref="DRAWINGS">FIG. 1</figref>, the cache management module <b>110</b> may further include a cache hit tracking database <b>111</b>. The cache hit tracking database <b>111</b> may store data regarding the number of cache hits associated with various LBAs of the drives <b>104</b>. The cache hit tracking database <b>111</b> may maintain various data structures for tracking cache hits. For example, such a clusterHitStruct data structure for a given clusterID for an LBA range (i.e. a &#x201c;cluster&#x201d;) may be as follows:</p>
<p id="p-0019" num="0018">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="63pt" align="left"/>
<colspec colname="1" colwidth="154pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>struct{</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="77pt" align="left"/>
<colspec colname="1" colwidth="140pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>UINT64_hitCount;</entry>
</row>
<row>
<entry/>
<entry>time_t_lastHitTime;</entry>
</row>
<row>
<entry/>
<entry>UINT64_hitFrequency;</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="63pt" align="left"/>
<colspec colname="1" colwidth="154pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>} clusterHitStruct;</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
<br/>
where hitCount is a count of I/O requests <b>107</b> associated with a given LBA range, hitFrequency is a frequency of hits to that LBA range and t_lastHitTime is a system clock time associated with a most recent cache hit associated with the LBA range. The size of the LBA range associated with a clusterHitStruct data structure may be user-configurable to allow for adaptable granularity of data migration management.
</p>
<p id="p-0020" num="0019">Upon receipt of an I/O request <b>107</b> associated with a given LBA range, the cache management module <b>110</b> may determine if a cache hit results and, if so, may increment the hitCount value of the clusterHitStruct data structure and modify the hitFrequency value accordingly.</p>
<p id="p-0021" num="0020">The cache management module <b>110</b> may monitor the state of the cache hit tracking database <b>111</b>. Upon exceeding a cache hit threshold (e.g. exceeding a specified number of cache hits, exceeding a specified cache hit frequency, etc.), for a particular LBA range in a given storage pool <b>105</b> those LBAs may be designated as a hot-spot and subject to relocation to a storage pool having enhanced performance characteristics. Alternately, LBAs ranges having cache hit values below a threshold (e.g. less than a specified number of cache hits, failing to meet a specified cache hit frequency, etc.), for a particular LBA range in a given storage pool <b>105</b> those LBAs may be designated as a cold-spot and subject to relocation to a storage pool having lesser performance characteristics.</p>
<p id="p-0022" num="0021">LBA ranges that are infrequently accessed as reflected by a) clusterIDs having very low hitCount and/or b) clusterIDs having no entry in the map. The former case implies that read I/Os on the LBA range within a specific cluster is very low and the latter implies that no read I/Os have ever occurred on the cluster. In either case, when low ranked clusters are detected by the cache management module <b>110</b>, the cache management module <b>110</b> may move the data associated with those clusters from high-performance storage pool <b>105</b>A a low performance storage pool <b>105</b>B. Conversely, if a high hit count is detected by the cache management module <b>110</b> for a cluster residing on low performance disks, the cache management module <b>110</b> may move the data of that cluster to high performance disks. Varying levels of data tiering (e.g. high/medium/low) may be employed providing an additional input factor to consider before moving low ranking clusters.</p>
<p id="p-0023" num="0022">The cache management module <b>110</b> may apply additional sampling logic on deciding which clusters to move between high performance &#x26; low performance disks. A weighting may be applied to clusters depending on when the last hit on the cluster was detected and/or the hit count on adjacent clusters. Such a weighting scheme may come into play when there are many clusters to be moved around but there aren't enough free blocks within the desired storage pool <b>105</b>. For example, a weighting W may be a function:</p>
<p id="p-0024" num="0023">
<tables id="TABLE-US-00002" num="00002">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="21pt" align="left"/>
<colspec colname="1" colwidth="196pt" align="left"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
<entry>W = f(hit_count, hit_time, hit_frequency,</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="182pt" align="left"/>
<tbody valign="top">
<row>
<entry/>
<entry>hit_frequency_and_count_of_adjacent_clusters)</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="1" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0025" num="0024">The data movement mechanism may be further enhanced to ensure that specific administrator-initiated activities result in data allocation from a particular performance storage tier. For example, when administrator initiates a snapshot on a volume the snapshot repository may stored on a lower performance disks. In this scenario, the above described cache hit-based data migration methodologies would be disregarded and the specific operation being performed on a volume may dictate the performance tier being used.</p>
<p id="p-0026" num="0025">Additionally, the caching algorithm itself may be enhanced to monitor for data blocks/clusters that have a huge cache hits and ensure that those blocks are never cleared from the cache.</p>
<p id="p-0027" num="0026">It is notable that the solution proposed here operates at the block level and may not require any host based software. As such, the solution may be agnostic to the host or the host based application that is accessing the data blocks. No software porting across platforms may be required. Additionally, all metadata related to the frequently used blocks may be stored on the storage array <b>102</b> itself in a persistent store and hence available across reboots and recoverable even in the case of a catastrophic failure.</p>
<p id="p-0028" num="0027">The solution proposed here being block based overcomes problems with typical file system-based tiering solutions. Specifically, if only a part of a file is accessed regularly, the file system-based tiering solutions may not mark the un-accessed part of the file for tiering. With the proposed solution, since the firmware is agnostic about the contents of the data blocks very fine tiering can be achieved with un-accessed parts of a file being moved to lower performance storage.</p>
<p id="p-0029" num="0028">Further, the solution proposed here may be entirely automated with a storage administrator only having to mark out the logical volumes for which the cache hits are to be tracked and the size of the LBA ranges to be monitored for potential relocation. Once enabled, the controller firmware may automatically trigger data movement at the backend for un-accessed (or lesser accessed) LBAs. Conversely, since the data relocation system proposed herein is based on cache hits, when a data block located on a lower performance storage starts having a higher cache hit, those data blocks may be moved back to higher performance storage.</p>
<p id="p-0030" num="0029">It is believed that the present invention and many of its attendant advantages will be understood by the foregoing description. It may be also believed that it will be apparent that various changes may be made in the form, construction and arrangement of the components thereof without departing from the scope and spirit of the invention or without sacrificing all of its material advantages. The form herein before described being merely an explanatory embodiment thereof. It may be the intention of the following claims to encompass and include such changes.</p>
<p id="p-0031" num="0030">The foregoing detailed description may include set forth various embodiments of the devices and/or processes via the use of block diagrams, flowcharts, and/or examples. Insofar as such block diagrams, flowcharts, and/or examples contain one or more functions and/or operations, it will be understood by those within the art that each function and/or operation within such block diagrams, flowcharts, or examples may be implemented, individually and/or collectively, by a wide range of hardware, software, firmware, or virtually any combination thereof. In one embodiment, several portions of the subject matter described herein may be implemented via Application Specific Integrated Circuits (ASICs), Field Programmable Gate Arrays (FPGAs), digital signal processors (DSPs), or other integrated formats. However, those skilled in the art will recognize that some aspects of the embodiments disclosed herein, in whole or in part, may be equivalently implemented in integrated circuits, as one or more computer programs running on one or more computers (e.g., as one or more programs running on one or more computer systems), as one or more programs running on one or more processors (e.g., as one or more programs running on one or more microprocessors), as firmware, or as virtually any combination thereof, and that designing the circuitry and/or writing the code for the software and or firmware would be well within the skill of one of skill in the art in light of this disclosure.</p>
<p id="p-0032" num="0031">In addition, those skilled in the art will appreciate that the mechanisms of the subject matter described herein may be capable of being distributed as a program product in a variety of forms, and that an illustrative embodiment of the subject matter described herein applies regardless of the particular type of signal bearing medium used to actually carry out the distribution. Examples of a signal bearing medium include, but may be not limited to, the following: a recordable type medium such as a floppy disk, a hard disk drive, a Compact Disc (CD), a Digital Video Disk (DVD), a digital tape, a computer memory, etc.; and a transmission type medium such as a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link (e.g., transmitter, receiver, transmission logic, reception logic, etc.), etc.).</p>
<p id="p-0033" num="0032">Those having skill in the art will recognize that the state of the art may include progressed to the point where there may be little distinction left between hardware, software, and/or firmware implementations of aspects of systems; the use of hardware, software, and/or firmware may be generally (but not always, in that in certain contexts the choice between hardware and software may become significant) a design choice representing cost vs. efficiency tradeoffs. Those having skill in the art will appreciate that there may be various vehicles by which processes and/or systems and/or other technologies described herein may be effected (e.g., hardware, software, and/or firmware), and that the preferred vehicle will vary with the context in which the processes and/or systems and/or other technologies may be deployed. For example, if an implementer determines that speed and accuracy may be paramount, the implementer may opt for a mainly hardware and/or firmware vehicle; alternatively, if flexibility may be paramount, the implementer may opt for a mainly software implementation; or, yet again alternatively, the implementer may opt for some combination of hardware, software, and/or firmware. Hence, there may be several possible vehicles by which the processes and/or devices and/or other technologies described herein may be effected, none of which may be inherently superior to the other in that any vehicle to be utilized may be a choice dependent upon the context in which the vehicle will be deployed and the specific concerns (e.g., speed, flexibility, or predictability) of the implementer, any of which may vary. Those skilled in the art will recognize that optical aspects of implementations will typically employ optically oriented hardware, software, and or firmware.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for dynamic storage tiering comprising:
<claim-text>receiving an input/output (I/O) request from a host device;</claim-text>
<claim-text>determining whether the I/O request results in a cache hit;</claim-text>
<claim-text>comparing at least one of a number of cache hits associated with data associated with the I/O request or a frequency of cache hits associated with data associated with the I/O request to a threshold range; and</claim-text>
<claim-text>relocating data associated with the I/O request between a higher-performance storage device and lower-performance storage device according to a comparison of at least one of a number of cache hits associated with data associated with the I/O request or a frequency of cache hits associated with data associated with the I/O request to a threshold range.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the data associated with the I/O request comprises logical block address (LBA)-level data.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>,
<claim-text>wherein the determining whether the I/O request results in a cache hit further comprises:
<claim-text>applying a weighting to a first LBA range according to cache hit data associated with the first LBA range; and</claim-text>
<claim-text>applying a weighting to a second LBA range according to cache hit data associated with the second LBA range, and</claim-text>
</claim-text>
<claim-text>wherein the relocating data associated with the I/O request between a higher-performance storage device and lower-performance storage device according to a comparison of at least one of a number of cache hits associated with data associated with the I/O request or a frequency of cache hits associated with data associated with the I/O request to a threshold range further comprises:
<claim-text>relocating either the first LBA range or the second LBA range according to relative weightings of the first LBA range and the second LBA range.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the first LBA range is adjacent to the second LBA range.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein a weighting is a function of at least one of: a cache hit count associated with the LBA range; a cache hit time; a cache hit frequency; or a cache hit count associated with a second LBA range adjacent to the LBA range.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>receiving a user input;</claim-text>
<claim-text>retaining data associated with the I/O request in a present storage location when at least one of a number of cache hits associated with data associated with the I/O request and a frequency of cache hits associated with data associated with the I/O is outside a user-defined threshold range.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the user input is indicative of a volume snapshot operation.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the present storage location is a lower performance storage location.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A system for dynamic storage tiering comprising:
<claim-text>means for receiving an input/output (I/O) request from a host device;</claim-text>
<claim-text>means for determining whether the I/O request results in a cache hit;</claim-text>
<claim-text>means for comparing at least one of a number of cache hits associated with data associated with the I/O request or a frequency of cache hits associated with data associated with the I/O request to a threshold range; and</claim-text>
<claim-text>means for relocating data associated with the I/O request between a higher-performance storage device and lower-performance storage device according to a comparison of at least one of a number of cache hits associated with data associated with the I/O request or a frequency of cache hits associated with data associated with the I/O request to a threshold range.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the data associated with the I/O request comprises logical block address (LBA)-level data.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>,
<claim-text>wherein the means for determining whether the I/O request results in a cache hit further comprises:
<claim-text>means for applying a weighting to a first LBA range according to cache hit data associated with the first LBA range; and</claim-text>
<claim-text>means for applying a weighting to a second LBA range according to cache hit data associated with the second LBA range, and</claim-text>
</claim-text>
<claim-text>wherein the means for relocating data associated with the I/O request between a higher-performance storage device and lower-performance storage device according to a comparison of at least one of a number of cache hits associated with data associated with the I/O request or a frequency of cache hits associated with data associated with the I/O request to a threshold range further comprises:</claim-text>
<claim-text>means for relocating either the first LBA range or the second LBA range according to relative weightings of the first LBA range and the second LBA range.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first LBA range is adjacent to the second LBA range.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein a weighting is a function of at least one of: a cache hit count associated with the LBA range; a cache hit time; a cache hit frequency; or a cache hit count associated with a second LBA range adjacent to the LBA range.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:
<claim-text>means for receiving a user input;</claim-text>
<claim-text>means for retaining data associated with the I/O request in a present storage location when at least one of a number of cache hits associated with data associated with the I/O request and a frequency of cache hits associated with data associated with the I/O is outside a user-defined threshold range.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the user input is indicative of a volume snapshot operation.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the present storage location is a lower performance storage location. </claim-text>
</claim>
</claims>
</us-patent-grant>
