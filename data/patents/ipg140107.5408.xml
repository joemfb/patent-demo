<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626508-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626508</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13203371</doc-number>
<date>20100210</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2009-044842</doc-number>
<date>20090226</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>7</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>17</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>15</main-group>
<subgroup>06</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>21</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>25</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>7</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>30</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704254</main-classification>
<further-classification>704231</further-classification>
<further-classification>704235</further-classification>
<further-classification>704249</further-classification>
<further-classification>704243</further-classification>
<further-classification>704241</further-classification>
<further-classification>704251</further-classification>
<further-classification>704270</further-classification>
<further-classification>704246</further-classification>
<further-classification>707770</further-classification>
<further-classification>707707</further-classification>
<further-classification>707749</further-classification>
<further-classification>707738</further-classification>
<further-classification>707706</further-classification>
<further-classification>707711</further-classification>
<further-classification>707781</further-classification>
<further-classification>707713</further-classification>
<further-classification>707769</further-classification>
</classification-national>
<invention-title id="d2e71">Speech search device and speech search method</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5577162</doc-number>
<kind>A</kind>
<name>Yamazaki</name>
<date>19961100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704232</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5867816</doc-number>
<kind>A</kind>
<name>Nussbaum</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704232</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>2007/0083371</doc-number>
<kind>A1</kind>
<name>Jeong et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704256</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>2007/0143110</doc-number>
<kind>A1</kind>
<name>Acero et al.</name>
<date>20070600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>2007/0208561</doc-number>
<kind>A1</kind>
<name>Choi et al.</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2007/0225975</doc-number>
<kind>A1</kind>
<name>Imoto</name>
<date>20070900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704233</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2008/0208597</doc-number>
<kind>A1</kind>
<name>Chino et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704277</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2008/0221893</doc-number>
<kind>A1</kind>
<name>Kaiser</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704257</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2008/0255841</doc-number>
<kind>A1</kind>
<name>Hanazawa et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2008/0294431</doc-number>
<kind>A1</kind>
<name>Miyamoto et al.</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704231</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2009/0099841</doc-number>
<kind>A1</kind>
<name>Chen</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704  9</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2009/0281807</doc-number>
<kind>A1</kind>
<name>Hirose et al.</name>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2010/0063819</doc-number>
<kind>A1</kind>
<name>Emori</name>
<date>20100300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704251</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2011/0307254</doc-number>
<kind>A1</kind>
<name>Hunt et al.</name>
<date>20111200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704235</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>CN</country>
<doc-number>1604185</doc-number>
<kind>A</kind>
<date>20050400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>JP</country>
<doc-number>5 35292</doc-number>
<date>19930200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>JP</country>
<doc-number>2005 257954</doc-number>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>JP</country>
<doc-number>2005257954</doc-number>
<kind>A</kind>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>WO</country>
<doc-number>2005 122002</doc-number>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00020">
<othercit>Satoru Takabayashi, Writing Assistance through Search Techniques, Feb. 9, 2001, Department of Information Processing, Graduate School of Information Science, Nara Institute of Science and Technology, Master's Thesis.</othercit>
</nplcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00021">
<othercit>Kuriki, G., et al., &#x201c;Open Vocabulary Spoken Document Retrieval using Phone Sequence Obtained from a General Speech Recognizer,&#x201d; The Institute of Electronics, Information and Communication Engineers, IEICE Technical Report, SP2008-53, pp. 61-66, (Jul. 2008) (with English Abstract).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00022">
<othercit>Yamasita, T., et al., &#x201c;Full Text Approximate String Search using Suffix Arrays,&#x201d; IPSJ SIG Technical Reports, pp. 23-30, (Sep. 11, 1997) (with English Abstract).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00023">
<othercit>Kanda, N., et al., &#x201c;Open-Vocabulary Keyword Detection from Super-Large Scale Speech Database,&#x201d; IEEE, MMSP 2008, pp. 939-944, (2008).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00024">
<othercit>Thambiratnam, K., et al., &#x201c;Dynamic Match Phone-Lattice Searches for Very Fast and Accurate Unrestricted Vocabulary Keyword Spotting,&#x201d; IEEE, ICASSP 2005, pp. I-465-I-468, (2005).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00025">
<othercit>International Search Report issued Apr. 20, 2010 in PCT/JP10/051937 filed Feb. 10, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00026">
<othercit>Combined Chinese Office Action and Search Report issued Jan. 15, 2013 in Chinese Patent Application No. 201080009141.X (with English Translation of Categories of Cited Documents).</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>10</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704  9</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704231-257</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704275</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704270</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>7042701</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707706</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707738</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707749</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707711</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707781</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707770</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707769</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707713</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707780</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707736</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>707748</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>18</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120036159</doc-number>
<kind>A1</kind>
<date>20120209</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Katsurada</last-name>
<first-name>Koichi</first-name>
<address>
<city>Toyohashi</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Nitta</last-name>
<first-name>Tsuneo</first-name>
<address>
<city>Toyohashi</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Teshima</last-name>
<first-name>Shigeki</first-name>
<address>
<city>Toyohashi</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Katsurada</last-name>
<first-name>Koichi</first-name>
<address>
<city>Toyohashi</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Nitta</last-name>
<first-name>Tsuneo</first-name>
<address>
<city>Toyohashi</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Teshima</last-name>
<first-name>Shigeki</first-name>
<address>
<city>Toyohashi</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Oblon, Spivak, McClelland, Maier &#x26; Neustadt, L.L.P.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>National University Corporation TOYOHASHI UNIVERSITY OF TECHNOLOGY</orgname>
<role>03</role>
<address>
<city>Toyohasi-shi</city>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Shah</last-name>
<first-name>Paras D</first-name>
<department>2659</department>
</primary-examiner>
<assistant-examiner>
<last-name>Thomas-Homescu</last-name>
<first-name>Anne</first-name>
</assistant-examiner>
</examiners>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/JP2010/051937</doc-number>
<kind>00</kind>
<date>20100210</date>
</document-id>
<us-371c124-date>
<date>20110922</date>
</us-371c124-date>
</pct-or-regional-filing-data>
<pct-or-regional-publishing-data>
<document-id>
<country>WO</country>
<doc-number>WO2010/098209</doc-number>
<kind>A </kind>
<date>20100902</date>
</document-id>
</pct-or-regional-publishing-data>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Provided are a speech search device, the search speed of which is very fast, the search performance of which is also excellent, and which performs fuzzy search, and a speech search method. Not only the fuzzy search is performed, but also the distance between phoneme discrimination features included in speech data is calculated to determine the similarity with respect to the speech using both a suffix array and dynamic programming, and an object to be searched for is narrowed by means of search keyword division based on a phoneme and search thresholds relative to a plurality of the divided search keywords, the object to be searched for is repeatedly searched for while increasing the search thresholds in order, and whether or not there is the keyword division is determined according to the length of the search keywords, thereby implementing speech search, the search speed of which is very fast and the search performance of which is also excellent.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="239.78mm" wi="197.10mm" file="US08626508-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="241.98mm" wi="198.12mm" file="US08626508-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="231.99mm" wi="162.64mm" orientation="landscape" file="US08626508-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="203.96mm" wi="143.93mm" orientation="landscape" file="US08626508-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="226.57mm" wi="144.44mm" orientation="landscape" file="US08626508-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="219.46mm" wi="154.60mm" orientation="landscape" file="US08626508-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="228.94mm" wi="189.74mm" file="US08626508-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="248.16mm" wi="182.03mm" file="US08626508-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="234.87mm" wi="176.11mm" file="US08626508-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="246.46mm" wi="161.12mm" file="US08626508-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="241.22mm" wi="168.99mm" file="US08626508-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="238.42mm" wi="167.30mm" file="US08626508-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="134.45mm" wi="169.42mm" file="US08626508-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">TECHNICAL FIELD</heading>
<p id="p-0002" num="0001">The present invention relates to a speech search device and a speech search method. More specifically, it relates to a device and a method for searching for speeches speedily and efficiently.</p>
<heading id="h-0002" level="1">BACKGROUND ART</heading>
<p id="p-0003" num="0002">With the prevalence of a broadband line and the advancement in information and communications technology, opportunities for utilizing contents of speeches and moving images on a web are increasing and the number of contents is rapidly increasing. Accordingly, to efficiently search for data of the speeches and the moving images on the web and to use them, speech search technologies are indispensable. In particular, a high-speed search system is desired.</p>
<p id="p-0004" num="0003">To meet such a need, conventional technologies described in Non-patent Literatures 1 and 2 propose a method for performing high-speed search by creating index data from a speech database and utilizing it.</p>
<p id="p-0005" num="0004">Further, the conventional technology described in Patent Literature 1 describes combining a suffix array and dynamic programming for the purpose of elimination of spelling inconsistency in document search and its speed-up.</p>
<heading id="h-0003" level="1">CITATION LIST</heading>
<p id="p-0006" num="0000">
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0005">Patent Literature</li>
    <li id="ul0001-0002" num="0006">Patent Literature 1: International Patent Publication No. 2005/122002</li>
</ul>
</p>
<heading id="h-0004" level="1">Non-Patent Literature</heading>
<p id="p-0007" num="0000">
<ul id="ul0002" list-style="none">
    <li id="ul0002-0001" num="0007">Non-patent Literature 1: N. Kanda, et al., &#x201c;Open-Vocabulary Keyword Detection from Super-Large Scale Speech Database,&#x201d; IEEE MMSP 2008, pp. 939-944, 2008.</li>
    <li id="ul0002-0002" num="0008">Non-patent Literature 2: K. Thambiratnam &#x26; S. Sridharan, &#x201c;Dynamic Match Phone-Lattice Searches For Very Fast And Accurate Unrestricted Vocabulary Keyword Spotting,&#x201d; ICASSP 2005, vol. 1, pp. 465-468, 2005.</li>
</ul>
</p>
<heading id="h-0005" level="1">SUMMARY OF INVENTION</heading>
<heading id="h-0006" level="1">Technical Problem</heading>
<p id="p-0008" num="0009">The conventional technology described in the aforementioned Patent Literature 1 would target document search but not fuzzy search for a phoneme string by means of speech recognition in accordance with the present invention. In particular, it is self-evident that time required in calculation increases remarkably in the case of combining a simple suffix array and dynamic programming.</p>
<p id="p-0009" num="0010">On the other hand, in the aforementioned conventional technologies (Non-patent Literatures 1 and 2), if the speech database is of a large scale, the conventional speech search speed-up methods must create index data commensurate with the scale of the database. This leads to a need for provision of a high-speed secondary storage, which is undesirable in terms of costs.</p>
<p id="p-0010" num="0011">Further, as compared to the main storage, the secondary storage needs a longer time in access and so has a disadvantage in that its search is slowed down.</p>
<p id="p-0011" num="0012">Moreover, the index data is typically created from a word or a sub-word, which approach is based on the presumption that a search keyword (or sub-keyword) and the word or sub-word should agree with each other completely, so that there is a possibility that the existing speech recognition method subject to mis-recognition could not provide sufficient search performances.</p>
<p id="p-0012" num="0013">In view of the above problems, it is an object of the present invention to provide a speech search device and a speech search method that performs fuzzy search without requiring a secondary storage and at a high search speed and low search costs and has good search performances.</p>
<heading id="h-0007" level="1">Solution to Problem</heading>
<p id="p-0013" num="0014">In speech search in accordance with the present invention, specifically, the speech search device and the speech search method for performing fuzzy search has the following configurations.</p>
<p id="p-0014" num="0015">The invention of claim <b>1</b> provides a speech search device for receiving a speech as an input and searching for speech data obtained by sampling the input speech, the speech search device comprising a database-use speech recognizer that recognizes the speech recorded in the speech database, a speech-use phoneme string generation unit that generates a phoneme string from a word string recognized by the database-use speech recognizer, a suffix array generation unit that generates a suffix array from the phoneme string generated by the speech-use phoneme generation unit, an input device that inputs a search keyword, an input phoneme generation unit that generates the phoneme string from the search keyword input by the input device, a speech search unit that searches the suffix array for the search keyword by dynamic programming, and an output device that outputs a result of the search by the speech search unit, wherein the speech search unit includes means for setting first threshold to be used in search and means for searching for a search object by dynamic programming by using the first threshold.</p>
<p id="p-0015" num="0016">This speech search device, which receives a speech as an input and searches for speech data obtained by sampling the input speech, performs fuzzy search by using a suffix array and dynamic programming. It performs matching with a search keyword in units of a phoneme and so enables search even if the search keyword does not completely agree with a word or sub-word registered in an index.</p>
<p id="p-0016" num="0017">The invention of claim <b>2</b> provides the speech search device according to claim <b>1</b>, wherein the speech search unit further comprises means for, if the search keyword is at least a predetermined length, dividing the search keyword based on the phoneme, and means for determining a second threshold to be used in search for the keyword divided by the search keyword division means, from the first threshold, and wherein the search object search means searches for the object by dynamic programming by using the second threshold.</p>
<p id="p-0017" num="0018">In addition to the invention of claim <b>1</b>, the speech search device having the above configuration divides a search keyword, changes the first threshold to be used in search to permit matching at least two positions, determines the number of divided phonemes in accordance with the length of the keyword, and determines whether to divide or not, thereby performing high-speed search. This alteration of the first threshold to be used in search so that matching may be performed at least two positions is carried out in accordance with the following first equation (that is, Math. 1)</p>
<p id="p-0018" num="0019">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msup>
          <mi>t</mi>
          <mi>&#x2032;</mi>
        </msup>
        <mo>=</mo>
        <mrow>
          <mfrac>
            <mi>p</mi>
            <mrow>
              <mi>p</mi>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </mfrac>
          <mo>&#x2062;</mo>
          <mi>t</mi>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mrow>
          <mi>Math</mi>
          <mo>.</mo>
          <mstyle>
            <mspace width="0.8em" height="0.8ex"/>
          </mstyle>
          <mo>&#x2062;</mo>
          <mn>1</mn>
        </mrow>
        <mo>]</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0019" num="0020">In this equation, p is the number of divisions, t is an original first threshold obtained by the means for determining a search threshold for the plurality of divided search keywords, and t&#x2032; is a second threshold obtained after alteration by the threshold alteration means.</p>
<p id="p-0020" num="0021">The invention of claim <b>3</b> provides the speech search device according to claim <b>1</b> or <b>2</b>, wherein the speech search unit comprises threshold adjustment means for repeatedly performing search as increasing the first threshold consecutively and consecutively present a result of the search.</p>
<p id="p-0021" num="0022">The above configuration is provided with a threshold adjustment function to repeatedly perform search as consecutively increasing the first threshold to be used in search by iterative lengthening search (a kind of iterative deepening search) and consecutively present a result of the search. It performs an improvement in sensory search speed by consecutively presenting new search results as updating the threshold while a user of the device is confirming initially presented search results.</p>
<p id="p-0022" num="0023">The invention of claim <b>4</b> provides the speech search device according to anyone of claims <b>1</b> to <b>3</b>, in which the speech search unit further includes means for determining whether or not keyword division is necessary based on the length of the search keyword and keyword division means for determining the number of the phonemes after the keyword is divided.</p>
<p id="p-0023" num="0024">The device having the above configuration can determine whether or not keyword division is necessary based on the length of the search keyword and determine the number of the phonemes after the keyword is divided.</p>
<p id="p-0024" num="0025">The invention of claim <b>5</b> provides the speech search device according to any one of claims <b>1</b> to <b>4</b>, wherein the means for searching for the search object by dynamic programming further comprises means for calculating similarity between the phonemes by using an inter-phoneme distance based on phoneme discrimination features in dynamic programming.</p>
<p id="p-0025" num="0026">The above inter-phoneme distance may come in, for example, a Hamming distance of a difference between the phoneme discrimination features. Accordingly, in the above configuration, the inter-phoneme similarity would be calculated using the Hamming distance.</p>
<p id="p-0026" num="0027">The invention of claim <b>6</b> provides a speech search method for receiving a speech as an input and searching for speech data obtained by sampling the input speech, the speech search method comprising the steps of creating a suffix array by converting the speech data into a phoneme string, receiving a search keyword and converting it into the phoneme string, setting a first threshold to be used in search, searching for a search object by dynamic programming by using the first, and outputting a result of the search performed in the searching step.</p>
<p id="p-0027" num="0028">The speech search method having the above configuration performs fuzzy search by using a suffix array and dynamic programming (hereinafter referred to as DP matching in some cases). It performs matching with a search keyword in units of a phoneme and so enables search even if the search keyword does not completely agree with a word or sub-word registered in an index.</p>
<p id="p-0028" num="0029">The invention of claim <b>7</b> provides the speech search method according to claim <b>6</b>, further comprising the steps of, if the search keyword is at least a predetermined length, dividing this search keyword based on the phoneme, and determining a second threshold to be used in search for the keyword divided in the search keyword dividing step, from the first threshold, wherein the search object searching step searches for the object by dynamic programming by using the second threshold.</p>
<p id="p-0029" num="0030">The speech search method having the above configuration divides a search keyword, changes the first threshold to be used in search to permit matching at least two positions, determines the number of divided phonemes in accordance with the length of the keyword, and determines whether to divide or not, to prevent exponential explosion of processing time, thereby performing high-speed search. It is to be noted that the second threshold to be used in search for the divided keyword can be determined on the basis of Equation 1 given in Math. 1.</p>
<p id="p-0030" num="0031">The invention of claim <b>8</b> provides the speech search method according to claim <b>6</b> or <b>7</b>, further comprising a threshold adjusting step of repeatedly performing search as increasing the first threshold consecutively.</p>
<p id="p-0031" num="0032">The speech search method having the above configuration is provided with a threshold adjustment function to repeatedly perform search as consecutively increasing the first threshold to be used in search by iterative lengthening search (a kind of iterative deepening search) and consecutively present a result of the search. Initial search having the small first threshold approximates binary search owing to features of the suffix array and so is very speedy in performance.</p>
<p id="p-0032" num="0033">The invention of claim <b>9</b> provides the speech search method according to any one of claims <b>6</b> to <b>8</b>, further comprising a step of determining whether or not keyword division is necessary based on the length of the search keyword and a keyword division step of determining the number of the phonemes after the keyword is divided.</p>
<p id="p-0033" num="0034">The speech search method having the above configuration is processed so as to enable determining whether or not keyword division is necessary based on the length of the search keyword and determining the number of the phonemes after the keyword is divided.</p>
<p id="p-0034" num="0035">The invention of claim <b>10</b> provides the speech search method of any one according to claims <b>6</b> to <b>9</b>, wherein the step of searching for the search object includes a step of calculating similarity between the phonemes by using an inter-phoneme distance based on phoneme discrimination features in dynamic programming.</p>
<p id="p-0035" num="0036">The speech search method having the above configuration enables such processing as calculating similarity between the phonemes by using an inter-phoneme distance (for example, Hamming distance of a difference between the phoneme discrimination features) based on phoneme discrimination features in the dynamic programming.</p>
<heading id="h-0008" level="1">Advantageous Effects of Invention</heading>
<p id="p-0036" num="0037">The present invention does not need a large data region and so can eliminate the need for a high-speed secondary storage to reduce costs necessary for preparation of secondary storage. That is, it is possible to provide a speech search device and a speech search method that has a high search speed and low search costs as well as good search performances.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading>
<p id="p-0037" num="0038"><figref idref="DRAWINGS">FIG. 1</figref> is a flowchart of speech search according to the present invention;</p>
<p id="p-0038" num="0039"><figref idref="DRAWINGS">FIG. 2</figref> is a block diagram of an internal configuration of the speech search according to the present invention;</p>
<p id="p-0039" num="0040"><figref idref="DRAWINGS">FIG. 3</figref> is an explanatory view of creating a suffix array from a speech database according to the present invention;</p>
<p id="p-0040" num="0041"><figref idref="DRAWINGS">FIG. 4</figref> is an explanatory view of fuzzy search by use of dynamic programming (DP matching) in the suffix array according to the present invention;</p>
<p id="p-0041" num="0042"><figref idref="DRAWINGS">FIG. 5</figref> is an explanatory view of keyword division and speech search according to the present invention;</p>
<p id="p-0042" num="0043"><figref idref="DRAWINGS">FIG. 6</figref> is a graph of a time that elapses until a first search result is output in a case where a first threshold is minimized by using a six-phoneme search keyword as an object according to an example of the present invention. A horizontal axis gives the first threshold and a vertical axis gives a recall ratio, a precision ratio, and a processing time in search;</p>
<p id="p-0043" num="0044"><figref idref="DRAWINGS">FIG. 7</figref> is a graph of the time that elapses until the first search result is output in a case where the first threshold is minimized by using a 12-phoneme search keyword as an object according to an example of the present invention. A horizontal axis gives the first threshold and a vertical axis gives the recall ratio, the precision ratio, and the processing time in search;</p>
<p id="p-0044" num="0045"><figref idref="DRAWINGS">FIG. 8</figref> is a graph of the time that elapses until the first search result is output in a case where the first threshold is minimized by using a 18-phoneme search keyword as an object according to an example of the present invention. A horizontal axis gives the first threshold and a vertical axis gives the recall ratio, the precision ratio, and the processing time in search;</p>
<p id="p-0045" num="0046"><figref idref="DRAWINGS">FIG. 9</figref> is a graph of the time that elapses until the first search result is output in a case where the first threshold is minimized by using a 24-phoneme search keyword as an object according to an example of the present invention. A horizontal axis gives the first threshold and a vertical axis gives the recall ratio, the precision ratio, and the processing time in search;</p>
<p id="p-0046" num="0047"><figref idref="DRAWINGS">FIG. 10</figref> is a graph of the time that elapses until half of correct keywords are detected by using the six-phoneme search keyword as an object according to an example of the present invention. A horizontal axis gives the first threshold and a vertical axis gives the recall ratio, the precision ratio, and the processing time in search;</p>
<p id="p-0047" num="0048"><figref idref="DRAWINGS">FIG. 11</figref> is a graph of the time that elapses until half of correct keywords are detected by using the 12-phoneme search keyword as an object according to an example of the present invention. A horizontal axis gives the first threshold and a vertical axis gives the recall ratio, the precision ratio, and the processing time in search;</p>
<p id="p-0048" num="0049"><figref idref="DRAWINGS">FIG. 12</figref> is a graph of the time that elapses until a first threshold is set to an initial value of 0.0 to search for a search keyword including 6 to 24 phonemes and a group of search results are presented to a user according an example of the present invention. A horizontal axis gives a speech-equivalent time (unit: hour) for an artificial speech database (Mainichi Daily News corpus) and a vertical axis gives a search processing time (unit: millisecond);</p>
<p id="p-0049" num="0050"><figref idref="DRAWINGS">FIG. 13</figref> is a graph of the time that elapses until the first threshold is updated from a state in <figref idref="DRAWINGS">FIG. 12</figref> to 0.2 to then search for the search keyword including 6 to 24 phonemes and a group of search results are presented to the user according to an example of the present invention. A horizontal axis gives a speech-equivalent time (unit: hour) for the artificial speech database (Mainichi Daily News corpus) and a vertical axis gives a search processing time (unit: millisecond);</p>
<p id="p-0050" num="0051"><figref idref="DRAWINGS">FIG. 14</figref> is a graph of the time that elapses until the first threshold is updated from a state in <figref idref="DRAWINGS">FIG. 13</figref> further to 0.4 to then search for the search keyword including 6 to 24 phonemes and a group of search results are presented to the user according to an example of the present invention. A horizontal axis gives a speech-equivalent time (unit: hour) for the artificial speech database (Mainichi Daily News corpus) and a vertical axis gives a search processing time (unit: millisecond);</p>
<p id="p-0051" num="0052"><figref idref="DRAWINGS">FIG. 15</figref> is a graph of the time that elapses until the first threshold is set to an initial value of 0.0 to search for the search keyword including 6 to 24 phonemes and a group of search results are presented to the user in an implementation environment different from <figref idref="DRAWINGS">FIG. 12</figref> according to an example of the present invention. A horizontal axis gives a speech-equivalent time (unit:hour) for the artificial speech database (Mainichi Daily News corpus) and a vertical axis gives a search processing time (unit:millisecond);</p>
<p id="p-0052" num="0053"><figref idref="DRAWINGS">FIG. 16</figref> is a graph of the time that elapses until the first threshold is updated from a state in <figref idref="DRAWINGS">FIG. 15</figref> to 0.2 to then search for the search keyword including 6 to 24 phonemes and a group of search results are presented to the user according to an example of the present invention. A horizontal axis gives a speech-equivalent time (unit:hour) for the artificial speech database (Mainichi Daily News corpus) and a vertical axis gives a search processing time (unit:millisecond);</p>
<p id="p-0053" num="0054"><figref idref="DRAWINGS">FIG. 17</figref> is a graph of the time that elapses until the first threshold is updated from a state in <figref idref="DRAWINGS">FIG. 16</figref> further to 0.4 to then search for the search keyword including 6 to 24 phonemes and a group of search results are presented to the user according to an example of the present invention. A horizontal axis gives a speech-equivalent time (unit:hour) for the artificial speech database (Mainichi Daily News corpus) and a vertical axis gives a search processing time (unit: millisecond); and</p>
<p id="p-0054" num="0055"><figref idref="DRAWINGS">FIG. 18</figref> is a graph of the time that elapses until the first threshold is updated from a state in <figref idref="DRAWINGS">FIG. 17</figref> further to 1.0 to then search for the search keyword including 6 to 24 phonemes and a group of search results are presented to the user according to an example of the present invention. A horizontal axis gives a speech-equivalent time (unit: hour) for the artificial speech database (Mainichi Daily News corpus) and a vertical axis gives a search processing time (unit:millisecond).</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading>
<p id="p-0055" num="0056">The following will describe a speech search device and a speech search method in an embodiment of the present invention with reference to the drawings. It is to be noted that these drawings are used to explain technological features that can be employed in the present disclosure and constitutions of an apparatus and flowcharts of a variety of processing are just illustrative and not intended to be restrictive unless otherwise specified.</p>
<p id="p-0056" num="0057">A description will be given of an embodiment of the speech search method according to the present invention with reference to a flowchart in <figref idref="DRAWINGS">FIG. 1</figref>. The present embodiment receives a speech as an input upon start and performs fuzzy search on speech data obtained by sampling this input speech (by using 16 sampling bits and a sampling frequency of 44.1 kHz, for example) by using both a suffix array and DP matching.</p>
<p id="p-0057" num="0058">First, speech data recorded in a speech database is converted into a phoneme string (a), to create a suffix array from this phoneme string (a) (S<b>11</b>). Next, a search keyword is received and converted into a phoneme string (S<b>12</b>). Simultaneously with this processing, a first threshold (written as threshold <b>1</b> in <figref idref="DRAWINGS">FIG. 1</figref>) to be used in search is set (S<b>12</b>). In the processing, if the search keyword converted into the phoneme string has at least a predetermined length (for example, at least 9 phonemes), it is to be divided and, otherwise, not to be divided, based on which presumption a determination is made as to whether to divide it or not (S<b>13</b>). If it is determined that it should be divided, this search keyword is divided into a predetermined number of phonemes (S<b>14</b>). In this case, the number of the post-division phonemes can be determined beforehand. For example, by setting the number of the post-division phonemes to 3, if the search keyword includes 9 phonemes, it can be divided into three pieces every 3 phonemes. Further, if the search keyword includes 10 to 12 phonemes, it can be divided into four pieces.</p>
<p id="p-0058" num="0059">The divided keywords are determined as to whether there is similarity among them by calculating a distance between phoneme discrimination features included in the speech data.</p>
<p id="p-0059" num="0060">That is, a second threshold (written as threshold <b>2</b> in <figref idref="DRAWINGS">FIG. 1</figref>) is determined from the first threshold by using the equation given in Math. 1 and DP matching is performed on the keywords divided using this second threshold in the suffix array (S<b>15</b>). A result of this processing is saved as a first stage candidate (b) temporarily (S<b>15</b>), so that a final candidate (c) is determined on the basis of a positional relationship of this first stage candidate (b) (S<b>16</b>). Further, DP matching is performed on the result of this final candidate (c) in a suffix array (a) by using the first threshold, a result of which is output (presented to the user) (S<b>16</b>). This ends primary search.</p>
<p id="p-0060" num="0061">In the present embodiment, after the primary search ends, the first threshold is updated to a little larger value (for example, by adding 0.2), to repeat the searching step again in processing (S<b>17</b>, S<b>18</b>). Since the first threshold is updated to a little larger value, the second threshold calculated on the basis of the first threshold also becomes a little larger value. Accordingly, search is performed for similar words (words with similar phoneme strings) having a certain degree of distance between the phoneme discrimination features. It is to be noted that the repetition of the searching step can be processed to end if the first threshold reaches a predetermined value or the total number of the search results reaches a predetermined value (S<b>18</b>). For example, it can be processed to end if the first threshold reaches 1.4 or the total number of the search results reaches 100.</p>
<p id="p-0061" num="0062">Next, if the search keyword is short (for example, the number of the phonemes is eight or less), DP matching is performed in the suffix array (a) by using the first threshold without dividing the search keyword (S<b>19</b>). In this case, only the first threshold is used. Then, a result obtained in such a manner is output (presented to the user) as it is (S<b>19</b>). Since the search keyword is not divided, it is unnecessary to reference the positional relationship of the matching result.</p>
<p id="p-0062" num="0063">It is to be noted that even in a case where the search keyword is not divided, the first threshold is updated to a little larger value (for example, by adding 0.2) (S<b>20</b>), to repeat the searching step again (S<b>21</b>). This is done so in order to search for similar words (words with similar phoneme strings) having a longer distance between the phoneme discrimination features. It is to be noted that the repetition of the searching step can be processed to end if the updated threshold reaches a predetermined value or the number of the search results reaches a predetermined number.</p>
<p id="p-0063" num="0064">By such a search method, initial search by use of a small first threshold approximates binary search in search conditions, so that high-speed search can be performed for phoneme strings very close to the search keyword. Further, by gradually increasing the first threshold, a kind of iterative deepening search can be performed. Moreover, by consecutively outputting (presenting to the user) the first threshold before updating it, the phoneme strings can be output sequentially in order in which they are closer to the search keyword. Although the present embodiment exemplified a case where a boundary with which to determine whether the search keyword is long or short in keyword division would be set to at least 9 phonemes, the boundary with which to determine whether the search keyword is long or short can be set to 18 if the number of the post-division phonemes is set to 6. If the number of the post-division phonemes is small, the number of the first stage candidates (b) becomes enormous, so that the processing rate may possibly slow down. To solve this problem, the number of the post-division phonemes can be adjusted, to speed up the search further.</p>
<p id="p-0064" num="0065">It is to be noted that the above search method can be constituted so that the processing may end without updating the first threshold. In this case, although phoneme strings obtained by search are restricted to those similar to a search keyword, the first threshold can be set a little larger so that more phoneme strings may be searched for in one searching step.</p>
<p id="p-0065" num="0066">Further, in keyword division, although the step (S<b>13</b>) is provided to determine whether or not keyword division is necessary, such a step can be omitted in processing so that the keyword would be divided into a predetermined number of phonemes or not be divided. If such a processing as to divide it into the predetermined number of phonemes is selected, it must be divided into at least 3 phonemes in order to calculate the second threshold from the first threshold by the equation given in Math. 1, so that it is necessary to give a step of determining whether the number of divided keyword pieces after the keyword is divided into the predetermined number of phonemes is less than 3 or not less than 3.</p>
<p id="p-0066" num="0067">In contrast, the embodiment of the speech search device according to the present invention is configured as shown in the internal constitution block diagram in <figref idref="DRAWINGS">FIG. 2</figref>. In the present embodiment, means for performing fuzzy search is realized by storing a large-scale speech data sampled beforehand (for example, the number of sampling bits: 16, sampling frequency: 44.1 kHz) in a speech database <b>25</b>, and using both a suffix array creation unit <b>28</b> and a speech search unit <b>29</b> that performs DP matching.</p>
<p id="p-0067" num="0068">A speech search device <b>31</b> in the present embodiment is provided with the speech database <b>25</b>, a database-use speech recognizer <b>26</b>, a speech-use phoneme string generation unit <b>27</b>, and the suffix array creation unit <b>28</b>, to create a suffix array from speech data. On the other hand, it is also provided with input devices <b>21</b> and <b>24</b> and a phoneme string generation unit <b>23</b> in order to create a phoneme string of an input search keyword. One of the input devices <b>21</b> and <b>24</b> is a speech input device (microphone, for example) <b>21</b> and the other is a character input device (keyboard, for example) <b>24</b>. In configuration, both of those two different input means or only either one of them may be mounted. However, in the case of mounting the speech input device (microphone, for example) <b>21</b>, it is necessary to mount a speech recognizer <b>22</b>. Then, a keyword input as a word string or obtained by converting speech into a word string is converted into a phoneme string in the phoneme string generation unit <b>23</b>. Description of the &#x201c;speech-/character-use phoneme string generation unit&#x201d; in <figref idref="DRAWINGS">FIG. 2</figref> is intended to accommodate both of a case where the input is speech and a case where the input is a character.</p>
<p id="p-0068" num="0069">As shown in <figref idref="DRAWINGS">FIG. 2</figref>, in configuration, information of a suffix array created from speech data and information of a phoneme string of an input search keyword undergo search processing in the speech search unit <b>29</b>. The speech search unit <b>29</b> is provided with means for setting a first threshold to be used in search, means for, if the search keyword has at least a predetermined length, dividing this search keyword based on phonemes, means for determining a second threshold from the first threshold, the second threshold being used in search for keywords divided by the search keyword division means, and means for searching for a search object by dynamic programming by using either one of the first and second thresholds.</p>
<p id="p-0069" num="0070">Therefore, the means for calculating a distance between phoneme discrimination features included in speech data to determine similarity is realized by the speech search unit <b>29</b>. On the other hand, the means for dividing the input search keyword based on phonemes, the means for changing the first threshold according to Equation 1 (Math. 1) to obtain the second threshold so that at least two of the divided search keywords may match, and the means for searching for a search object determined by the first threshold and the second threshold are all realized by the speech search unit <b>29</b>.</p>
<p id="p-0070" num="0071">Further, the means for repeatedly performing search as consecutively increasing the first threshold to be used in search is realized by the speech search unit <b>29</b>, which realizes also the threshold adjustment means for consecutively presenting search results. Simultaneously, the means for consecutively outputting (presenting to the user) the search results is realized by a display device (for example, display) <b>30</b> or a speech output device (for example, speaker) <b>31</b>.</p>
<p id="p-0071" num="0072">Further, the means for determining whether or not keyword division is necessary based on the length of the search keyword is realized by the speech search unit <b>29</b>, while the means for determining the number of phonemes after the keyword is divided is realized by the speech-/character-use phoneme string generation unit <b>23</b> and the speech search unit <b>29</b>.</p>
<p id="p-0072" num="0073">In the speech search device in the present embodiment, as shown in <figref idref="DRAWINGS">FIG. 2</figref>, as speech search results, information such as characters and images about search is displayed on the display device <b>30</b> (for example, display), while speech information is reproduced as voices from the speech output device <b>31</b> (for example, speaker). Only either one of these may be mounted in configuration.</p>
<p id="p-0073" num="0074">The speech processing device <b>32</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> may be realized using a personal computer in which an ROM, an RAM (hereinafter referred to as memory), a CPU, an HDD, and a speech input/output interface (capable of processing at a sampling frequency of 44.1 kHz with 16 sampling bits, for example) are electrically interconnected significantly with a system bus. It is possible to store the speech database in the HDD, connect the speech input device <b>21</b> and the speech output device <b>31</b> to the speech input/output interface, compose the means other than those mentioned above as software by using the C# language or C++ language and store them in the HDD so that the software may be read into the memory upon activation, and mainly interlock the memory and the CPU with each other via the system bus, thereby realizing necessary means.</p>
<p id="p-0074" num="0075">Next, a description will be given of a specific method of speech search with reference to <figref idref="DRAWINGS">FIG. 3 to 5</figref>.</p>
<p id="p-0075" num="0076"><figref idref="DRAWINGS">FIG. 3</figref> is an explanatory view of creating a suffix array from the speech database. Speech data stored in the speech database <b>25</b> is converted into a word string by using the database-use speech recognizer <b>26</b> to further convert the word string into a phoneme string (a) by using the speech-use phoneme string generation unit <b>27</b>. Next, a suffix array is created from the phoneme string by using the suffix array generation unit <b>28</b> and saved in the memory or the HDD.</p>
<p id="p-0076" num="0077">If a search keyword is received in a speech (it is input by the speech input device <b>21</b>), it is converted into a word string by using the speech recognizer <b>22</b> and converted into a phoneme string by using the speech-/character-use phoneme string generation unit <b>23</b>. Also if it is received in a text (character string) (it is input by the character input device <b>24</b>), it is converted into a phoneme string by using the speech-/character-use phoneme string generation unit <b>23</b>. An average value of the first threshold per phoneme to be used in search is set to a small value (for example, 0.0) by the speech search unit <b>29</b>.</p>
<p id="p-0077" num="0078"><figref idref="DRAWINGS">FIG. 4</figref> shows an explanatory view of fuzzy search by use of DP matching in the aforementioned suffix array. After the keyword is divided into an optimal number of phonemes or not divided, search is performed in the suffix array by using DP matching. The DP matching uses the first threshold in a case where the keyword is not divided and a value (second threshold) obtained by changing the first threshold by using Equation 1 (Math. 1) in a case where the keyword is divided. In such a manner, a first stage candidate (b) of the search result is obtained. In the case of not dividing, (b) is presented to the user as the result by using the display device <b>30</b> and the speech output device <b>31</b>.</p>
<p id="p-0078" num="0079"><figref idref="DRAWINGS">FIG. 5</figref> shows an explanatory view of keyword division and speech search. If the phoneme is divided, at least two of the divided keywords are searched for, so that a final candidate (c) is obtained based on the positional relationship of the search result.</p>
<p id="p-0079" num="0080">DP matching is performed on the final candidate (c) by using the phoneme string (a) and the first threshold, to present the search result to the user by using the display device <b>30</b> and the speech output device <b>31</b>. After the search result is presented, the first threshold is updated to a little larger value (for example, by adding 0.2) and then a return is made to the DP matching using the first threshold.</p>
<heading id="h-0011" level="1">EXAMPLE 1</heading>
<p id="p-0080" num="0081">A search experiment was conducted on speech data in the CSJ (Corpus of Spontaneous Japanese) (male speaker, 390 hours) as an object by constituting the speech search device <b>32</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> by using the C# language in a personal computer (Intel (registered trademark), Pentium (registered trademark) D, 2.8 GHz, memory capacity: 4 GB) and the results are shown in <figref idref="DRAWINGS">FIGS. 6 to 9</figref> as a time that elapses until the first search result is output in a case where the first threshold was minimized (0.0). In <figref idref="DRAWINGS">FIGS. 6 to 9</figref>, the horizontal axis gives the first threshold and the vertical axis gives a recall ratio, a precision ratio, and a processing time in search. The time that elapsed until the first search result was output was several milliseconds in a case where the first threshold was minimized in conditions of 6 phonemes (see <figref idref="DRAWINGS">FIG. 6</figref>), 12 phonemes (see <figref idref="DRAWINGS">FIG. 7</figref>), and 18 phonemes (see <figref idref="DRAWINGS">FIG. 8</figref>), and 24 phonemes (see <figref idref="DRAWINGS">FIG. 9</figref>) included in the search keyword.</p>
<heading id="h-0012" level="1">EXAMPLE 2</heading>
<p id="p-0081" num="0082"><figref idref="DRAWINGS">FIGS. 10 and 11</figref> show the time that elapses until half of correct keywords included in the aforementioned corpus are detected. In <figref idref="DRAWINGS">FIGS. 10 and 11</figref>, the horizontal axis gives the first threshold and the vertical axis gives a recall ratio, a precision ratio, and a processing time in search. The time that elapsed until the half of the correct keywords were detected was one second or less in conditions of 6 phonemes (see <figref idref="DRAWINGS">FIG. 10</figref>) and 12 phonemes (see <figref idref="DRAWINGS">FIG. 11</figref>) included in the search keyword.</p>
<heading id="h-0013" level="1">EXAMPLE 3</heading>
<p id="p-0082" num="0083">A search experiment was conducted on newspaper article data comparable to 10000 speech-equivalent hours as an object by constituting the speech search device <b>32</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> by using the C# language in the personal computer (Intel (registered trademark), Pentium (registered trademark) D, 2.8 GHz, memory capacity: 4 GB) and the results are shown in <figref idref="DRAWINGS">FIGS. 12 to 14</figref>. Non-patent Literature 1 describes that it takes 2.17 seconds to search a 2031-hour speech database for a 5.2-mora search keyword (in a range of 5 phonemes through 11 phonemes). On the other hand, in the present invention, as shown in <figref idref="DRAWINGS">FIG. 12</figref>, the first threshold was set to 0.0 and a search keyword of 6 phonemes through 24 phonemes was searched for, to find that it took several milliseconds through 120 milliseconds to present a first search result group to the user. Further, the first threshold was updated to 0.2 and then a search keyword of 6 phonemes through 24 phonemes was searched for, to find that it took several milliseconds through 130 milliseconds to present a newly obtained search result group to the user as shown in <figref idref="DRAWINGS">FIG. 13</figref>. Further, the first threshold was updated to 0.4 and then a search keyword of 6 phonemes through 24 phonemes was searched for, to find that it took several tens of milliseconds through 600 milliseconds to present a newly obtained search result group to the user as shown in <figref idref="DRAWINGS">FIG. 14</figref>. From these, it is known that speech search could be conducted speedily.</p>
<heading id="h-0014" level="1">EXAMPLE 4</heading>
<p id="p-0083" num="0084">A search experiment was conducted on newspaper article data comparable to 10000 speech-equivalent hours as an object by constituting the speech search device <b>32</b> shown in <figref idref="DRAWINGS">FIG. 2</figref> by using the C++ language in a personal computer (Intel (registered trademark), Core2Duo, E8600, 3.3 GHz, memory capacity: 8 GB) and the results are shown in <figref idref="DRAWINGS">FIG. 15 to 18</figref>. As shown in <figref idref="DRAWINGS">FIG. 15</figref>, the first threshold was set to 0.0 and a search keyword of 6 phonemes through 24 phonemes was searched for, to find that it took several milliseconds to present a first search result group to the user. Further, as shown in <figref idref="DRAWINGS">FIG. 16</figref>, the first threshold was updated to 0.2 and then a search keyword of 6 phonemes through 24 phonemes was searched for, to find that it took several milliseconds to present a newly obtained search result group to the user. Further, as shown in <figref idref="DRAWINGS">FIG. 17</figref>, the first threshold was updated to 0.4 and then a search keyword of 6 phonemes through 24 phonemes was searched for, to find that it took several milliseconds through 27 milliseconds to present a newly obtained search result group to the user. Further, as shown in <figref idref="DRAWINGS">FIG. 18</figref>, the first threshold was updated to 1.0 and then a search keyword of 6 phonemes through 24 phonemes was searched for, to find that it took ten and more milliseconds through about one second to present a newly obtained search result group to the user. From these, it is known that speech search could be performed speedily.</p>
<p id="h-0015" num="0000">Reference Signs List</p>
<p id="p-0084" num="0000">
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0085"><b>21</b> speech input device</li>
    <li id="ul0003-0002" num="0086"><b>22</b> speech recognizer</li>
    <li id="ul0003-0003" num="0087"><b>23</b> speech-/character-use phoneme string generation unit</li>
    <li id="ul0003-0004" num="0088"><b>24</b> character input device</li>
    <li id="ul0003-0005" num="0089"><b>25</b> speech database</li>
    <li id="ul0003-0006" num="0090"><b>26</b> database-use speech recognizer</li>
    <li id="ul0003-0007" num="0091"><b>27</b> speech-use phoneme string generation unit</li>
    <li id="ul0003-0008" num="0092"><b>28</b> suffix array generation unit</li>
    <li id="ul0003-0009" num="0093"><b>29</b> speech search unit</li>
    <li id="ul0003-0010" num="0094"><b>30</b> display device</li>
    <li id="ul0003-0011" num="0095"><b>31</b> speech output device</li>
    <li id="ul0003-0012" num="0096"><b>32</b> speech search device</li>
</ul>
</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08626508-20140107-M00001.NB">
<img id="EMI-M00001" he="6.35mm" wi="76.20mm" file="US08626508-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08626508-20140107-M00002.NB">
<img id="EMI-M00002" he="6.35mm" wi="76.20mm" file="US08626508-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08626508-20140107-M00003.NB">
<img id="EMI-M00003" he="6.35mm" wi="76.20mm" file="US08626508-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08626508-20140107-M00004.NB">
<img id="EMI-M00004" he="6.35mm" wi="76.20mm" file="US08626508-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A speech search device for receiving input speech and searching for speech data obtained by sampling the input speech, the speech search device comprising:
<claim-text>a database-use speech recognizer that recognizes the speech data, which is recorded in a speech database to obtain a word string;</claim-text>
<claim-text>a speech-use phoneme string generator unit that generates a phoneme string from the word string obtained by the database-use speech recognizer;</claim-text>
<claim-text>a suffix array generator that generates a suffix array from the phoneme string generated by the speech-use phoneme string generator;</claim-text>
<claim-text>an input device that inputs a search keyword;</claim-text>
<claim-text>an input phoneme generator that generates a keyword phoneme string from the search keyword input by the input device;</claim-text>
<claim-text>a speech searcher that sets a first threshold to be used in a search of the suffix array, that searches the suffix array for the keyword phoneme string, using dynamic programming, based on the first threshold, wherein the speech searcher iteratively adjusts the first threshold, performs the search based on the adjusted first threshold, and presents a result of the search during each iteration, wherein the adjusted first threshold is determined based on:</claim-text>
</claim-text>
<claim-text>
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
  <mrow>
    <msup>
      <mi>t</mi>
      <mi>&#x2032;</mi>
    </msup>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mi>p</mi>
        <mrow>
          <mi>p</mi>
          <mo>-</mo>
          <mn>1</mn>
        </mrow>
      </mfrac>
      <mo>&#x2062;</mo>
      <mi>t</mi>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>in which p is a number of divisions with respect to the search keyword, t is the first threshold, and t&#x2032; is the adjusted first threshold; and
<claim-text>an output device that outputs the result of the search by the speech searcher, during each iteration.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The speech search device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the speech searcher divides the search keyword based on the keyword phoneme string when the search keyword is at least a predetermined length, determines an adjusted first threshold, to be used in a search for the divided search keyword, from the first threshold, and
<claim-text>searches for the divided search keyword by dynamic programming by using the adjusted first threshold.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The speech search device according to one of <claim-ref idref="CLM-00001">claims 1</claim-ref> and <claim-ref idref="CLM-00002">2</claim-ref>,
<claim-text>wherein the speech searcher determines</claim-text>
<claim-text>whether keyword division is necessary based on the length of the search keyword, and</claim-text>
<claim-text>determines a resultant number of phonemes after the search keyword is divided.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The speech search device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the speech searcher calculates similarity between phonemes in the keyword phoneme string by using an inter-phoneme distance based on phoneme discrimination features in the dynamic programming.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A speech search method for receiving an input speech and searching for speech data obtained by sampling the input speech, the speech search method comprising the steps of:
<claim-text>creating a suffix array by converting the speech data into a phoneme string:</claim-text>
<claim-text>receiving a search keyword and converting the search keyword into a keyword phoneme string;</claim-text>
<claim-text>setting a first threshold to be used in a search of the suffix array for the keyword phoneme string;</claim-text>
<claim-text>searching, by a hardware processor, for a search object in the search of the suffix array, using dynamic programming based on the first threshold and outputting a result of the searching; and</claim-text>
<claim-text>an iterative threshold adjusting step of adjusting the first threshold, searching for the search object based on the adjusted first threshold, and outputting the result of the searching during each iteration, wherein the adjusted first threshold is determined based on:</claim-text>
</claim-text>
<claim-text>
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mrow>
  <mrow>
    <msup>
      <mi>t</mi>
      <mi>&#x2032;</mi>
    </msup>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mi>p</mi>
        <mrow>
          <mi>p</mi>
          <mo>-</mo>
          <mn>1</mn>
        </mrow>
      </mfrac>
      <mo>&#x2062;</mo>
      <mi>t</mi>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>in which p is a number of divisions with respect to the search keyword, t is the first threshold, and t&#x2032; is the adjusted first threshold.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The speech search method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising the steps of:
<claim-text>when the keyword phoneme string is at least a predetermined length, dividing the keyword phoneme string based on phoneme units; and</claim-text>
<claim-text>determining an adjusted first threshold, based on the first threshold, replacing the first threshold with the adjusted first threshold, which is to be used in the search, which is based on the divided keyword phoneme string.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The speech search method according to <claim-ref idref="CLM-00005">claim 5</claim-ref> or <claim-ref idref="CLM-00006">6</claim-ref>, further comprising:
<claim-text>a step of determining whether to divide the keyword phoneme string based on a length of the keyword phone string; and a</claim-text>
<claim-text>keyword dividing step of determining the number of the phoneme units resulting from dividing the keyword phoneme string.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The speech search method according to <claim-ref idref="CLM-00005">claim 5</claim-ref> or <claim-ref idref="CLM-00006">6</claim-ref>, wherein the step of searching for the search object includes a step of calculating similarity between the phoneme units of the keyword phoneme string by using an inter-phoneme distance based on phoneme discrimination features included in the speech data, in the dynamic programming.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. A non-transitory computer-readable storage medium having computer readable program codes embodied in the computer readable storage medium that, when executed cause a computer to execute:
<claim-text>receiving an input speech and storing speech data obtained by sampling the input speech;</claim-text>
<claim-text>creating a suffix array by converting the speech data into a phoneme string; receiving a search keyword and converting the search keyword into a keyword phoneme string;</claim-text>
<claim-text>setting a first threshold to be used in a search of the suffix array for the keyword phoneme string;</claim-text>
<claim-text>searching, by a hardware processor, for a search object in the search of the suffix array, using dynamic programming based on the first threshold and outputting a result of the searching; and</claim-text>
<claim-text>an iterative threshold adjusting step of adjusting the first threshold, searching for the search object based on the adjusted first threshold, and outputting the result of the searching during each iteration, wherein the adjusted first threshold is determined based on:</claim-text>
</claim-text>
<claim-text>
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mrow>
  <mrow>
    <msup>
      <mi>t</mi>
      <mi>&#x2032;</mi>
    </msup>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mi>p</mi>
        <mrow>
          <mi>p</mi>
          <mo>-</mo>
          <mn>1</mn>
        </mrow>
      </mfrac>
      <mo>&#x2062;</mo>
      <mi>t</mi>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
</maths>
</claim-text>
<claim-text>in which p is a number of divisions with respect to the search keyword, t is the first threshold, and t&#x2032; is the adjusted first threshold.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The speech search device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the speech searcher divides the search keyword and, based on the adjusted first threshold, performs the search based on the divided search keyword, and wherein the result of the search at each iteration matches at least two positions. </claim-text>
</claim>
</claims>
</us-patent-grant>
