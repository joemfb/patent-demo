<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626493-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626493</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13871025</doc-number>
<date>20130426</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>15</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>25</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>21</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20130101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>10</class>
<subclass>L</subclass>
<main-group>21</main-group>
<subgroup>06</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>704200</main-classification>
<further-classification>704270</further-classification>
<further-classification>704276</further-classification>
<further-classification>704278</further-classification>
</classification-national>
<invention-title id="d2e51">Insertion of sounds into audio content according to pattern</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4461024</doc-number>
<kind>A</kind>
<name>Rengger et al.</name>
<date>19840700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5204905</doc-number>
<kind>A</kind>
<name>Mitome</name>
<date>19930400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704260</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5220681</doc-number>
<kind>A</kind>
<name>Belgin</name>
<date>19930600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5751885</doc-number>
<kind>A</kind>
<name>O'Loughlin et al.</name>
<date>19980500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5918213</doc-number>
<kind>A</kind>
<name>Bernard et al.</name>
<date>19990600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5931901</doc-number>
<kind>A</kind>
<name>Wolfe et al.</name>
<date>19990800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709206</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5983186</doc-number>
<kind>A</kind>
<name>Miyazawa et al.</name>
<date>19991100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>6088123</doc-number>
<kind>A</kind>
<name>Adler et al.</name>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>6181743</doc-number>
<kind>B1</kind>
<name>Bailleul</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>6198832</doc-number>
<kind>B1</kind>
<name>Maes et al.</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6229550</doc-number>
<kind>B1</kind>
<name>Gloudemans et al.</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6252971</doc-number>
<kind>B1</kind>
<name>Wang</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6311214</doc-number>
<kind>B1</kind>
<name>Rhoads</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>6332123</doc-number>
<kind>B1</kind>
<name>Kaneko et al.</name>
<date>20011200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>704276</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>6359573</doc-number>
<kind>B1</kind>
<name>Taruguchi et al.</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>6373530</doc-number>
<kind>B1</kind>
<name>Birks et al.</name>
<date>20020400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>6563937</doc-number>
<kind>B1</kind>
<name>Wendt</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>6564380</doc-number>
<kind>B1</kind>
<name>Murphy</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>6611830</doc-number>
<kind>B2</kind>
<name>Shinoda et al.</name>
<date>20030800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>6674861</doc-number>
<kind>B1</kind>
<name>Xu et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>6735699</doc-number>
<kind>B1</kind>
<name>Sasaki et al.</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>6739773</doc-number>
<kind>B2</kind>
<name>Spano</name>
<date>20040500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>6748237</doc-number>
<kind>B1</kind>
<name>Bates et al.</name>
<date>20040600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>6771794</doc-number>
<kind>B1</kind>
<name>Osaka</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>6839673</doc-number>
<kind>B1</kind>
<name>Choi et al.</name>
<date>20050100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>6868229</doc-number>
<kind>B2</kind>
<name>Balogh</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>6940993</doc-number>
<kind>B2</kind>
<name>Jones et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>6975746</doc-number>
<kind>B2</kind>
<name>Davis et al.</name>
<date>20051200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>6983057</doc-number>
<kind>B1</kind>
<name>Ho et al.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>6996533</doc-number>
<kind>B2</kind>
<name>Ikeda et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>7020304</doc-number>
<kind>B2</kind>
<name>Alattar et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>7024016</doc-number>
<kind>B2</kind>
<name>Rhoads et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>7068297</doc-number>
<kind>B2</kind>
<name>Jones et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>7073127</doc-number>
<kind>B2</kind>
<name>Zhao et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>7095871</doc-number>
<kind>B2</kind>
<name>Jones et al.</name>
<date>20060800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>7110566</doc-number>
<kind>B2</kind>
<name>Pelly et al.</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>7190808</doc-number>
<kind>B2</kind>
<name>Goldberg et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382100</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>7206820</doc-number>
<kind>B1</kind>
<name>Rhoads et al.</name>
<date>20070400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>7246313</doc-number>
<kind>B2</kind>
<name>Sung et al.</name>
<date>20070700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>7280970</doc-number>
<kind>B2</kind>
<name>Tamir et al.</name>
<date>20071000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>7302162</doc-number>
<kind>B2</kind>
<name>Beaton</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>7304227</doc-number>
<kind>B2</kind>
<name>Hoguchi</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>7305468</doc-number>
<kind>B2</kind>
<name>Douceur et al.</name>
<date>20071200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>7373513</doc-number>
<kind>B2</kind>
<name>Levy</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>7443982</doc-number>
<kind>B2</kind>
<name>Stone et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>7444353</doc-number>
<kind>B1</kind>
<name>Chen et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>7460252</doc-number>
<kind>B2</kind>
<name>Campbell et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>8055910</doc-number>
<kind>B2</kind>
<name>Kocher et al.</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2001/0002931</doc-number>
<kind>A1</kind>
<name>Maes et al.</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>2002/0027994</doc-number>
<kind>A1</kind>
<name>Katayama et al.</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>2002/0033844</doc-number>
<kind>A1</kind>
<name>Levy et al.</name>
<date>20020300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>2002/0080964</doc-number>
<kind>A1</kind>
<name>Stone et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>2002/0129255</doc-number>
<kind>A1</kind>
<name>Tsuchiyama et al.</name>
<date>20020900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>2004/0166873</doc-number>
<kind>A1</kind>
<name>Simic et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>2004/0236574</doc-number>
<kind>A1</kind>
<name>Ativanichayaphong et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>14</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>704270</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704276</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704200</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>704278</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11203705</doc-number>
<date>20050815</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8452604</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13871025</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20130238342</doc-number>
<kind>A1</kind>
<date>20130912</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>AT&#x26;T Intellectual Property I, L.P.</orgname>
<address>
<city>Atlanta</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Tischer</last-name>
<first-name>Steven N.</first-name>
<address>
<city>Atlanta</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<last-name>Zimmerman, PLLC</last-name>
<first-name>Scott P.</first-name>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>AT&#x26;T Intellectual Property I, L.P.</orgname>
<role>02</role>
<address>
<city>Atlanta</city>
<state>GA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Yen</last-name>
<first-name>Eric</first-name>
<department>2658</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">Sounds are inserted into audio content according to a pattern. A library stores humanly perceptible voice sounds. Pattern control information is received that is associated with a device recording the audio content. A pattern is retrieved and washing machine sounds are inserted into the audio content according to the pattern. The humanly perceptible voice sounds are inserted into the audio content according to the pattern to generate a signed audio recording.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="161.63mm" wi="180.76mm" file="US08626493-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="158.24mm" wi="183.30mm" file="US08626493-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="207.60mm" wi="182.03mm" file="US08626493-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="160.70mm" wi="171.70mm" file="US08626493-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="205.06mm" wi="197.36mm" file="US08626493-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="186.52mm" wi="183.90mm" file="US08626493-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application is a continuation of U.S. application Ser. No. 11/203,750, filed Aug. 15, 2005, now issued as U.S. Pat. No. 8,452,604 and incorporated herein by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0003" num="0002">Recent years have witnessed the proliferation of personal media creation and distribution, such as audio podcasting, web logs (&#x201c;blogs&#x201d;), RSS (Really Simple Syndication) feeds, web cameras, and the like. With the expanding range of distribution possibilities for such content, the ability to determine and ascertain ownership of content may become more problematic. For example, such content may be edited, re-recorded, sampled or otherwise processed, which may make it difficult to determine the origin of the content.</p>
<p id="p-0004" num="0003">One way to manage content distribution is to restrict access to it. For example, several techniques for digital rights management (DRM) have been developed to prevent unauthorized use of content. DRM is used, for example, by digital media publishers to allow them to control the duplication and dissemination of their proprietary content. Potential problems of DRM include potential vulnerability to attacks and overprotection of content by preventing &#x201c;fair use.&#x201d; In addition, DRM may not be particularly useful or desirable, as an individual content creator may lack the ability to institute DRM measures, and access limitations generally run counter to the paradigm of free distribution of content among individuals.</p>
<p id="p-0005" num="0004">Schemes exist for the digital watermarking of content, such as JPEG images or video files. Some conventional visual watermarking techniques, for example, embed identifying data within a compressed image or video file. Typically, the embedding is done in a manner that minimizes the creation of visual artifacts when an image is generated from the compressed file. While such techniques may be effective in maintaining the identity of content, they may be limited in their applicability.</p>
<heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0006" num="0005">Some embodiments of the present invention provide methods of identifying audio and/or visual content. Recognizable visual and/or audio artifacts, such as recognizable sounds, are introduced into visual and/or audio content in an identifying pattern to generate a signed visual and/or audio recording for distribution over a digital communications medium. A library of images and/or sounds may be provided, and the image and/or sounds from the library may be selectively inserted to generate the identifying pattern. The images and/or sounds may be inserted responsive to one or more parameters associated with creation of the visual and/or audio content. For example, the images and/or sounds may be inserted responsive to a personal identifier, a personal characteristic, a location descriptor, a temporal metric, a device identifier and/or an ambient environment parameter. A representation of the identifying pattern may be generated and stored in a repository, e.g., an independent repository configured to maintain creative rights information. The stored pattern may be retrieved from the repository and compared to an unidentified visual and/or audio recording to determine an identity thereof.</p>
<p id="p-0007" num="0006">Additional embodiments of the present invention provide systems for identifying audio and/or visual content. A system includes a signature generator configured to introduce recognizable visual and/or audio artifacts into visual and/or audio content in an identifying pattern to generate a signed visual and/or audio recording for distribution over a digital communications medium. The system may further include a library of images and/or sounds, and the signature generator may be configured to selectively insert the image and/or sounds from the library to generate the identifying pattern. For example, the signature generator may be configured to insert the images and/or sounds responsive to one or more parameters associated with creation of the visual and/or audio content. The signature generator may be further configured to generate a representation of the identifying pattern for storage in a repository.</p>
<p id="p-0008" num="0007">In additional embodiments of the present invention, computer program products for identifying audio and/or visual content are provided. A computer program product may include computer program code embodied in a storage medium, the computer program code including program code configured to introduce recognizable visual and/or audio artifacts into visual and/or audio content in an identifying pattern to generate a signed visual and/or audio recording for distribution over a digital communications medium. The program code configured to introduce recognizable visual and/or audio artifacts into visual and/or audio content in an identifying pattern to generate a signed visual and/or audio recording for distribution over a digital communications medium may include program code configured to instantiate a library of images and/or sounds and program code configured to selectively insert images and/or sounds from the library to generate the identifying pattern. For example, the program code configured to selectively insert images and/or sounds from the library to generate the identifying pattern may include program code configured to insert the images and/or sounds responsive to one or more parameters associated with creation of the visual and/or audio content. The computer program code may further include program code configured to generate a representation of the identifying pattern for storage in a repository</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. 1</figref> is a schematic block diagram illustrating a content signature generator and exemplary operations thereof according to some embodiments of the present invention.</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. 2</figref> is a schematic block diagram illustrating exemplary operations for inserting recognizable audio artifacts into audio content according to some embodiments of the present invention.</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. 3</figref> is a signal diagram illustrating exemplary operations for inserting recognizable audio artifacts into audio content according to further embodiments of the present invention.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 4</figref> is a schematic block diagrams illustrating an exemplary electronic device including a content signature generator according to some embodiments of the present invention.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 5</figref> is a schematic block diagram illustrating a client-server arrangement for a content signature information storage and retrieval according to some embodiments of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF EMBODIMENTS OF THE PRESENT INVENTION</heading>
<p id="p-0014" num="0013">The present invention now will be described more fully hereinafter with reference to the accompanying figures, in which embodiments of the invention are shown. This invention may, however, be embodied in many alternate forms and should not be construed as limited to the embodiments set forth herein.</p>
<p id="p-0015" num="0014">Accordingly, while the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that there is no intent to limit the invention to the particular forms disclosed, but on the contrary, the invention is to cover all modifications, equivalents, and alternatives falling within the spirit and scope of the invention as defined by the claims. Like numbers refer to like elements throughout the description of the figures.</p>
<p id="p-0016" num="0015">The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein, the singular forms &#x201c;a&#x201d;, &#x201c;an&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms &#x201c;comprises&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated selectivity features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other selectivity features, integers, steps, operations, elements, components, and/or groups thereof. As used herein the term &#x201c;and/or&#x201d; includes any and all combinations of one or more of the associated listed items.</p>
<p id="p-0017" num="0016">Unless otherwise defined, all terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this invention belongs. It will be further understood that terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein.</p>
<p id="p-0018" num="0017">The present invention is described below with reference to block diagrams and/or other illustrations of methods and/or systems according to embodiments of the invention. It should also be noted that in some alternate implementations, the functions/acts noted in the blocks may occur out of the order noted in the flowcharts. For example, two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order, depending upon the functionality/acts involved.</p>
<p id="p-0019" num="0018">According to various exemplary embodiments of the present invention, media content, i.e., visual and/or audio content, may be &#x201c;signed&#x201d; by altering it in ways that may survive downstream alterations, such that it may be later processed to discover, for example, the identity of the originator and/or owner of the content. In some embodiments, recognizable additional content (e.g., sounds) may be inserted into the content according to identifying pattern (e.g., a periodic, aperiodic, pseudorandom or other identifiable pattern), and a separate representation of the pattern may be generated and stored for use in later identification processes. The additional content may be contextual, i.e., content consistent with the content being signed, and/or the additional content may be non-contextual, such as out-of-place sounds and/or imagery. The stored representation may be used, for example, to facilitate search and discovery of the origin of the content by allowing the matching of specific items not otherwise obvious in the instantiation.</p>
<p id="p-0020" num="0019">For example, a person generating a recording of a story for a podcast may introduce recognizable audio artifacts, e.g., fake background noises and/or audio effects, such as fade-outs, static or echoes, according to a pattern. The pattern may be based on, for example, characteristics of the person creating the record, the device making the recording and/or parameters associated with the environment in which the recording was made. The signature could be later be detected to determine, for example, the origin of particular content, even if it has been re-recorded and incorporated into other content as if it were the work of someone else.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 1</figref> illustrates a content signature generator <b>100</b> and exemplary operations thereof according to some embodiments of the present invention. The content signature generator <b>100</b> is configured to insert recognizable visual and/or audio artifacts <b>102</b> into visual and/or audio content <b>101</b> according to pattern control information <b>103</b>, producing a visual and/or audio recording <b>104</b> that includes an identifying pattern of recognizable visual and/or audio artifacts. The content signal generator <b>100</b> is further operative to produce a separate representation <b>105</b> of the identifying pattern, which may be provided, for example, to a repository of rights information. It will be understood that the artifacts <b>102</b> may be contextual and/or non-contextual, and the pattern may have a periodic, aperiodic, pseudorandom or other character. For example, the pattern could be a periodic pattern of discrete, individually humanly perceptible artifacts or a &#x201c;burst&#x201d; of multiple artifacts that may be perceived by a human as a single perceptual event.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 2</figref> illustrates a content signature generator <b>200</b> and exemplary operations thereof according to further embodiments of the present invention. The content signature generator <b>200</b> is configured to insert audio artifacts in the form of various commonly recognized sounds <b>202</b>, such as those associated with a washing machine, hotel lobby, office, woods, or sea shore, and/or or commonly recognized audio effects, such as static or echoes, into source audio content <b>201</b>. It will be understood that the sounds <b>202</b> may be consistent with the context of the source audio content <b>201</b> and/or may be non-contextual. The source audio content <b>201</b> may be, for example, a live input (e.g. a microphone or mixer feed) and/or pre-recorded audio content. The sounds <b>202</b> are inserted in the source audio content in a pattern based on pattern control information <b>203</b> associated with an originator of the content, e.g., the originator's name or other identifier, an identifier for a device used in creating the content, a date, time, or location, and/or a characteristic of a source of the content, e.g., a voice formant. The content signature generator <b>200</b> thus produces a &#x201c;signed&#x201d; audio recording <b>204</b> including the pattern of inserted sounds. The signed audio recording <b>204</b> may subsequently distributed over a communications medium, e.g., transmitted over the Internet, stored on a recording medium, or the like. The content signature generator <b>200</b> also generates a representation <b>205</b> of the audio content pattern, which may be transmitted, for example, to a repository of creative rights information, as described below with reference to <figref idref="DRAWINGS">FIG. 5</figref>.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. 3</figref> illustrates exemplary operations for insertion of sound artifacts according to further embodiments of the present invention. A library <b>310</b> of predetermined sounds associated with a particular commonly recognized scene or environment, e.g., a laundry room, may be provided. As shown, several discrete sounds may be associated with such an environment, e.g., a &#x201c;clunk&#x201d; associated with a washing machine solenoid operation, a start of a motor, a slam of a door, an inrush of water, and the like. Sounds from the library <b>310</b> may be selectively inserted into an audio feed to according to a pattern to produce a &#x201c;signed&#x201d; audio recording <b>320</b>.</p>
<p id="p-0024" num="0023">It will be understood that the present invention is not limited to insertion of &#x201c;environmental&#x201d; sound sets, such as those described above with reference to <figref idref="DRAWINGS">FIGS. 2 and 3</figref>. For example, in some embodiments, an identifying pattern of utterances arranged into phrases. The utterances could be from a library of utterances associated with a particular individual, such as the &#x201c;voice files&#x201d; described in Published U.S. Patent Application No. 2004/0111271 to Tischer (U.S. patent application Ser. No. 10/012,946, filed Dec. 10, 2001).</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. 4</figref> illustrates an exemplary implementation of content signatures in an electronic device <b>400</b>, such as a personal computer, notebook computer, personal digital assistant (PDA), media recorder, wireless communications device (e.g., cell phone) or the like. A content signature generator <b>412</b> may be instantiated in the device <b>400</b> using program code configured to execute on a processor <b>410</b>, e.g., a microprocessor, microcontroller or the like. The processor <b>410</b> interoperates with a memory <b>420</b>, for example, semiconductor memory, magnetic storage and/or optical storage, and with input/output (I/O) circuitry <b>430</b>. The memory <b>420</b> may be used, for example, to store an artifact library <b>423</b> (e.g., a repository of MPEG or WAV files), and identity information <b>424</b>, such as an identifier for the device <b>400</b> and/or a user of the device <b>400</b>, for use in generation of signed recordings by the content signature generator <b>412</b>. As shown, content to be &#x201c;signed&#x201d; by the content signature generator <b>412</b> may include unsigned recordings <b>421</b> stored in the memory <b>420</b> and/or content <b>401</b> provided to the processor <b>430</b> via the I/O circuitry <b>430</b>. The I/O circuitry <b>430</b> may also receive sensor inputs <b>402</b> for use by the content signature generator <b>412</b>. Signed recordings <b>422</b> may be stored in the memory <b>420</b> and/or transmitted to a communications medium via the I/O circuitry <b>430</b>. Representations <b>425</b> of patterns of artifacts introduced into the signed recordings <b>422</b> may also be stored in the memory <b>420</b> and/or transmitted to a communications medium via the I/O circuitry <b>430</b>.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 5</figref> illustrates a client-server configuration for distributing content pattern representations according to further embodiments of the present invention. At an originating client <b>510</b>, a content signature generator <b>512</b> generates representations <b>515</b> of patterns of visual and/or audio artifacts introduced into content recordings. The representations <b>515</b> are transmitted to a repository <b>522</b> at a repository server <b>520</b> via a network <b>540</b>.</p>
<p id="p-0027" num="0026">The repository <b>522</b> may be, for example, a database of patterns maintained by an independent party that administers or maintains creative rights information, such that parties wishing to include received content in their works may inquire as to the origin of the content. For example, an inquiring client <b>530</b> wishing to identify the origin of particular content <b>533</b> may, for example, transmit the content <b>533</b> to the repository server <b>520</b> over the network <b>540</b>. As shown, the repository server <b>520</b> may include a content signature determiner <b>524</b> that is configured to determine if the content <b>533</b> includes a pattern of recognizable artifacts stored in the repository <b>522</b>, and to provide identity info <b>535</b> to the inquiring client <b>530</b> upon detection of a match.</p>
<p id="p-0028" num="0027">It will be appreciated that the implementation shown in <figref idref="DRAWINGS">FIG. 5</figref> is provided for purposes of illustration, and that content signature information storage and distribution may be implemented in any of a number of different ways in various embodiments of the present invention. For example, in some embodiments, the repository <b>522</b> may be distributed across multiple nodes that communicate with the server <b>520</b>. In further embodiments, the content signature determiner <b>524</b> may be positioned at inquiring client <b>530</b> (or other node), and may retrieve pattern information from the repository server <b>520</b> to make comparisons at the client <b>530</b>.</p>
<p id="p-0029" num="0028">In the drawings and specification, there have been disclosed exemplary embodiments of the invention. Although specific terms are employed, they are used in a generic and descriptive sense only and not for purposes of limitation, the scope of the invention being defined by the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>The invention claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method of identifying audio content, comprising:
<claim-text>storing in memory a library of humanly perceptible voice sounds;</claim-text>
<claim-text>receiving, by a processor, pattern control information associated with a device recording the audio content;</claim-text>
<claim-text>retrieving a pattern from the memory that is associated with the pattern control information;</claim-text>
<claim-text>inserting, by the processor, washing machine sounds into the audio content according to the pattern;</claim-text>
<claim-text>retrieving humanly perceptible voice sounds from the library of humanly perceptible voice sounds; and</claim-text>
<claim-text>inserting, by the processor, the humanly perceptible voice sounds into the audio content according to the pattern to generate a signed audio recording for distribution over a digital communications medium.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising querying the library of humanly perceptible voice sounds.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising receiving a location of the device recording the audio content.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising querying for the location of the device recording the audio content.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising retrieving a name of a person generating the signed audio recording.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A system for identifying audio content, the system comprising:
<claim-text>a processor; and</claim-text>
<claim-text>memory storing code that when executed causes the processor to perform operations, the operations comprising:</claim-text>
<claim-text>storing a library of humanly perceptible voice sounds;</claim-text>
<claim-text>receiving pattern control information associated with a device recording the audio content;</claim-text>
<claim-text>retrieving a pattern that is associated with the pattern control information;</claim-text>
<claim-text>inserting washing machine sounds into the audio content according to the pattern;</claim-text>
<claim-text>retrieving humanly perceptible voice sounds from the library of humanly perceptible voice sounds; and</claim-text>
<claim-text>inserting the humanly perceptible voice sounds into the audio content according to the pattern to generate a signed audio recording for distribution over a digital communications medium.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. The system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the operations further comprise querying the library of humanly perceptible voice sounds.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the operations further comprise receiving a location of the device recording the audio content.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The system according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the operations further comprise querying for the location of the device recording the audio content.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the operations further comprise retrieving a name of a person generating the signed audio recording.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. A memory storing program code that when executed causes a processor to perform operations, the operations comprising:
<claim-text>storing a library of humanly perceptible voice sounds;</claim-text>
<claim-text>receiving pattern control information associated with a device recording the audio content;</claim-text>
<claim-text>retrieving a pattern that is associated with the pattern control information;</claim-text>
<claim-text>inserting washing machine sounds into the audio content according to the pattern;</claim-text>
<claim-text>retrieving humanly perceptible voice sounds from the library of humanly perceptible voice sounds; and</claim-text>
<claim-text>inserting the humanly perceptible voice sounds into the audio content according to the pattern to generate a signed audio recording for distribution over a digital communications medium.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The memory according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the operations further comprise querying the library of humanly perceptible voice sounds.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. The memory according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the operations further comprise receiving a location of the device recording the audio content.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The memory according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the operations further comprise querying for the location of the device recording the audio content.</claim-text>
</claim>
</claims>
</us-patent-grant>
