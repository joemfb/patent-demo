<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626700-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626700</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13097615</doc-number>
<date>20110429</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>183</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>17</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>7</main-group>
<subgroup>08</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>706 59</main-classification>
<further-classification>706 16</further-classification>
</classification-national>
<invention-title id="d2e53">Context aware device execution for simulating neural networks in compute unified device architecture</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>5819245</doc-number>
<kind>A</kind>
<name>Peterson et al.</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>706 16</main-classification></classification-national>
</us-citation>
</us-references-cited>
<number-of-claims>8</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>None</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>8</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61329995</doc-number>
<date>20100430</date>
</document-id>
</us-provisional-application>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Monraz</last-name>
<first-name>Xavier</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Lewi</last-name>
<first-name>Jeremy M.</first-name>
<address>
<city>Burlingame</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Monraz</last-name>
<first-name>Xavier</first-name>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Lewi</last-name>
<first-name>Jeremy M.</first-name>
<address>
<city>Burlingame</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Pillsbury Winthrop Shaw Pittman LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>The Intellisis Corporation</orgname>
<role>02</role>
<address>
<city>San Diego</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Chaki</last-name>
<first-name>Kakali</first-name>
<department>2122</department>
</primary-examiner>
<assistant-examiner>
<last-name>Sitiriche</last-name>
<first-name>Luis</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A system, method, and computer program product are provided for efficient allocation of attributes corresponding to neurons or connections of multiple types using a common data structure. A map file is generated by a pre-processor in order to map an attribute of a neuron or connection to a particular location within the common data structure based on a type associated with the neuron or connection, while allowing a neuron or connection of a different type to map its own attribute to that same particular location. Additionally, kernel code can be written to reference attribute names made available by the map file in order to provide reusability of code.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="219.12mm" wi="171.96mm" file="US08626700-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="226.31mm" wi="162.14mm" orientation="landscape" file="US08626700-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="242.15mm" wi="170.35mm" orientation="landscape" file="US08626700-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="244.52mm" wi="168.40mm" orientation="landscape" file="US08626700-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="234.27mm" wi="170.69mm" orientation="landscape" file="US08626700-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="196.68mm" wi="144.02mm" file="US08626700-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="235.29mm" wi="171.03mm" file="US08626700-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="186.77mm" wi="159.43mm" file="US08626700-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="240.88mm" wi="195.07mm" file="US08626700-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">The present application claims the benefit of U.S. Provisional Application No. 61/329,995, filed Apr. 30, 2010, entitled &#x201c;Context Aware Device Execution for Simulating Neural Networks in Compute Unified Device Architecture&#x201d;, which is incorporated herein by reference in its entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND OF INVENTION</heading>
<p id="p-0003" num="0002">1. Field of the Invention</p>
<p id="p-0004" num="0003">The present invention relates generally to Single Instruction Multiple Data (&#x201c;SIMD&#x201d;) and Single Instruction Multiple Thread (&#x201c;SIMT&#x201d;) architectures and, in particular, processing of neural networks using a SIMD/SIMT architecture.</p>
<p id="p-0005" num="0004">2. Description of the Background Art</p>
<p id="p-0006" num="0005">Improving resource utilization of algorithms for SIMD architectures requires careful consideration of memory access patterns. In order to maintain a high level of parallelism, each resource has to maintain uniform reads and writes at any given instant. Additionally, conditional operations based on the content of memory need to be structured in a manner that reduces the divergence of processes or threads that would result in serialization of operations.</p>
<p id="p-0007" num="0006">Divergence in a SIMD/SIMT architecture refers to the divergence of two or more processors or threads. Divergence occurs when a set of processors or threads executing simultaneously encounters an instruction and/or data which causes a subset of threads to branch into a different execution path than the other threads. As a result, the threads are no longer synchronized (i.e., executing the same instructions) and can therefore not be run simultaneously on a SIMD/SIMT architecture. These considerations should be kept in mind when parallelizing operations associated with neural network processing.</p>
<p id="p-0008" num="0007">Accordingly, what is desired is an efficient memory structure for maintaining high parallelization of neural network operations and improving utilization of compute resources in a SIMD architecture.</p>
<heading id="h-0003" level="1">SUMMARY OF INVENTION</heading>
<p id="p-0009" num="0008">Embodiments of the invention include a method comprising generating a map file from a type definition associating a neuron or connection with an attribute, the map file configured to allocate a position in a buffer to the attribute and to allocate the position in the buffer to an additional attribute of an additional neuron or connection of a different type, and building executable code from kernel code and the map file.</p>
<p id="p-0010" num="0009">Further features and advantages of the invention, as well as the structure and operation of various embodiments of the invention, are described in detail below with reference to the accompanying drawings. It is noted that the invention is not limited to the specific embodiments described herein. Such embodiments are presented herein for illustrative purposes only. Additional embodiments will be apparent to persons skilled in the relevant art(s) based on the teachings contained herein.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0010">The accompanying drawings, which are incorporated herein and form a part of the specification, illustrate embodiments of the present invention and, together with the description, further serve to explain the principles of the invention and to enable a person skilled in the relevant art to make and use the invention.</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. 1</figref> is a processor architecture illustrating hierarchy of elements of a GPU, in accordance with an embodiment of the present invention.</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. 2</figref> is a data structure illustrating individual state vectors corresponding to J, each having all of the fields associated with each type, in accordance with an embodiment of the present invention.</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. 3</figref> is an abstract representation of a matrix data structure used to store state vectors for neurons and/or connections, in accordance with an embodiment of the present invention.</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. 4</figref> is a data structure illustrating vector padding to prevent thread divergence within a warp, in accordance with an embodiment of the present invention.</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart illustrating steps by which a data structure is allocated, in accordance with an embodiment of the present invention.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a schematic representation of interactions between various elements in an executable build process, in accordance with an embodiment of the present invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart illustrating the steps by which executable code is created from kernel code and connection or neuron type definitions, in accordance with an embodiment of the present invention.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 8</figref> depicts an example computer system in which embodiments of the present invention may be implemented.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0020" num="0019">The present invention will now be described with reference to the accompanying drawings. In the drawings, generally, like reference numbers indicate identical or functionally similar elements. Additionally, generally, the left-most digit(s) of a reference number identifies the drawing in which the reference number first appears.</p>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<heading id="h-0006" level="1">I. Introduction</heading>
<p id="p-0021" num="0020">The following detailed description of the present invention refers to the accompanying drawings that illustrate exemplary embodiments consistent with this invention. Other embodiments are possible, and modifications can be made to the embodiments within the spirit and scope of the invention. Therefore, the detailed description is not meant to limit the invention. Rather, the scope of the invention is defined by the appended claims.</p>
<p id="p-0022" num="0021">It would be apparent to one of skill in the art that the present invention, as described below, can be implemented in many different embodiments of software, hardware, firmware, and/or the entities illustrated in the figures. Any actual software code with the specialized control of hardware to implement the present invention is not limiting of the present invention. Thus, the operational behavior of the present invention will be described with the understanding that modifications and variations of the embodiments are possible, and within the scope and spirit of the present invention.</p>
<p id="p-0023" num="0022">One skilled in the relevant arts will appreciate that the techniques described herein can be applied to a variety of SIMD architectures, including the SIMT architecture provided by the Compute Unified Device Architecture (&#x201c;CUDA&#x201d;) developed by NVIDIA C<smallcaps>ORPORATION </smallcaps>of Santa Clara, Calif. Reference to either SIMD or SIMT in this context is therefore intended to describe non-limiting applicability to a wide range of SIMD and other architectures, and is not limited to applications specific to CUDA.</p>
<p id="p-0024" num="0023">At a very basic level, a neural network operates essentially as a set of discrete elements (neurons) that are connected to one another. Each neuron is typically connected to a small fraction of all the neurons in the network. A set of states and parameters, potentially changing with time, are associated with each neuron and connection. This association can be expressed by saying that each neuron is described by some vector, {right arrow over (r)}<sub>j</sub>, where the subscript identifies the neuron. Similarly, each connection is described by its own vector {right arrow over (w)}<sub>k </sub>where the subscript k identifies the connection.</p>
<p id="p-0025" num="0024">In a neural network, each connection is directed. This means that, for each connection, one neuron is designated as the start of the connection (&#x201c;pre-synaptic neuron&#x201d;) and the other neuron is designated as the terminal for the connection (&#x201c;post-synaptic neuron&#x201d;). The pre- and post-synaptic nomenclature is obtained from the neuroscience community, and refers to the fact that biological molecules (neuro-transmitters) travel from the pre-synaptic neuron to the post-synaptic neuron. A neural network is therefore defined by the following data:</p>
<p id="h-0007" num="0000">R={{right arrow over (r)}<sub>i</sub>: i&#x3b5;1, 2, . . . , N<sub>r</sub>}</p>
<p id="h-0008" num="0000">W={{right arrow over (w)}<sub>m</sub>: m&#x3b5;1, 2, . . . N<sub>w</sub>} C={(i, j, m): m&#x3b5;1, 2, . . . , N<sub>w</sub>}</p>
<p id="p-0026" num="0025">Here, N<sub>r </sub>is the number of neurons in the network and N<sub>w </sub>is the number of connections. R is the set of state vectors describing the neurons. W is the set of state vectors describing the connections. C is a set of ordered triplets. Each of these triplets describes one connection in the network. The first value in this triplet, i, specifies the index of the pre-synaptic neuron. j is the index of the post-synaptic neuron. m is an index which identifies the connection, i.e. m has a unique value for each ordered triplet (i,j,m). Each of these indexes corresponds to a state vector {right arrow over (r)}<sub>i</sub>, {right arrow over (r)}<sub>j</sub>, {right arrow over (w)}<sub>m </sub>which contains the states and parameters of the pre-synaptic, post-synaptic and connection respectively. In accordance with an embodiment of the present invention, there is a one-to-one association between the weights and the connections. Each connection has one weight associated with it.</p>
<p id="p-0027" num="0026">A neural network operates by iteratively updating the state vectors for the neurons and connections. Each iteration is referred to as an epoch or time step. At each time step we update the state vector for each neuron j by evaluating some function &#x192;<sub>j </sub>for that neuron. After updating all neurons, each connection m is updated by evaluating some function g<sub>m</sub>. Input is provided to the network by allowing the function &#x192;<sub>j </sub>for some neurons to depend on an input signal {right arrow over (I)}. The key idea of a neural network is that the connections determine which neurons directly influence the state vectors of other neurons. Furthermore, this interaction is modulated by the state values of that connection, {right arrow over (w)}<sub>m</sub>. This is described in more detail below.</p>
<p id="p-0028" num="0027">At each time step, the state vector for neuron j is updated by executing some function &#x192;<sub>j</sub>. The output of this function is the new value for the neuron's state vector, {right arrow over (r)}<sub>j</sub>. This function depends on the current value of the state vector {right arrow over (r)}<sub>j </sub>as well as all connections (and the associated pre-synaptic neuron) for which neuron j is the post-synaptic neuron. We can therefore write the update for neuron j as</p>
<p id="p-0029" num="0028">{right arrow over (r)}<sub>j</sub>&#x2190;&#x192;<sub>j</sub>({right arrow over (r)}<sub>i</sub>,C<sub>j</sub>) where C<sub>j</sub>={({right arrow over (r)}<sub>i</sub>, {right arrow over (w)}<sub>m</sub>): for all m such that (i, j, m)&#x3b5;C}</p>
<p id="p-0030" num="0029">The set C<sub>j </sub>denotes the set of all pairs of pre-synaptic state vectors {right arrow over (r)}<sub>i </sub>and connection state vectors {right arrow over (w)}<sub>m </sub>for which there exists a connection from neuron i to neuron j. The arrow means the value of {right arrow over (r)}<sub>j </sub>is replaced with the result of evaluating &#x192;<sub>j</sub>.</p>
<p id="p-0031" num="0030">The function &#x192;<sub>j </sub>can be almost any function that satisfies the following constraints. For each connection, the function &#x192;<sub>j </sub>can only depend on some intermediary values, {right arrow over (a)}<sub>m</sub>, which are a function only of ({right arrow over (r)}<sub>i</sub>, {right arrow over (r)}<sub>j</sub>, {right arrow over (w)}<sub>m</sub>) for connection m. Furthermore, it must be possible to serially accumulate the contributions to {right arrow over (r)}<sub>j </sub>from all connections using a fixed amount of memory which is independent of the number of connections. This is described more exactly with the following pseudo code for &#x192;<sub>j </sub></p>
<p id="p-0032" num="0031">Function &#x192;<sub>j </sub></p>
<p id="p-0033" num="0032">For each ({right arrow over (r)}<sub>i</sub>, {right arrow over (w)}<sub>m</sub>)&#x3b5;C<sub>j </sub>
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0033">{right arrow over (a)}<sub>j</sub>&#x2190;&#x3bc;<sub>m</sub>({right arrow over (r)}<sub>i</sub>, {right arrow over (r)}<sub>j</sub>, {right arrow over (w)}<sub>m</sub>)</li>
        <li id="ul0002-0002" num="0034">{right arrow over (b)}<sub>j</sub>&#x2190;&#x3bd;<sub>j</sub>({right arrow over (a)}<sub>j</sub>, {right arrow over (b)}<sub>j</sub>)</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0034" num="0035">Return &#x3be;<sub>j</sub>({right arrow over (r)}<sub>j</sub>, {right arrow over (b)}<sub>j</sub>)</p>
<p id="p-0035" num="0036">Here &#x3bc;<sub>m</sub>, &#x3bd;<sub>j </sub>and &#x3be;<sub>j </sub>are some vector-valued functions which return vectors. {right arrow over (a)}<sub>j </sub>is the contribution to neuron j from connection m and pre-synaptic neuron i. {right arrow over (b)}<sub>j </sub>quantifies the total effect of all contributions. The only restriction on &#x192;<sub>j </sub>is that it must be possible to compute the effect of all contributions to neuron j by processing each connection one by one. After processing each connection, {right arrow over (b)}<sub>j </sub>is updated, and is sufficient to describe the contributions of all connections processed so far. The &#x201c;For&#x201d; loop therefore accumulates the contributions of all connections. After iterating over all connections terminating on neuron j, the new value for {right arrow over (r)}<sub>j </sub>is computed by evaluating function &#x3be;<sub>j</sub>. This function depends on the current state j, {right arrow over (r)}<sub>j</sub>, and the combined effect of all of neuron j's connections as measured by {right arrow over (b)}<sub>j</sub>. As a result, this demonstrates that the amount of memory needed to implement &#x192;<sub>j </sub>is independent of the number of connections.</p>
<p id="p-0036" num="0037">In the pseudo code for &#x192;<sub>j</sub>, no order for processing the connections is given. However, for some functions/connections the connections might be processed in a specific order.</p>
<p id="p-0037" num="0038">A fundamental feature of neural networks is that they can learn and adapt. Mathematically, this means the states and parameters of the neurons and connections can change with time as a function of the input and their dynamics. For the neuron, learning involves choosing appropriate functions &#x3bc;<sub>m</sub>, &#x3bd;<sub>j</sub>, and &#x3be;<sub>j</sub>. For connections, learning involves letting the state vector {right arrow over (w)}<sub>m </sub>change with time. In accordance with an embodiment of the present invention, at each time step the connection is updated by:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0039">{right arrow over (w)}<sub>m</sub>&#x2190;&#x3c8;<sub>m</sub>({right arrow over (r)}<sub>i</sub>, {right arrow over (r)}<sub>j</sub>, {right arrow over (w)}<sub>m</sub>)</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0038" num="0040">Here &#x3c8;<sub>m </sub>is some function which determines how the connection changes. The only restriction on &#x3c8;<sub>m </sub>is that it is a function of the connection, {right arrow over (w)}<sub>m </sub>and the pre and post-synaptic neurons ({right arrow over (r)}<sub>i</sub>, {right arrow over (r)}<sub>j</sub>).</p>
<p id="p-0039" num="0041">In the description of a neural network above, each neuron and connection has a unique set of functions to describe its evolution. In practice, most networks contain a few types of connections and neurons. Each type of neuron or connection would be associated with a particular function &#x192; for the neuron type or &#x3c8; for the connection.</p>
<p id="p-0040" num="0042">Applying a neural network to a problem, such as classifying images, entails choosing suitable quantities to encode in the state vectors ({right arrow over (r)}<sub>i</sub>, {right arrow over (r)}<sub>j</sub>, {right arrow over (w)}<sub>m</sub>) as well as the appropriate functions &#x3bc;<sub>j</sub>, &#x3bd;<sub>m</sub>, &#x3be;<sub>j</sub>, and &#x3c8;<sub>m </sub>for updating these quantities.</p>
<p id="p-0041" num="0043">Neural networks provide a very general framework in which a multitude of algorithms may be chosen depending on the choice of functions and the quantities encoded in the state vectors. These include, by way of example and not limitation, slow feature analysis, ICA, PCA, and sparse coding. The functions need not be deterministic.</p>
<p id="p-0042" num="0044">The networks described above are very heterogeneous (i.e. they contain many different types of neurons and connections). Each connection and neuron has its own set of equations (i.e. the functions &#x3bc;, &#x3bd;, &#x3be;) which are used to update the neuron/connection. Similarly each connection and neuron might store different numbers and types of parameters (i.e. the state vectors {right arrow over (r)}<sub>j </sub>and {right arrow over (w)}<sub>m </sub>would have different lengths and store different values at each entry depending on the neuron/connection type).</p>
<p id="p-0043" num="0045">Heterogeneity provides an impediment to maximizing the usage of computational resources of a SIMD architecture. A SIMD architecture simultaneously executes a single instruction on multiple data. To illustrate these challenges, an illustrative example is provided in order to consider how a homogenous network with one type of neuron and one type of connection could be parallelized for a SIMD architecture.</p>
<p id="p-0044" num="0046">Exemplary high-level pseudo code for running a homogeneous neural network is provided below. The subscripts on the functions h, &#x3c8;, g, k are deleted in the pseudocode because the functions are assumed to be the same for all instances, since the network is assumed to be homogenous.</p>
<p id="p-0045" num="0047">For each {right arrow over (r)}<sub>j</sub>&#x3b5;R
<ul id="ul0005" list-style="none">
    <li id="ul0005-0001" num="0000">
    <ul id="ul0006" list-style="none">
        <li id="ul0006-0001" num="0048">For each ({right arrow over (r)}<sub>i</sub>, {right arrow over (w)}<sub>m</sub>)&#x3b5;C<sub>j </sub>
        <ul id="ul0007" list-style="none">
            <li id="ul0007-0001" num="0049">{right arrow over (a)}<sub>j</sub>&#x2190;&#x3bc;({right arrow over (r)}<sub>i</sub>, {right arrow over (r)}<sub>j</sub>, {right arrow over (w)}<sub>m</sub>)</li>
            <li id="ul0007-0002" num="0050">{right arrow over (w)}<sub>m</sub>&#x2190;&#x3c9;({right arrow over (r)}<sub>i</sub>, {right arrow over (r)}<sub>j</sub>, {right arrow over (w)}<sub>m</sub>)</li>
            <li id="ul0007-0003" num="0051">{right arrow over (b)}<sub>j</sub>&#x2190;&#x3bd;({right arrow over (a)}<sub>j</sub>, {right arrow over (b)}<sub>j</sub>)</li>
        </ul>
        </li>
        <li id="ul0006-0002" num="0052">{right arrow over (r)}<sub>j</sub>&#x2190;&#x3be;({right arrow over (r)}<sub>j</sub>, {right arrow over (b)}<sub>j</sub>)</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0046" num="0053">To parallelize this code, iterations of the outer-most loop (which is executed once for each neuron) are executed in parallel. This means that N<sub>p </sub>neurons are processed in parallel, where N<sub>p </sub>is the number of threads or processors available in a target architecture.</p>
<p id="p-0047" num="0054">The above implementation presents a challenge for SIMD architectures, as each neuron j will have a variable number of incoming connections (i.e. the number of connections, |C<sub>j</sub>|, will vary for each neuron). As a result, the number of iterations for the inner loop would vary for each neuron, such that one thread may have to execute the inner loop five times while another thread would have to execute the inner loop ten times. As a result, it would no longer be possible to maintain synchronization of the two threads, presenting implementation problems for a SIMD architecture.</p>
<p id="p-0048" num="0055">One way to solve this problem is to sort the neurons by the number of connections and to simultaneously process all neurons with a given number of connections. This is illustrated by the following example pseudo-code:</p>
<p id="p-0049" num="0056">For N<sub>c</sub>=0, 1, . . . , max<sub>j</sub>|C<sub>j</sub>|
<ul id="ul0008" list-style="none">
    <li id="ul0008-0001" num="0000">
    <ul id="ul0009" list-style="none">
        <li id="ul0009-0001" num="0057">R<sub>N</sub><sub><sub2>c</sub2></sub>={{right arrow over (r)}<sub>j</sub>: where |C<sub>j</sub>|=N<sub>c</sub>}</li>
        <li id="ul0009-0002" num="0058">For {right arrow over (r)}<sub>j</sub>&#x3b5;R<sub>N</sub><sub><sub2>c </sub2></sub>
        <ul id="ul0010" list-style="none">
            <li id="ul0010-0001" num="0059">For ({right arrow over (r)}<sub>i</sub>, {right arrow over (w)}<sub>m</sub>)&#x3b5;C<sub>j </sub>
            <ul id="ul0011" list-style="none">
                <li id="ul0011-0001" num="0060">{right arrow over (a)}<sub>j</sub>&#x2190;&#x3bc;({right arrow over (r)}<sub>i</sub>, {right arrow over (r)}<sub>j</sub>, {right arrow over (w)}<sub>m</sub>)</li>
                <li id="ul0011-0002" num="0061">{right arrow over (w)}<sub>m</sub>&#x2190;&#x3c8;({right arrow over (r)}<sub>i</sub>, {right arrow over (r)}<sub>j</sub>, {right arrow over (w)}<sub>m</sub>)</li>
                <li id="ul0011-0003" num="0062">b<sub>j</sub>&#x2190;&#x3bd;({right arrow over (a)}<sub>j</sub>, {right arrow over (b)}<sub>j</sub>)</li>
                <li id="ul0011-0004" num="0063">{right arrow over (r)}<sub>j</sub>&#x2190;&#x3be;({right arrow over (r)}<sub>j</sub>, {right arrow over (b)}<sub>j</sub>)</li>
            </ul>
            </li>
        </ul>
        </li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0050" num="0064">In the outer loop, every possible value of incoming connections for the network is considered. In the second loop iterating over R<sub>N</sub><sub><sub2>c</sub2></sub>, all neurons having N<sub>c </sub>incoming connections are considered. Since all such neurons have the same number of incoming connections, the inner most loop over C<sub>j </sub>would have to be executed the same number of times. This provides the ability to parallelize the second loop, as for a given value of N<sub>c</sub>, each thread or processor in a SIMD architecture would process a different neuron with N<sub>c</sub>, incoming connections. Since the innermost loop would be executed the same number of times for all threads, and the functions are the same for all threads, all threads would be synchronized.</p>
<p id="p-0051" num="0065">The proposed solution, discussed in detail herein, goes well beyond this basic architecture in order to efficiently handle heterogeneous neural networks on SIMD/SIMT platforms. Context Aware Device Execution (&#x201c;CADE&#x201d;) is an architecture comprising several elements, in accordance with an embodiment of the present invention, including data structures for efficient representation of neural networks on SIMD architectures, mechanisms for accessing these data structures within the code (or &#x201c;kernels&#x201d;, functions which are executed in parallel on different data in a SIMD/SIMT architecture) executed on each processor in a SIMD/SIMT architecture, a modular organization of the various source files and build processes in order to facilitate reprogrammability and the definition of new elements, and an enhanced or optimal ordering of neurons and connections to maximize the utilization of compute resources on a SIMD architecture.</p>
<heading id="h-0009" level="1">II. CUDA Architecture</heading>
<p id="p-0052" num="0066">In an embodiment, an NVIDIA Graphics Processing Unit (&#x201c;GPU&#x201d;) performs General Purpose GPU (&#x201c;GPGPU&#x201d;) computation through CUDA. CUDA presents the GPU as a hierarchical device composed of cores, multiprocessors, and ultimately a grid. To take advantage of the given hardware model, the software model used on top of these devices has to conform to CUDA's SIMT paradigm.</p>
<p id="p-0053" num="0067">The lowest level of managed execution in CUDA is a block. A multiprocessor processes a number of blocks which each contain a number of threads. A multiprocessor executes each block independently of the rest and, similarly, a block is split into groupings of threads termed &#x201c;warps&#x201d;. The threads of a warp are executed in parallel and, therefore, any divergence within a warp results in serialization of CUDA core operations. Because of the nature of this architecture, significant performance improvements are achieved by minimizing the amount of divergence in a warp. By way of example, and not limitation, in NVIDIA's Fermi architecture, the warp size is 32 threads. Accordingly, any implementation on CUDA in this architecture should be expected not to diverge within every set of 32 threads.</p>
<p id="p-0054" num="0068"><figref idref="DRAWINGS">FIG. 1</figref> is a processor architecture <b>100</b> illustrating hierarchy of elements of a GPU <b>102</b>, in accordance with an embodiment of the present invention. The GPU is comprised of multiprocessors <b>104</b>, each of which has CUDA cores <b>106</b>, in accordance with a further embodiment of the present invention.</p>
<p id="p-0055" num="0069">Software executed in multiprocessors <b>104</b> is called a kernel. A kernel tells the device what to do with one specific thread in the entire thread pool, such as what data to access, what mathematical operations to perform, and where to write out results. Further information regarding development on CUDA is found in the &#x201c;NVIDIA CUDA Programming Guide&#x201d; from NVIDIA CORPORATION, Santa Clara, Calif. (2010) (http://developer.download.nvidia.com/compute/cuda/1<sub>&#x2014;</sub>0/NVIDIA_CUDA_Programming_Guide<sub>&#x2014;</sub>1.0.pdf).</p>
<p id="p-0056" num="0070">There are also preferred access patterns to data to minimize the total number of memory operations to increase throughput. These operations are meant to facilitate keeping the multiprocessors loaded with global data as efficiently as possible. The time it takes to access data in global memory compared to memory on chip is about 2 orders of magnitude. Hiding this global memory latency can only be accomplished by taxing the arithmetic units and loading the global memory as quickly as possible. When bank conflicts occur, memory request operations are queued up to be performed serially which also negatively affects throughput.</p>
<heading id="h-0010" level="1">III. Na&#xef;ve Implementation of a Heterogeneous Neural Network in a SIMD Architecture</heading>
<p id="p-0057" num="0071">A na&#xef;ve implementation of a heterogeneous network is to represent it as a homogenous network consisting of super neurons and connections. These super neurons and connections are formed by concatenating together all of the code and data structures for the different types of neurons and connections.</p>
<p id="p-0058" num="0072">Suppose we have three types of neurons (Type A, Type B, and Type C). Each Type has associated with it a different set of fields. A single state vector {right arrow over (r)}<sub>j </sub>is defined, capable of representing all neuron Types. In order to handle this, {right arrow over (r)}<sub>j </sub>has one component for each field of Type A, Type B, and Type C.</p>
<p id="p-0059" num="0073"><figref idref="DRAWINGS">FIG. 2</figref> is a data structure <b>200</b> illustrating individual state vectors <b>208</b><i>a</i>, <b>208</b><i>b</i>, <b>210</b><i>a</i>, <b>210</b><i>b</i>, <b>212</b><i>a</i>, <b>212</b><i>b</i>, <b>208</b><i>c</i>, <b>210</b><i>c</i>, <b>212</b><i>c</i>, <b>212</b><i>d</i>, <b>210</b><i>d</i>, and <b>212</b><i>e </i>corresponding to {right arrow over (r)}<sub>j</sub>, each having all of the fields associated with each type (e.g., Type A fields <b>202</b>, Type B fields <b>204</b>, and Type C fields <b>206</b>), in accordance with an embodiment of the present invention. Rows 1-4 <b>202</b> are used by connections of Type A while rows 5-6 <b>204</b> are used by Type B, and rows 7-9 <b>206</b> are used by Type C.</p>
<p id="p-0060" num="0074">The column headings are used to indicate which type each column is an instance of. For example, columns <b>208</b><i>a </i>and <b>208</b><i>b </i>are instances of Type A. Column <b>210</b><i>c </i>is an instance of Type B. Thus, column <b>208</b><i>a </i>only uses the first 4 rows because it is an instance of Type A. By way of illustration, all of the columns designated <b>208</b> are of Type A, <b>210</b> are of Type B, and <b>212</b> are of Type C.</p>
<p id="p-0061" num="0075">Note the matrix of data structure <b>200</b> described here provides an abstract representation of the data structure representing the connection and neuron state vectors. In practice there is probably one such matrix for the neurons and a separate one for the connections. Both, however, would have the same abstract structure, in accordance with an embodiment of the present invention. Furthermore, one skilled in the relevant arts would recognize that this matrix could be implemented a number of ways, such as using a set of one-dimensional arrays with one array for each row, a two-dimensional array, or an array of structures with one field in the structure for each row. The precise nature of the data structure <b>200</b> of <figref idref="DRAWINGS">FIG. 2</figref> is therefore provided by way of example, and not limitation.</p>
<p id="p-0062" num="0076">Similarly, each neuron type has associated with it its own set of functions, e.g. k corresponding to Type A, k<sub>b </sub>corresponding to Type B, and k, corresponding to Type C, by way of example and not limitation. A single homogenous k function for all neurons can therefore be created by concatenating the code together as illustrated by the following pseudo-code:</p>
<p id="p-0063" num="0077">Def k({right arrow over (r)}<sub>j</sub>,{right arrow over (b)}<sub>j</sub>):
<ul id="ul0012" list-style="none">
    <li id="ul0012-0001" num="0000">
    <ul id="ul0013" list-style="none">
        <li id="ul0013-0001" num="0078">If {right arrow over (r)}<sub>j </sub>is type A:
        <ul id="ul0014" list-style="none">
            <li id="ul0014-0001" num="0079">q&#x2190;k<sub>A</sub>({right arrow over (r)}<sub>j</sub>,{right arrow over (b)}<sub>j</sub>)</li>
        </ul>
        </li>
        <li id="ul0013-0002" num="0080">If {right arrow over (r)}<sub>j </sub>is type B:
        <ul id="ul0015" list-style="none">
            <li id="ul0015-0001" num="0081">q&#x2190;k<sub>B</sub>({right arrow over (r)}<sub>j</sub>,{right arrow over (b)}<sub>j</sub>)</li>
        </ul>
        </li>
        <li id="ul0013-0003" num="0082">If {right arrow over (r)}<sub>j </sub>is type C:
        <ul id="ul0016" list-style="none">
            <li id="ul0016-0001" num="0083">q&#x2190;k<sub>A</sub>({right arrow over (r)}<sub>j</sub>,{right arrow over (b)}<sub>j</sub>)</li>
        </ul>
        </li>
        <li id="ul0013-0004" num="0084">Return q</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0064" num="0085">This na&#xef;ve implementation has two major disadvantages. First, as <figref idref="DRAWINGS">FIG. 2</figref> shows (see, e.g., the A fields <b>202</b> for Type B vectors <b>210</b><i>a </i>and <b>210</b><i>b</i>), a significant amount of space is wasted in the data structure. In particular, as the number of types increases, an increasing fraction of space in the data structure is wasted. Second, the super functions, such as super function k of the above pseudo-code, end up containing several branch statements, such as the three branch statements in the pseudo-code example. For a given neuron, only one IF statement is executed since each neuron has a single type. In most SIMD architectures, this branching is likely to lead to inefficient implementations which in the worst case provide no speedup over a serial implementation.</p>
<p id="p-0065" num="0086">In an exemplary embodiment, three parallel processing resources are available, all executing the same super function k as in an exemplary SIMD architecture, with the neuron executing on the first processing resource being of Type A, the neuron on the second processing resource being of Type B, and the neuron on the third processing resource being of Type C. The first processing resource will drop into the IF statement for Type A. On most SIMD architectures, the other processors will have to block while the first processing resource completes its work so that all processing resources can remain synchronized. Similarly, when a second thread of execution operating on the second processing resource enters into the IF statement for Type B, the other two processing resources will block while the second processing resource executes.</p>
<p id="p-0066" num="0087">To prevent this type of inefficient usage, the neurons should be ordered such that each processor is executing a neuron or connection of the same type, thereby eliminating divergence without the blocking situations caused by this na&#xef;ve implementation.</p>
<heading id="h-0011" level="1">IV. Efficient Data Structure</heading>
<p id="p-0067" num="0088">CADE provides an approach to efficiently store the state vectors for the connections and neurons, in accordance with an embodiment of the present invention. <figref idref="DRAWINGS">FIG. 3</figref> is an abstract representation of a matrix data structure <b>300</b> used to store state vectors for neurons and/or connections, in accordance with an embodiment of the present invention. As noted above, in a typical application, there would be one such matrix for the neurons and another matrix for the connections.</p>
<p id="p-0068" num="0089">The contents of data structure <b>300</b> are similar to those of data structure <b>200</b> of <figref idref="DRAWINGS">FIG. 2</figref>, but the amount of wasted space has been greatly reduced. This is accomplished by determining a number of rows for the matrix <b>300</b> corresponding to a maximum number of fields from each of Types A <b>308</b>, B <b>310</b>, or C <b>312</b>, in accordance with an embodiment of the present invention. Thus, in the non-limiting example shown in <figref idref="DRAWINGS">FIG. 3</figref>, the number of fields <b>302</b> is determined to be four, based on the largest number of fields being the four fields <b>202</b> of <figref idref="DRAWINGS">FIG. 2</figref> associated with Type A. As any other Type (e.g., Type B, having two fields, and Type C, having three fields) can be represented in the space of four fields, this allows data structure <b>300</b> to avoid allocation of unused space. In accordance with an embodiment of the present invention, each field corresponds to a B-byte data word, where each field is of equal size as the other fields.</p>
<p id="p-0069" num="0090">As different data is now stored using equivalent fields (e.g., the first field of a Type A vector <b>308</b> and the first field of a Type B vector <b>310</b>), a different mechanism than absolute position is needed in order to distinguish the contents of a particular field. In the na&#xef;ve implementation of data structure <b>200</b> of <figref idref="DRAWINGS">FIG. 2</figref>, it was known a priori that data for a Type B vector <b>210</b><i>a </i>was stored strictly in the B fields <b>204</b>.</p>
<p id="p-0070" num="0091">In data structure <b>300</b>, the contents of a particular field of K fields <b>302</b> are instead determined based on the type of neuron or connection, in accordance with an embodiment of the present invention. Each column in the matrix represents an instance of a particular type of neuron or connection (i.e., a vector). A column could be represented, by way of example and not limitation, by having a separate array designating the type for each column or by designating the first B-byte word to represent the type and using K+1 rows in the matrix, although one skilled in the relevant arts will appreciate that other ways of designating the type for a particular column may be used. Assignments of each of the K fields <b>302</b> to different parameters for a given neuron or connection type is determined by map files, which are generated from neuron or connection definition files, as will be described in further detail below.</p>
<p id="p-0071" num="0092">An alternative implementation to the data structure <b>300</b> of <figref idref="DRAWINGS">FIG. 3</figref> would be to have separate matrices for each type, representing each unique type by a matrix with a number of rows dependent on the type, or alternatively using an array of structures with a unique structure for each type. However, when copying or moving data there is a need to be aware of the different types of neurons or connections, making the code for managing the data in this alternative implementation more complex. Furthermore, it generally would be necessary to issue a separate copy/move operation for each type. In many architectures, doing block reads (i.e. writing/reading contiguous memory) is significantly more efficient than random reads. Thus, an advantage of the data structure <b>300</b> of <figref idref="DRAWINGS">FIG. 3</figref> is that it is possible to read/write connections or neurons using block reads/writes without needing to have knowledge about their type.</p>
<p id="p-0072" num="0093">Moreover, data structure <b>300</b> of <figref idref="DRAWINGS">FIG. 3</figref> provides easier lookup of a particular neuron or connection over separate structures for each type. For example, to retrieve the data for the i<sup>th </sup>neuron or connection, it is found at the i<sup>th </sup>column of the matrix data structure <b>300</b>. In contrast, using separate matrices for each type, it would be necessary to know a priori which type of neuron or connection the i<sup>th </sup>neuron or connection was, and then look it up in the appropriate matrix.</p>
<p id="p-0073" num="0094">In accordance with an embodiment of the present invention, an algorithm for constructing a mapping, such as shown by data structure <b>300</b>, is to use hierarchical clustering. Nodes are constructed for each neuron or connection type being mapped. Additionally, a list of unique fields formed from the union of the fields from all neuron or connection types is also constructed. This list of fields is then sorted in ascending order based on the number of types which have each field, in accordance with an embodiment of the present invention.</p>
<p id="p-0074" num="0095">Hierarchical clustering is then applied to cluster the types and form trees based on this sorted list of fields, in accordance with an embodiment of the present invention. For each field, each of the corresponding neuron or connection types having this field is selected. For each of the selected types, the tree structure is traversed up to the root node to identify all types containing this field. A common parent node is then added to all these root nodes, in accordance with an embodiment of the present invention. As a result, all of the types containing this field are now joined together by a common ancestor, although one skilled in the relevant arts will appreciate that some of the types may already have been in the same tree structure. This new node is then labeled with the field. The aforementioned steps are then repeated until all fields have been processed.</p>
<p id="p-0075" num="0096">The end result of this clustering is a tree or set of trees, with each tree representing a disjoint set of fields (i.e., the fields in one tree do not belong to any of the types in another tree), in accordance with an embodiment of the present invention.</p>
<p id="p-0076" num="0097">In accordance with an embodiment of the present invention, each field is assigned to an integer K recursively by the following exemplary non-limiting algorithm:
<ul id="ul0017" list-style="none">
    <li id="ul0017-0001" num="0000">
    <ul id="ul0018" list-style="none">
        <li id="ul0018-0001" num="0098">1. For each of the trees produced in the above initialize K=0.</li>
        <li id="ul0018-0002" num="0099">2. Take the root node of this tree and assign the field associated with this node a value of K (K will be the offset to which the field is mapped).</li>
        <li id="ul0018-0003" num="0100">3. Remove this node from the tree, producing one or more subtrees.</li>
        <li id="ul0018-0004" num="0101">4. If the subtrees just contain the type nodes then stop; processing is complete.</li>
        <li id="ul0018-0005" num="0102">5. Otherwise, recursively process each subtree by setting K to K+1 (i.e remove the subtree's root node, assign the corresponding field to K+1, and repeat this process on the subtrees until the remaining subtrees just contain the type nodes).</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0077" num="0103">One advantage of this algorithm is that fields shared by multiple types are assigned to the same position, in accordance with an embodiment of the present invention, minimizing divergence on a SIMD/SMIT architecture. In a neural network the kernels for connections need access to the fields of its pre- and post-synaptic neurons. In many instances, a number of neuron types will all have a common field, such as &#x201c;activity.&#x201d; Furthermore, it may be desirable to define a connection that behaves exactly the same between any two neurons containing the field &#x201c;activity&#x201d; (e.g a connection that takes the pre-synaptic activity, scales it, and adds it to the post-synaptic activity).</p>
<p id="p-0078" num="0104">The algorithm above ensures that all types defining the activity of a neuron store the activity in the same location. Thus, when processing this activity connection for a set of neurons, it is possible to perform coordinated reads, where the same position in the buffer is read, by way of example and not limitation, to access the &#x201c;activity&#x201d; field for all connections regardless of actual neuron type. Coordinated memory access typically provides improved access speeds on many computer architectures.</p>
<heading id="h-0012" level="1">V. Ordering of Neurons and Connections</heading>
<p id="p-0079" num="0105">As noted above, the CUDA multiprocessor operated on groups of threads at a time, the groups of threads known as &#x201c;warps&#x201d;. Parallelism can be exploited, in accordance with an embodiment of the present invention, at the warp level, and at the block level (groups of warps). The efficient parallel processing of threads of a warp, based on the reasons discussed above, is obtained by preventing divergence within any given warp of threads that may cause serialization of operations among the threads. This is accomplished by maintaining the same execution code within each warp.</p>
<p id="p-0080" num="0106"><figref idref="DRAWINGS">FIG. 4</figref> is a data structure <b>400</b> illustrating vector padding to prevent thread divergence within a warp, in accordance with an embodiment of the present invention. By aligning all the neurons or connections (depending on the contents of the data structure <b>400</b>) of a certain type into groups of the warp size, there will be an implicit guarantee that no neuron processed in any given warp will diverge from the other neurons in that warp. <figref idref="DRAWINGS">FIG. 4</figref> illustrates how CADE would allocate context specific fields. Black boxes <b>409</b> and <b>411</b> in <figref idref="DRAWINGS">FIG. 4</figref> denote virtualized padding. Virtualized padding is accomplished, in accordance with an embodiment of the present invention, through the use of an array &#x201c;warp_arr&#x201d; which contains either the index in the data to use, or an invalid value which tells the kernel that this index is used as padding, although one skilled in the relevant arts will recognize that other techniques for indicating to a processing resource that padding is present.</p>
<p id="p-0081" num="0107">A multiprocessor inside the GPU schedules warps of threads within a block of threads to be executed at a time. The padding scheme of <figref idref="DRAWINGS">FIG. 4</figref> prevents divergence within a warp by pushing any thread that would not take the same path as all other threads in the current warp up to the next warp. For example, the execution of Type A <b>408</b> neurons or connections would diverge from that of Type B <b>410</b> neurons or connections. Accordingly, it is desirable to have all of the threads of execution for Type A <b>408</b> occur within a single warp, without also having to process any threads of execution for Type B <b>410</b>.</p>
<p id="p-0082" num="0108">In the non-limiting exemplary embodiment shown in <figref idref="DRAWINGS">FIG. 4</figref>, a warp size <b>404</b> of five threads is depicted, although one skilled in the relevant arts will realize that any warp size <b>404</b> can be used based on, for example, the processing resources available to be allocated in parallel. Since there are only three Type A <b>408</b> vectors corresponding to individual neurons or connections available for processing, padding <b>409</b> is introduced, resulting in the total of Type A <b>408</b> vectors and padding <b>409</b> being equal to warp size <b>404</b>. Padding <b>409</b> has the effect of pushing execution of Type B <b>410</b> neurons or connections to the next warp. Similarly, with four Type B <b>410</b> neurons or connections, padding <b>411</b> is needed to match the warp size <b>404</b> of five, pushing execution of Type C <b>412</b> neurons to the next warp.</p>
<p id="p-0083" num="0109">Below is a non-limiting exemplary code sectopm demonstrating the use of warp_arr and how the virtualized padding would be used:</p>
<p id="p-0084" num="0110">_global_void</p>
<p id="p-0085" num="0111">updateCell(int number_of_cells, int* cell_types, int* warp_arr) {
<ul id="ul0019" list-style="none">
    <li id="ul0019-0001" num="0000">
    <ul id="ul0020" list-style="none">
        <li id="ul0020-0001" num="0112">//thread and block information is automatically available</li>
        <li id="ul0020-0002" num="0113">int local_inx=threadIdx.x+blockIdx.x* blockDim.x;</li>
        <li id="ul0020-0003" num="0114">if (local_inx&#x3c;number_of_cells) {
        <ul id="ul0021" list-style="none">
            <li id="ul0021-0001" num="0115">int cell_inx=warp_arr[local_inx];</li>
            <li id="ul0021-0002" num="0116">int type=cell_types [cell_inx];</li>
            <li id="ul0021-0003" num="0117">switch(type) {</li>
            <li id="ul0021-0004" num="0118">case BASIC_CELL:
            <ul id="ul0022" list-style="none">
                <li id="ul0022-0001" num="0119">basic_cell_update(cell_inx);</li>
                <li id="ul0022-0002" num="0120">break;</li>
            </ul>
            </li>
            <li id="ul0021-0005" num="0121">case FIRING_THRESH_CELL:
            <ul id="ul0023" list-style="none">
                <li id="ul0023-0001" num="0122">firing_thresh_cell_update(cell_inx);</li>
                <li id="ul0023-0002" num="0123">break;</li>
            </ul>
            </li>
            <li id="ul0021-0006" num="0124">case PADDING:
            <ul id="ul0024" list-style="none">
                <li id="ul0024-0001" num="0125">break;</li>
            </ul>
            </li>
            <li id="ul0021-0007" num="0126">}</li>
        </ul>
        </li>
        <li id="ul0020-0004" num="0127">}</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0086" num="0128">}</p>
<p id="p-0087" num="0129">The above code would be called when processing the neurons and the corresponding function called, e.g. firing_thresh_cell_update, would execute in a homogeneous warp of threads with the same type of neurons, in accordance with an embodiment of the present invention.</p>
<p id="p-0088" num="0130"><figref idref="DRAWINGS">FIG. 5</figref> is a flowchart <b>500</b> illustrating steps by which data structure <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref> is allocated, in accordance with an embodiment of the present invention. The method begins at step <b>502</b>, and proceeds to step <b>504</b> where a maximum number of fields needed to represent any neuron or connection is determined. As noted above, in the case of data structure <b>200</b> of <figref idref="DRAWINGS">FIG. 2</figref>, the Type A neurons or connections required the use of four A fields <b>202</b>, whereas only two and three fields were needed by Types B and C, respectively. Accordingly, data structure <b>300</b> of <figref idref="DRAWINGS">FIG. 3</figref> is created, as per step <b>506</b>, having a number of fields <b>302</b> corresponding to this maximum number (four, in the case of <figref idref="DRAWINGS">FIG. 2</figref>).</p>
<p id="p-0089" num="0131">At step <b>508</b>, neuron or connection processing is interleaved, by grouping neurons and connections of a same type together (e.g., Type A <b>408</b>, Type B <b>410</b>, Type C <b>412</b> of <figref idref="DRAWINGS">FIG. 4</figref>), in accordance with an embodiment of the present invention. Then, at step <b>510</b>, padding <b>409</b> and <b>411</b> is introduced to each group of neurons or connections, as needed, in order for the sum of the padding and the number of neurons or connections of each type to equal the warp size <b>404</b> to prevent thread divergence. The method then ends at step <b>512</b>.</p>
<heading id="h-0013" level="1">VI. Data Access Inside Kernels</heading>
<p id="p-0090" num="0132">Now that a data structure like data structure <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref>, with k-fields <b>402</b>, is being used, a methodology by which to access a neural element's appropriate fields is needed. Each neural update function is type specific, such that the fields and parameters available when updating a particular neuron or connection are fixed, in accordance with an embodiment of the present embodiment. For example, a developer writing a neural update function for a Type A neuron would know that Type A has a field named &#x201c;activity&#x201d;. However, without some sort of mapping, the developer would not know which field of the k-fields <b>402</b> of data structure <b>400</b> is being used to store the &#x201c;activity&#x201d; data, as it must share the same row of data structure <b>400</b> with other fields for other types. As will be described in further detail herein, a set of mechanisms and abstractions are implemented using map files and a runtime API that allow a developer of neural update functions to determine which row in the data structure stores which field needed by the neural update function.</p>
<p id="p-0091" num="0133"><figref idref="DRAWINGS">FIG. 6</figref> illustrates a schematic representation <b>600</b> of interactions between various elements in an executable build process, in accordance with an embodiment of the present invention.</p>
<p id="p-0092" num="0134">For each type of neuron or connection, there is a corresponding definition in a type definition file <b>602</b>, in accordance with an embodiment of the present invention. This definition assigns some attributes to each type, such as a name. Furthermore, for each type, a list of all the parameters and fields for that type is provided. These fields correspond to values that are stored in the K B-byte words <b>402</b> of <figref idref="DRAWINGS">FIG. 4</figref> allocated to each instance of a neuron or connection.</p>
<p id="p-0093" num="0135">Each field <b>402</b> for a given type is associated with several attributes, such as, by way of example and not limitation, the name for that field, the type, the size in bytes, a default value, etc. One skilled in the relevant arts will appreciate that the exact nature of the data associated with a type will vary depending on a given implementation, and the above attributes are provided by way of example, and not limitation.</p>
<p id="p-0094" num="0136">In accordance with an embodiment of the present invention, type definitions can be structured similarly to an XML file using a set of nested nodes, where each node has attributes. A non-limiting exemplary abstract representation of how data is stored in a type definition following the notation of an XML file is:</p>
<p id="p-0095" num="0137">&#x3c;neuron&#x3e;
<ul id="ul0025" list-style="none">
    <li id="ul0025-0001" num="0000">
    <ul id="ul0026" list-style="none">
        <li id="ul0026-0001" num="0138">&#x3c;type_name&#x3e;&#x3c;/type_name&#x3e;</li>
        <li id="ul0026-0002" num="0139">. . . other attributes of a neuron . . .</li>
        <li id="ul0026-0003" num="0140">&#x3c;field&#x3e;
        <ul id="ul0027" list-style="none">
            <li id="ul0027-0001" num="0141">&#x3c;name&#x3e;Voltage &#x3c;/name&#x3e;</li>
            <li id="ul0027-0002" num="0142">&#x3c;type&#x3e;Float32&#x3c;/name&#x3e;</li>
            <li id="ul0027-0003" num="0143">. . . other attributes of this field</li>
        </ul>
        </li>
        <li id="ul0026-0004" num="0144">&#x3c;/field&#x3e;</li>
        <li id="ul0026-0005" num="0145">. . . other fields</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0096" num="0146">&#x3c;/neuron&#x3e;</p>
<p id="p-0097" num="0147">&#x3c;conn&#x3e;
<ul id="ul0028" list-style="none">
    <li id="ul0028-0001" num="0000">
    <ul id="ul0029" list-style="none">
        <li id="ul0029-0001" num="0148">&#x3c;conn_name&#x3e;voltage_dependent_conn &#x3c;/conn_name&#x3e;</li>
        <li id="ul0029-0002" num="0149">&#x3c;post_type&#x3e;voltage &#x3c;/post_type&#x3e;</li>
        <li id="ul0029-0003" num="0150">. . . other attributes of a connection . . .</li>
        <li id="ul0029-0004" num="0151">&#x3c;field&#x3e;
        <ul id="ul0030" list-style="none">
            <li id="ul0030-0001" num="0152">&#x3c;name&#x3e;half wave rectified &#x3c;/name&#x3e;</li>
            <li id="ul0030-0002" num="0153">&#x3c;type&#x3e;bool &#x3c;/type&#x3e;</li>
            <li id="ul0030-0003" num="0154">. . . other attributes of this field</li>
            <li id="ul0030-0004" num="0155">&#x3c;/field&#x3e;</li>
        </ul>
        </li>
        <li id="ul0029-0005" num="0156">. . . other fields</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0098" num="0157">&#x3c;/conn&#x3e;</p>
<p id="p-0099" num="0158">These type definitions <b>602</b> are provided to pre-processor <b>604</b>, in accordance with an embodiment of the present invention. Pre-processor <b>604</b> determines the number of K B-byte words needed by identifying the neuron or connection type that uses the most data, as previously discussed in relation to the four A fields <b>202</b> of <figref idref="DRAWINGS">FIG. 2</figref>. The pre-processing stage then systematically determines which words in the K word buffer for each neuron or connection type should store which parameters. This information is output from the pre-processing stage in the form of map files <b>606</b>, in accordance with an embodiment of the present invention.</p>
<p id="p-0100" num="0159">Map files <b>606</b> define which field is stored in which row for each type. Thus, a developer of a neural update function can look at the map files and hard code the row number from the K fields <b>402</b> of <figref idref="DRAWINGS">FIG. 4</figref> into the neural update function. However, a dynamic approach allows for the mapping to change each time pre-processor <b>604</b> is executed, and provides additional clarity over hard-coded constants. By accounting for mapping changes, a developer retains the flexibility to add or delete fields for a specific type without the need to re-key hard-coded row number constants.</p>
<p id="p-0101" num="0160">By way of example, and not limitation, supposing connection Type A has fields a1 and a2, the map files <b>606</b> might then specify that a1 is stored in the first B-byte word and a2 is stored in the second B-byte word in the buffers (corresponding to fields <b>402</b> of <figref idref="DRAWINGS">FIG. 4</figref> for each column) allocated to each instance of Type A. A non-limiting exemplary map file using XML-like syntax to provide an abstract illustration of the content of map files <b>606</b> would be:</p>
<p id="p-0102" num="0161">&#x3c;mapconn type=voltage_dependent_conn&#x3e;
<ul id="ul0031" list-style="none">
    <li id="ul0031-0001" num="0000">
    <ul id="ul0032" list-style="none">
        <li id="ul0032-0001" num="0162">&#x3c;entry&#x3e;
        <ul id="ul0033" list-style="none">
            <li id="ul0033-0001" num="0163">&#x3c;field_name&#x3e;have wave rectified &#x3c;/field_name&#x3e;</li>
            <li id="ul0033-0002" num="0164">&#x3c;row&#x3e;0&#x3c;/row&#x3e;</li>
        </ul>
        </li>
        <li id="ul0032-0002" num="0165">&#x3c;/entry&#x3e;</li>
        <li id="ul0032-0003" num="0166">. . . other entries for fields . . .</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0103" num="0167">&#x3c;/mapconn&#x3e;</p>
<p id="p-0104" num="0168">In practice, map files <b>606</b> would include (i.e., incorporate for compilation) files that can be referenced by kernel code <b>608</b>, in accordance with an embodiment of the present invention. By way of example, and not limitation, map files <b>606</b> can include some C header files defining C preprocessor macros, such as a header file named &#x201c;voltage_dependent_conn.h&#x201d;, the contents of which could be a macro:</p>
<p id="p-0105" num="0169">#define voltage_dependent_conn 0</p>
<p id="p-0106" num="0170">For each field name used by the voltage dependent connection there would be a C macro with that name, in accordance with an embodiment of the present invention, the value of which would be the row of the matrix which stored that field. In the above example, the field name &#x201c;voltage_dependent_conn&#x201d; can then be used throughout kernel code <b>608</b> without the need to know a priori the corresponding row for the field. One skilled in the relevant arts will appreciate that a number of different methodologies for mapping fields for reusable application in kernel code <b>608</b> exist, and the aforementioned solution is provided by way of example, and not limitation.</p>
<p id="p-0107" num="0171">A characteristic of the map files <b>606</b> and a runtime API, which provides routines for listing the different types of neurons or connections and the fields associated with each, is that they implement a mapping which maps the field names to the appropriate row in the data structure <b>400</b> illustrated in <figref idref="DRAWINGS">FIG. 4</figref>. This mapping allows a developer of a neural update function encoded in kernel code <b>608</b> to refer to fields by their name, such as through the use of the name &#x201c;voltage_dependent_conn&#x201d; provided by the exemplary macro above. Then, at compile time or runtime, these references by name are automatically converted into the required instructions necessary to read the appropriate row from data structure <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0108" num="0172">In the above exemplary map file <b>606</b>, the kernel code <b>608</b> uses macros which in turn use map files' <b>606</b> data to determine the appropriate location of a field. Since kernel code <b>608</b> has the equations for the corresponding neural element type, it has no need to know which of the k-fields correspond to which field of the neural element, in accordance with an embodiment of the present invention. In kernel code, it is possible to access a field via the following code for a specific neural element i:</p>
<p id="p-0109" num="0173">int k_offset=ROWMAP(fieldname);</p>
<p id="p-0110" num="0174">TYPE(fieldname) field_name_var=(TYPE(fieldname))data[k_offset, i];</p>
<p id="p-0111" num="0175">In the above code segment, ROWMAP and TYPE would be macros that rely on map files <b>606</b> to actually determine the appropriate locations of the fieldname in question. If the map file <b>606</b> included #defines with the location of field names, e.g.</p>
<p id="p-0112" num="0176">#define voltage_dependent_conn_offset 0</p>
<p id="p-0113" num="0177">#define voltage_dependent_conn_type_str &#x201c;float&#x201d;</p>
<p id="h-0014" num="0000">with voltage_dependent_conn as the field name, then the macros could be written as follows:</p>
<p id="p-0114" num="0178">#define ROWMAP(X) X ##_offset</p>
<p id="p-0115" num="0179">#define TYPE(X) X ##_type_str</p>
<p id="p-0116" num="0180">The specific neural element type kernel code <b>608</b> then makes use of the retrieved fields based on whatever equations or other operations are relevant to the data contained in those fields for the particular type corresponding to the kernel code <b>608</b>. This constitutes the glue by which the runtime determined data structure and field allocation is connected with the generic field-location-agnostic neural element execution code.</p>
<p id="p-0117" num="0181">The map files <b>606</b> and kernel code <b>608</b> are combined and used to build the actual executable code <b>612</b> through application of compiler/builder <b>610</b> (e.g., gcc NVIDIA compiler, gnu linker, etc.), in accordance with an embodiment of the present invention. As noted above, the executable code includes a runtime API, which is a set of objects and functions that can be used at runtime to gain introspection into the neuron or connection types, in accordance with a further embodiment of the present invention. The runtime API provides routines for listing the different types of neurons or connections implemented and the fields associated with each neuron type or connection type. For each field, its name as well as its type can be obtained via the runtime API, by way of example and not limitation. For each neuron, by way of further example and not limitation, a list of allowed connections can be obtained.</p>
<p id="p-0118" num="0182"><figref idref="DRAWINGS">FIG. 7</figref> is a flowchart <b>700</b> illustrating the steps by which executable code is created from kernel code and connection or neuron type definitions, in accordance with an embodiment of the present invention. The method begins at step <b>702</b> and proceeds to step <b>704</b> where the connection or neuron type definitions <b>602</b> of <figref idref="DRAWINGS">FIG. 6</figref> are pre-processed by pre-processors <b>604</b>, as described above, in order to generate map files <b>606</b>, in accordance with an embodiment of the present invention.</p>
<p id="p-0119" num="0183">At step <b>706</b>, executable code <b>612</b> is compiled and built from kernel code <b>608</b> and the generated map files <b>606</b>, in accordance with an embodiment of the present invention. The kernel code <b>608</b> is presented in the form of source files containing code for the neural update functions k<sub>j</sub>, g<sub>j</sub>, h<sub>j</sub>, &#x3c8;<sub>m</sub>, discussed above, in accordance with an embodiment of the present invention. One instance of each neural update function is developed for each type of neuron or connection. To access the data in the data structure <b>400</b> of <figref idref="DRAWINGS">FIG. 4</figref>, the kernel code <b>608</b> can either use information in the map files <b>606</b> or information available at runtime via the runtime API <b>612</b>, in accordance with an embodiment of the present invention. The method then ends at step <b>708</b>.</p>
<heading id="h-0015" level="1">VII. Example Computer System Implementation</heading>
<p id="p-0120" num="0184">Various aspects of the present invention can be implemented by software, firmware, hardware, or a combination thereof. <figref idref="DRAWINGS">FIG. 8</figref> illustrates an example computer system <b>800</b> in which the present invention, or portions thereof, can be implemented as computer-readable code. For example, the methods illustrated by flowcharts <b>500</b> of <figref idref="DRAWINGS">FIG. 5</figref>, <b>600</b> of <figref idref="DRAWINGS">FIGS. 6</figref>, and <b>700</b> of <figref idref="DRAWINGS">FIG. 7</figref>, can be implemented in system <b>800</b>. Various embodiments of the invention are described in terms of this example computer system <b>800</b>. After reading this description, it will become apparent to a person skilled in the relevant art how to implement the invention using other computer systems and/or computer architectures.</p>
<p id="p-0121" num="0185">Computer system <b>800</b> includes one or more processors, such as processor <b>804</b>. Processor <b>804</b> can be a special purpose or a general purpose processor. Processor <b>804</b> is connected to a communication infrastructure <b>806</b> (for example, a bus or network).</p>
<p id="p-0122" num="0186">Computer system <b>800</b> also includes a main memory <b>808</b>, preferably random access memory (RAM), and may also include a secondary memory <b>810</b>. Secondary memory <b>810</b> may include, for example, a hard disk drive <b>812</b>, a removable storage drive <b>814</b>, and/or a memory stick. Removable storage drive <b>814</b> may comprise a floppy disk drive, a magnetic tape drive, an optical disk drive, a flash memory, or the like. The removable storage drive <b>814</b> reads from and/or writes to a removable storage unit <b>818</b> in a well known manner. Removable storage unit <b>818</b> may comprise a floppy disk, magnetic tape, optical disk, etc. that is read by and written to by removable storage drive <b>814</b>. As will be appreciated by persons skilled in the relevant art(s), removable storage unit <b>818</b> includes a computer usable storage medium having stored therein computer software and/or data.</p>
<p id="p-0123" num="0187">In alternative implementations, secondary memory <b>810</b> may include other similar means for allowing computer programs or other instructions to be loaded into computer system <b>800</b>. Such means may include, for example, a removable storage unit <b>822</b> and an interface <b>820</b>. Examples of such means may include a program cartridge and cartridge interface (such as that found in video game devices), a removable memory chip (such as an EPROM, or PROM) and associated socket, and other removable storage units <b>822</b> and interfaces <b>820</b> that allow software and data to be transferred from the removable storage unit <b>822</b> to computer system <b>800</b>.</p>
<p id="p-0124" num="0188">Computer system <b>800</b> may also include a communications interface <b>824</b>. Communications interface <b>824</b> allows software and data to be transferred between computer system <b>800</b> and external devices. Communications interface <b>824</b> may include a modem, a network interface (such as an Ethernet card), a communications port, a PCMCIA slot and card, or the like. Software and data transferred via communications interface <b>824</b> are in the form of signals that may be electronic, electromagnetic, optical, or other signals capable of being received by communications interface <b>824</b>. These signals are provided to communications interface <b>824</b> via a communications path <b>826</b>. Communications path <b>826</b> carries signals and may be implemented using wire or cable, fiber optics, a phone line, a cellular phone link, an RF link or other communications channels.</p>
<p id="p-0125" num="0189">In this document, the terms &#x201c;computer program medium&#x201d; and &#x201c;computer usable medium&#x201d; are used to generally refer to media such as removable storage unit <b>818</b>, removable storage unit <b>822</b>, and a hard disk installed in hard disk drive <b>812</b>. Signals carried over communications path <b>826</b> can also embody the logic described herein. Computer program medium and computer usable medium can also refer to memories, such as main memory <b>808</b> and secondary memory <b>810</b>, which can be memory semiconductors (e.g. DRAMs, etc.). These computer program products are means for providing software to computer system <b>800</b>.</p>
<p id="p-0126" num="0190">Computer programs (also called computer control logic) are stored in main memory <b>808</b> and/or secondary memory <b>810</b>. Computer programs may also be received via communications interface <b>824</b>. Such computer programs, when executed, enable computer system <b>800</b> to implement the present invention as discussed herein. In particular, the computer programs, when executed, enable processor <b>804</b> to implement the processes of the present invention, such as the steps in the methods illustrated by flowcharts <b>500</b> of <figref idref="DRAWINGS">FIG. 5</figref>, <b>600</b> of <figref idref="DRAWINGS">FIGS. 6</figref>, and <b>700</b> of <figref idref="DRAWINGS">FIG. 7</figref>, discussed above. Accordingly, such computer programs represent controllers of the computer system <b>800</b>. Where the invention is implemented using software, the software may be stored in a computer program product and loaded into computer system <b>800</b> using removable storage drive <b>814</b>, interface <b>820</b>, hard drive <b>812</b> or communications interface <b>824</b>.</p>
<p id="p-0127" num="0191">The invention is also directed to computer program products comprising software stored on any computer useable medium. Such software, when executed in one or more data processing device, causes a data processing device(s) to operate as described herein. Embodiments of the invention employ any computer useable or readable medium, known now or in the future. Examples of computer useable mediums include, but are not limited to, primary storage devices (e.g., any type of random access memory), secondary storage devices (e.g., hard drives, floppy disks, CD ROMS, ZIP disks, tapes, magnetic storage devices, optical storage devices, MEMS, nanotechnological storage device, etc.), and communication mediums (e.g., wired and wireless communications networks, local area networks, wide area networks, intranets, etc.).</p>
<heading id="h-0016" level="1">VIII. Conclusion</heading>
<p id="p-0128" num="0192">While various embodiments of the present invention have been described above, it should be understood that they have been presented by way of example only, and not limitation. It will be understood by those skilled in the relevant art(s) that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined in the appended claims. It should be understood that the invention is not limited to these examples. The invention is applicable to any elements operating as described herein. Accordingly, the breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method of organizing one or more data structures in physical storage media for efficient access by a parallel processing architecture, the method being implemented in a computer system that includes a physical processor and physical storage media, the method comprising:
<claim-text>determining the number of pre-synaptic connections for individual ones of a set of neurons, the set of neurons being connected through a set of connections, wherein the set of neurons includes a first neuron and a second neuron;</claim-text>
<claim-text>grouping, by the physical processor, individual ones of a set of neurons together in a set of groups based on the determined number of pre-synaptic connections per neuron such that individual ones of the set of groups include neurons having the same number of re-synaptic connections, wherein the set of groups includes a first group and a second group, wherein the first group includes the first neuron and the second neuron; and</claim-text>
<claim-text>processing the set of neurons by evaluating connections in parallel per group such that a first connection of the first neuron is evaluated in parallel with the first connection of the second neuron.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein individual neurons are associated with one or more fields of data that are needed to process neurons, wherein individual fields correspond to a fixed amount of physical storage media, the method further comprising:
<claim-text>determining the number of fields of data associated with individual ones of the set of neurons;</claim-text>
<claim-text>determining a maximum number of fields of data such that individual ones of the set of neurons are associated with at most the maximum number of fields;</claim-text>
<claim-text>forming a data structure by an allocation of physical storage media for individual ones of the set of neurons based on the determined maximum number of fields; and</claim-text>
<claim-text>storing the fields associated with the set of neurons in the data structure,</claim-text>
<claim-text>wherein processing the set of neurons includes accessing the data structure.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:
<claim-text>determining a maximum number of connections for individual ones of the set of neurons such that individual ones of the set of neurons are connected through at most the maximum number of connections,</claim-text>
<claim-text>wherein the allocation includes padding based on the determined maximum number of connections.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the padding is performed such that one or more parameters indicative of efficiency of accessing the physical storage media are improved in comparison to an allocation of physical storage media without padding.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the set of neurons further includes a third neuron and a fourth neuron, wherein the second group includes the third neuron and the fourth neuron, wherein processing the set of neurons is performed by evaluating the first group in parallel with the second group such that a first connection of the first neuron is evaluated in parallel with the first connection of the third neuron.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein individual ones of the set of neurons are associated with a neuron type, wherein grouping individual ones of a set of neurons together in a set of groups is performed such that individual ones of the set of groups include neurons having the same neuron type.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A computer-implemented method of organizing one or more data structure in physical storage media for efficient access by a parallel processing architecture, the method being implemented in a computer system that includes a physical processor and physical storage media, the method comprising:
<claim-text>determining a number of fields of data associated with individual ones of a set of neurons, wherein fields are needed to process neurons, wherein individual ones of the set of neurons are associated with a neuron type;</claim-text>
<claim-text>determining a maximum number of fields of data such that individual ones of the set of neurons are associated with at most the maximum number of fields;</claim-text>
<claim-text>grouping, by the physical processor, individual ones of a set of neurons together in a set of groups such that individual ones of the set of groups include neurons having the same neuron type and the same number of pre-synaptic connections,</claim-text>
<claim-text>forming a data structure by an allocation of physical storage media for an individual group based on the determined maximum number of fields, wherein the allocation includes padding based on the number of pre-synaptic connections for neurons in the individual group, and furthermore repeating allocation for individual ones of the set of groups;</claim-text>
<claim-text>storing the fields associated with the set of neurons in the data structure; and</claim-text>
<claim-text>processing the set of neurons by evaluating connections in parallel per group, wherein processing the set of neurons includes accessing the data structure.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>generating a map file from a type definition associating individual ones of the set of neurons with fields, the map file configured to allocate positions in the data structure to the fields; and</claim-text>
<claim-text>building executable code from kernel code and the map file.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
