<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625925-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625925</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>12577453</doc-number>
<date>20091012</date>
</document-id>
</application-reference>
<us-application-series-code>12</us-application-series-code>
<us-term-of-grant>
<us-term-extension>336</us-term-extension>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>40</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>34</subgroup>
<symbol-position>L</symbol-position>
<classification-value>N</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>382274</main-classification>
</classification-national>
<invention-title id="d2e55">Distortion of digital images using spatial offsets from image reference points</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4881130</doc-number>
<kind>A</kind>
<name>Hayashi</name>
<date>19891100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>5309245</doc-number>
<kind>A</kind>
<name>Hayashi et al.</name>
<date>19940500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>5404316</doc-number>
<kind>A</kind>
<name>Klingler et al.</name>
<date>19950400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5404439</doc-number>
<kind>A</kind>
<name>Moran et al.</name>
<date>19950400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5412767</doc-number>
<kind>A</kind>
<name>Long</name>
<date>19950500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5425137</doc-number>
<kind>A</kind>
<name>Mohan et al.</name>
<date>19950600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5436733</doc-number>
<kind>A</kind>
<name>Terada et al.</name>
<date>19950700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5459586</doc-number>
<kind>A</kind>
<name>Nagasato et al.</name>
<date>19951000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5465160</doc-number>
<kind>A</kind>
<name>Kamo et al.</name>
<date>19951100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5471572</doc-number>
<kind>A</kind>
<name>Buchner</name>
<date>19951100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>5473740</doc-number>
<kind>A</kind>
<name>Kasson</name>
<date>19951200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>5489921</doc-number>
<kind>A</kind>
<name>Dorff et al.</name>
<date>19960200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>5490239</doc-number>
<kind>A</kind>
<name>Myers</name>
<date>19960200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>5506946</doc-number>
<kind>A</kind>
<name>Bar</name>
<date>19960400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>5542003</doc-number>
<kind>A</kind>
<name>Wofford</name>
<date>19960700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>5548705</doc-number>
<kind>A</kind>
<name>Moran et al.</name>
<date>19960800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>5581670</doc-number>
<kind>A</kind>
<name>Bier</name>
<date>19961200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>5586239</doc-number>
<kind>A</kind>
<name>Ueda</name>
<date>19961200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>5611027</doc-number>
<kind>A</kind>
<name>Edgar</name>
<date>19970300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>5617114</doc-number>
<kind>A</kind>
<name>Bier et al.</name>
<date>19970400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>5623592</doc-number>
<kind>A</kind>
<name>Carlson</name>
<date>19970400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>5638496</doc-number>
<kind>A</kind>
<name>Sato</name>
<date>19970600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>5684509</doc-number>
<kind>A</kind>
<name>Hatanaka et al.</name>
<date>19971100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>5734761</doc-number>
<kind>A</kind>
<name>Bagley</name>
<date>19980300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>5793885</doc-number>
<kind>A</kind>
<name>Kasson</name>
<date>19980800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>5818455</doc-number>
<kind>A</kind>
<name>Stone et al.</name>
<date>19981000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>5874967</doc-number>
<kind>A</kind>
<name>West et al.</name>
<date>19990200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>5877772</doc-number>
<kind>A</kind>
<name>Nomura</name>
<date>19990300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>5923792</doc-number>
<kind>A</kind>
<name>Shyu et al.</name>
<date>19990700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>5949429</doc-number>
<kind>A</kind>
<name>Bonneau et al.</name>
<date>19990900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>6002401</doc-number>
<kind>A</kind>
<name>Baker</name>
<date>19991200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>6038348</doc-number>
<kind>A</kind>
<name>Carley</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>6043824</doc-number>
<kind>A</kind>
<name>Bier</name>
<date>20000300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>6069626</doc-number>
<kind>A</kind>
<name>Cline et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>6069629</doc-number>
<kind>A</kind>
<name>Paterson et al.</name>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>6072501</doc-number>
<kind>A</kind>
<name>Bier</name>
<date>20000600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>6130676</doc-number>
<kind>A</kind>
<name>Wise et al.</name>
<date>20001000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>6175663</doc-number>
<kind>B1</kind>
<name>Huang</name>
<date>20010100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>6201548</doc-number>
<kind>B1</kind>
<name>Cariffe</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>6208753</doc-number>
<kind>B1</kind>
<name>Braudaway et al.</name>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382162</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>6229544</doc-number>
<kind>B1</kind>
<name>Cragun</name>
<date>20010500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345418</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>6249353</doc-number>
<kind>B1</kind>
<name>Yoshida et al.</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>6252579</doc-number>
<kind>B1</kind>
<name>Rosenberg et al.</name>
<date>20010600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>6301586</doc-number>
<kind>B1</kind>
<name>Yang et al.</name>
<date>20011000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>  1  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>6335733</doc-number>
<kind>B1</kind>
<name>Keren et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345418</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>6466228</doc-number>
<kind>B1</kind>
<name>Ulrich et al.</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345619</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>6480203</doc-number>
<kind>B1</kind>
<name>Carter et al.</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345619</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>6486914</doc-number>
<kind>B1</kind>
<name>Anderson</name>
<date>20021100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>6504575</doc-number>
<kind>B1</kind>
<name>Ramirez et al.</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00050">
<document-id>
<country>US</country>
<doc-number>6535301</doc-number>
<kind>B1</kind>
<name>Kuwata</name>
<date>20030300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00051">
<document-id>
<country>US</country>
<doc-number>6577330</doc-number>
<kind>B1</kind>
<name>Tsuda</name>
<date>20030600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00052">
<document-id>
<country>US</country>
<doc-number>6590583</doc-number>
<kind>B2</kind>
<name>Soohoo</name>
<date>20030700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00053">
<document-id>
<country>US</country>
<doc-number>6681056</doc-number>
<kind>B1</kind>
<name>Tseng et al.</name>
<date>20040100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382282</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00054">
<document-id>
<country>US</country>
<doc-number>6710782</doc-number>
<kind>B2</kind>
<name>Ruff et al.</name>
<date>20040300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345619</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00055">
<document-id>
<country>US</country>
<doc-number>6728421</doc-number>
<kind>B2</kind>
<name>Kokemohr</name>
<date>20040400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00056">
<document-id>
<country>US</country>
<doc-number>6792158</doc-number>
<kind>B1</kind>
<name>Brumley</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00057">
<document-id>
<country>US</country>
<doc-number>6801227</doc-number>
<kind>B2</kind>
<name>Bocionek</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00058">
<document-id>
<country>US</country>
<doc-number>6804406</doc-number>
<kind>B1</kind>
<name>Chen</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00059">
<document-id>
<country>US</country>
<doc-number>6828991</doc-number>
<kind>B2</kind>
<name>Nason et al.</name>
<date>20041200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00060">
<document-id>
<country>US</country>
<doc-number>6850249</doc-number>
<kind>B1</kind>
<name>Gu</name>
<date>20050200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00061">
<document-id>
<country>US</country>
<doc-number>6865300</doc-number>
<kind>B2</kind>
<name>Kokemohr</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00062">
<document-id>
<country>US</country>
<doc-number>6868190</doc-number>
<kind>B1</kind>
<name>Morton</name>
<date>20050300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00063">
<document-id>
<country>US</country>
<doc-number>6892359</doc-number>
<kind>B1</kind>
<name>Nason et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00064">
<document-id>
<country>US</country>
<doc-number>6897879</doc-number>
<kind>B2</kind>
<name>Lyapunov et al.</name>
<date>20050500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345613</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00065">
<document-id>
<country>US</country>
<doc-number>6941359</doc-number>
<kind>B1</kind>
<name>Beaudoin et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00066">
<document-id>
<country>US</country>
<doc-number>6966036</doc-number>
<kind>B2</kind>
<name>Nason et al.</name>
<date>20051100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00067">
<document-id>
<country>US</country>
<doc-number>6982695</doc-number>
<kind>B1</kind>
<name>Canova, Jr.</name>
<date>20060100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00068">
<document-id>
<country>US</country>
<doc-number>7009600</doc-number>
<kind>B2</kind>
<name>Jones</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00069">
<document-id>
<country>US</country>
<doc-number>7013028</doc-number>
<kind>B2</kind>
<name>Gont et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382113</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00070">
<document-id>
<country>US</country>
<doc-number>7019753</doc-number>
<kind>B2</kind>
<name>Rappaport et al.</name>
<date>20060300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00071">
<document-id>
<country>US</country>
<doc-number>7031547</doc-number>
<kind>B2</kind>
<name>Kokemohr</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00072">
<document-id>
<country>US</country>
<doc-number>7136790</doc-number>
<kind>B1</kind>
<name>Hobbs</name>
<date>20061100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00073">
<document-id>
<country>US</country>
<doc-number>7346226</doc-number>
<kind>B2</kind>
<name>Shyshkin</name>
<date>20080300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382275</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00074">
<document-id>
<country>US</country>
<doc-number>7602968</doc-number>
<kind>B2</kind>
<name>Kokemohr</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00075">
<document-id>
<country>US</country>
<doc-number>7602991</doc-number>
<kind>B2</kind>
<name>Kokemohr</name>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00076">
<document-id>
<country>US</country>
<doc-number>2002/0093514</doc-number>
<kind>A1</kind>
<name>Edwards et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00077">
<document-id>
<country>US</country>
<doc-number>2002/0118209</doc-number>
<kind>A1</kind>
<name>Hylen</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00078">
<document-id>
<country>US</country>
<doc-number>2003/0020733</doc-number>
<kind>A1</kind>
<name>Yin</name>
<date>20030100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00079">
<document-id>
<country>US</country>
<doc-number>2003/0095697</doc-number>
<kind>A1</kind>
<name>Wood et al.</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382131</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00080">
<document-id>
<country>US</country>
<doc-number>2004/0225968</doc-number>
<kind>A1</kind>
<name>Look et al.</name>
<date>20041100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00081">
<document-id>
<country>US</country>
<doc-number>2008/0101711</doc-number>
<kind>A1</kind>
<name>Kalker et al.</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382254</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00082">
<document-id>
<country>US</country>
<doc-number>2010/0027908</doc-number>
<kind>A1</kind>
<name>Kokemohr</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382274</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00083">
<document-id>
<country>US</country>
<doc-number>2010/0039448</doc-number>
<kind>A1</kind>
<name>Kokemohr</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345647</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00084">
<document-id>
<country>EP</country>
<doc-number>886437</doc-number>
<kind>A2</kind>
<date>19981200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00085">
<document-id>
<country>JP</country>
<doc-number>07162677</doc-number>
<kind>A</kind>
<date>19950600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00086">
<document-id>
<country>JP</country>
<doc-number>10091761</doc-number>
<kind>A</kind>
<date>19980400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00087">
<document-id>
<country>JP</country>
<doc-number>11146219</doc-number>
<kind>A</kind>
<date>19990500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00088">
<document-id>
<country>JP</country>
<doc-number>2000151985</doc-number>
<kind>A</kind>
<date>20000500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00089">
<document-id>
<country>JP</country>
<doc-number>2001-67469</doc-number>
<kind>A</kind>
<date>20010300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00090">
<othercit>Bier et al., &#x201c;A Taxonomy of See-Through Tools&#x201d;, (Apr. 24 28) ACM, pp. 358-364, Proceedings of CHI 94, New York.K, 1994.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00091">
<othercit>Bier et al., &#x201c;A Taxonomy of See-Through Tools: The Video&#x201d;, (Apr. 24 28) ACM, Proceedings of CHI 94, New York, 1994.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00092">
<othercit>Chiyo Date et al., Sentakuhanni-hen, Mac Fan Special 14, Feb. 22, 2001, pp. 51-70, Mainichi Communications, Japan.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00093">
<othercit>Stone et al., &#x201c;The Movable Filter as a User Interface Tool&#x201d;, (Apr. 24 28) ACM, pp. 306-312, Proceedings of CHI 94, New York, 1994.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00094">
<othercit>Kokemohr, N., U.S. Appl. No. 11/279,958 (now US Patent No. 7,602,968), Office Action dated Aug. 11, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00095">
<othercit>Kokemohr, N., U.S. Appl. No. 11/832,599 (now US Patent No. 7,602,991), Office Action dated Sep. 29, 2008.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00096">
<othercit>Kokemohr, N., U.S. Appl. No. 12/577,176, Office Action dated Mar. 23, 2010.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>6</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>382162</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382164</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382165</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382214</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382219-221</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382254</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382260-264</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382274</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382276</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382284</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345581</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345582</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345619</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345620</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345630</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345639</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345647</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345650</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345671</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358  19</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358296</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358401</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358443</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358448</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>358450</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715777</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715808</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715814</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715839</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>715856</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34833302</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>5</number-of-drawing-sheets>
<number-of-figures>5</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11279958</doc-number>
<date>20060417</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7602968</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>12577453</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>11072609</doc-number>
<date>20050303</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>7031547</doc-number>
<date>20060418</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11279958</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10824664</doc-number>
<date>20040413</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6865300</doc-number>
<date>20050308</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>11072609</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<division>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>10280897</doc-number>
<date>20021024</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>6728421</doc-number>
<date>20040427</date>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>10824664</doc-number>
</document-id>
</child-doc>
</relation>
</division>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>60336498</doc-number>
<date>20011024</date>
</document-id>
</us-provisional-application>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20100027908</doc-number>
<kind>A1</kind>
<date>20100204</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Kokemohr</last-name>
<first-name>Nils</first-name>
<address>
<city>Hamburg</city>
<country>DE</country>
</address>
</addressbook>
<residence>
<country>DE</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Kokemohr</last-name>
<first-name>Nils</first-name>
<address>
<city>Hamburg</city>
<country>DE</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>IP Spring</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Google Inc.</orgname>
<role>02</role>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Alavi</last-name>
<first-name>Amir</first-name>
<department>2668</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for distorting a digital image comprising receiving the coordinates of one or more than one image reference point defined by a user within the digital image, receiving one or more than one spatial offset assigned by the user and associated with the coordinates of the one or more than one defined image reference point, providing a mixing function algorithm embodied on a computer-readable medium for distorting the digital image, calculating an offset matrix by applying the mixing function algorithm based on the one or more than one spatial offset and the coordinates of the one or more than one defined image reference point; and distorting the digital image by application of the offset matrix. A graphic tag may be associated with each of the defined image reference points and displayed over the digital image, and the assignment of the spatial offset may be accomplished by movement of the graphic tag with the pointing device. Abstract image reference points may be used to limit distortion.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="172.47mm" wi="134.45mm" file="US08625925-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="165.27mm" wi="117.43mm" file="US08625925-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="133.10mm" wi="109.56mm" file="US08625925-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="187.88mm" wi="138.94mm" file="US08625925-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="122.77mm" wi="114.38mm" file="US08625925-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="99.06mm" wi="143.34mm" file="US08625925-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">The present Application is a continuation of U.S. Ser. No. 11/279,958, filed Apr. 17, 2006, now U.S. Pat. No. 7,602,968, which is a continuation of U.S. Ser. No. 11/072,609, filed Mar. 3, 2005, now U.S. Pat. No. 7,031,547, which is a continuation of U.S. Ser. No. 10/824,664, filed Apr. 13, 2004, now U.S. Pat. No. 6,865,300, which is a division of U.S. Ser. No. 10/280,897, filed Oct. 24, 2002, now U.S. Pat. No. 6,728,421, which claims the benefit of U.S. Provisional Patent Application No. 60/336,498 filed Oct. 24, 2001, the content of which are all incorporated by reference in this disclosure in their entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">BACKGROUND</heading>
<p id="p-0003" num="0002">The present Application relates to distortion of a digital image using spatial offsets from User Definable Image Reference Points. User Definable Image Reference Points are described in U.S. Pat. Nos. 7,031,547; 6,865,300; and 6,728,421, the contents of which are incorporated by reference in this disclosure in their entirety.</p>
<p id="p-0004" num="0003">It is a well-known problem to correct color, contrast, sharpness, or other specific digital image attributes in a digital image. It is also well-known to those skilled in image-editing that it is difficult to perform multiple color, contrast, and other adjustments while maintaining a natural appearance of the digital image.</p>
<p id="p-0005" num="0004">At the current stage of image-editing technology, computer users can only apply relatively basic functions to images in a single action, such as increasing the saturation of all pixels of an image, removing a certain colorcast from the entire image, or increasing the image's overall contrast. Well-known image-editing tools and techniques such as layer masks can be combined with existing image adjustment functions to apply such image changes selectively. However, current methods for image editing are still limited to one single image adjustment at a time. More complex tools such as the Curves functions provided in image editing programs such as Adobe Photoshop&#xae; provide the user with added control for changing image color, but such tools are difficult to apply, and still very limited as they apply an image enhancement globally to the image.</p>
<p id="p-0006" num="0005">Additional image editing tools also exist for reading or measuring color values in the digital image. In its current release, Adobe Photoshop&#xae; offers a feature that enables the user to place and move up to four color reference points in an image. Such color reference points read properties (limited to the color values) of the image area in which they are placed. It is known to those skilled in the art that the only purpose of such color reference points is to display the associated color values; there is no image operation associated with such reference points. The reference points utilized in image-editing software are merely offered as a control tool for measuring an image's color values at a specific point within the image.</p>
<p id="p-0007" num="0006">In other implementations of reference points used for measuring color in specific image regions, image-editing applications such as Adobe Photoshop&#xae;, Corel Draw&#xae;, and Pictographics iCorrect 3.0&#xae;, allow the user to select a color in the image by clicking on a specific image point during a color enhancement and perform an operation on the specific color with which the selected point is associated. For example, the black-point adjustment in Adobe Photoshop&#xae; allows the user to select a color in the image and specify the selected color as black, instructing the software to apply a uniform color operation to all pixels of the image, so that the desired color is turned into black. This method is not only available for black-point operations, but for pixels that are intended to be white, gray (neutral), skin tone, or sky, etc.</p>
<p id="p-0008" num="0007">While each of these software applications provide methods for reading a limited number of colors and allow for one single operation which is applied globally and uniformly to the image and which only applies one uniform color cast change based on the read information, none of the methods currently used allow for the placement of one or more graphical representations of image reference points (IRPs) in the image that can read color or image information, be assigned an image editing function, be associated with one or more image reference points (IRPs) in the image to perform image-editing functions, be moved, or be modified by the user such that multiple related and unrelated operations can be performed.</p>
<p id="p-0009" num="0008">What is needed is a method to enable a user to easily attach IRPs to the image and at the same time define how the user wants the image to be distorted or altered.</p>
<heading id="h-0003" level="1">SUMMARY</heading>
<p id="p-0010" num="0009">A method for distorting or alteration of a digital image is disclosed comprising the steps of receiving the coordinates of one or more than one image reference point defined by a user within the digital image; receiving one or more than one spatial offset assigned by the user and associated with the coordinates of the one or more than one defined image reference point; providing a mixing function algorithm embodied on a computer-readable medium for distorting the digital image; calculating an offset matrix by applying the mixing function algorithm based on the one or more than one spatial offset and the coordinates of the one or more than one defined image reference point; and distorting the digital image by application of the offset matrix.</p>
<p id="p-0011" num="0010">The spatial offset may be assigned by the user by movement of a pointing device, such as a mouse. A graphic tag (<b>30</b>, <b>32</b>, <b>34</b>) may be associated with an image reference point and displayed over the digital image. The assignment of the spatial offset may be accomplished by movement of the graphic tag with the pointing device.</p>
<p id="p-0012" num="0011">Abstract image reference points may be used for limiting distortion.</p>
<p id="p-0013" num="0012">A method for distortion or alteration of a digital image is disclosed comprising determining one or more sets of pixel characteristics, receiving for each pixel characteristic set, a spatial offset, providing a mixing function algorithm embodied on a computer-readable medium for distorting the digital image, calculating an offset matrix by applying the mixing function algorithm based on the one or more sets of pixel characteristics and the received spatial offsets, and distorting the digital image by application of the offset matrix.</p>
<p id="p-0014" num="0013">A computer readable medium is provided having contents for causing a computer-based information handling system to perform the steps of the invention, thereby transforming the digital image.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0015" num="0014">These and other features, aspects, and advantages of the present invention will become better understood with reference to the following description, illustrations, equations, appended claims, and accompanying drawings where:</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 1</figref> is a screen shot of a digital image in an image processing program, illustrating one embodiment usable in the graphical user interface of the present invention.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 2</figref> is a screen shot of a digital image in an image processing program, illustrating another embodiment usable in the graphical user interface of the present invention.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 3</figref> is a flow chart of the steps of the application of a mixing function in accord with the disclosure.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 4</figref> is an illustration of one embodiment of a dialog box usable in the graphical user interface of the present invention.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 5</figref> is an illustration of one embodiment of a dialog box implementing simplified user control over weights usable in the graphical user interface of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0005" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0021" num="0020">The method and user interface of the present invention is usable as a plug-in supplemental program, as an independent module that may be integrated into any commercially available image processing program such as Adobe Photoshop&#xae;, or into any image processing device that is capable of modifying and displaying an image, such as a color copier or a self service photo print kiosk, as a dynamic library file or similar module that may be implemented into other software programs whereby image measurement and modification may be useful, or as a stand alone software program. These are all examples, without limitation, of image processing of a digital image. Although embodiments of the invention which adjust color, contrast, noise reduction, and sharpening are described, the present invention is useful for altering any attribute or feature of the digital image.</p>
<p id="p-0022" num="0021">Furthermore, it will become clear with regard to the current invention that the user interface for the current invention may have various embodiments, which will become clear later in this disclosure.</p>
<p id="h-0006" num="0000">The Graphical User Interface</p>
<p id="p-0023" num="0022">The user interface component of the present invention provides methods for setting IRPs in an image. Those skilled in the art will find that multiple methods or implementations of a user interface are useful with regard to the current invention.</p>
<p id="p-0024" num="0023">In one preferred embodiment of a user interface, an implementation of the present invention allows the user to set a variety of types of IRPs in an image, which can be shown as graphic tags <b>10</b> floating over the image, as shown in <figref idref="DRAWINGS">FIG. 1</figref>. <figref idref="DRAWINGS">FIG. 1</figref> is a screen shot of a digital image in an image processing program.</p>
<p id="p-0025" num="0024">This method enables the user to move the IRPs in the image for the purpose of adjusting the location of such IRPs and thus the effect of each IRP on the image.</p>
<p id="p-0026" num="0025">In another preferred embodiment, IRPs could be invisible within the preview area of the image and identified placed elsewhere as information boxes <b>12</b> within the interface, as shown in <figref idref="DRAWINGS">FIG. 2</figref>, but associated with a location (shown by arrow). In this embodiment of the user interface, graphic tags <b>10</b> do not &#x201c;float&#x201d; over the image as in <figref idref="DRAWINGS">FIG. 1</figref>. However, as it will become clear later in this disclosure that it is the location that Image Reference Points [IRPs] identifies and the related function that are significant, and that the graphical representations of the IRPs are useful as a convenience to the user to indicate the location of the IRP function. (<figref idref="DRAWINGS">FIG. 2</figref> is a screen shot of a digital image in an image processing program.)</p>
<p id="p-0027" num="0026">In both <figref idref="DRAWINGS">FIG. 1</figref> and <figref idref="DRAWINGS">FIG. 2</figref>, the IRPs serve as a graphical representation of an image modification that will be applied to an area of the image.</p>
<p id="p-0028" num="0027">The graphical user interface is embodied on a computer-readable medium for execution on a computer for image processing of a digital image. A first interface receives the coordinates of each of a plurality of image reference points defined by a user within the digital image, and a second interface receives an image editing function assigned by the user and associated with either the coordinates of each of the plurality of defined image reference points, or the image characteristics of one or more pixels neighboring the coordinates of each of the plurality of defined image reference points.</p>
<p id="p-0029" num="0028">In a further embodiment, the second interface receives an image editing function assigned by the user and associated with both the coordinates of each of the plurality of defined image reference points, and the image characteristics of one or more pixels neighboring the coordinates of each of the plurality of defined image reference points.</p>
<p id="p-0030" num="0029">In a further alternative optional embodiment, a third interface displays a graphical icon or graphical tag <b>10</b> at the coordinates of one or more than one of the plurality of defined image reference points. Additionally optionally, the third interface permits repositioning of the graphical icon.</p>
<p id="p-0031" num="0030">In further embodiments, a fourth interface displays the assigned image editing function. The second interface may further receive an image area associated with the coordinates of one or more than one of the plurality of defined image reference points. The second interface may further receive a color area associated with the coordinates of one or more than one of the plurality of defined image reference points.</p>
<p id="p-0032" num="0031">In an alternative embodiment, the first interface receives the coordinates of a single image reference point defined by a user within the digital image, and the second interface receives an image editing function assigned by the user and associated with both the coordinates of the defined image reference point, and the image characteristics of one or more pixels neighboring the coordinates of the defined image reference point.</p>
<p id="h-0007" num="0000">Mixing Functions</p>
<p id="p-0033" num="0032">A central function of the present invention is the &#x201c;Mixing Function,&#x201d; which modifies the image based on the values and settings of the IRPs and the image modifications associated with the IRPs. With reference to this disclosure, a &#x201c;Mixing Function&#x201d; is an algorithm that defines to what extent a pixel is modified by each of the IRPs and its related image modification function.</p>
<p id="p-0034" num="0033">It will be evident to those skilled in the art that there are many possible mixing functions, as will be shown in this disclosure.</p>
<p id="p-0035" num="0034">The method for applying the mixing function is shown in <figref idref="DRAWINGS">FIG. 3</figref>. Begin with receiving <b>14</b> the IRPs in the image; test <b>16</b> to determine whether abstract IRPs are being used. If so, load <b>18</b> the abstract IRPs and then select <b>20</b> the first pixel to be processed; if not select <b>20</b> the first pixel to be processed. Then apply <b>22</b> the mixing function according to this disclosure, and test <b>24</b> whether all pixels chosen to be processed have been processed. If so, the method is completed <b>26</b>, if not, the next pixel is selected <b>28</b> and step <b>22</b> is repeated.</p>
<p id="h-0008" num="0000">Using the Pythagoras Distance Approach</p>
<p id="p-0036" num="0035">In one embodiment of the mixing function, the Pythagoras equation can be used. Those skilled in the art will find that this is more suitable for IRPs that are intended to perform local color correction or similar changes to an image.</p>
<p id="p-0037" num="0036">In step <b>22</b>, apply the image modification to a greater extent, if the location of the IRP is close to that of the current pixel, or apply it to a lesser extent, if the location of the IRP is further away from the current pixel, using the Pythagoras equation to measure the distance, often also referred to as distance in Euclidian space.</p>
<p id="h-0009" num="0000">Using Color Curves</p>
<p id="p-0038" num="0037">In another embodiment, a mixing function could be created with the use of color curves. To create the function:</p>
<p id="p-0039" num="0038">Step <b>22</b>.<b>1</b>.<b>1</b>. Begin with the first channel of the image (such as the Red channel).</p>
<p id="p-0040" num="0039">Step <b>22</b>.<b>1</b>.<b>2</b>. All IRPs will have an existing brightness which is the brightness of the actual channel of the pixel where the IRP is located, and a desired brightness, which is the brightness of the actual channel of the same pixel after the image modification associated with its IRP has been applied. Find the optimal polynomial function that matches these values. For example, if the red channel has an IRP on a pixel with a value of 20, which changes the pixel's value to 5, and there is a second IRP above a pixel with the value of 80, which changes that channel luminosity to 90, all that is needed is to find a function that meets the conditions &#x192;(20)=5 and &#x192;(80)=90.</p>
<p id="p-0041" num="0040">Step <b>22</b>.<b>1</b>.<b>3</b>. Apply this function to all pixels of the selected channel.</p>
<p id="p-0042" num="0041">Step <b>22</b>.<b>1</b>.<b>4</b>. If all channels have not been modified, select the next channel and proceed with step <b>22</b>.<b>1</b>.<b>2</b>.</p>
<p id="h-0010" num="0000">Using Segmentation to Create the Mixing Function</p>
<p id="p-0043" num="0042">In a further embodiment, the mixing function can be created using segmentation. To create the function:</p>
<p id="p-0044" num="0043">Step <b>22</b>.<b>2</b>.<b>1</b>. Segment the image using any appropriate segmentation algorithm.</p>
<p id="p-0045" num="0044">Step <b>22</b>.<b>2</b>.<b>2</b>. Begin with IRP <b>1</b>.</p>
<p id="p-0046" num="0045">Step <b>22</b>.<b>2</b>.<b>3</b>. Apply the filter associated with that IRP to the segment where it is located.</p>
<p id="p-0047" num="0046">Step <b>22</b>.<b>2</b>.<b>4</b>. Select the next IRP.</p>
<p id="p-0048" num="0047">Step <b>22</b>.<b>2</b>.<b>5</b>. Unless all IRPs have been processed, proceed with step <b>22</b>.<b>2</b>.<b>3</b>.</p>
<p id="p-0049" num="0048">If there is a segment that contains two IRPs, re-segment the image with smaller segments, or re-segment the area into smaller segments.</p>
<p id="h-0011" num="0000">Using Multiple Segmentations</p>
<p id="p-0050" num="0049">In a still further embodiment of the current invention, the mixing function can be created using multiple segmentation. To create the function:</p>
<p id="p-0051" num="0050">Step <b>22</b>.<b>3</b>.<b>1</b>. Make &#x201c;n&#x201d; different segmentations of the image, e.g., n=4, where the first segmentation is rougher, (having few but larger segments), and the following segmentations are finer, (using more by smaller segments per image).</p>
<p id="p-0052" num="0051">Step <b>22</b>.<b>3</b>.<b>2</b>. Begin with IRP <b>1</b>.</p>
<p id="p-0053" num="0052">Step <b>22</b>.<b>3</b>.<b>3</b>. Apply the image modification of that IRP at 1/nth opacity to all pixels in the segment that contains the current IRP of the first segmentation, then apply the image modification at 1/nth opacity to all pixels in the segment containing the IRP of the second segmentation. Continue for all n segmentations.</p>
<p id="p-0054" num="0053">Step <b>22</b>.<b>3</b>.<b>4</b>. Select the next IRP.</p>
<p id="p-0055" num="0054">Step <b>22</b>.<b>3</b>.<b>5</b>. Unless all IRPs have been processed, proceed with step <b>22</b>.<b>3</b>.<b>3</b>.</p>
<p id="p-0056" num="0055">Those skilled in the art will know that several segmenting algorithms may be used, and the &#x201c;roughness&#x201d; (size of segments) within the equation can be defined by a parameter.</p>
<p id="h-0012" num="0000">Using a Classification Method</p>
<p id="p-0057" num="0056">A classification method from pattern recognition science may be used to create another embodiment of the mixing function. To create the function:</p>
<p id="p-0058" num="0057">Step <b>22</b>.<b>4</b>.<b>1</b>. Choose a set of characteristics, such as saturation, x-coordinate, y-coordinate, hue, and luminance.</p>
<p id="p-0059" num="0058">Step <b>22</b>.<b>4</b>.<b>2</b>. Using existing methods of pattern recognition, classify all pixels of the image, i.e., every pixel is assigned to an IRP based on the characteristics, and assuming that the IRPs are centers of clusters.</p>
<p id="p-0060" num="0059">Step <b>22</b>.<b>4</b>.<b>3</b>. Modify each pixel with the image modification associated with the IRP to which the pixel has been classified.</p>
<p id="h-0013" num="0000">Using a &#x201c;Soft&#x201d; Classification Method</p>
<p id="p-0061" num="0060">In an even further embodiment of the current invention, it may be useful to modify the classification method to adjust for similarity of pixel attributes.</p>
<p id="p-0062" num="0061">Typically, a pixel will not match the attributes of one IRP to a degree of 100%. One pixel's attributes might, for example, match one IRP to 50%, another IRP to 30% and a third IRP only to 20%. In the current embodiment using soft classification, the algorithm would apply the effect of the first IRP to a degree of 50%, the second IRP's effect at 30%, and the third IRP's effect to 20%. By utilizing this &#x201c;Soft&#x201d; Classification, one pixel is not purely associated with the most similar IRP.</p>
<p id="p-0063" num="0062">One preferred embodiment that is described in detail later in this disclosure will show an implementation that follows a similar concept as described here.</p>
<p id="h-0014" num="0000">Using an Expanding Areas Method</p>
<p id="p-0064" num="0063">In another embodiment of the mixing function, an expanding areas method could be used to create a mixing function. To create the function:</p>
<p id="p-0065" num="0064">Step <b>22</b>.<b>5</b>.<b>1</b>. Associate each IRP with an &#x201c;area&#x201d; or location within the image. Initially, this area is only the pixel where the IRP is positioned.</p>
<p id="p-0066" num="0065">Step <b>22</b>.<b>5</b>.<b>2</b>. Apply the following to all IRP areas: Consider all pixels that touch the area. Among those, find the one whose attributes (color, saturation, luminosity) are closest to the initial pixel of the area. While comparing the attributes, minimize for the sum of differences of all attributes. Add this pixel to the area and assign the current area size in pixels to it. The initial pixel is assigned with a value of 1, the next added pixel is assigned a value of 2, the next with a value of 3, etc., until each pixel has been assigned a value.</p>
<p id="p-0067" num="0066">Step <b>22</b>.<b>5</b>.<b>3</b>. Repeat step <b>22</b>.<b>5</b>.<b>2</b> until all areas have expanded to the full image size.</p>
<p id="p-0068" num="0067">Step <b>22</b>.<b>5</b>.<b>4</b>. Apply all modifications of all IRPs to that pixel while increasing the application for those with smaller values.</p>
<p id="h-0015" num="0000">One Preferred Mixing Function</p>
<p id="p-0069" num="0068">In one preferred embodiment, a mixing function uses a set of attributes for each pixel (luminosity, hue, etc.). These attributes are compared to the attributes of the area where an IRP is positioned, and the Mixing Function applies those IRPs image modifications more whose associated attributes are similar to the actual pixel, and those IRPs image modifications less whose associated characteristics are very different from the actual pixel.</p>
<p id="p-0070" num="0069">Unless otherwise specified, capitalized variables will represent large structures (such as the image I) or functions, while non-capitalized variables refer to one-dimensional, real numbers.</p>
<p id="h-0016" num="0000">Definition of the Key Elements</p>
<p id="p-0071" num="0070">A &#x201c;Pixel-Difference-Based IRP Image Modification,&#x201d; from now on called an &#x201c;IRP Image Modification,&#x201d; may be represented by a 7-tuple, as shown in Equation 1, where m is the amount of IRPs that will be made use of, and the number n is the amount of analyzing functions as explained later.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(<i>F</i><sub>1 . . . m</sub><i>,R</i><sub>1 . . . m</sub><i>,I,A</i><sub>1 . . . n</sub><i>,D,V,C</i><sub>1 . . . m</sub>)&#x2003;&#x2003;[1]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0072" num="0071">The first value, F<sub>1 . . . m </sub>is a set of the &#x201c;Performing Functions.&#x201d; Each of these functions is an image modification function, which may be called with three parameters as shown in Equation 2.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>I&#x2032;</i><sub>xy</sub><i>=F</i>(<i>I,x,y</i>)&#x2003;&#x2003;[2]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0073" num="0072">In Equation 2 the result I&#x2032;<sub>xy </sub>is the pixel that has been calculated by F. I is the image on which F is applied, and x and y are the coordinates of the pixel in I that F is applied to. Such a performing function could be &#x201c;darken pixels by 30%,&#x201d; for example, as shown in <figref idref="DRAWINGS">FIG. 1</figref>. In image science, these modifications are often called filters.</p>
<p id="p-0074" num="0073">The second value in Equation 1, R<sub>1 . . . m </sub>is a number of m tuples. Each tuple represents values of an IRP, and is a set of pixel characteristics. Such a tuple R consists of 2*n+1 values, as in Equation [3].
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>((<i>g</i><sub>1 </sub><i>. . . g</i><sub>n</sub>),<i>g</i>*,(<i>w</i><sub>1 </sub><i>. . . w</i><sub>n</sub>))&#x2003;&#x2003;[3]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0075" num="0074">F<sub>1 . . . m </sub>and R<sub>1 . . . m </sub>together represent the IRPs that the user has created. I will explain later how the IRPs that the user has placed can be converted into the functions and values F<sub>1 . . . m </sub>and R<sub>1 . . . m</sub>. Later in this disclosure I indicate that a function F and a tuple R are &#x201c;associated&#x201d; with each other and with an IRP if they F and R together represent an IRP.</p>
<p id="p-0076" num="0075">The third value I in Equation 1 is the image with the pixels I<sub>xy</sub>. This image can be of any type, i.e., grayscale, Lab, CMYK, RGB, or any other image representation that allows Performing Functions (Equation [2]) or analyzing functions (Equation [4]) to be performed on the image.</p>
<p id="p-0077" num="0076">The fourth element A<sub>1 . . . n </sub>in Equation 1 is a set of n &#x201c;Analyzing Functions&#x201d; as represented in Equation [4].
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sub>n</sub>(<i>I,x,y</i>)=<i>k</i>&#x2003;&#x2003;[4]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0078" num="0077">These functions, unlike the Performing Functions F, calculate a single real number k for each pixel. These functions extract comparable attributes out of the image, such as saturation, luminance, horizontal location or vertical location, amount of noise in the region around the coordinates x, y, and so forth. The number n is the amount of Analyzing Functions.</p>
<p id="p-0079" num="0078">The function's results need to be comparable. That is, the difference of the results of two different pixels applied to the same Analyzing Function can be represented by a number. For example, if p<sub>1 </sub>is a dark pixel and p<sub>2 </sub>is a bright pixel, and A is a function that calculates the luminance of a pixel, then |A(p<sub>1</sub>)&#x2212;A(p<sub>2</sub>)| is an easy measure for the luminosity difference of both pixels. Note: Analyzing Functions in this disclosure refer to functions that calculate characteristics of an image, and must not be confused with the mathematical term &#x201c;analytic functions.&#x201d; The result of an Analyzing Function applied to a pixel will for further reference in this disclosure be called a &#x201c;Characteristic&#x201d; of the pixel.</p>
<p id="p-0080" num="0079">The Analyzing Functions can analyze the color of a point x, y in the image I, the structure of the point x, y in the image I, and the location of a point x, y in the image I itself.</p>
<p id="p-0081" num="0080">Later in this disclosure I refer to &#x201c;Color Analyzing Functions,&#x201d; &#x201c;Structure Analyzing Functions&#x201d; and &#x201c;Location Analyzing Functions.&#x201d; Color Analyzing Functions are any functions on the pixel's values itself, such as r, g and b, while Structure Analyzing Functions also take the values and differences of a group of pixel around the point x, y into account, and Location Analyzing Functions are any functions on x and y.</p>
<p id="p-0082" num="0081">For example, the Analyzing Function A(I, x, y)=x+y is a Location Analyzing Function of the pixel. An example of a Color Analyzing Function would be A(I, x, y)=I<sub>xy(r)</sub>+I<sub>xy(g)</sub>+I<sub>xy(b)</sub>, where r, g and b refer to the RGB channels of the image. An example of a Structure Analyzing Function would be A(I, x, y)=I<sub>xy(r)</sub>&#x2212;I<sub>(x+1)y(r)</sub>. Note: These three categories of Analyzing Functions are not disjoint. For example, the function A(I,x,y)=I<sub>xy(r)</sub>&#x2212;I<sub>(x+1)(y&#x2212;2)(g)</sub>+x is a Color Analyzing Function, a Structure Analyzing Function, and a Location Analyzing Function simultaneously.</p>
<p id="p-0083" num="0082">&#x201c;Normalizing&#x201d; the Analyzing Functions and limiting the range of possible values such that their results have approximately the range of 0 . . . 100 will simplify the process.</p>
<p id="p-0084" num="0083">The fifth element D in Equation 1 is a &#x201c;Difference Function&#x201d; which can compare two vectors of n values against each other and provides a single number that is larger the more the two vectors of n values differ and zero if the two sets of n numbers are identical. In doing so, the function D is capable of weighing each individual number of the two sets with a weight vector (w<sub>1 . . . n</sub>) as in Equation [5].
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>d=D</i>((<i>a</i><sub>1 . . . n</sub>),(<i>b</i><sub>1 . . . n</sub>),(w<sub>1 . . . n</sub>))&#x2003;&#x2003;[5]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0085" num="0084">D is defined as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>D</i>((<i>a</i><sub>1 . . . n</sub>),(<i>b</i><sub>1 . . . n</sub>),(<i>w</i><sub>1 . . . n</sub>))=&#x2225;(<i>a</i><sub>1</sub><i>*w</i><sub>1</sub><i>&#x2212;b</i><sub>1</sub><i>*w</i><sub>1</sub>), . . . ,(<i>a</i><sub>n</sub><i>*w</i><sub>n</sub><i>&#x2212;b</i><sub>n</sub><i>*w</i><sub>n</sub>)&#x2225;&#x2003;&#x2003;[6]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
where &#x2225;&#x2022;&#x2225; refers to any norm, such as the distance in Euclidian space, which is also known as &#x2225;&#x2022;&#x2225;<sub>2</sub>.
</p>
<p id="p-0086" num="0085">In other words, the more a<sub>1 . . . n </sub>and b<sub>1 . . . n </sub>differ, the higher the result of the Difference Function D, while the weights w<sub>1 . . . n </sub>control the importance of each element of the vectors of a and b. By setting elements of w to zero, D will disregard the according elements of a and b.</p>
<p id="p-0087" num="0086">Suitable Difference Functions in this implementation are:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>D</i>((<i>a</i><sub>1 . . . n</sub>),(<i>b</i><sub>1 . . . n</sub>),(<i>w</i><sub>1 . . . n</sub>))=|<i>a</i><sub>1</sub><i>&#x2212;b</i><sub>1</sub><i>|*w</i><sub>1</sub><i>+|a</i><sub>2</sub><i>&#x2212;b</i><sub>2</sub><i>|*w</i><sub>2</sub><i>+ . . . +|a</i><sub>n</sub><i>&#x2212;b</i><sub>n</sub><i>|*w</i><sub>n</sub>&#x2003;&#x2003;[7]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>D</i>((<i>a</i><sub>1 . . . n</sub>),(<i>b</i><sub>1 . . . n</sub>),(<i>w</i><sub>1 . . . n</sub>))<sup>2</sup>=(<i>a</i><sub>1</sub><i>*w</i><sub>1</sub><i>&#x2212;b</i><sub>1</sub><i>*w</i><sub>1</sub>)<sup>2</sup>+ . . . +(<i>a</i><sub>n</sub><i>*w</i><sub>n</sub><i>&#x2212;b</i><sub>n</sub><i>*w</i><sub>n</sub>)<sup>2</sup>&#x2003;&#x2003;[8]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0088" num="0087">The weighed Pythagoras function [8] leads to better results than the simple function [7], while function [8] provides for accelerated processing. To those skilled in the art, the norms used in [7] and [8] may also be known as &#x2225;&#x2022;&#x2225;<sub>1 </sub>and &#x2225;&#x2022;&#x2225;<sub>2</sub>.</p>
<p id="p-0089" num="0088">A function D* that is derived from the function D is defined as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>D</i>*((<i>a</i><sub>1 . . . n</sub>),(<i>b</i><sub>1 . . . n</sub>),(<i>w</i><sub>1 . . . n</sub>),<i>g</i>*)=<i>D</i>((<i>a</i><sub>1 . . . n</sub>),(<i>b</i><sub>1 . . . n</sub>),(<i>w</i><sub>1 . . . n</sub>))+<i>g*</i>&#x2003;&#x2003;[9]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0090" num="0089">In other words: D* measures the difference of a<sub>1 . . . n </sub>and b<sub>1 . . . n</sub>, weighed with w<sub>1 . . . n</sub>, and adds the real number g* to the result.</p>
<p id="p-0091" num="0090">For accelerated performance or for simpler implementation, another Difference Function <o ostyle="single">D</o> or <o ostyle="single">D</o>* can be made use of which does not utilize weights. Systems as described in this disclosure that do not utilize weights are easier to use and faster to compute, but less flexible. <o ostyle="single">D</o> and <o ostyle="single">D</o>* are defined as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i> <o ostyle="single">D</o></i>((<i>a</i><sub>1 . . . n</sub>),(<i>b</i><sub>1 . . . n</sub>))=<i>D</i>((<i>a</i><sub>1 . . . n</sub>),(<i>b</i><sub>1 . . . n</sub>),(1,1, . . . ,1))&#x2003;&#x2003;[10]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i> <o ostyle="single">D</o></i>*((<i>a</i><sub>1 . . . n</sub>),(<i>b</i><sub>1 . . . n</sub>),<i>g</i>*)=<i>D</i>*((<i>a</i><sub>1 . . . n</sub>),(<i>b</i><sub>1 . . . n</sub>),(1,1, . . . ,1),<i>g</i>*)&#x2003;&#x2003;[11]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0092" num="0091">The sixth element, V, in Equation 1 is an &#x201c;Inversion Function&#x201d; that has the following characteristics with V:<img id="CUSTOM-CHARACTER-00001" he="3.56mm" wi="3.56mm" file="US08625925-20140107-P00001.TIF" alt="custom character" img-content="character" img-format="tif"/><sub>0</sub><sup>+</sup>&#x2192;<img id="CUSTOM-CHARACTER-00002" he="3.13mm" wi="3.56mm" file="US08625925-20140107-P00002.TIF" alt="custom character" img-content="character" img-format="tif"/><sup>+</sup>:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>V</i>(<i>x</i>)&#x3e;0 for all <i>x&#x2267;</i>0<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>V</i>(<i>y</i>)&#x3c;<i>V</i>(<i>x</i>) for all <i>x&#x3c;y </i>for all <i>x, y&#x2267;</i>0<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>lim x</i>&#x2192;&#x221e; of <i>V</i>(<i>x</i>)=0.<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0093" num="0092">The Gaussian bell curve or V(x)=1/(x+0.001) are such functions. Note: V(x)=1/x is not appropriate as the result of V(0) would not be defined.</p>
<p id="p-0094" num="0093">In one preferred embodiment, the function in Equation [12] is used, where t is any number that is approximately between 1 and 1000. The value t=50 is a good value to start with, if the Analyzing Functions are normalized to a range of 0 . . . 100 as referred to in the section on &#x201c;Normalizing Analyzing Functions&#x201d; that follows equation [4] in this disclosure.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>V</i>(<i>x</i>)=0.5<sup>(x/t)</sup>&#x2003;&#x2003;[12]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0095" num="0094">The Inversion function will be used later in this disclosure to calculate an &#x201c;Inverse Difference&#x201d; between two tuples a<sub>1 . . . n </sub>and b<sub>1 . . . n </sub>by calculating V(D*((a<sub>1 . . . n</sub>), (b<sub>1 . . . n</sub>), (w<sub>1 . . . n</sub>), g*)) or V(D((a<sub>1 . . . n</sub>), (b<sub>1 . . . n</sub>), (w<sub>1 . . . n</sub>))) or V( <o ostyle="single">D</o>*((a<sub>1 . . . n</sub>), (b<sub>1 . . . n</sub>), g*)) or V( <o ostyle="single">D</o>((a<sub>1 . . . n</sub>), (b<sub>1 . . . n</sub>))). The purpose of this Inverse Difference is to provide a high value if similarity between the tuples a<sub>1 . . . n </sub>and b<sub>1 . . . n </sub>is detected, and a low value, if the tuples a<sub>1 . . . n </sub>and b<sub>1 . . . n </sub>are different.</p>
<p id="p-0096" num="0095">The seventh element, C<sub>1 . . . m</sub>, in equation [1] is a set of m &#x201c;Controlling Functions&#x201d;. Each of those Controlling Functions has m parameters and needs to suit the following conditions:</p>
<p id="p-0097" num="0096">C<sub>i</sub>(p<sub>1 </sub>. . . p<sub>m</sub>)&#x2267;0 for all p<sub>1 </sub>. . . p<sub>m </sub>and for all 1&#x2266;i&#x2266;m (all p<sub>1 </sub>. . . p<sub>m </sub>will never be negative).</p>
<p id="p-0098" num="0097">C<sub>i</sub>(p<sub>1 </sub>. . . p<sub>m</sub>) is high if p<sub>i </sub>has a high value compared to the mean of p<sub>1 </sub>. . . p<sub>m </sub></p>
<p id="p-0099" num="0098">C<sub>i</sub>(p<sub>1 </sub>. . . p<sub>m</sub>) is low if p<sub>i </sub>has a low value compared to the mean of p<sub>1 </sub>. . . p<sub>m </sub></p>
<p id="p-0100" num="0099">C<sub>1</sub>+C<sub>2</sub>+ . . . +C<sub>m </sub>is always 1.</p>
<p id="p-0101" num="0100">C<sub>i</sub>(p<sub>1 </sub>. . . p<sub>m</sub>)=C<sub>&#x3a0;(i)</sub>(p<sub>&#x3a0;(1) </sub>. . . p<sub>&#x3a0;(m)</sub>) with &#x3a0; being any permutation &#x3a0;:(1 . . . m)&#x2192;(&#x3a0;(1) . . . &#x3a0;(m)).</p>
<p id="p-0102" num="0101">A recommended equation for such a controlling function would be as shown in Equation [13].
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>C</i><sub>i</sub>(<i>p</i><sub>1 </sub><i>. . . p</i><sub>m</sub>)=<i>p</i><sub>i</sub>/(<i>p</i><sub>1</sub><i>+p</i><sub>2</sub><i>+ . . . +p</i><sub>m</sub>)&#x2003;&#x2003;[13]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0103" num="0102">The purpose of a controlling function C<sub>i </sub>is to provide a large number (close to 1) if the i<sup>th </sup>element of the parameters p<sub>1 </sub>. . . p<sub>m </sub>is high relative to the other parameters, and a small value (close to 0) if the i<sup>th </sup>element of the parameters p<sub>1 </sub>. . . p<sub>m </sub>is relatively low, and to &#x201c;down-scale&#x201d; a tuple of m elements so that their sum is 1.0, while the relations between the elements of the m-tuple are constrained. If the Controlling Functions are applied to a set of m Inverse Differences, the m results of the Controlling Functions will be referred to as &#x201c;Controlled Inverse Differences&#x201d; later in this disclosure.</p>
<p id="h-0017" num="0000">Setting the Elements F, R and A</p>
<p id="p-0104" num="0103">The following section describes the manner in which the user-defined (or otherwise defined) m IRPs can be converted into its associated Performing Functions F and tuples R.</p>
<p id="p-0105" num="0104">Note: In contrary to F and R, the last four elements of the tuple (A, D, V, C) are functions that are defined by the programmer when a system using IRPs is created, and are predefined or only slightly adjustable by the user. However, to a certain extent, a system may give the user control over the functions A, D, V and C; and there can be certain components of the first two elements F<sub>1 . . . m </sub>and R<sub>1 . . . m </sub>that will be set by the application without user influence. This will become clearer later in this disclosure.</p>
<p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. 4</figref> provides a sample image of an image in a dialog box an image processing program, for the purposes of illustrating modifications to an image using the current invention. For example, graphic tag <b>30</b> representing IRP R<sub>1 </sub>in <figref idref="DRAWINGS">FIG. 4</figref>, placed on the left apple, will be to increase saturation, graphic tag <b>32</b> representing IRP R<sub>2</sub>, placed on the right apple, will be decrease saturation, and graphic tag <b>34</b> representing IRP R<sub>3</sub>, placed on the sky, will darken its associated image component.</p>
<p id="p-0107" num="0106">To do so, three performing functions F<sub>1 </sub>. . . F<sub>3 </sub>are necessary, where F<sub>1 </sub>increases the saturation, F<sub>2 </sub>decreases the saturation, and F<sub>3 </sub>is an image darkening image modification.</p>
<p id="p-0108" num="0107">The system should typically allow the user to set such a Performing Function before or after the user places an IRP in the image. In such cases, the user first defines the type of the performing function (such as &#x201c;sharpen,&#x201d; or &#x201c;darken,&#x201d; or &#x201c;increase saturation,&#x201d; etc.) and then the user defines the behavior of the function (such as &#x201c;sharpen to 100%,&#x201d; or &#x201c;darken by 30 levels,&#x201d; etc.).</p>
<p id="p-0109" num="0108">In the current example, three tuples R<sub>1 </sub>. . . R<sub>3 </sub>are necessary. For each IRP, there is always one tuple R and one Performing Function F. It is not necessary, however, that all Performing Functions are different. As previously disclosed, IRPs in the current invention are used to store Characteristics of an individual pixel or a particular area in an image. As such, using the current example of modifying <figref idref="DRAWINGS">FIG. 4</figref>, three IRPs are necessary: an IRP that stores the Characteristics of the first apple, an IRP that stores the Characteristics of the second apple, and an IRP that stores the Characteristics for the sky.</p>
<p id="p-0110" num="0109">This can typically be done by reading the Characteristics of the image location where the user has placed an IRP. If a user has placed an IRP on the image coordinate location x, y in the image I, the values of R=((g<sub>1 </sub>. . . g<sub>n</sub>), g*, (w<sub>1 </sub>. . . w<sub>n</sub>)) can be calculated as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>g</i><sub>1 </sub><i>. . . g</i><sub>n</sub><i>=A</i><sub>1</sub>(<i>I,x,y</i>) . . . <i>A</i><sub>n</sub>(<i>I,x,y</i>)&#x2003;&#x2003;[14]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>g*=</i>0<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>w<sub>1 </sub>. . . w<sub>n</sub>=default value, for example, 1.<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0111" num="0110">The user may have control over the values of R after they were initially filled. This control may be allowed to varying extents, such as weights only versus all variables.</p>
<p id="p-0112" num="0111">In our example the two red apples will be modified differently. Presumably, both apples have the same color and the same structure, and each only differs in its location. The sky, containing the third IRP, has a different location than the apples, and also a different color.</p>
<p id="p-0113" num="0112">As we now see that both location and color are relevant for differentiating between the three relevant image areas, it will be obvious that what is needed is at least one or more Location Analyzing Functions and one or more Color Analyzing Functions. In cases where the application allows the user only to perform global color changes, it would be sufficient to choose only Color Analyzing Functions.</p>
<p id="p-0114" num="0113">Some Analyzing Functions are as follows, where I<sub>xy(r) </sub>refers to the red channel's value of the image I at the location x,y and so forth.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sub>1</sub>(<i>I,x,y</i>)=<i>x</i>&#x2003;&#x2003;[15a]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sub>2</sub>(<i>I,x,y</i>)=<i>y</i>&#x2003;&#x2003;[15b]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sub>3</sub>(<i>I,x,y</i>)=<i>I</i><sub>xy(r)</sub>&#x2003;&#x2003;[15c]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sub>4</sub>(<i>I,x,y</i>)=<i>I</i><sub>xy(g)</sub>&#x2003;&#x2003;[15d]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sub>5</sub>(<i>I,x,y</i>)=<i>I</i><sub>xy(b)</sub>&#x2003;&#x2003;[15e]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0115" num="0114">A<sub>1 </sub>and A<sub>2 </sub>are Location Analyzing Functions and A<sub>3 </sub>through A<sub>5 </sub>are Color Analyzing Functions.</p>
<p id="p-0116" num="0115">Note: A<sub>3 </sub>through A<sub>5</sub>, which only provide the red, green, and blue values, are suitable functions for a set of color-dependent analytical functions. For even better performance it is recommended to derive functions that calculate luminosity, saturation, etc., independently. Using the channels of the image in Lab color mode is appropriate. However, the following Analyzing Functions are also examples of appropriate Analyzing Functions, where the capitalized variables X, Y, R, G, B represent the maximum possible values for the coordinates or the color channels.
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sub>1</sub>(<i>I,x,y</i>)=<i>x*</i>100/<i>X</i>&#x2003;&#x2003;[16a]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sub>2</sub>(<i>I,x,y</i>)=<i>y*</i>100/<i>Y</i>&#x2003;&#x2003;[16b]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sub>3</sub>(<i>I,x,y</i>)=(<i>I</i><sub>xy(r)</sub><i>+I</i><sub>xy(g)</sub><i>+I</i><sub>xy(b)</sub>)*100/(<i>R+G+B</i>)&#x2003;&#x2003;[16c]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sub>4</sub>(<i>I,x,y</i>)=100*(<i>I</i><sub>xy(r)</sub><i>&#x2212;I</i><sub>xy(g)</sub>)/(<i>R+G</i>)+50&#x2003;&#x2003;[16d]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>A</i><sub>5</sub>(<i>I,x,y</i>)=100*(<i>I</i><sub>xy(r)</sub><i>&#x2212;I</i><sub>xy(b)</sub>)/(<i>R+B</i>)+50&#x2003;&#x2003;[16e]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0117" num="0116">Equations [16] shows Analyzing Functions that are also normalized to a range of 0 . . . 100 (see the description for normalizing Analyzing Functions after equation [4]) Normalizing the Analyzing Functions aids in the implementation, as normalized Analyzing Functions have the advantage that their results always have the same range, regardless of the image size or other image characteristics. The Analyzing Functions found in Equations [15] will be used throughout this disclosure when discussing values from R<sub>1 . . . m </sub></p>
<p id="p-0118" num="0117">Note: It may not be useful to adjust the set of Analyzing Functions from image to image. It may be preferable to use one set of Analyzing Functions that is suitable for many or all image types. When the current invention is used for standard color enhancements, the Analyzing Functions of Equations [16] are good to start with.</p>
<p id="h-0018" num="0000">A Closer Look at IRPs</p>
<p id="p-0119" num="0118">As previously discussed in this disclosure, the tuples R of an IRP store the information of the Characteristics of the region to which an operation will be applied, the region of interest. These tuples R acquire the Characteristics typically by applying the n analytical functions to the image location I<sub>xy </sub>where the IRP was placed, as in equation [14].</p>
<p id="p-0120" num="0119">In the current embodiment, the Difference Function D* will compare the values g<sub>1 </sub>. . . g<sub>n </sub>of each IRP to the results of the n Analyzing Functions for all pixels in the image, using the weights w<sub>1 </sub>. . . w<sub>n</sub>.</p>
<p id="p-0121" num="0120">For example, if the pixel in the middle of the left apple has the coordinates (10, 100) and the RGB color 150, 50, 50 (red), then the Analyzing Functions A<sub>1 </sub>. . . A<sub>n </sub>of this pixel will have the values A<sub>1</sub>=10, A<sub>2</sub>=100, A<sub>3</sub>=150, A<sub>4</sub>=50, A<sub>5</sub>=50, therefore, the values g<sub>1 </sub>. . . g<sub>n </sub>will be set to (10, 10, 150, 50, 50).</p>
<p id="p-0122" num="0121">g* is set to zero for this IRP.</p>
<p id="p-0123" num="0122">The weights will control the significance of the individual elements of g<sub>1 </sub>. . . g<sub>5</sub>. See Equations [6], [7] and [8]. For example, if the weights w<sub>1 </sub>. . . w<sub>5 </sub>are set to (10,10,1,1,1), the location related information, gained through A<sub>1 </sub>and A<sub>2</sub>, will be more significant than the color related information from A<sub>3 </sub>through A<sub>5</sub>. (This IRP would be more location dependent than color dependent).</p>
<p id="p-0124" num="0123">If, however, w<sub>1 </sub>. . . w<sub>5</sub>=(0,0,3,3,0) is set, only the red and green channels of the pixel information would be considered by the Difference Function, and the IRP would not differentiate between the location of a pixel or its blue channel. As previously mentioned, in <figref idref="DRAWINGS">FIG. 4</figref> the location-dependent and color-dependent Characteristics play a role in differentiating the apples from each other and from the sky. Therefore, we will use equal weights for all 5 characteristics.</p>
<p id="p-0125" num="0124">Setting all weights all to 1, the first IRP would be:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>1</sub>=(<i>g</i>1 . . . <i>g</i>5,<i>g*,w</i>1 . . . <i>w</i>5)=((10,100,150,50,50),0,(1,1,1,1,1))<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0126" num="0125">(the first apple at the coordinate 10,100 with the color 150,50,50)</p>
<p id="p-0127" num="0126">The second and third IRP could have values such as
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>2</sub>=((190,100,150,50,50),0,(1,1,1,1,1))<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0128" num="0127">(the second apple at the coordinate 190,100 with the color 150,50,50)
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>3</sub>=((100,10,80,80,200),0,(1,1,1,1,1)).<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0129" num="0128">(the sky at the coordinate 100,10 with the color 80,80,200)</p>
<p id="h-0019" num="0000">The Mixing Function</p>
<p id="p-0130" num="0129">An abbreviation related to the Difference Function follows. The purpose of the Difference Function is to calculate a value that indicates how &#x201c;different&#x201d; a pixel in the image is from the Characteristics that a certain IRP is associated with.</p>
<p id="p-0131" num="0130">The &#x201c;Difference&#x201d; between an IRP R=((g<sub>1 </sub>. . . g<sub>n</sub>), g*, (w<sub>1 </sub>. . . w<sub>n</sub>)) and a pixel I<sub>xy </sub>can be written as follows:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>|<i>R&#x2212;I</i><sub>xy</sub><i>|=D</i>*((<i>g</i><sub>1 </sub><i>. . . g</i><sub>n</sub>),(<i>A</i><sub>1</sub>(<i>I,x,y</i>), . . . ,<i>A</i><sub>n</sub>(<i>I,x,y</i>)),(<i>w</i><sub>1</sub><i>, . . . , w</i><sub>n</sub>), <i>g</i>*)&#x2003;&#x2003;[17]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0132" num="0131">The Difference referred to in this embodiment is always the result of the Difference function, and should not be confused with the &#x201c;spatial&#x201d; distance between two pixels in an image.</p>
<p id="p-0133" num="0132">If, for ease of implementation or for faster computing of the Mixing Function, the Difference Functions D, <o ostyle="single">D</o> or <o ostyle="single">D</o>* are used, the abbreviation would be:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>|<i>R&#x2212;I</i><sub>xy</sub><i>|=D</i>((<i>g</i><sub>1 </sub><i>. . . g</i><sub>n</sub>),(<i>A</i><sub>1</sub>(<i>I,x,y</i>), . . . ,<i>A</i><sub>n</sub>(<i>I,x,y</i>)),(<i>w</i><sub>1</sub><i>, . . . , w</i><sub>n</sub>))&#x2003;&#x2003;[18]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>|<i>R&#x2212;I</i><sub>xy</sub><i>|= <o ostyle="single">D</o></i>((<i>g</i><sub>1 </sub><i>. . . g</i><sub>n</sub>),(<i>A</i><sub>1</sub>(<i>I,x,y</i>), . . . ,<i>A</i><sub>n</sub>(<i>I,x,y</i>)))&#x2003;&#x2003;[19]<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>|<i>R&#x2212;I</i><sub>xy</sub><i>|= <o ostyle="single">D</o></i>*((<i>g</i><sub>1 </sub><i>. . . g</i><sub>n</sub>),(<i>A</i><sub>1</sub>(<i>I,x,y</i>), . . . ,<i>A</i><sub>n</sub>(<i>I,x,y</i>)),<i>g</i>*)&#x2003;&#x2003;[20]<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0134" num="0133">Given the 7-tupel of an IRP based image modification (F<sub>1 . . . m</sub>, R<sub>1 . . . m</sub>, I, A<sub>1 . . . n</sub>, D, V, C) then the modified image I*xy is as show in Equation [21].</p>
<p id="p-0135" num="0134">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msubsup>
          <mi>I</mi>
          <mi>xy</mi>
          <mo>*</mo>
        </msubsup>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>m</mi>
          </munderover>
          <mo>&#x2062;</mo>
          <mrow>
            <mrow>
              <msub>
                <mi>F</mi>
                <mi>i</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>I</mi>
                  <mo>,</mo>
                  <mi>x</mi>
                  <mo>,</mo>
                  <mi>y</mi>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>*</mo>
            <mrow>
              <msub>
                <mi>C</mi>
                <mi>i</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>V</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mo>&#xf603;</mo>
                        <mrow>
                          <msub>
                            <mi>R</mi>
                            <mn>1</mn>
                          </msub>
                          <mo>-</mo>
                          <msub>
                            <mi>I</mi>
                            <mi>xy</mi>
                          </msub>
                        </mrow>
                        <mo>&#xf604;</mo>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>,</mo>
                  <mi>&#x2026;</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>,</mo>
                  <mrow>
                    <mi>V</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mo>&#xf603;</mo>
                        <mrow>
                          <msub>
                            <mi>R</mi>
                            <mi>m</mi>
                          </msub>
                          <mo>-</mo>
                          <msub>
                            <mi>I</mi>
                            <mi>xy</mi>
                          </msub>
                        </mrow>
                        <mo>&#xf604;</mo>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mn>21</mn>
        <mo>]</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0136" num="0135">Apply this equation to each pixel in the image I, to receive the processed image I*, where all Performing Functions were applied to the image according to the IRPs that the user has set. This equation compares the n Characteristics of each pixel x, y against all IRPs, and applies those Performing Functions F<sub>i </sub>to a greater extent to the pixel, whose IRPs have similar Characteristics, while the Controlling Function ensures that the sum of all functions F<sub>i </sub>does not exceed unwanted ranges.</p>
<p id="p-0137" num="0136">In an even further preferred embodiment of the current invention, equation [22] would be used.</p>
<p id="p-0138" num="0137">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msubsup>
          <mi>I</mi>
          <mi>xy</mi>
          <mo>*</mo>
        </msubsup>
        <mo>=</mo>
        <mrow>
          <msub>
            <mi>I</mi>
            <mi>xy</mi>
          </msub>
          <mo>+</mo>
          <mrow>
            <munderover>
              <mo>&#x2211;</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>m</mi>
            </munderover>
            <mo>&#x2062;</mo>
            <mrow>
              <mi>&#x394;</mi>
              <mo>&#x2062;</mo>
              <mstyle>
                <mspace width="0.3em" height="0.3ex"/>
              </mstyle>
              <mo>&#x2062;</mo>
              <mrow>
                <msub>
                  <mi>F</mi>
                  <mi>i</mi>
                </msub>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>I</mi>
                    <mo>,</mo>
                    <mi>x</mi>
                    <mo>,</mo>
                    <mi>y</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>*</mo>
              <mrow>
                <mi>V</mi>
                <mo>&#x2061;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mo>&#xf603;</mo>
                    <mrow>
                      <msub>
                        <mi>R</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>-</mo>
                      <msub>
                        <mi>I</mi>
                        <mi>xy</mi>
                      </msub>
                    </mrow>
                    <mo>&#xf604;</mo>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mn>22</mn>
        <mo>]</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0139" num="0138">In contrast to equation [21], equation [22] requires that the Inversion Function V does not exceed values of approximately 1. The Gaussian Bell curve V(x)=e<sup>&#x2212;x</sup><sup><sup2>2 </sup2></sup>or 1/(x+1) or equation [12] could be such functions. The function &#x394;F expresses the difference between the original and modified image (where I&#x2032;<sub>xy</sub>=I<sub>xy</sub>+&#x394;F(I, x, y) instead of I&#x2032;<sub>xy</sub>=F(I, x, y), see Equation 2).</p>
<p id="p-0140" num="0139">When comparing Equation [21] and [22], the terms V(|R<sub>i</sub>&#x2212;Ixy|) represent the Inverse Difference of the currently processed tuple R<sub>i </sub>and the pixel I<sub>xy</sub>. Only equation [21] uses Controlled Inverse Differences. If equation [21] is used, each pixel in the image will be filtered with a 100% mix of all Performing Functions, regardless if an image region contains a large or a small number of IRPs. The more IRPs that are positioned in the image, the less effect an individual IRP will have if Equation [21] is used. If Equation [22] is used, the IRPs will not show this competitive nature. That is, each IRP will modify the image to a certain extent regardless whether it is placed amidst many other IRPs or not. Therefore, if Equation [22] is used, placing multiple IRPs in an image area will increase the total amount of image modification in this area.</p>
<heading id="h-0020" level="1">Further Embodiments</heading>
<p id="p-0141" num="0140">In a further embodiment, the concept of &#x201c;Abstract IRPs&#x201d; can be used to enhance the performed image modification, or to change the behavior of the image modification.</p>
<p id="p-0142" num="0141">Abstract IRPs are similar to other IRPs as they are pairs of a Performing Function F and a set of values R. Both Abstract IRPs and IRPs may be used together to modify an image. Abstract IRPs, however, are not &#x201c;user defined&#x201d; IRPs or IRPs that are placed in the image by the user. The function of an Abstract IRP can be to limit the local &#x201c;effect&#x201d; or intensity of an IRP. In this regard, Abstract IRPs are typically not &#x201c;local&#x201d;, i.e., they affect the entire image. Abstract IRPs can be implemented in a manner that the user turns a user-controlled element on or off as illustrated later, so that the Abstract IRPs are not presented as IRPs to the user, as shown in <figref idref="DRAWINGS">FIG. 4</figref>.</p>
<p id="p-0143" num="0142">Note: The use of Abstract IRPs as disclosed below requires that equation [21] is implemented as the mixing function, and that the Difference function is implemented as shown in equation [17] or [20].</p>
<p id="p-0144" num="0143">In <figref idref="DRAWINGS">FIG. 4</figref> the user has positioned graphic tags <b>30</b>, <b>32</b>, and <b>34</b> representing IRPs R<sub>1 </sub>. . . R<sub>3</sub>. Controls <b>36</b>, <b>38</b>, and <b>40</b> indicate a set of three possible user controls. When control <b>36</b> is used, the application would use one additional pre-defined Abstract IRP in the image modification. Such pre-defined, Abstract IRPs could, for example, be IRPs R<sub>4 </sub>through R<sub>6 </sub>as described below.</p>
<p id="p-0145" num="0144">When the check box in control <b>36</b> is enabled, Abstract IRP R<sub>4</sub>, is utilized. Without the use of an Abstract IRP, when an image has an area such as the cactus <b>42</b> which is free of IRPs, this area will still be filtered by a 100% mix of the effects of all IRPs (see equation [19] and the Controlling Function C). In the current image example, the cactus <b>42</b> would be affected by a mix of the IRPs R<sub>1 </sub>. . . R<sub>3</sub>, although the user has placed no IRP on the cactus.</p>
<p id="p-0146" num="0145">To remedy this, Abstract IRP R<sub>4 </sub>is utilized which makes use of the g* value. Note: g* is used as described below when the mixing function of equation [21] is being implemented.</p>
<p id="p-0147" num="0146">The Abstract IRP could have zero weights and a g* value greater than zero, such as
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>4</sub>=((0,0,0,0,0),50,(0,0,0,0,0))<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0148" num="0147">The Difference Function |R<sub>4</sub>&#x2212;I<sub>xy</sub>| will return nothing but 50 whatever the Characteristics of the pixel I<sub>xy </sub>might be. The value of g* should be in the range of 1 to 1000. 50 is a good value to start with.</p>
<p id="p-0149" num="0148">The purpose of this IRP and its R<sub>4 </sub>is that pixels in areas free of IRPs, such as in the middle of the cactus <b>42</b>, will have a lower Difference to R<sub>4 </sub>(which is constantly set to 50) than to R<sub>1 </sub>. . . R<sub>3</sub>. For pixels in image areas where one or more IRPs are set, R<sub>4 </sub>will not be the IRP with the lowest Difference, as a different IRP will likely have a lower Difference. In other words: areas free of non-Abstract IRPs are controlled predominantly by R<sub>4</sub>, and areas that do contain non-Abstract IRPs will be affected to a lesser extent by R<sub>4</sub>. If the Performing Function F<sub>4 </sub>is set to a function that does not change the image (F<sub>4</sub>(I,x,y)=Ixy), R<sub>4 </sub>ensures that areas free of IRPs will remain mainly unaffected.</p>
<p id="p-0150" num="0149">In order to make Abstract IRP R<sub>4 </sub>more effective (i.e., IRPs R<sub>1 </sub>. . . R<sub>3 </sub>less effective), g* can be lowered, and the value g* in R<sub>4 </sub>can be raised to make the &#x201c;active&#x201d; IRPs R<sub>1 </sub>. . . R<sub>3 </sub>more effective. A fixed value for g* in R<sub>4 </sub>may be implemented if the system that is programmed is designed for image retouchers with average skills for example, and applications designed for advanced users may permit the user to change the setting of g*.</p>
<p id="p-0151" num="0150">In an even further embodiment of the current invention, Abstract IRPs could be used whereby an IRP has weights equaling zero for the location dependent parameters, and values for g<sub>1 </sub>. . . g<sub>n </sub>which would represent either black or white, combined with a Performing Function which does not affect the image.</p>
<p id="p-0152" num="0151">Two of such Abstract IRPs&#x2014;one for black, one for white&#x2014;would be suitable to ensure that black and white remain unaffected. Such Abstract IRPs could be:
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>5</sub>=((0,0,255,255,255),0,(0,0,1,1,1))<?in-line-formulae description="In-line Formulae" end="tail"?>
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>R</i><sub>6</sub>=((0,0,0,0,0),0,(0,0,1,1,1))<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0153" num="0152">As with R<sub>4 </sub>and F<sub>4</sub>, the Performing Functions F<sub>5 </sub>and F<sub>6 </sub>would also be functions that do not perform any image modification, so the IRPs 5 and 6 would ensure that colors such as black and white remain mainly unaffected by the IRPs that the user places.</p>
<p id="p-0154" num="0153">As shown in control <b>38</b> and control <b>40</b>, these Abstract IRPs can be implemented providing the user with the ability to turn checkboxes or similar user controls on or off. Such checkboxes control the specified function that the Abstract IRPs would have on the image. When the associated checkbox is turned on, the application uses this Abstract IRP This process is referred to as &#x201c;load abstract IRPs&#x201d; in step <b>18</b> of <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0155" num="0154">It is not necessary that all Abstract IRPs are associated with a Performing Function that leaves the image unaffected. If for instance an implementation is programmed that allows the user to sharpen the image, an abstract IRP such as R above can be implemented, where the associated Performing Function F4 sharpens the image to 50%. The user could then place IRPs whose Performing Functions sharpen the image to for instance to 0%, 25%, 75% or 100% in the image. This would mean that the image is sharpened to an individual extent where the user has set IRPs, and to 50% anywhere else.</p>
<p id="p-0156" num="0155">In an even further embodiment, the IRP based image modification can be used in combination with a further, global image modification I&#x2032;<sub>xy</sub>=M(I, x, y), where M is an image filter, combining the IRP based image modification and the uniform image modification M as shown in Equation [23].</p>
<p id="p-0157" num="0156">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msubsup>
          <mi>I</mi>
          <mi>xy</mi>
          <mo>*</mo>
        </msubsup>
        <mo>=</mo>
        <mrow>
          <mi>M</mi>
          <mo>(</mo>
          <mrow>
            <msub>
              <mi>I</mi>
              <mi>xy</mi>
            </msub>
            <mo>+</mo>
            <mrow>
              <munderover>
                <mo>&#x2211;</mo>
                <mrow>
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>m</mi>
              </munderover>
              <mo>&#x2062;</mo>
              <mrow>
                <mrow>
                  <msub>
                    <mi>F</mi>
                    <mi>i</mi>
                  </msub>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>I</mi>
                      <mo>,</mo>
                      <mi>x</mi>
                      <mo>,</mo>
                      <mi>y</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>*</mo>
                <mrow>
                  <mi>V</mi>
                  <mo>&#x2061;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mo>&#xf603;</mo>
                      <mrow>
                        <msub>
                          <mi>R</mi>
                          <mi>i</mi>
                        </msub>
                        <mo>-</mo>
                        <msub>
                          <mi>I</mi>
                          <mi>xy</mi>
                        </msub>
                      </mrow>
                      <mo>&#xf604;</mo>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mn>23</mn>
        <mo>]</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0158" num="0157">Equation [23] is derived from equation [21]. Equation [22] could also be utilized for this embodiment. The current embodiment is useful for a variety of image filter types M, especially those that lead to unwanted image contrast when applied, causing what is known to those skilled in the art as &#x201c;blown-out areas&#x201d; of a digital image. Such image filters M could be color to black and white conversions, increasing the overall contrast, inverting the image, applying a strong stylistic effect, a solarization filter, or other strong image modifications.</p>
<p id="p-0159" num="0158">Applying such an image modification such as a color to black and white conversion without the current invention, the user would first convert the image to black and white, inspect areas of the resulting black and white image that are too dark or too bright, then undo the image modification, make changes to the original image to compensate for the filter application, and then re-apply the image modification, until the resulting image no longer has the unwanted effects.</p>
<p id="p-0160" num="0159">While implementing this filter in combination with an IRP based image modification as shown in Equation [23], the user can modify contrast and color of the image as the image modification M is applied, such as in the example of the black and white conversion, thus accelerating the method of application by the user for the black and white conversion process and providing improved results.</p>
<p id="h-0021" num="0000">Offset Vectors</p>
<p id="p-0161" num="0160">In an even further embodiment, the Performing Functions F<sub>i </sub>can be replaced with &#x201c;Offset Vectors&#x201d; S<sub>i</sub>=(&#x394;x<sub>i </sub>&#x394;y<sub>i</sub>)<sup>T</sup>, where S<sub>1 . . . m </sub>are the m Offset Vectors associated with the m IRPs, and &#x394;x and &#x394;y are any real numbers. In this case, the user would define such an Offset Vector of an IRP for instance by defining a direction and a length, or by dragging an IRP symbol with a mouse button different from the standard mouse button. The mixing function, for instance if derived from equation [21], would then be</p>
<p id="p-0162" num="0161">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>S</mi>
          <mi>xy</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <munderover>
            <mo>&#x2211;</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>m</mi>
          </munderover>
          <mo>&#x2062;</mo>
          <mrow>
            <msub>
              <mi>S</mi>
              <mi>i</mi>
            </msub>
            <mo>*</mo>
            <mrow>
              <msub>
                <mi>C</mi>
                <mi>i</mi>
              </msub>
              <mo>&#x2061;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mi>V</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mo>&#xf603;</mo>
                        <mrow>
                          <msub>
                            <mi>R</mi>
                            <mn>1</mn>
                          </msub>
                          <mo>-</mo>
                          <msub>
                            <mi>I</mi>
                            <mi>xy</mi>
                          </msub>
                        </mrow>
                        <mo>&#xf604;</mo>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>,</mo>
                  <mi>&#x2026;</mi>
                  <mo>&#x2062;</mo>
                  <mstyle>
                    <mspace width="0.8em" height="0.8ex"/>
                  </mstyle>
                  <mo>,</mo>
                  <mrow>
                    <mi>V</mi>
                    <mo>&#x2061;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mo>&#xf603;</mo>
                        <mrow>
                          <msub>
                            <mi>R</mi>
                            <mi>m</mi>
                          </msub>
                          <mo>-</mo>
                          <msub>
                            <mi>I</mi>
                            <mi>xy</mi>
                          </msub>
                        </mrow>
                        <mo>&#xf604;</mo>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>[</mo>
        <mn>24</mn>
        <mo>]</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
</maths>
</p>
<p id="p-0163" num="0162">Of course, if this function is assembled of vectors of <img id="CUSTOM-CHARACTER-00003" he="3.56mm" wi="3.56mm" file="US08625925-20140107-P00003.TIF" alt="custom character" img-content="character" img-format="tif"/>, the result is a matrix S<sub>xy </sub>of the same horizontal and vertical dimensions as the image, whose elements are vectors with two elements. For further reference, I refer to this matrix as an &#x201c;Offset Matrix&#x201d;.</p>
<p id="p-0164" num="0163">Using this implementation, the user can easily attach IRPs to regions in the image and at the same time define in which directions the user wants these regions to be distorted or altered.</p>
<p id="p-0165" num="0164">The result of the mixing function is an offset matrix that contains information relating to in which direction a pixel of the original image I needs to be distorted to achieve the distorted image I<sup>d</sup>. The benefit of calculating the Offset Matrix this way is that the Offset Matrix adapts to the features of the image, provided that the vectors R<sub>1 . . . m </sub>have weights other than zero for pixel luminosity, chrominance, and structure Characteristics. The image I<sup>d </sup>can be calculated the following way:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0165">(1) Reserve some memory space for I<sup>d</sup>, and flag all of its pixels.</li>
        <li id="ul0002-0002" num="0166">(2) Select the first coordinate (x,y) in I.</li>
        <li id="ul0002-0003" num="0167">(3) Write the values (such as r,g,b) of the pixel I<sub>xy </sub>into the picture I<sup>d </sup>at the location (x,y)+S<sub>xy</sub>, and un-flag the pixel at that location in I<sup>d</sup>.</li>
        <li id="ul0002-0004" num="0168">(4) Unless all pixels in I are considered, select next coordinate (x,y) and proceed with step (3).</li>
        <li id="ul0002-0005" num="0169">(5) Select first pixel in I<sup>d </sup>that is still flagged.</li>
        <li id="ul0002-0006" num="0170">(6) Assign the values (such as r,g,b) of the closest non-flagged pixel to this pixel. If multiple non-flagged pixels are equally close, select the values of that pixel that was created using the lowest Offset Vector S<sub>xy</sub>.</li>
        <li id="ul0002-0007" num="0171">(7) If flagged pixels are left, select next flagged pixel in I<sup>d </sup>and proceed with step (6).</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0166" num="0172">In other words, copy each pixel from I into I<sup>d </sup>while using the elements of the Offset Matrix S for offsetting that pixel. Those areas that remain empty in I<sup>d </sup>shall be filled with the pixel values neighbored to the empty area in I<sup>d</sup>, while values of pixels that were moved to the least extent during the copy process shall be preferred.</p>
<heading id="h-0022" level="1">Further Embodiments</heading>
<p id="p-0167" num="0173">In a further embodiment, a plurality or IRPs can be saved and applied to one or more different images. In batch processing applications, this plurality of IRPs can be stored and applied to multiple images. In such an embodiment, it is important that IRPs whose weights for location-dependent characteristics are zero.</p>
<p id="p-0168" num="0174">In a further embodiment, the user may be provided with simplified control over the weights of an IRP by using a unified control element. In Equations [15] and Equations [16], five Characteristics are utilized, two of which are location dependent Characteristics sourcing from Location Analyzing Functions.</p>
<p id="p-0169" num="0175">In creating such a unified control element, one control element controls these two weights. This unified control element could be labeled &#x201c;location weight,&#x201d; instead of the two elements &#x201c;horizontal location weight&#x201d; and &#x201c;vertical location weight.&#x201d;</p>
<p id="p-0170" num="0176">In a further embodiment, user control elements may be implemented that display different values for the weights as textual descriptions instead of numbers, as such numbers are often confusing to users. Those skilled in the art will recognize that it may be confusing to users that low values for weights lead to IRPs that have more influence on the image, and vice versa. Regarding weights for location-dependent Characteristics (such as w<sub>1 </sub>and w<sub>2 </sub>in the current example), the user could be allowed to choose one out of five pre-defined weights for textual descriptions of different values for the location dependent weights w<sub>1 </sub>and w<sub>2 </sub>as show in Table 1.</p>
<p id="p-0171" num="0177">
<tables id="TABLE-US-00001" num="00001">
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="offset" colwidth="119pt" align="left"/>
<colspec colname="1" colwidth="14pt" align="center"/>
<colspec colname="2" colwidth="84pt" align="center"/>
<thead>
<row>
<entry/>
<entry namest="offset" nameend="2" rowsep="1">TABLE 1</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
<row>
<entry/>
<entry>w<sub>1</sub></entry>
<entry>w<sub>2</sub></entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="2" align="center" rowsep="1"/>
</row>
</thead>
<tbody valign="top">
<row>
<entry/>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="offset" colwidth="35pt" align="left"/>
<colspec colname="1" colwidth="84pt" align="left"/>
<colspec colname="2" colwidth="14pt" align="char" char="."/>
<colspec colname="3" colwidth="84pt" align="char" char="."/>
<tbody valign="top">
<row>
<entry/>
<entry>&#x201c;global&#x201d;</entry>
<entry>0</entry>
<entry>0</entry>
</row>
<row>
<entry/>
<entry>&#x201c;almost global&#x201d;</entry>
<entry>0.3</entry>
<entry>0.3</entry>
</row>
<row>
<entry/>
<entry>&#x201c;default&#x201d;</entry>
<entry>1</entry>
<entry>1</entry>
</row>
<row>
<entry/>
<entry>&#x201c;local&#x201d;</entry>
<entry>3</entry>
<entry>3</entry>
</row>
<row>
<entry/>
<entry>&#x201c;very local&#x201d;</entry>
<entry>8</entry>
<entry>8</entry>
</row>
<row>
<entry/>
<entry namest="offset" nameend="3" align="center" rowsep="1"/>
</row>
</tbody>
</tgroup>
</table>
</tables>
</p>
<p id="p-0172" num="0178"><figref idref="DRAWINGS">FIG. 5</figref> illustrates how such simplified user control over weights may be implemented in an image processing program.</p>
<p id="p-0173" num="0179">In a further embodiment, the user control over weights could be simplified to such an extent that there are only two types of weights for IRPs that the user can choose from: &#x201c;strong&#x201d; that utilizes weight vectors such as (1,1,1,1,1) and &#x201c;weak&#x201d; that utilizes weight vectors such as (3,3,3,3,3). Note: As mentioned before, large weights make the area that an IRP has influence on smaller, and vice versa.</p>
<p id="p-0174" num="0180">For example, the user may place IRPs in the sky with an associated enhancement to increase the saturation of an area identified by one or more IRPs. In the same image, the user may place additional IRPs with an assigned function to decrease contrast, identifying changes in contrast and the desired changes in contrast based on the location of each individual IRP. In a preferred embodiment, IRPs may include a function that weights the intensity of the image-editing function as indicated by the user.</p>
<p id="p-0175" num="0181">In a different implementation of the invention, IRPs could be placed to identify a color globally across the image, and using an associated command, increase the saturation of the identified color.</p>
<p id="p-0176" num="0182">In a still further preferred embodiment, IRPs could be used to provide varying degrees of sharpening across a digital image. In such an implementation, multiple IRP's could be placed within specific image regions or image characteristics, such as the eyes, the skin, and hair of a portrait, and different sharpening intensities assigned to each IRP and applied to the digital image while considering the presence of color and/or contrast and the relative difference of each IRP from one another to provide the desired image adjustment.</p>
<p id="p-0177" num="0183">All features disclosed in the specification, including the claims, abstract, and drawings, and all the steps in any method or process disclosed, may be combined in any combination, except combinations where at least some of such features and/or steps are mutually exclusive. Each feature disclosed in the specification, including the claims, abstract, and drawings, can be replaced by alternative features serving the same, equivalent or similar purpose, unless expressly stated otherwise. Thus, unless expressly stated otherwise, each feature disclosed is one example only of a generic series of equivalent or similar features.</p>
<p id="p-0178" num="0184">This invention is not limited to particular hardware described herein, and any hardware presently existing or developed in the future that permits processing of digital images using the method disclosed can be used, including for example, a digital camera system.</p>
<p id="p-0179" num="0185">A computer readable medium is provided having contents for causing a computer-based information handling system to perform the steps described herein, and to display the graphical user interface disclosed herein.</p>
<p id="p-0180" num="0186">The term memory block refers to any possible computer-related image storage structure known to those skilled in the art, including but not limited to RAM, Processor Cache, Hard Drive, or combinations of those, including dynamic memory structures. Preferably, the methods and graphical user interface disclosed will be embodied in a computer program (not shown) either by coding in a high level language, or by preparing a filter which is complied and available as an adjunct to an image processing program. For example, in a preferred embodiment, the methods and graphical user interface is compiled into a plug-in filter that can operate within third party image processing programs such as Adobe Photoshop&#xae;.</p>
<p id="p-0181" num="0187">Any currently existing or future developed computer readable medium suitable for storing data can be used to store the programs embodying the afore-described interface, methods and algorithms, including, but not limited to hard drives, floppy disks, digital tape, flash cards, compact discs, and DVDs. The computer readable medium can comprise more than one device, such as two linked hard drives. This invention is not limited to the particular hardware used herein, and any hardware presently existing or developed in the future that permits image processing can be used.</p>
<p id="p-0182" num="0188">Any currently existing or future developed computer readable medium suitable for storing data can be used, including, but not limited to hard drives, floppy disks, digital tape, flash cards, compact discs, and DVDs. The computer readable medium can comprise more than one device, such as two linked hard drives, in communication with the processor.</p>
<p id="p-0183" num="0189">A method for image processing of a digital image has disclosed comprising the steps of determining one or more sets of pixel characteristics; determining for each pixel characteristic set, an image editing function; providing a mixing function algorithm embodied on a computer-readable medium for modifying the digital image; and processing the digital image by applying the mixing function algorithm based on the one or more pixel characteristic sets and determined image editing functions. In one embodiment, the mixing function algorithm comprises a difference function. Optionally, the difference function algorithm calculates a value based on the difference of between pixel characteristics and one of the one or more determined pixel characteristic sets. In another embodiment, the mixing function algorithm includes a controlling function for normalizing the calculations.</p>
<p id="p-0184" num="0190">In a further embodiment, the method adds the step of determining for each pixel characteristic set, a set of weighting values, and the processing step further comprises applying the mixing function algorithm based on the determined weighting value set.</p>
<p id="p-0185" num="0191">In a further embodiment, a first pixel characteristic set is determined, and at least one characteristic in the first pixel characteristic set is location dependent, and at least one characteristic in the first pixel characteristic set is either color dependent, or structure dependent, or both. Alternatively, a first pixel characteristic set is determined, and at least two different characteristics in the first pixel characteristic set are from the group consisting of location dependent, color dependent, and structure dependent.</p>
<p id="p-0186" num="0192">A method for processing of a digital image has been disclosed, comprising the steps of receiving the coordinates of one or more than one image reference point defined by a user within the digital image; receiving one or more than one image editing function assigned by the user and associated with the coordinates of the one or more than one defined image reference point; providing a mixing function algorithm embodied on a computer-readable medium for modifying the digital image; and processing the digital image by applying the mixing function algorithm based on the one or more than one assigned image editing function and the coordinates of the one or more than one defined image reference point. The method may optionally further comprise displaying a graphical icon at the coordinates of a defined image reference point.</p>
<p id="p-0187" num="0193">A mixing function algorithm suitable to the invention has been described, and exemplar alternative embodiments are disclosed, including a group consisting of a Pythagoras distance approach which calculates a geometric distance between each pixel of the digital image to the coordinates of the one or more than one defined image reference point, a color curves approach, a segmentation approach, a classification approach, an expanding areas approach, and an offset vector approach. Optionally, the segmentation approach comprises multiple segmentation, and additionally optionally the classification approach adjusts for similarity of pixel attributes. The mixing function algorithm may optionally operate as a function of the calculated geometric distance from each pixel of the digital image to the coordinates of the defined image reference points.</p>
<p id="p-0188" num="0194">Optionally, the disclosed method further comprises receiving one or more assigned image characteristics associated with the coordinates of a defined image reference point, and wherein the mixing function algorithm calculates a characteristic difference between the image characteristics of a pixel of the digital image and the assigned image characteristics. The mixing function algorithm may also calculate a characteristic difference between the image characteristics of a pixel and the image characteristics of one or more pixels neighboring the coordinates of one or more defined image reference point.</p>
<p id="p-0189" num="0195">Additionally, optionally other steps may be added to the method. For example, the method may further comprise receiving one or more weighting values, and the processing step further comprising applying the mixing function algorithm based on weighting values; or further comprise receiving one or more regions of interest associated with the coordinates of one or more defined image reference point; or further comprise the step of providing an graphical user interface comprising a first interface to receive the coordinates of the one or more defined image reference points, and a second interface to receive the one or more assigned image editing functions.</p>
<p id="p-0190" num="0196">A method for processing of a digital image comprising pixels having image characteristics has been disclosed comprising the steps defining the location of image reference points within the digital image; determining image editing functions; and processing the digital image by applying the determined image editing functions based upon either the location of the defined image reference points, or the image characteristics of the pixels at the location of the defined image reference points, or both.</p>
<p id="p-0191" num="0197">A method for image processing of a digital image has also been disclosed comprising the steps of providing one or more than one image processing filter; setting the coordinates of one or more than one image reference point within the digital image; providing a mixing function algorithm embodied on a computer-readable medium for modifying the digital image; and processing the digital image by applying the mixing algorithm based on the one or more than one image processing filter and the coordinates of the one or more than one set image reference point. Optionally, various filters may be used, including but not limited to a noise reduction filter, a sharpening filter, or a color change filter.</p>
<p id="p-0192" num="0198">Also, any element in a claim that does not explicitly state &#x201c;means for&#x201d; performing a specified function or &#x201c;step for&#x201d; performing a specified function, should not be interpreted as a &#x201c;means&#x201d; or &#x201c;step&#x201d; clause as specified in 35 U.S.C. &#xa7;112.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US08625925-20140107-M00001.NB">
<img id="EMI-M00001" he="8.47mm" wi="76.20mm" file="US08625925-20140107-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US08625925-20140107-M00002.NB">
<img id="EMI-M00002" he="8.47mm" wi="76.20mm" file="US08625925-20140107-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US08625925-20140107-M00003.NB">
<img id="EMI-M00003" he="8.47mm" wi="76.20mm" file="US08625925-20140107-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US08625925-20140107-M00004.NB">
<img id="EMI-M00004" he="8.47mm" wi="76.20mm" file="US08625925-20140107-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-claim-statement>I claim:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A method for distorting a digital image comprising the steps of: receiving the coordinates of one or more than one image reference point defined by a user within the digital image;
<claim-text>providing a computer-based information handling system to perform the steps of:</claim-text>
<claim-text>receiving one or more than one spatial offset assigned by the user and associated with the coordinates of the one or more than one defined image reference point;</claim-text>
<claim-text>providing a mixing function algorithm embodied on a computer-readable medium for distorting the digital image;</claim-text>
<claim-text>calculating an offset matrix by applying the mixing function algorithm based on the one or more than one spatial offset and the coordinates of the one or more than one defined image reference point; and</claim-text>
<claim-text>distorting the digital image by application of the offset matrix.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the spatial offset is assigned by the user by movement of a pointing device.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, where a graphic tag is associated with each of the defined image reference points and displayed over the digital image, and the assignment of the spatial offset comprises movement of the graphic tag with the pointing device.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the calculating step further comprises applying the mixing function algorithm based on an abstract image reference point for limiting distortion.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. A method for distortion of a digital image comprising the steps of:
<claim-text>providing a computer-based information handling system to perform the steps of:</claim-text>
<claim-text>determining one or more sets of pixel characteristics;</claim-text>
<claim-text>receiving for each pixel characteristic set, a spatial offset;</claim-text>
<claim-text>providing a mixing function algorithm embodied on a computer-readable medium for distorting the digital image;</claim-text>
<claim-text>calculating an offset matrix by applying the mixing function algorithm based on the one or more sets of pixel characteristics and the received spatial offsets, and</claim-text>
<claim-text>distorting the digital image by application of the offset matrix.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A non-transitory computer readable medium having contents for causing a computer-based information handling system to perform the steps of receiving the coordinates of one or more than one image reference point defined by a user within the digital image; receiving one or more than one spatial offset assigned by the user and associated with the coordinates of the one or more than one defined image reference point; applying a mixing function algorithm for distorting the digital image; calculating an offset matrix by applying the mixing function algorithm based on the one or more than one spatial offset and the coordinates of the one or more than one defined image reference point; and distorting the digital image by application of the offset matrix.</claim-text>
</claim>
</claims>
</us-patent-grant>
