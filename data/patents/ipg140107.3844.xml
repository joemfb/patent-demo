<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08624911-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08624911</doc-number>
<kind>B1</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13344605</doc-number>
<date>20120105</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<us-term-extension>24</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>G</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>345582</main-classification>
<further-classification>345581</further-classification>
<further-classification>345583</further-classification>
<further-classification>345584</further-classification>
<further-classification>345587</further-classification>
<further-classification>345611</further-classification>
<further-classification>345612</further-classification>
</classification-national>
<invention-title id="d2e53">Texture-based polygon antialiasing</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>4679040</doc-number>
<kind>A</kind>
<name>Yan</name>
<date>19870700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>4780711</doc-number>
<kind>A</kind>
<name>Doumas</name>
<date>19881000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>4831447</doc-number>
<kind>A</kind>
<name>Lake</name>
<date>19890500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>5255352</doc-number>
<kind>A</kind>
<name>Falk</name>
<date>19931000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345582</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>5278678</doc-number>
<kind>A</kind>
<name>Harrington</name>
<date>19940100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>5487142</doc-number>
<kind>A</kind>
<name>Nakayama et al.</name>
<date>19960100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>5548693</doc-number>
<kind>A</kind>
<name>Shinya</name>
<date>19960800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>5561723</doc-number>
<kind>A</kind>
<name>DesJardins et al.</name>
<date>19961000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>5668940</doc-number>
<kind>A</kind>
<name>Steiner et al.</name>
<date>19970900</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>5719595</doc-number>
<kind>A</kind>
<name>Hoddie et al.</name>
<date>19980200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345611</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>6337686</doc-number>
<kind>B2</kind>
<name>Wong et al.</name>
<date>20020100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>6466206</doc-number>
<kind>B1</kind>
<name>Deering</name>
<date>20021000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>6999100</doc-number>
<kind>B1</kind>
<name>Leather et al.</name>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2003/0210251</doc-number>
<kind>A1</kind>
<name>Brown</name>
<date>20031100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345611</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2004/0189664</doc-number>
<kind>A1</kind>
<name>Frisken et al.</name>
<date>20040900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345611</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2007/0273706</doc-number>
<kind>A1</kind>
<name>Berger</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345582</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2008/0025641</doc-number>
<kind>A1</kind>
<name>Lee</name>
<date>20080100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382296</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2008/0320414</doc-number>
<kind>A1</kind>
<name>Kan et al.</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>17</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>345641</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345582-588</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345611-616</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>7</number-of-drawing-sheets>
<number-of-figures>8</number-of-figures>
</figures>
<us-related-documents>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61430165</doc-number>
<date>20110105</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61430532</doc-number>
<date>20110106</date>
</document-id>
</us-provisional-application>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>61559713</doc-number>
<date>20111114</date>
</document-id>
</us-provisional-application>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Miller</last-name>
<first-name>James B.</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Miller</last-name>
<first-name>James B.</first-name>
<address>
<city>Sunnyvale</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Fish &#x26; Richardson P.C.</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Google Inc.</orgname>
<role>02</role>
<address>
<city>Mountain View</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Xiao</last-name>
<first-name>Ke</first-name>
<department>2677</department>
</primary-examiner>
<assistant-examiner>
<last-name>Tran</last-name>
<first-name>Kim-Thanh T</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A computer-implemented method includes identifying a bit-mapped image of a line or polygon shape; mapping the image to a texture map that is slightly large in at least one dimension than the bit-mapped image; overlaying the bit-mapped image and the texture map; computing pixel shading for pixels between an outer edge of the bit-mapped image and the texture map by measuring a distance from particular ones of the pixels to an idealized line near an edge of the bit-mapped image; and displaying the bit-mapped image with pixels at its edge shaded according to the computed pixel shading.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="174.41mm" wi="233.00mm" file="US08624911-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="245.79mm" wi="152.91mm" orientation="landscape" file="US08624911-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="237.57mm" wi="182.20mm" orientation="landscape" file="US08624911-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="214.80mm" wi="168.40mm" file="US08624911-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="203.62mm" wi="136.40mm" file="US08624911-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="250.70mm" wi="207.60mm" orientation="landscape" file="US08624911-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="244.77mm" wi="189.82mm" orientation="landscape" file="US08624911-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="248.07mm" wi="193.04mm" orientation="landscape" file="US08624911-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application claims priority under 35 U.S.C. &#xa7;119(e) to U.S. Provisional Patent Application Ser. No. 61/430,165, filed on Jan. 5, 2011, to U.S. Provisional Patent Application Ser. No. 61/430,532, filed on Jan. 6, 2011, and to U.S. Provisional Patent Application Ser. No. 61/559,713, filed Nov. 14, 2011, each of which the entire contents are incorporated by reference as if set forth fully herein.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">This document relates to mechanisms for antialiasing lines, polygons, and other objects for display on a computer system.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Lines or other edges drawn on pixelated video displays are prone to &#x201c;jaggy&#x201d; artifacts, where the viewer can see the edge stair-step between the pixels that are drawn with color of the item and the color of the area around the item. For example, if a black box is displayed at a rotation angle of 45 degrees on a white background, all of the edges of the box will have a visible saw-tooth edge if a user of a computing device looks closely. This is a natural outcome of using displays with discrete pixels, because there is no practical way to represent on such displays a continuous line that is diagonal to the pixel layout. Instead, a line is represented just a series of separate pixels, or small rectangular (usually square) areas on the display surface. In short, drawing with squares is not smooth.</p>
<p id="p-0005" num="0004">This artifact is present in general, but can be particularly noticeable when the pixels on a display are relatively large (for example, on low-resolution or low-dpi displays). The artifact may also be particularly noticeable when a line is drawn at a small angle relative to the pixel layout (e.g., lines close to, but not on, the horizontal and vertical), because the steps are particularly long in such a situation. The problem is how to make lines look smoother, or &#x201c;antialiased.&#x201d; General antialiasing techniques involve having some pixels represent partial coverage of a line by having translucent variations of the line color. The net effect is that the line &#x201c;fades out&#x201d; at the edges, and looks smoother by avoiding having such hard edges between the on-the-line and off-the-line pixels on the display along the stair-step or saw-tooth edge.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0006" num="0005">This document discusses systems and techniques for efficient antialiasing of lines and other objects, which may enable effective antialiasing even on devices that have relatively low graphic computing power, such as tablet computers and mobile telephones. The techniques discussed here use GPU shaders or commonly available RGBA texture interpolation features of graphics or CPU hardware to accomplish the task of rendering a line or other object such as the outer periphery of a two-dimensional item that is displayed on a contrasting background. Minimizing the amount of information transmitted down to the GPU helps efficiency, as does minimizing the calculations done by the shaders at the vertex and pixel level.</p>
<p id="p-0007" num="0006">The techniques provide a fast way to draw visually correct antialiased lines (and other objects) using GPU shaders. They use a small set of data to send down to the shaders (minimizing the bandwidth necessary to get the data there) and a relatively small set of computations to determine how to draw the affected pixels. They also use the graphics engine's interpolator to compute interpolated values for the pixels in between vertices. But they use that interpolated value differently than do other more expensive approaches and they end up requiring far less data to get a similar result compared to other techniques.</p>
<p id="p-0008" num="0007">By the techniques described here, one first matches a polygon to a texture map that has a high-quality alpha component (such as using RGBA). The polygon is mapped to an area in the texture map that has a shape that matches the shape of the polygon, and a border that has an alpha value that transitions from opaque (e.g., a value of 255) to transparent (e.g., a value of 0) along a small number of pixels around an edge of the polygon. For example, for a line, a quad may be formed around the line that represents the bounding area of the anti-aliased line (or other rectangular object). This quad will encompass both the opaque region of the line interior as well as the fading antialiased region at the line's edges on the texture map.</p>
<p id="p-0009" num="0008">For a given rendering of a polygon with antialiased edges, the process then identifies a line along an edge of the polygon (where the line is a construction line used for computations, but is not part of the image and is not displayed). That line may be drawn, for example, between vertices on the ends of a straight edges of the polygon that is to be antialiased, and may pass through or very close to the saw-teeth that make up a diagonal edge of the polygon.</p>
<p id="p-0010" num="0009">A &#x201c;distance&#x201d; float value may then be identified for pixels of the map outside of the polygon. The distance between each such map pixel (e.g., a center point of each pixel) and the line may be conveniently computed, and may be used to assign an alpha channel value to the pixel, which value controls the level of transparency of the pixel, and thus results in an antialiasing effect if the values are provided to the shader of a GPU or other such structure.</p>
<p id="p-0011" num="0010">The techniques discussed here may, in certain implementations, provide one or more advantages. For example, certain implementations may require far less data than traditional approaches for computing alpha values. Also, the computation of the alpha values may be simplified while still producing adequate quality, so that, for example, the antialiasing computations may occur in software rather than having to be performed in hardware. Such ability can be particularly useful where computational power is limited, such as on small mobile devices.</p>
<p id="p-0012" num="0011">In one implementation, a computer-implemented method is disclosed that comprises identifying a bit-mapped image of a line or polygon shape; mapping the image to a texture map that is slightly larger in at least one dimension than the bit-mapped image; overlaying the bit-mapped image and the texture map; computing pixel shading for pixels between an outer edge of the bit-mapped image and the texture map by measuring a distance from particular ones of the pixels to a line near an edge of the bit-mapped image; and displaying the bit-mapped image with pixels at its edge shaded according to the computed pixel shading. The texture map may be an integer number of pixels larger than the bit-mapped image around a periphery of the bit-mapped image. Also, the texture map can be an integer number of pixels larger than the bit-mapped image along every peripheral edge of the bit-mapped image. The method can also include determining a rotation angle of the bit-mapped image in three-dimensional space. In addition, the method can include determining a rotation angle of the bit-mapped image in two-dimensional space and identifying endpoints of an edge of the bit-mapped image and creating the line using the identified endpoints.</p>
<p id="p-0013" num="0012">In certain aspects, the distance is determined based on a distance between a point on a pixel and a point on the line that is closest to the point on the pixel.</p>
<p id="p-0014" num="0013">In another implementation, a computer-implemented system is disclosed that comprises one or more digital images. The system also comprises an antialiasing module programmed to generate a texture map that is a defined size larger than a first digital image of the one or more digital images to be displayed by the system, and to compute shading values for pixels between an outer edge of the first digital image and an outer edge of the texture map as a function of a distance between a particular pixel and an idealized line near an edge of the first digital image. Moreover, the system comprises a renderer to receive inputs generated from the antialiasing module and to generate a display of the first digital image having antialiased edges.</p>
<p id="p-0015" num="0014">In some aspects, the texture map is an integer number of pixels larger than the first digital image around a periphery of the first digital image, or an integer number of pixels larger than the first digital image along two opposed edges, but a same dimension as the first digital image along two other opposed edges. In addition, the antialising module can be programmed to determine a rotation angle of the polygon shape in three-dimensional space. Moreover, the antialiasing module can be programmed to determine a rotation angle of the polygon shape in two-dimensional space, and to identify endpoints of an edge of the bit-mapped image and creating the line using the identified endpoints.</p>
<p id="p-0016" num="0015">In yet another implementation, one or more tangible, recordable storage media have recorded thereon instructions. When the instructions are executed, they perform operations that comprise identifying a shape in the form of a line or polygon to be displayed on a computing device; mapping the shape a texture map that is slightly larger in at least one dimension than the identified shape; overlaying the identified shape and the texture map with each other; computing pixel shading for pixels on the texture map between an outer edge of the identified shape and an edge of the texture map by measuring a distance from particular ones of the pixels to a line near an edge of the identified shape; and displaying the identified shape with pixels at its edge shaded according to the computed pixel shading. The texture map can be an integer number of pixels larger than the identified shape around a periphery of the identified shape, or an integer number of pixels larger than the identified shape along two opposed edges, but a same dimension as the identified shape along two other opposed edges. Also, the operations can include identifying endpoints of an edge of the bit-mapped image and creating the line using the identified endpoints. As another example, the distance can be determined based on a distance between a point on a pixel and a point on the line that is closest to the point on the pixel.</p>
<p id="p-0017" num="0016">The details of one or more embodiments are set forth in the accompanying drawings and the description below. Other features and advantages will be apparent from the description and drawings, and from the claims.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">DESCRIPTION OF DRAWINGS</heading>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. 1A and 1B</figref> show example screen shots of a media player display having antialiased edges.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. 2</figref> is a depiction of antialiasing performed on a square bitmapped image.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. 3</figref> is a close-up showing antialiasing calculations for an edge of a polygon.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. 4</figref> is a flow chart of an antialiasing process.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. 5</figref> is a schematic diagram of a system for displaying antialiased objects.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. 6 and 7</figref> show examples computer devices that can be used to implement the techniques described here.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<p id="p-0024" num="0023">Like reference symbols in the various drawings indicate like elements.</p>
<heading id="h-0006" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. 1A and 1B</figref> show example screen shots of a media player display having antialiased edges. In particular, the figure shows a carousel user interface for selecting a media item. <figref idref="DRAWINGS">FIG. 1A</figref> shows a display of books having a slight three-dimensional effect (their edges show to the user), while <figref idref="DRAWINGS">FIG. 1B</figref> shows a display of album covers, and there is still a three-dimensional effect of the albums disappearing into the distance, but side edges are not shown because traditional album sleeves have no appreciable thickness.</p>
<p id="p-0026" num="0025">Referring now to <figref idref="DRAWINGS">FIG. 1A</figref>, a number of books are shown in a carousel arrangement. The carousel effect is created by angling each of the items as it approaches the left or right edge of the display (and showing the middle item dead on), so that the items appear visually to be moving along a curved path that presents its inside surface in front of the user, and wraps around (off the screen) behind the user's head. The user can pan the carousel left or right to see other titles by dragging a finger on a touch-sensitive display on which the graphics are shown here.</p>
<p id="p-0027" num="0026">It can be appreciated that the top and bottom edges, at least, of the books will need to be angled very slightly from the horizontal in order to create the three-dimensional visual effect. Such slight angles could create annoying jaggies, particularly by the jaggies changing position on each edge as a user moves the books back and forth.</p>
<p id="p-0028" num="0027">As shown, the books have depth and dimension. Also, the rightmost and leftmost books can be seen along their edges or spines. Thus, the application for rendering the books may receive from an external source an indication of the number of pages in each of the books, and may create a rectangular model for each book that is proportionate to the number of pages, so that each book will look in the presentation as it would in real life, at least from a thickness perspective. In certain instances (e.g., reference books), a maximum page number may be imposed and any books reported as having more than the maximum will be displayed as if they have the maximum. In a similar manner, whether a book is hardcover or softcover can be determined by the application, and the rendering may differ based on such a determination. For example, the front and back covers may be rendered to overhang the pages for a hardcover book, whereas they may be rendered as being flush (and thinner) for a softcover book.</p>
<p id="p-0029" num="0028">Referring now to <figref idref="DRAWINGS">FIG. 1B</figref>, there is shown a plurality of album covers in a carousel implementation that takes a slightly different form. As shown here, the carousel disappears off the top right corner of the display and into the distance. The album covers move past the user and effectively over the user's left shoulder as they move off the display in the other direction. Navigation from one album to the next may occur by touch input and dragging or flicking on the albums themselves in one of the two directions for the carousel, or by selecting &#x201c;backward&#x201d; and &#x201c;forward&#x201d; buttons shown on the display, whose selection will be accompanied by the carousel indexing forward or backward one album cover. Again, to maintain a three-dimensional effect, and because the like of album covers is moving from one side of the display to the other between album covers, the top and bottom edges of the album covers will be angled slightly from the horizontal, and may suffer jaggies, particularly when the background contrasts greatly with the particular album cover. Also, the album covers may be caused to tip slight in the vertical when they are moved, so as to give them a little extra feeling of motion&#x2014;e.g., when a user drags in a direction on an album, it may tip slight about its lower edge in that direction, so as to give the visual effect that the user is pulling on a physical album.</p>
<p id="p-0030" num="0029">Thus, each of these applications of a graphical user interface may have need of antialiasing function in order to make the image that the user sees more pleasing visually. The description below discusses example processes and structures that may be employed to provide antialiasing in situations like those shown in these carousels. In applying the techniques below, images for the fronts of books and album covers, and of spines of books, may be obtained, and a computing device may apply a peripheral ring of pixels or texels around the image that results for each such object, where the ring (which may be one pixel wide or several pixels) can be made transparent. The images may then be passed, with a model and information identifying an orientation of the model to a standard graphics processor, which may texture the ring as a blend of adjacent pixels. Also, the system may indicate to the graphics processing that it is supposed to use blending in such a situation. Such actions may occur using well known APIs for particular graphics processors. Particular example techniques are described in more detail below.</p>
<p id="p-0031" num="0030">The techniques for providing antialiasing along edges of the books, which are described below, may operate satisfactorily in the situations discussed above because the orientation of the books and albums relative to the viewer is relatively fixed. In particular, if a transparent ring were provided around an image for a spine, and a user were able to rotate the book for viewing from the spine end, the user may see a transparent vertical line along each side of the spine. A light colored background could bleed through that line. Such an issue could be particularly annoying if the user rotated the book back and forth through that particular orientation. However, where the orientation of the books (or albums) is restricted to being almost completely front-on, such problems do not arise, and the antialiasing discussed here and claimed below may provide a visually appealing representation of the 3D models of the books and albums as they are moved back and forth along the curve of the carousel.</p>
<p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. 2</figref> is a depiction of antialiasing performed on a square bitmapped image, such as the front of an album cover or front of a book as shown in <figref idref="DRAWINGS">FIGS. 1A and 1B</figref>. An image <b>200</b><i>a </i>in the figure represents a basic bit-mapped image. The image may take any appropriate form, and in this example may be considered to be a piece of album cover art that is to be displayed on a computing device and to be scrollable through what appears to a viewer of the device to be a circular or arced carousel (e.g., if the user swipes on a touchscreen with her finger to spin the carousel), like those of <figref idref="DRAWINGS">FIGS. 1A and 1B</figref>. Such rotation of the carousel may cause individual album covers, made up of various different bit maps, to rotate in three dimensional space slightly. As a result, the edges of the image become slightly angled relative to vertical and horizontal as shown by rotated image <b>200</b><i>b. </i></p>
<p id="p-0033" num="0032">The lower portion of the figure shows an example for removing the &#x201c;jaggies&#x201d; shown at the edge of image <b>200</b><i>b </i>by the use of a lightweight antialiasing technique. This technique may be used in computing alpha values for pixels at and around the edge of an image during any particular point in its motion (and can be updated every time the image is moved and thus needs to be re-rendered).</p>
<p id="p-0034" num="0033">Specifically, a texture map <b>204</b><i>a </i>may be generated that is slightly larger than the image <b>202</b><i>a</i>, such as by being one or two pixels wider and taller on each edge (so four pixels larger in each dimension). The map <b>204</b><i>a </i>may be the same size as image <b>200</b><i>a</i>, and image <b>204</b><i>a </i>may simply be a version of image <b>200</b><i>a </i>that has been shrunk in each dimension by two to four pixels. The image <b>200</b><i>a </i>may also be referred to more generally as a polygon, and the techniques described here may more generally be performed along each edge of the polygon (generally external peripheral edges for solid polygons having no holes in them).</p>
<p id="p-0035" num="0034">The polygon has texture coordinates that are chosen so that the texture coordinates of the polygon are mapped to match the generated texture including border pixels. Such selection of texture coordinates may be relevant in situations in which the polygon does not map to the entire area of the texture.</p>
<p id="p-0036" num="0035">In the middle in the lower portion of the figure, the alpha shading values for the texture map are shown. In particular, the portion of the map <b>204</b><i>a </i>that is outside the edge of the image may be given a value of 0, for transparent. In contrast, portions of the texture map <b>204</b><i>a </i>that overlap with the shrunken image <b>202</b><i>a </i>may be entirely opaque, with a value of 255. Thus, if the combination were rendered at this point, it would look simply like the figure, since the main part of the figure would be fully rendered and the area around the edge would be invisible, or fully transparent.</p>
<p id="p-0037" num="0036">The lower right rendition shows the image rotated in three dimensional space, with antialiasing performed along the edges of the image. In particular, the jaggies on the image <b>202</b><i>b </i>have been smoothed by filling in particular pixels between the edge of the image <b>202</b><i>b </i>and the texture map <b>204</b><i>b </i>and by shading the pixels. In particular, the previously transparent pixels with an alpha of 0 may be provided with a positive alpha value that causes them to be less transparent and more opaque. Such a &#x201c;middle ground&#x201d; transparency may result in a visual softening of the edge of the image&#x2014;an antialiasing effect.</p>
<p id="p-0038" num="0037">The level of shading, via the selected alpha value, may be a function of each pixel's distance from a hypothetical or idealized line that may run along an edge of the image, such as the outer edge of the sawteeth that is created by the jaggies or an inner edge of the sawteeth. The ideal line may also be offset from those positions and shading values may be adjusted accordingly so as to create a shading process that properly antialiases the edge. Thus, various approaches may be made for determining the distance, which may be measured by an absolute or relative value, such as a normalized value set between 0 and 1. Such value may then be resolved into an alpha value (or the alpha value itself may be from 0 to 1), where a greater distance from the ideal line away from the object receives a lower value (a value that is closer to transparent).</p>
<p id="p-0039" num="0038">The calculation of alpha values may thus be performed for each of the pixels along an edge (and measured form an ideal line), and for pixels along other edges in need of antialiasing (and thus measured from other ideal lines). In certain circumstances, the alpha value for a pixel may be inferred from values of other pixels such as its neighboring pixels, rather than computing its value directly from its distance from the ideal line.</p>
<p id="p-0040" num="0039">The solid portion of the image is shown by <b>206</b> in the blown-up area, while the shaded pixel is shown at <b>208</b>, and a portion of the texture map that was left transparent (and would not be visible when the image is displayed on a computer) is shown by <b>210</b>. In other implementations where there are multiple rows of pixels outside the outer edge of the underlying polygon or image, each of the rows may have a progressively lower alpha value as they move away from the edge and into free space. Such a process may result in the edge being softer, but also smoother. The final antialiased image may, in practice, be generated by a graphical processing unit (GPU) that may operate according to a standard such as OpenGL ES 1.0, 2.0, and OpenGL. The alpha calculations may be performed separately, such as on a general microprocessor executing software for computing the alpha values in the manner discussed here. The alpha values and other information needed for generating the image may then be passed to the GPU. Such operations may be performed each time an item such as the image <b>200</b><i>a </i>is moved on a display (e.g., if a user rotates one of the carousels in FIGS. <b>1</b>A or <b>1</b>B slightly, the antialiasing would be performed again to reflect the new angles of the edge s of the objects shown in the carousels).</p>
<p id="p-0041" num="0040">Thus, by this general technique, antialiasing may be performed in a lightweight manner, particularly for situations in the object whose edges are being antialiased is constrained in its level of rotation so that it stays close to a straight-on orientation on a display of a computer. The following figure provides additional discussion for the performance of computing alpha values around the edge of an object such as a polygon to be displayed on a computer.</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. 3</figref> is a close-up showing antialiasing calculations for an edge of a polygon. The computations are similar to those shown in <figref idref="DRAWINGS">FIG. 2</figref>, but are described here in more detail. Generally, this view better shows hypothetical lines that may be drawn and the computation of pixels in an antialiasing area. The left line <b>306</b> is termed an ideal edge, and at that point, alpha is equal to 1. The line <b>306</b> is not actually displayed, but is instead a construction line used for performing geometric calculations that can lead to the generation of alpha values for pixels in the texture map. The right line <b>310</b> is the polygon edge where the alpha value is 0. The polygon edge is rendered in the area between the lines, and the alpha is computed according to the distance between the line <b>306</b> (or another appropriate line) and the particular pixel that is being provided a transparency value. The location of the pixel may be taken at the middle of the pixel or at another location on or off of the pixel, as long as the selected location allows a determination to be made about the pixels relative distance from the lines <b>306</b> or another appropriate location that provides an indication of how far the pixel is from the outer edge of the polygon.</p>
<p id="p-0043" num="0042">Pixel <b>304</b>, for example, shows the computation of alpha as a function of a distance to the idealized edge, which value may be normalized into a range from 0 to 1. The function is ideally related to a projected distance in screen space&#x2014;approximated in the texture map with the N-pixel border in texture, where alpha=0. The value of N is selected to relate to a worst case (smallest) projected polygon for a process. A large N results in fuzzy edges and needs to be optimized for a given application. Such selection may be made to lessen potential problems of the projection minimizing the textures. For example, when the projected area of a polygon is less than the area of the texture map, OpenGL will apply a user-defined minimization filter that may skip over idealized border pixels. As a particular example, if the area is 2:1 reduced, then two border pixels may be needed. A 4:1 reduction would require four border pixels.</p>
<p id="p-0044" num="0043">Also, although the distance computation is shown here as a distance from a point on the pixel to a closest point on the line <b>306</b> (which results in a right angle with the line), the distance measurement made be made by other techniques that still result in the relative distance of a pixel from the edge of the polygon being a factor or the factor in an alpha value that is assigned to the pixel (and potentially to other, neighboring pixels).</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. 4</figref> is a flow chart of an antialiasing process. The process begins at box <b>400</b>, where a bit-mapped image (or other form of polygon) is obtained. For example, in an album art carousel example, images that represent album covers may be obtained from local or remote storage and may be provided for display on a device.</p>
<p id="p-0046" num="0045">The process then, at box <b>302</b>, maps texture maps to the relevant images. An image may be determined to be relevant if it has an edge that is at an angle to horizontal or vertical, such that the edge requires antialiasing. As described above, the texture maps may be selected to be N pixels larger on one or more sides of the image, than is the image in size. Alternatively, an image map may be selected that has an area that matches the image or other form of polygon, and the image may be reduced in size by several pixels</p>
<p id="p-0047" num="0046">At box <b>404</b>, the process computes shape parameters for the rotated polygon, so as to generate a set of pixels that represents the unantialiased shape. Such computation may involve identifying the coordinates of edges of the rendered object, whether as straight lines or curves. Upon identifying the coordinates, the process may determine which edges are at an angle that might require the provision of antialiasing. The shading for pixels along the jaggies, in the antialiasing region, may then be computed in the manners discussed above, according to a function of their distance from an idealized line that is set up a determined distance from an edge (e.g., the outer or inner edge of the sawteeth along the image) along the outer edge of the polygon.</p>
<p id="p-0048" num="0047">The rotated and antialiased polygon is then rendered and displayed at box <b>408</b>. Such operations may use a variety of forms of hardware and use data derived form the foregoing steps of the process.</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. 5</figref> is a schematic diagram of a system <b>500</b> for displaying antialiased objects. In general, the system <b>500</b> allows a computer device, such as a portable device <b>502</b>, to present information via a graphical user interface, where the edges of displayed objects may be antialiased according to the techniques described above.</p>
<p id="p-0050" num="0049">Referring now more specifically to components of the system <b>500</b>, the device <b>502</b> is arranged to communicate through a wireless network and then the network <b>506</b>, such as the Internet, with various hosted services. In this example, the hosted service that is being used is an image service provided by image server <b>508</b>. Interaction between computing devices and an image server system is well-known and may include the receipt of image files by the device <b>502</b>, where the file may include a header that identifies meta data and a body of the file defines data for generating the image on a display. In some examples, the images may be of album cover art and may be accessed for display on the device <b>502</b>.</p>
<p id="p-0051" num="0050">The device <b>502</b> has been loaded with a media player application <b>516</b> for playing media files such as songs and movies on the device <b>502</b>, and also for allowing a user to select items for playing via a carousel interface on a display <b>504</b> of the device. Other applications on the device may require antialiasing for other purposes, and the media player application <b>516</b>. is discussed here for purposes of illustration only.</p>
<p id="p-0052" num="0051">The display <b>504</b> on the device <b>502</b> shows album covers in the carousel display, similar to the displays in <figref idref="DRAWINGS">FIGS. 1A and 1B</figref>, where individual album covers are rotated as the carousel is moved.</p>
<p id="p-0053" num="0052">Particular components shown in schematic form as executing on the device <b>502</b> may affect the manner in which the application <b>516</b> is displayed on the device <b>502</b>. For example, a display controller <b>514</b> may control the manner in which different applications access the display of device <b>502</b>, and the manner in which their content is displayed on that display. For example, the display controller <b>514</b> may identify which application is currently the focus of the device <b>502</b> and may display content from that application in preference to content from other applications. The display controller <b>514</b> may also identify which user interface elements, such as elements generated by particular activities of an application, are to be displayed at a current time on the display of device <b>502</b>.</p>
<p id="p-0054" num="0053">An input controller <b>522</b> may interact with the display controller <b>514</b> and may identify various inputs received on the device <b>502</b>. Such input, for example, may include touch-based inputs on a touch sensitive portion of the display on device <b>502</b>. The location of a particular touch input as sent by the input controller <b>502</b> may be coordinated with locations of items in the user interface displayed by the display controller <b>514</b>, so that a user intention for a touch input may be discerned. For example, a user dragging on the albums in the carousel may be identified as relating to a user intent to rotate the images through the curve of the carousel, and to fetch and display additional images that are not currently being displayed.</p>
<p id="p-0055" num="0054">An antialiasing module <b>526</b> is also provided on the device <b>502</b> and may interact with the display controller <b>514</b> and a GPU <b>520</b> to provide antialiasing of edges on objects that are displayed by various applications executing on the device <b>502</b>. The antialiasing module may obtain information that defines the locations and angles of edges, and may construct ideal lines along such edges, generate texture maps having different alpha values at different locations on the maps, and identify alpha values to apply to the maps based on the distances of particular pixels or groups of pixels as a function of the distance of those pixels from the outer edge of the object that is being antialiased. The antialiasing module <b>526</b> may, for example, use techniques like those discussed above for <figref idref="DRAWINGS">FIGS. 2 and 3</figref>.</p>
<p id="p-0056" num="0055">The antialiasing module may execute on a microprocessor on the device <b>502</b>, and the microprocessor may pass alpha values and other relevant information about the rendering process to the GPU <b>520</b>. The GPU may then operating in a standard manner to turn such information into a rich display that can be shown on the device <b>502</b>.</p>
<p id="p-0057" num="0056">A wireless interface <b>518</b> may be used to receive image data and send such data back and forth to the image server <b>508</b> through the network <b>506</b>. The media player application <b>516</b> may be programmed according to an application programming interface to access information that is passing through the wireless interface <b>518</b>. For example, the media player application <b>516</b> may register with an operating system that is executing on the device <b>502</b> to be notified when images are received from the image server <b>506</b> and to be provided with data from such messages so that they can be displayed on the device <b>502</b> in a quickly-updated manner.</p>
<p id="p-0058" num="0057">An images database <b>524</b> may also be the source of images to be displayed on the device <b>502</b>. The database <b>524</b> may store images persistently or may cache images obtained from image server <b>506</b>. For example, if a user of the media application <b>516</b> performs a search for &#x201c;U2,&#x201d; images for each of the albums made by the band U2 may be pre-fetch, and five of those images may be shown immediately on the display <b>504</b>. As the user manipulates the carousel, other of the images may be obtained from the database <b>524</b>, and antialiasing may be performed on whatever images are currently being displayed.</p>
<p id="p-0059" num="0058">In this manner, the system <b>500</b> can display a variety of items and provide an improved visual display through a relatively lightweight process for antialiasing edges of objects that are displayed. The antialiasing service may be provided for all applications, for only those that request it (with an understanding that the particular type of antialiasing may not be appropriate for all uses), or only to predefined applications, including by incorporating the antialiasing into the code for the application itself. As a result, the system <b>500</b> may significantly improve the operation of device <b>502</b> and the experience of a user of the device <b>502</b> without substantial additional overhead being needed on the device <b>502</b>.</p>
<p id="p-0060" num="0059">Referring now to <figref idref="DRAWINGS">FIG. 6</figref>, a conceptual diagram of a system that may be used to implement the systems and methods described in this document is illustrated. In the system, a computing device <b>610</b> can wirelessly communicate with base station <b>640</b>, which can provide the mobile computing device with wireless access to numerous hosted services <b>660</b> through a network <b>650</b>.</p>
<p id="p-0061" num="0060">In this illustration, the mobile computing device <b>610</b> is depicted as a tablet device that includes a touchscreen display <b>612</b> for presenting content to a user of the mobile computing device <b>610</b> and receiving touch-based user inputs. Other visual, auditory, and tactile output components may also be provided (e.g., LED lights, a speaker for providing tonal, voice-generated, or recorded output, or vibrating mechanisms for tactile output), as may various different input components (e.g., keyboard, physical buttons, trackballs, accelerometers, gyroscopes, and magnetometers).</p>
<p id="p-0062" num="0061">Example visual output mechanism in the form of display device <b>612</b> may take the form of an LED or AMOLED display with resistive or capacitive touch capabilities, for displaying video, graphics, images, and text, and coordinating user touch inputs locationally with the displayed information so that user contact above a displayed item may be associated with the item by the device <b>610</b>. The mobile computing device <b>610</b> may take alternative forms also, including as a laptop computer, a mobile telephone, a slate computer, a personal digital assistant, an embedded system (e.g., a car navigation system), a desktop personal computer, or a computerized workstation.</p>
<p id="p-0063" num="0062">An example mechanism for receiving user-input includes a &#x201c;virtual&#x201d; keyboard displayed on the touchscreen display <b>612</b> or a physical keyboard (not shown), which may be a full qwerty keyboard or a traditional keypad that includes keys for the digits &#x2018;0-9&#x2019;, &#x2018;*&#x2019;, and &#x2018;#.&#x2019; The keyboard receives input when a user physically contacts or depresses a keyboard key. User manipulation of a trackball or interaction with a trackpad enables the user to supply directional and rate of rotation information to the device <b>610</b> (e.g., to manipulate a position of a cursor on the display device <b>612</b>).</p>
<p id="p-0064" num="0063">The device <b>610</b> may be able to determine a position of physical contact with the touchscreen display device <b>612</b> (e.g., a position of contact by a finger or a stylus). Using the touchscreen <b>612</b>, various &#x201c;virtual&#x201d; input mechanisms may be produced, where a user interacts with a graphical user interface element depicted on the touchscreen <b>612</b> by contacting the graphical user interface element. An example of a &#x201c;virtual&#x201d; input mechanism is a &#x201c;software keyboard,&#x201d; where a keyboard is displayed on the touchscreen and a user selects keys by pressing a region of the touchscreen <b>612</b> that corresponds to each key.</p>
<p id="p-0065" num="0064">The mobile computing device <b>610</b> may include mechanical or touch sensitive physical buttons (not shown). Additionally, the mobile computing device may include buttons for adjusting volume output by the one or more speakers (not shown), and a button for turning the mobile computing device on or off. A microphone allows the device <b>610</b> to convert audible sounds into an electrical signal that may be digitally encoded and stored in computer-readable memory, or transmitted to another computing device. The device <b>610</b> may also include a digital compass, an accelerometer, proximity sensors, and ambient light sensors.</p>
<p id="p-0066" num="0065">An operating system may provide an interface between the mobile computing device's hardware (e.g., the input/output mechanisms and a processor executing instructions retrieved from computer-readable medium) and software. Example operating systems include the ANDROID mobile device platform; APPLE IPHONE/MAC OS X operating systems; MICROSOFT WINDOWS 7/WINDOWS MOBILE operating systems; SYMBIAN operating system; RIM BLACKBERRY operating system; PALM WEB operating system; a variety of UNIX-flavored operating systems; or a proprietary operating system for computerized devices. The operating system may provide a platform for the execution of application programs that facilitate interaction between the computing device and a user.</p>
<p id="p-0067" num="0066">The mobile computing device <b>610</b> may present a graphical user interface with the touchscreen <b>612</b>. A graphical user interface is a collection of one or more graphical interface elements and may be static (e.g., the display appears to remain the same over a period of time), or may be dynamic (e.g., the graphical user interface includes graphical interface elements that animate without user input).</p>
<p id="p-0068" num="0067">A graphical interface element may be text, lines, shapes, images, or combinations thereof. For example, a graphical interface element may be an icon that is displayed on the desktop and the icon's associated text. In some examples, a graphical interface element is selectable with user-input. For example, a user may select a graphical interface element by pressing a region of the touchscreen that corresponds to a display of the graphical interface element. In some examples, the user may manipulate a trackball to highlight a single graphical interface element as having focus. User-selection of a graphical interface element may invoke a pre-defined action by the mobile computing device. In some examples, selectable graphical interface elements further or alternatively correspond to a button on the keyboard. User-selection of the button may invoke the pre-defined action.</p>
<p id="p-0069" num="0068">In some examples, the operating system provides a &#x201c;desktop&#x201d; user interface that is displayed upon turning on the computing device <b>610</b>, activating the device <b>610</b> from a sleep mode, upon &#x201c;unlocking&#x201d; the mobile computing device <b>610</b>, or upon receiving user-selection of a physical button on the computing device <b>610</b>. The desktop graphical interface may display several icons that, when selected with user-input, invoke corresponding application programs. An invoked application program may present a graphical interface that replaces the desktop graphical interface until the application program terminates or is hidden from view.</p>
<p id="p-0070" num="0069">User-input may manipulate a sequence of mobile computing device <b>610</b> operations. For example, a single-action user input (e.g., a single tap of the touchscreen, swipe across the touchscreen, contact with a button, or combination of these at a same time) may invoke an operation that changes a display of the user interface. Without the user-input, the user interface may not have changed at a particular time. For example, a multi-touch user input with the touchscreen <b>812</b> may invoke a mapping application to &#x201c;zoom-in&#x201d; on a location, even though the mapping application may have by default zoomed-in after several seconds.</p>
<p id="p-0071" num="0070">The desktop graphical interface can also display &#x201c;widgets.&#x201d; A widget is one or more graphical interface elements that are associated with an application program that has been executed, and that display on the desktop content controlled by the executing application program. A widget's application program may start with the mobile telephone. Further, a widget may not take focus of the full display. Instead, a widget may only &#x201c;own&#x201d; a small portion of the desktop, displaying content and receiving touchscreen user-input within the portion of the desktop.</p>
<p id="p-0072" num="0071">The computing device <b>610</b> may include one or more location-identification mechanisms. A location-identification mechanism may include a collection of hardware and software that provides the operating system and application programs an estimate of the mobile telephone's geographical position. A location-identification mechanism may employ satellite-based positioning techniques, base station transmitting antenna identification, multiple base station triangulation, internet access point IP location determinations, inferential identification of a user's position based on search engine queries, and user-supplied identification of location (e.g., by &#x201c;checking in&#x201d; to a location).</p>
<p id="p-0073" num="0072">The computing device <b>610</b> may include other application modules and hardware. Where the computing device <b>610</b> is a mobile telephone, a call handling unit may receive an indication of an incoming telephone call and provide a user capabilities to answer the incoming telephone call. A media player may allow a user to listen to music or play movies that are stored in local memory of the computing device <b>610</b>. The device <b>610</b> may include a digital camera sensor, and corresponding image and video capture and editing software. An internet browser may enable the user to view content from a web page by typing in an addresses corresponding to the web page or selecting a link to the web page.</p>
<p id="p-0074" num="0073">The computing device <b>610</b> may include an antenna to wirelessly communicate information with the base station <b>640</b>. The base station <b>640</b> may be one of many base stations in a collection of base stations (e.g., a mobile telephone cellular network) that enables the computing device <b>610</b> to maintain communication with a network <b>650</b> as the computing device <b>610</b> is geographically moved. The computing device <b>610</b> may alternatively or additionally communicate with the network <b>650</b> through a Wi-Fi router or a wired connection (e.g., Ethernet, USB, or FIREWIRE). The computing device <b>610</b> may also wirelessly communicate with other computing devices using BLUETOOTH protocols, or may employ an ad-hoc wireless network.</p>
<p id="p-0075" num="0074">A service provider that operates the network of base stations may connect the mobile computing device <b>610</b> to the network <b>650</b> to enable communication between the mobile computing device <b>610</b> and other computerized devices that provide services <b>560</b>. Although the services <b>660</b> may be provided over different networks (e.g., the service provider's internal network, the Public Switched Telephone Network, and the Internet), network <b>650</b> is illustrated as a single network. The service provider may operate a server system <b>652</b> that routes information packets and voice data between the computing device <b>610</b> and computing devices associated with the services <b>660</b>.</p>
<p id="p-0076" num="0075">The network <b>650</b> may connect the computing device <b>610</b> to the Public Switched Telephone Network (PSTN) <b>662</b> in order to establish voice or fax communication between the mobile computing device <b>610</b> and another computing device. For example, the service provider server system <b>652</b> may receive an indication from the PSTN <b>662</b> of an incoming call for the computing device <b>610</b>. Conversely, the computing device <b>610</b> may send a communication to the service provider server system <b>652</b> initiating a telephone call with a telephone number that is associated with a device accessible through the PSTN <b>662</b>.</p>
<p id="p-0077" num="0076">The network <b>650</b> may connect the computing device <b>610</b> with a Voice over Internet Protocol (VoIP) service <b>664</b> that routes voice communications over an IP network, as opposed to the PSTN. For example, a user of the computing device <b>610</b> may invoke a VoIP application and initiate a call using the program. The service provider server system <b>652</b> may forward voice data from the call to a VoIP service, which may route the call over the internet to a corresponding computing device, potentially using the PSTN for a final leg of the connection.</p>
<p id="p-0078" num="0077">An application store <b>666</b> may provide a user of the computing device <b>610</b> the ability to browse a list of remotely stored application programs that the user may download over the network <b>650</b> and install on the computing device <b>610</b>. The application store <b>666</b> may serve as a repository of applications developed by third-party application developers. An application program that is installed on the computing device <b>610</b> may be able to communicate over the network <b>650</b> with server systems that are designated for the application program. For example, a VoIP application program may be downloaded from the Application Store <b>666</b>, enabling the user to communicate with the VoIP service <b>664</b>.</p>
<p id="p-0079" num="0078">The computing device <b>610</b> may access content on the internet <b>668</b> through network <b>650</b>. For example, a user of the computing device <b>610</b> may invoke a web browser application that requests data from remote computing devices that are accessible at designated universal resource locations. In various examples, some of the services <b>660</b> are accessible over the internet.</p>
<p id="p-0080" num="0079">The computing device <b>610</b> may communicate with a personal computer <b>670</b>. For example, the personal computer <b>670</b> may be the home computer for a user of the computing device <b>610</b>. Thus, the user may be able to stream media from his personal computer <b>670</b>. The user may also view the file structure of his personal computer <b>670</b>, and transmit selected documents between the computerized devices.</p>
<p id="p-0081" num="0080">A voice recognition service <b>672</b> may receive voice communication data recorded with the computing device's microphone (not shown), and translate the voice communication into corresponding textual data. In some examples, the translated text is provided to a search engine as a web query, and responsive search engine search results are transmitted to the computing device <b>610</b>.</p>
<p id="p-0082" num="0081">The computing device <b>610</b> may communicate with a social network <b>674</b>. The social network may include numerous members, some of which have agreed to be related as acquaintances. Application programs on the computing device <b>610</b> may access the social network <b>674</b> to retrieve information based on the acquaintances of the user of the mobile computing device. For example, an &#x201c;address book&#x201d; application program may retrieve telephone numbers for the user's acquaintances. In various examples, content may be delivered to the computing device <b>610</b> based on social network distances from the user to other members. For example, advertisement and news article content may be selected for the user based on a level of interaction with such content by members that are &#x201c;close&#x201d; to the user (e.g., members that are &#x201c;friends&#x201d; or &#x201c;friends of friends&#x201d;).</p>
<p id="p-0083" num="0082">The computing device <b>610</b> may access a personal set of contacts <b>676</b> through network <b>660</b>. Each contact may identify an individual and include information about that individual (e.g., a phone number, an email address, and a birthday). Because the set of contacts is hosted remotely to the computing device <b>610</b>, the user may access and maintain the contacts <b>676</b> across several devices as a common set of contacts.</p>
<p id="p-0084" num="0083">The computing device <b>610</b> may access cloud-based application programs <b>678</b>. Cloud-computing provides application programs (e.g., a word processor or an email program) that are hosted remotely from the computing device <b>610</b>, and may be accessed by the device <b>610</b> using a web browser or a dedicated program. Example cloud-based application programs include GOOGLE DOCS word processor and spreadsheet service, GOOGLE GMAIL webmail service, and PICASA picture manager.</p>
<p id="p-0085" num="0084">Mapping service <b>680</b> can provide the computing device <b>610</b> with street maps, route planning information, and satellite images. An example mapping service is GOOGLE MAPS. The mapping service <b>680</b> may also receive queries and return location-specific results. For example, the computing device <b>610</b> may send an estimated location of the mobile computing device and a user-entered query for &#x201c;pizza places&#x201d; to the mapping service <b>680</b>. The mapping service <b>680</b> may return a street map with &#x201c;markers&#x201d; superimposed on the map that identify geographical locations of nearby &#x201c;pizza places.&#x201d;</p>
<p id="p-0086" num="0085">Turn-by-turn service <b>682</b> may provide the computing device <b>610</b> with turn-by-turn directions to a user-supplied destination. For example, the turn-by-turn service <b>682</b> may stream to device <b>610</b> a street-level view of an estimated location of the device, along with data for providing audio commands and superimposing arrows that direct a user of the device <b>610</b> to the destination.</p>
<p id="p-0087" num="0086">Various forms of streaming media <b>684</b> may be requested by the computing device <b>610</b>. For example, computing device <b>610</b> may request a stream for a pre-recorded video file, a live television program, or a live radio program. Example services that provide streaming media include YOUTUBE and PANDORA.</p>
<p id="p-0088" num="0087">A micro-blogging service <b>686</b> may receive from the computing device <b>610</b> a user-input post that does not identify recipients of the post. The micro-blogging service <b>686</b> may disseminate the post to other members of the micro-blogging service <b>686</b> that agreed to subscribe to the user.</p>
<p id="p-0089" num="0088">A search engine <b>688</b> may receive user-entered textual or verbal queries from the computing device <b>610</b>, determine a set of internet-accessible documents that are responsive to the query, and provide to the device <b>610</b> information to display a list of search results for the responsive documents. In examples where a verbal query is received, the voice recognition service <b>672</b> may translate the received audio into a textual query that is sent to the search engine.</p>
<p id="p-0090" num="0089">These and other services may be implemented in a server system <b>690</b>. A server system may be a combination of hardware and software that provides a service or a set of services. For example, a set of physically separate and networked computerized devices may operate together as a logical server system unit to handle the operations necessary to offer a service to hundreds of individual computing devices.</p>
<p id="p-0091" num="0090">In various implementations, operations that are performed &#x201c;in response&#x201d; to another operation (e.g., a determination or an identification) are not performed if the prior operation is unsuccessful (e.g., if the determination was not performed). Features in this document that are described with conditional language may describe implementations that are optional. In some examples, &#x201c;transmitting&#x201d; from a first device to a second device includes the first device placing data into a network for receipt by the second device, but may not include the second device receiving the data. Conversely, &#x201c;receiving&#x201d; from a first device may include receiving the data from a network, but may not include the first device transmitting the data.</p>
<p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. 7</figref> shows an example of a generic computer device <b>700</b> and a generic mobile computer device <b>750</b>, which may be used with the techniques described here. Computing device <b>700</b> is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. Computing device <b>750</b> is intended to represent various forms of mobile devices, such as tablet devices, personal digital assistants, cellular telephones, smartphones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.</p>
<p id="p-0093" num="0092">Computing device <b>700</b> includes a processor <b>702</b>, memory <b>704</b>, a storage device <b>706</b>, a high-speed interface <b>708</b> connecting to memory <b>704</b> and high-speed expansion ports <b>710</b>, and a low speed interface <b>712</b> connecting to low speed bus <b>714</b> and storage device <b>706</b>. Each of the components <b>702</b>, <b>704</b>, <b>706</b>, <b>708</b>, <b>710</b>, and <b>712</b>, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor <b>702</b> can process instructions for execution within the computing device <b>700</b>, including instructions stored in the memory <b>704</b> or on the storage device <b>706</b> to display graphical information for a GUI on an external input/output device, such as display <b>716</b> coupled to high speed interface <b>708</b>. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices <b>700</b> may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).</p>
<p id="p-0094" num="0093">The memory <b>704</b> stores information within the computing device <b>700</b>. In one implementation, the memory <b>704</b> is a volatile memory unit or units. In another implementation, the memory <b>704</b> is a non-volatile memory unit or units. The memory <b>704</b> may also be another form of computer-readable medium, such as a magnetic or optical disk.</p>
<p id="p-0095" num="0094">The storage device <b>706</b> is capable of providing mass storage for the computing device <b>700</b>. In one implementation, the storage device <b>706</b> may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. A computer program product can be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory <b>704</b>, the storage device <b>706</b>, memory on processor <b>702</b>, or a propagated signal.</p>
<p id="p-0096" num="0095">The high speed controller <b>708</b> manages bandwidth-intensive operations for the computing device <b>700</b>, while the low speed controller <b>712</b> manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only. In one implementation, the high-speed controller <b>708</b> is coupled to memory <b>704</b>, display <b>716</b> (e.g., through a graphics processor or accelerator), and to high-speed expansion ports <b>710</b>, which may accept various expansion cards (not shown). In the implementation, low-speed controller <b>712</b> is coupled to storage device <b>706</b> and low-speed expansion port <b>714</b>. The low-speed expansion port, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.</p>
<p id="p-0097" num="0096">The computing device <b>700</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server <b>720</b>, or multiple times in a group of such servers. It may also be implemented as part of a rack server system <b>724</b>. In addition, it may be implemented in a personal computer such as a laptop computer <b>722</b>. Alternatively, components from computing device <b>700</b> may be combined with other components in a mobile device (not shown), such as device <b>750</b>. Each of such devices may contain one or more of computing device <b>700</b>, <b>750</b>, and an entire system may be made up of multiple computing devices <b>700</b>, <b>750</b> communicating with each other.</p>
<p id="p-0098" num="0097">Computing device <b>750</b> includes a processor <b>752</b>, memory <b>764</b>, an input/output device such as a display <b>754</b>, a communication interface <b>766</b>, and a transceiver <b>768</b>, among other components. The device <b>750</b> may also be provided with a storage device, such as a microdrive or other device, to provide additional storage. Each of the components <b>750</b>, <b>752</b>, <b>764</b>, <b>754</b>, <b>766</b>, and <b>768</b>, are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.</p>
<p id="p-0099" num="0098">The processor <b>752</b> can execute instructions within the computing device <b>750</b>, including instructions stored in the memory <b>764</b>. The processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors. The processor may provide, for example, for coordination of the other components of the device <b>750</b>, such as control of user interfaces, applications run by device <b>750</b>, and wireless communication by device <b>750</b>.</p>
<p id="p-0100" num="0099">Processor <b>752</b> may communicate with a user through control interface <b>758</b> and display interface <b>756</b> coupled to a display <b>754</b>. The display <b>754</b> may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology. The display interface <b>756</b> may comprise appropriate circuitry for driving the display <b>754</b> to present graphical and other information to a user. The control interface <b>758</b> may receive commands from a user and convert them for submission to the processor <b>752</b>. In addition, an external interface <b>762</b> may be provide in communication with processor <b>752</b>, so as to enable near area communication of device <b>750</b> with other devices. External interface <b>762</b> may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.</p>
<p id="p-0101" num="0100">The memory <b>764</b> stores information within the computing device <b>750</b>. The memory <b>764</b> can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units. Expansion memory <b>774</b> may also be provided and connected to device <b>750</b> through expansion interface <b>772</b>, which may include, for example, a SIMM (Single In Line Memory Module) card interface. Such expansion memory <b>774</b> may provide extra storage space for device <b>750</b>, or may also store applications or other information for device <b>750</b>. Specifically, expansion memory <b>774</b> may include instructions to carry out or supplement the processes described above, and may include secure information also. Thus, for example, expansion memory <b>774</b> may be provide as a security module for device <b>750</b>, and may be programmed with instructions that permit secure use of device <b>750</b>. In addition, secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.</p>
<p id="p-0102" num="0101">The memory may include, for example, flash memory and/or NVRAM memory, as discussed below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory <b>764</b>, expansion memory <b>774</b>, memory on processor <b>752</b>, or a propagated signal that may be received, for example, over transceiver <b>768</b> or external interface <b>762</b>.</p>
<p id="p-0103" num="0102">Device <b>750</b> may communicate wirelessly through communication interface <b>766</b>, which may include digital signal processing circuitry where necessary. Communication interface <b>766</b> may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. The device may be provided with a subscriber identity module (SIM) card that stores a key for identifying a subscriber with a telecommunications carrier to enable communication through the carrier. Such communication may occur, for example, through radio-frequency transceiver <b>768</b>. In addition, short-range communication may occur, such as using a Bluetooth, WiFi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module <b>770</b> may provide additional navigation- and location-related wireless data to device <b>750</b>, which may be used as appropriate by applications running on device <b>750</b>.</p>
<p id="p-0104" num="0103">Device <b>750</b> may also communicate audibly using audio codec <b>760</b>, which may receive spoken information from a user and convert it to usable digital information. Audio codec <b>760</b> may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device <b>750</b>. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device <b>750</b>.</p>
<p id="p-0105" num="0104">The computing device <b>750</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone <b>780</b>. It may also be implemented as part of a smartphone <b>782</b>, personal digital assistant, tablet device <b>784</b>, or other similar mobile device.</p>
<p id="p-0106" num="0105">Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.</p>
<p id="p-0107" num="0106">These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms &#x201c;machine-readable medium&#x201d; &#x201c;computer-readable medium&#x201d; refers to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term &#x201c;machine-readable signal&#x201d; refers to any signal used to provide machine instructions and/or data to a programmable processor.</p>
<p id="p-0108" num="0107">To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.</p>
<p id="p-0109" num="0108">The systems and techniques described here can be implemented in a computing system that includes a back end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network (&#x201c;LAN&#x201d;), a wide area network (&#x201c;WAN&#x201d;), and the Internet.</p>
<p id="p-0110" num="0109">The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p>
<p id="p-0111" num="0110">A number of embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the invention. For example, much of this document has been described with respect to particular shapes or type of images, but other visible items that are rendered may also be provided with antialiasing.</p>
<p id="p-0112" num="0111">In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, or steps may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other embodiments are within the scope of the following claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A computer-implemented method, comprising:
<claim-text>identifying a bit-mapped image of a line or polygon shape, the image including a plurality of pixels;</claim-text>
<claim-text>mapping the bit-mapped image to a texture map that is larger in at least one dimension than the bit-mapped image;</claim-text>
<claim-text>overlaying the bit-mapped image and the texture map;</claim-text>
<claim-text>computing pixel shading for pixels between an outer edge of the bit-mapped image and the texture map by measuring a distance from particular ones of the pixels to a line near an edge of the bit-mapped image; and</claim-text>
<claim-text>displaying the bit-mapped image with the pixels between an edge of the bit-mapped image and an edge of the texture map at its edge shaded according to the computed pixel shading,</claim-text>
<claim-text>wherein the texture map is an integer number of pixels larger than the bit-mapped image in the at least one dimension.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the texture map is an integer number of pixels larger than the bit-mapped image along every peripheral edge of the bit-mapped image.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising determining a rotation angle of the bit-mapped image in three-dimensional space.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising determining a rotation angle of the bit-mapped image in two-dimensional space.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising identifying endpoints of an edge of the bit-mapped image and creating the line using the identified endpoints.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the distance is determined based on a distance between a point on a pixel and a point on the line that is closest to the point on the pixel.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A computer-implemented system, comprising:
<claim-text>one or more processors;</claim-text>
<claim-text>storage accessible by the one or more processors and storing one or more digital images that each include a plurality of pixels;</claim-text>
<claim-text>an antialiasing module programmed to execute on the one or more processors to:
<claim-text>generate a texture map that is made up of pixels and is a defined size larger than a first digital image, of the one or more digital images, to be displayed by the system, and</claim-text>
<claim-text>compute shading values for pixels between an outer edge of the first digital image and an outer edge of the texture map as a function of a distance between a particular pixel and an idealized line near an edge of the first digital image; and</claim-text>
</claim-text>
<claim-text>a renderer to receive inputs generated from the antialiasing module and to generate a display of the first digital image having antialiased edges,</claim-text>
<claim-text>wherein the texture map is an integer number of pixels larger than the bit-mapped image in the at least one dimension.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The computer-implemented system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the texture map is an integer number of pixels larger than the first digital image along two opposed edges, but a same dimension as the first digital image along two other opposed edges.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The computer-implemented system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the antialising module is programmed to determine a rotation angle of the polygon shape in three-dimensional space.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The computer-implemented system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the antialiasing module is programmed to determine a rotation angle of the polygon shape in two-dimensional space.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The computer-implemented system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the antialiasing module is programmed to identify endpoints of an edge of the bit-mapped image and creating the line using the identified endpoints.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The computer-implemented system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the distance is determined based on a distance between a point on a pixel and a point on the line that is closest to the point on the pixel.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. One or more non-transitory recordable storage media having recorded thereon instructions that, when executed, perform operations comprising:
<claim-text>identifying a shape in the form of a line or polygon to be displayed on a computing device;</claim-text>
<claim-text>mapping the shape to a texture map that is larger in at least one dimension than the identified shape;</claim-text>
<claim-text>overlaying the identified shape and the texture map with each other;</claim-text>
<claim-text>computing pixel shading for pixels on the texture map between an outer edge of the identified shape and an edge of the texture map by measuring a distance from particular ones of the pixels to a line near an edge of the identified shape; and</claim-text>
<claim-text>displaying the identified shape with pixels at its edge shaded according to the computed pixel shading,</claim-text>
<claim-text>wherein the texture map is an integer number of pixels larger than the identified shape in at least one dimension.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The non-transitory recordable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the texture map is an integer number of pixels larger than the identified shape along two opposed edges, but a same dimension as the identified shape along two other opposed edges.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The non-transitory recordable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the operations further comprise identifying endpoints of an edge of the identified shape and creating the line using the line using the identified endpoints.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The non-transitory recordable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the distance is determined based on a distance between a point on a pixel and a point on the line that is closest to the point on the pixel.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text>17. The non-transitory recordable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the texture map covers a plurality of separate identified shapes at the same time.</claim-text>
</claim>
</claims>
</us-patent-grant>
