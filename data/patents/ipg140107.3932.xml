<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08625000-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08625000</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13209489</doc-number>
<date>20110815</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>JP</country>
<doc-number>2010-183297</doc-number>
<date>20100818</date>
</priority-claim>
</priority-claims>
<us-term-of-grant>
<us-term-extension>302</us-term-extension>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>76</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>H</section>
<class>04</class>
<subclass>N</subclass>
<main-group>5</main-group>
<subgroup>222</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>K</subclass>
<main-group>9</main-group>
<subgroup>46</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>3482312</main-classification>
<further-classification>34833311</further-classification>
<further-classification>382190</further-classification>
</classification-national>
<invention-title id="d2e71">Image pickup apparatus that continuously takes images to obtain multiple images, control method therefor, and storage medium</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>7456874</doc-number>
<kind>B1</kind>
<name>Ono</name>
<date>20081100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348239</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>8199213</doc-number>
<kind>B2</kind>
<name>Hattori et al.</name>
<date>20120600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>8208732</doc-number>
<kind>B2</kind>
<name>Nakamura</name>
<date>20120600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382190</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>8254639</doc-number>
<kind>B2</kind>
<name>Tsujimura</name>
<date>20120800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382107</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>8289400</doc-number>
<kind>B2</kind>
<name>Brunner et al.</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482081</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>2003/0193610</doc-number>
<kind>A1</kind>
<name>Nozaki et al.</name>
<date>20031000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>348345</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>2006/0132623</doc-number>
<kind>A1</kind>
<name>Nozaki et al.</name>
<date>20060600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34823199</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>2006/0215041</doc-number>
<kind>A1</kind>
<name>Kobayashi</name>
<date>20060900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482201</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>2006/0221223</doc-number>
<kind>A1</kind>
<name>Terada</name>
<date>20061000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>34833305</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>2008/0122944</doc-number>
<kind>A1</kind>
<name>Zhang</name>
<date>20080500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>2008/0170761</doc-number>
<kind>A1</kind>
<name>Teng et al.</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382118</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>2009/0087099</doc-number>
<kind>A1</kind>
<name>Nakamura</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>2009/0324098</doc-number>
<kind>A1</kind>
<name>Nilsson</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382209</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>2010/0033590</doc-number>
<kind>A1</kind>
<name>Kawaguchi</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>2010/0039527</doc-number>
<kind>A1</kind>
<name>Kretz et al.</name>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>2010/0149361</doc-number>
<kind>A1</kind>
<name>Takeuchi</name>
<date>20100600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2010/0189356</doc-number>
<kind>A1</kind>
<name>Sugita</name>
<date>20100700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>382190</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2011/0013038</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20110100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2011/0181742</doc-number>
<kind>A1</kind>
<name>Nozaki et al.</name>
<date>20110700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482084</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2011/0205383</doc-number>
<kind>A1</kind>
<name>Shah</name>
<date>20110800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2011/0261219</doc-number>
<kind>A1</kind>
<name>Suzuki et al.</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2012/0300092</doc-number>
<kind>A1</kind>
<name>Kim et al.</name>
<date>20121100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>3482221</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>CN</country>
<doc-number>101419666</doc-number>
<kind>A</kind>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>CN</country>
<doc-number>101646019</doc-number>
<kind>A</kind>
<date>20100200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>JP</country>
<doc-number>2000209483</doc-number>
<kind>A</kind>
<date>20000700</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>JP</country>
<doc-number>2006053666</doc-number>
<kind>A</kind>
<date>20060200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>JP</country>
<doc-number>2009080529</doc-number>
<kind>A</kind>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>JP</country>
<doc-number>2009089077</doc-number>
<kind>A</kind>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>JP</country>
<doc-number>2009-253848</doc-number>
<kind>A</kind>
<date>20091000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>JP</country>
<doc-number>2009-272740</doc-number>
<kind>A</kind>
<date>20091100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>JP</country>
<doc-number>2012060292</doc-number>
<kind>A</kind>
<date>20120300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
</us-citation>
<us-citation>
<nplcit num="00032">
<othercit>Notice on the First Office Action for corresponding CN 2011102380514, dated Sep. 17, 2013. English translation provided.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>7</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>3482201</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482312</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>3482291</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34833305</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34833311</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>34833312</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382190</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382209</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>382224</main-classification>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>12</number-of-drawing-sheets>
<number-of-figures>15</number-of-figures>
</figures>
<us-related-documents>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120044384</doc-number>
<kind>A1</kind>
<date>20120223</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Shibagami</last-name>
<first-name>Genjiro</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
<residence>
<country>JP</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Shibagami</last-name>
<first-name>Genjiro</first-name>
<address>
<city>Tokyo</city>
<country>JP</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Rossi, Kimms &#x26; McDowell LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Canon Kabushiki Kaisha</orgname>
<role>03</role>
<address>
<country>JP</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Villecco</last-name>
<first-name>John</first-name>
<department>2661</department>
</primary-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">An image pickup apparatus capable of selecting an image more suited to a user from images that have been continuously taken. A face included in each of a plurality of images is chosen, and with respect to each of the images, an evaluation value for use in selecting one image from the plurality of images is calculated based on the face chosen in each of the images. An image with the highest evaluation value is selected, and when there are a plurality of images with the highest evaluation value, an image taken first is selected. The selected image is then recorded.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="168.06mm" wi="245.96mm" file="US08625000-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="248.07mm" wi="180.26mm" orientation="landscape" file="US08625000-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="219.03mm" wi="158.16mm" file="US08625000-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="195.33mm" wi="138.77mm" file="US08625000-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="198.12mm" wi="139.19mm" file="US08625000-20140107-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="232.07mm" wi="155.53mm" orientation="landscape" file="US08625000-20140107-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="219.46mm" wi="188.21mm" file="US08625000-20140107-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="174.41mm" wi="142.58mm" file="US08625000-20140107-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="256.46mm" wi="186.69mm" orientation="landscape" file="US08625000-20140107-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="232.66mm" wi="170.10mm" orientation="landscape" file="US08625000-20140107-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="232.41mm" wi="125.14mm" orientation="landscape" file="US08625000-20140107-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="263.48mm" wi="194.39mm" file="US08625000-20140107-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="206.42mm" wi="138.94mm" orientation="landscape" file="US08625000-20140107-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0002" num="0001">1. Field of the Invention</p>
<p id="p-0003" num="0002">The present invention relates to an image pickup apparatus, a control method, and a storage medium, and more particularly to an image pickup apparatus that continuously takes images to obtain multiple images, a control method therefor, and a storage medium storing a program for implementing the method.</p>
<p id="p-0004" num="0003">2. Description of the Related Art</p>
<p id="p-0005" num="0004">Conventionally, image processing techniques that enable images having a face with an expression desired to be remained is obtained from among continuously-taken images.</p>
<p id="p-0006" num="0005">For example, face images are detected from images, and facial expression evaluation values corresponding to respective facial expressions are calculated from the detected face images, and facial expressions are determined based on the relationship between the facial evaluation values and threshold values provided for the facial evaluation values (see, for example, Japanese Laid-Open Patent Publication (Kokai) No. 2009-253848). Also, there has been proposed a method that an image pickup means is provided for continuously obtaining a plurality of images, and the images are ranked using at least one of the following conditions, a state of a main subject, image composition, and a state of human faces as a predetermined condition (see, for example, Japanese Laid-Open Patent Publication (Kokai) No. 2009-272740).</p>
<p id="p-0007" num="0006">However, according to the invention described in Japanese Laid-Open Patent Publication (Kokai) No. 2009-253848, because threshold values are used for determination, facial expressions are determined in only second stages.</p>
<p id="p-0008" num="0007">Also, the invention described in Japanese Laid-Open Patent Publication (Kokai) No. 2009-272740 has the problem that the start timing of image pickup which is information reflecting the intent of a user is not taken into consideration although states of main subjects are taken into consideration. Specifically, even when each of a plurality of images that have been continuously taken are evaluated, and a highest ranked image is selected, this does not always suit to a user. For example, even when an image taken immediately after a release button for starting image pickup is pressed by a user is given a lower evaluation than an image taken several images later, there is only a small difference between them. In this case, the difference does not raise a problem for the user, and it can be thought that an image taken near the time at which the release button is intentionally operated by the user is likely to suit the user.</p>
<p id="p-0009" num="0008">Thus, the conventional arts have the problem that an image selected from continuously-taken images is not always an image suited to a user.</p>
<heading id="h-0002" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0010" num="0009">The present invention provides an image pickup apparatus capable of selecting an image more suited to a user can be selected from images that have been continuously taken, a control method for the image pickup apparatus, and a computer-readable storage medium storing a program for implementing the method.</p>
<p id="p-0011" num="0010">Accordingly, a first aspect of the present invention provides an image pickup apparatus that continuously takes images of a subject to obtain a plurality of images representing the subject, comprising a choosing unit configured to choose a face included in each of the plurality of images, a calculation unit configured to calculate an evaluation value, which is used to select one image from the plurality of images, based on the face chosen in each of the plurality of images by the choosing unit, with respect to each of the plurality of images, a selection unit configured to select an image with the highest evaluation value calculated by the calculation unit, and when there are a plurality of images with the highest evaluation value calculated by the calculation unit, select an image taken first, and a recording unit configured to record the image selected by the selection unit.</p>
<p id="p-0012" num="0011">Accordingly, a second aspect of the present invention provides a control method for an image pickup apparatus that continuously takes images of a subject to obtain a plurality of images representing the subject, comprising a choosing step of choosing a face included in each of the plurality of images, a calculation step of calculating an evaluation value, which is used to select one image from the plurality of images, based on the face chosen in each of the plurality of images in the choosing step, with respect to each of the plurality of images, a selection step of selecting an image with the highest evaluation value calculated in the calculation step, and when there are a plurality of images with the highest evaluation value calculated in the calculation step, selecting an image taken first, and a recording step of recording the image selected in the selection step.</p>
<p id="p-0013" num="0012">Accordingly, a third aspect of the present invention provides a computer-readable non-transitory storage medium storing a program for causing a computer to implement a control method for an image pickup apparatus that continuously takes images of a subject to obtain a plurality of images representing the subject, the control method comprising a choosing step of choosing a face included in each of the plurality of images, a calculation step of calculating an evaluation value, which is used to select one image from the plurality of images, based on the face chosen in each of the plurality of images in the choosing step, with respect to each of the plurality of images, a selection step of selecting an image with the highest evaluation value calculated in the calculation step, and when there are a plurality of images with the highest evaluation value calculated in the calculation step, selecting an image taken first, and a recording step of recording the image selected in the selection step.</p>
<p id="p-0014" num="0013">According to the present invention, an image more suited to the user can be selected from images that have been continuously taken.</p>
<p id="p-0015" num="0014">Further features of the present invention will become apparent from the following description of exemplary embodiments (with reference to the attached drawings).</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram schematically showing an exemplary arrangement of an image pickup apparatus according to an embodiment of the present invention.</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart showing an image data recording process carried out by a system controller appearing in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart showing the procedure of a face evaluation calculation process carried out in step S<b>202</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. 4A and 4B</figref> are flowcharts showing the procedure of a smile evaluation process carried out in step S<b>302</b> in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> are diagrams useful in explaining smile evaluation values assigned in the process in <figref idref="DRAWINGS">FIGS. 4A and 4B</figref>, in which <figref idref="DRAWINGS">FIG. 5A</figref> shows smile level, and <figref idref="DRAWINGS">FIG. 5B</figref> shows smile evaluation value.</p>
<p id="p-0021" num="0020"><figref idref="DRAWINGS">FIGS. 6A and 6B</figref> are flowcharts showing the procedure of an eye open evaluation process carried out in step S<b>303</b> in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. 7A and 7B</figref> are flowcharts showing the procedure of an eye open change detection process carried out in step S<b>507</b> in <figref idref="DRAWINGS">FIG. 6A</figref>.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. 8A to 8C</figref> are diagrams useful in explaining an eye open evaluation value assigned in the process in <figref idref="DRAWINGS">FIGS. 6A and 6B</figref>, in which <figref idref="DRAWINGS">FIG. 8A</figref> shows eye open level in the right eye, <figref idref="DRAWINGS">FIG. 8B</figref> shows eye open level in the left eye, and <figref idref="DRAWINGS">FIG. 8C</figref> shows eye open evaluation value.</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram useful in explaining a face evaluation value calculated in the process in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0004" level="1">DESCRIPTION OF THE EMBODIMENTS</heading>
<p id="p-0025" num="0024">The present invention will now be described in detail with reference to the drawings showing an embodiment thereof.</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. 1</figref> is a block diagram schematically showing an arrangement of an image pickup apparatus according to an embodiment of the present invention.</p>
<p id="p-0027" num="0026">Referring to <figref idref="DRAWINGS">FIG. 1</figref>, a digital camera <b>100</b> which is the image pickup apparatus according to the present embodiment includes a system controller <b>108</b>, an image pickup lens <b>101</b>, an image pickup device <b>102</b>, an A/D converter <b>103</b>, an image processing unit <b>104</b>, and a format converter <b>105</b>. The digital camera <b>100</b> further includes a DRAM <b>106</b>, an image recording unit <b>107</b>, and a face detection module <b>114</b>. The digital camera <b>100</b> further includes a smile detection module <b>115</b>, an eye region detection module <b>116</b>, an eye open detection module <b>117</b>, a console <b>109</b>, an image pickup mode SW <b>110</b>, a main SW <b>111</b>, and SWs <b>112</b> and <b>113</b>.</p>
<p id="p-0028" num="0027">The system controller <b>108</b> controls the entire digital camera <b>100</b>, and controls processes such as an image pickup sequence. The image pickup lens <b>101</b> includes a zoom mechanism, a focus lens mechanism, and a diaphragm shutter mechanism. The image pickup device <b>102</b> is a light-receiving means and a photoelectric conversion means for converting reflected light from a subject into an electric signal. The A/D converter <b>103</b> includes a CDS circuit that removes output noise from the image pickup device <b>102</b>, and a nonlinear amplification circuit that carries out nonlinear amplification before A/D conversion. The image processing unit <b>104</b> extracts signal components in a specific frequency band related to brightness from a signal output from the A/D converter <b>103</b>, and detects a focusing state of the subject. The format converter <b>105</b> converts a signal output from the image processing unit <b>104</b> into digital image data (hereafter referred to merely as an &#x201c;image&#x201d;). A DRAM <b>106</b> is a high-speed built-in memory (for example, a random access memory) in which an image output from the format converter <b>105</b> is recorded. The DRAM <b>106</b> is used as a high-speed buffer which is a temporary image storage means, or as a work memory for image compression and expansion. The image recording unit <b>107</b> includes a recording medium such as a memory card and its interface.</p>
<p id="p-0029" num="0028">The console <b>109</b> is for a user to operate the digital camera <b>100</b>. The console <b>109</b> includes a menu switch for configuring various settings of the digital camera <b>100</b> such as image pickup functions and image regeneration settings, a zoom lever for instructing the image pickup lens to perform a zooming operation, an operation mode switch for switching between an image pickup mode and a reproduction mode.</p>
<p id="p-0030" num="0029">The image pickup mode SW <b>110</b> is a switch for configuring settings such as determination as to whether or not to carry out face detection. The main SW <b>111</b> is a switch for turning on the power to the digital camera <b>100</b>. The SW <b>112</b> is a switch for taking image pickup standby actions such as AF (auto focus) and AE (auto exposure). The SW <b>113</b> is an image pickup switch for taking images after the SW <b>112</b> is operated. The SW <b>112</b> and the SW <b>113</b> are usually comprised of one button, and when the button is pressed halfway down, the SW <b>112</b> is turned on, and when the button is further pressed all the way down, the SW <b>113</b> is turned on.</p>
<p id="p-0031" num="0030">The face detection module <b>114</b> carries out face detection using an image signal processed by the image processing unit <b>104</b>, and sends detected one or more pieces of face information (for example, position, size, and reliability) to the system controller <b>108</b>. The face detection module <b>114</b> carries out a well-known face detection process on an image output from the image processing unit <b>104</b>, and detects a face region of a person included in an image taken by the image pickup device <b>102</b>. It should be noted that examples of the well-known face detection process include a method that a skin tone region is extracted from tone colors of respective pixels of an image, and a face is detected based on the degree of matching with a face contour plate prepared in advance. Moreover, there has been disclosed a method that a face is detected by extracting feature points of a face such as eyes, nose, and mouth using a known pattern recognition technique.</p>
<p id="p-0032" num="0031">The smile detection module <b>115</b> calculates a face smile level for the face detected by the face detection module <b>114</b>. Specifically, the smile detection module <b>115</b> obtains feature amounts required to calculate the degree of smiling such as contours of face constituents which constitute a face and include eyes, nose and mouth, and positions of the face constituents such as inner corners of eyes, outer corners of eyes, nostrils, corner of mouth, and lip. Examples of the method to obtain feature amounts include a method using template matching based on templates of respective face constituents, and a method using a determination unit for each face constituent obtained by machine learning using a number of sample images of face constituents. The smile detection module <b>115</b> calculates a smile level indicative of the degree to which a face smiles based on the above described feature amounts.</p>
<p id="p-0033" num="0032">The eye region detection module <b>116</b> detects an eye region from the face region detected by the face detection module <b>114</b>.</p>
<p id="p-0034" num="0033">The eye open detection module <b>117</b> detects the area of a brightness region corresponding to a black eye from a histogram in the eye region detected by the eye region detection module <b>116</b>. Then, the eye open detection module <b>117</b> calculates an eye open level, which is indicative of the degree to which the eyes open, according to the area of a region corresponding to a black eye relative to an eye region.</p>
<p id="p-0035" num="0034">It should be noted that methods for face detection, smile level calculation, and eye open level calculation are not limited to those described above, but various well-known methods may be used.</p>
<p id="p-0036" num="0035">With the arrangement described above, in the present embodiment, when the user operates the SW <b>113</b>, the system controller <b>108</b> starts continuously taking images. Then, system controller <b>108</b> records the obtained images on the DRAM <b>106</b>, selects one with the highest evaluation value, to be described later, from among them, and records the selected image in the image recording unit <b>107</b> which is a nonvolatile memory. It should be noted that when there are a plurality of pieces of image data with the highest evaluation value, an image taken at a time closest to the time at which the SW <b>113</b> was operated, that is, an image obtained first is selected from the plurality of pieces of image data. It should be noted that in the following description, continuously taking images may be referred to as &#x201c;continuous shooting&#x201d;.</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. 2</figref> is a flowchart showing an image data recording process carried out by the system controller <b>108</b> appearing in <figref idref="DRAWINGS">FIG. 1</figref>.</p>
<p id="p-0038" num="0037">The image data recording process is a process in which a plurality of images indicative of a subject are obtained by continuously shooting the subject in response to the SW <b>113</b> being turned on, and an image selected from the obtained plurality of images is recorded in the image recording unit <b>107</b>.</p>
<p id="p-0039" num="0038">Referring to <figref idref="DRAWINGS">FIG. 2</figref>, the system controller <b>108</b> carries out an image pickup process in which it reads out an electric charge signal from the image pickup device <b>102</b>, and writes taken images on a predetermined area of the DRAM <b>106</b> via the A/D converter <b>103</b>, the image processing unit <b>104</b>, and the format converter <b>105</b> (step S<b>201</b>). The system controller <b>108</b> carries out a face evaluation calculation process in <figref idref="DRAWINGS">FIG. 3</figref>, to be described later, to evaluate the taken images (step S<b>202</b>). Then, the system controller <b>108</b> determines whether or not to update an image to be recorded on the image recording unit <b>107</b> (hereafter referred to as &#x201c;to-be-recorded image data&#x201d;), that is, whether or not to select this image (step S<b>203</b>) (selection unit). When it is determined in the step S<b>203</b> that the to-be-recorded image data is not to be updated (NO in the step S<b>203</b>), the process proceeds to step S<b>205</b> with the step S<b>204</b> skipped. On the other hand, when it is determined in the step S<b>203</b> that the to-be-recorded image data is to be updated (YES in the step S<b>203</b>), the to-be-recorded image data is updated to the images subjected to the evaluation in the step S<b>202</b> (step S<b>204</b>). Then, the system controller <b>108</b> determines whether or not a predetermined number of images have been continuously taken (step S<b>205</b>). When it is determined that the predetermined number of images have been continuously taken (YES in the step S<b>205</b>), the system controller <b>108</b> causes the image recording unit <b>107</b> to record the to-be-recorded image data recorded on the DRAM <b>106</b> (step S<b>206</b>) (recording unit), and terminates the process.</p>
<p id="p-0040" num="0039">On the other hand, when it is determined in the step S<b>203</b> that the to-be-recorded image data is not to be updated (NO in the step S<b>203</b>), the system controller <b>108</b> proceeds the process to the step S<b>205</b>. When it is determined in the step S<b>205</b> that the predetermined number of images have not been continuously taken (NO in the S<b>205</b>), the system controller <b>108</b> repeatedly executes the step S<b>201</b> and the subsequent steps.</p>
<p id="p-0041" num="0040">A description will now be given of how it is determined in the step S<b>203</b> whether or not to update the to-be-recorded image data. One of conditions for update is the condition A that image data subjected to determination is &#x201c;the first image taken in sequential shooting&#x201d;. The other one of the conditions for update is the condition B that &#x201c;no minus evaluation value is assigned to all the faces subjected to the evaluation, and any of the faces subjected to the evaluation has a plus evaluation value for face and has a greater face evaluation value than all other evaluation values&#x201d;. When either one of the conditions A and B is satisfied, the to-be-recorded image data is updated. In the following description, an evaluation value for a face may be referred to as &#x201c;face evaluation value&#x201d;.</p>
<p id="p-0042" num="0041">When the face evaluation values are the same, this means that not &#x201c;greater than any other evaluation values&#x201d;, and hence the to-be-recorded image data is not updated, and an image close to the time at which image pickup is started is selected even when the face evaluation values are the same.</p>
<p id="p-0043" num="0042">According to the process in <figref idref="DRAWINGS">FIG. 2</figref>, when there are a plurality of images with the highest evaluation value, an image taken first is selected (YES in the step S<b>203</b>). As a result, the image that is temporary closest the time at which image pickup is started is selected, so that the image more suited to the user can be selected.</p>
<p id="p-0044" num="0043">A minus evaluation value means an evaluation value that is assigned to a face when the degree to which a face subjected to evaluation (hereafter referred to as &#x201c;evaluation target face&#x201d;) smiles lowers a predetermined amount or more, and further, when the degree to which the eyes open lowers a predetermined amount or more. A minus evaluation value is assigned when the evaluation target face changes from a smiling face to a normal face, and when the evaluation target face changes from an eye opening state to an eye closing state.</p>
<p id="p-0045" num="0044">A plus evaluation value means an evaluation value that is assigned to a face basically when the degree to which an evaluation target face smiles rises a predetermined amount or more, and further, when the degree to which the eyes open rises a predetermined amount or more. A plus evaluation value is assigned when an evaluation target face changes from a normal face to a smiling face, and when an evaluation target face changes from an eye opening state to an eye closing state.</p>
<p id="p-0046" num="0045">The above-mentioned evaluation target face is a face for which a smile level and an eye open level are calculated to calculate an evaluation value among faces detected from an image. The evaluation target face is determined from &#x201c;a face being present within a predetermined range of an image&#x201d;, &#x201c;a face with a predetermined size or larger size&#x201d;, &#x201c;a face focused at the start of image pickup&#x201d;, and so on. Namely, the evaluation target face is the face or faces of one or a plurality of persons intended as a subject by the user. From the second and subsequent images taken by continuous shooting, the same face as a face targeted for evaluation in the previous image pickup is preferentially adopted as an evaluation target face.</p>
<p id="p-0047" num="0046">Whether or not the faces are the same may be determined using a face recognition technique, or when the present evaluation target face is substantially the same in size and position as an evaluation target face in the previous image pickup, it may be determined that the present evaluation target face is the same face as an evaluation target face in the previous image pickup.</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. 3</figref> is a flowchart showing the procedure of the face evaluation calculation process carried out in the step S<b>202</b> in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0049" num="0048">Referring to <figref idref="DRAWINGS">FIG. 3</figref>, the system controller <b>108</b> detects faces from an image, and chooses evaluation target faces from the detected faces using the evaluation target face choosing method described above (step S<b>301</b>) (selection unit). The system controller <b>108</b> carries out a smile evaluation process in <figref idref="DRAWINGS">FIGS. 4A and 4B</figref>, to be described later, to do smile evaluation on any evaluation target face (step S<b>302</b>). The system controller <b>108</b> carries out an eye open evaluation process in <figref idref="DRAWINGS">FIG. 6</figref>, to be described later, to do a eye open evaluation on the same evaluation target face as the face on which the smile evaluation has been carried out (step S<b>303</b>). The system controller <b>108</b> then calculates a face evaluation value of the evaluation target face (step S<b>304</b>). In the present embodiment, the face evaluation value is calculated by adding up all the smile evaluation values and eye open evaluation values assigned to each evaluation target face.</p>
<p id="p-0050" num="0049">The system controller <b>108</b> then determines whether or not evaluations on all the evaluation target faces within the image have been completed (step S<b>305</b>). Upon determining that evaluations on all the evaluation target faces within the image have not been completed (NO in the step S<b>305</b>), the system controller <b>108</b> repeatedly carries out the processes in the step S<b>302</b> and the subsequent steps on the other evaluation target faces within the image. On the other hand, upon determining that evaluations on all the evaluation target faces within the image have been completed (YES in the step S<b>305</b>), the system controller <b>108</b> calculates image evaluation values (evaluation values) based on the faces selected from the respective images in the step S<b>301</b> (step S<b>306</b>) (calculation unit), and terminates the present process. The evaluation values are used to select one image from the plurality of images that have been taken.</p>
<p id="p-0051" num="0050">In the present embodiment, a face evaluation value is calculated by adding up all the evaluation values assigned to respective ones of a plurality of evaluation target faces in an image. Of course, when there is only one evaluation target face, a face evaluation value of this evaluation target face is an image evaluation value. When the number of persons is desired to be evaluated, an image evaluation value can be obtained by adding up face evaluation values of a plurality of evaluation target faces, and when the number of persons is not desired to be evaluated, an image evaluation value can be obtained by averaging face evaluation values of a plurality of evaluation target faces.</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIGS. 4A and 4B</figref> are flowcharts showing the procedure of the smile evaluation process carried out in the step S<b>302</b> in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0053" num="0052">Referring to <figref idref="DRAWINGS">FIGS. 4A and 4B</figref>, the system controller <b>108</b> obtains a smile level indicative of the degree of smiling with respect to an evaluation target face using the smile detection module <b>115</b> (step S<b>401</b>). The system controller <b>108</b> determines whether or not the present image is the first one of images that have been continuously taken (step S<b>402</b>). When it is determined that the present image is the first image (YES in the step S<b>402</b>), the system controller <b>108</b> assigns a basic evaluation value to it (step S<b>403</b>), and terminates the present process. On the other hand, when it is determined that the present image is not the first image (NO in the step S<b>402</b>), the system controller <b>108</b> determines whether or not the present evaluation target face is the same as the previous evaluation target face, that is, whether or not the present evaluation target face is the same face as an evaluation target face in the previous image pickup (step S<b>404</b>). When it is determined that the faces are not the same (NO in the step S<b>404</b>), the system controller <b>108</b> terminates the present process without assigning any evaluation value (step <b>410</b>). The reason why it is determined in the step S<b>404</b> whether or not the faces are the same is that the persons have to be the same so as to use a smile determination predetermined value because it varies among different individuals.</p>
<p id="p-0054" num="0053">On the other hand, when it is determined that the faces are the same (YES the step S<b>404</b>), the system controller <b>108</b> calculates a difference in smile level between the present image pickup and the previous image pickup (step S<b>405</b>). In the present embodiment, the smile level of an image targeted for comparison is compared to that in an image that has been previously taken, so that a change in facial expression is detected. On this occasion, the smile level of the target image may be compared to that in an image taken a predetermined time period ago, or may be compared to that in a taken image to be recorded. Thus, smile levels indicative of smiling degrees are obtained with respect to the same face in respective images.</p>
<p id="p-0055" num="0054">Then, the system controller <b>108</b> then calculates the ratio between the difference and a smile determination predetermined value (first predetermined value) for evaluating a change in facial expression (step S<b>406</b>). The system controller <b>108</b> determines whether or not the ratio is smaller than 1 and greater than &#x2212;1 (that is, whether or not the absolute value of the ratio is smaller than 1) (step S<b>407</b>). When it is determined that the absolute value of the ratio is smaller than 1 (YES in the step S<b>407</b>), the system controller <b>108</b> determines whether or not the evaluation target face has changed to a smile in the previous image for the first time (that is, whether or not the calculated ratio has become positive). When it is determined that the evaluation target face has not changed to a smile for the first time (NO in the step S<b>408</b>), the system controller <b>108</b> terminates the present process without assigning any evaluation value (step S<b>410</b>).</p>
<p id="p-0056" num="0055">On the other hand, when it is determined in the step S<b>407</b> that the absolute value of the ratio is not smaller than 1 (NO in the step S<b>407</b>), the system controller <b>108</b> determines whether or not the ratio is equal to or greater than 1 (step S<b>411</b>). When it is determined that the ratio is not equal to or greater than 1 (that is, the ratio is equal to or smaller than &#x2212;1) (NO in the step S<b>411</b>), the system controller <b>108</b> assigns a minus evaluation value (step S<b>412</b>) and terminates the present process. The minus evaluation value assigned in the step S<b>412</b> may be assigned according to the ratio obtained in the step S<b>406</b>. In this case, the larger the degree of change to a normal face, the greater in a minus direction an evaluation value to be assigned.</p>
<p id="p-0057" num="0056">On the other hand, when it is determined in the step S<b>408</b> that the evaluation target face has changed to a smile for the first time (YES in the step S<b>408</b>), the system controller <b>108</b> assigns a plus evaluation value (step S<b>409</b>) and terminates the present process. The plus evaluation value assigned in this step S<b>409</b> after the step S<b>408</b> is a uniform value.</p>
<p id="p-0058" num="0057">When it is determined that the ratio is equal to or greater than 1 (YES in the step S<b>411</b>), the system controller <b>108</b> assigns a plus evaluation value (step S<b>409</b>) and terminates the present process. The plus evaluation value assigned in this step S<b>409</b> after the step S<b>411</b> may be assigned according to the ratio obtained in the step S<b>406</b>. In this case, the larger the degree of change to a smile, the greater in a plus direction an evaluation value to be assigned.</p>
<p id="p-0059" num="0058">When in the process in <figref idref="DRAWINGS">FIG. 4B</figref>, it is determined that the evaluation target face has changed to a smile for the first time with respect to the previous image (YES in the step S<b>407</b> and YES in the step S<b>408</b>), the system controller <b>108</b> assigns a plus evaluation value only once even when the degree of the change is small. In an image in which a facial expression has just changed to a state determined as being a smile, the facial expression is likely to have reached a satisfactory smile level. Thus, by assigning a plus evaluation value only once even when the degree of smiling has not changed the smile determination predetermined value or more (step S<b>409</b>), a next image that has changed is likely to be selected.</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIGS. 5A and 5B</figref> are diagrams useful in explaining smile evaluation values assigned in the process in <figref idref="DRAWINGS">FIGS. 4A and 4B</figref>, in which <figref idref="DRAWINGS">FIG. 5A</figref> shows smile level, and <figref idref="DRAWINGS">FIG. 5B</figref> shows smile evaluation value.</p>
<p id="p-0061" num="0060">In the graph of <figref idref="DRAWINGS">FIG. 5A</figref>, the vertical axis represents smile level, and the horizontal axis represents the number of images that are continuously taken. In this graph, a white circle <b>601</b> indicates smile level in the first image. A difference <b>602</b> is a difference between smile level in the first image and a smile level in the second image. A line segment <b>603</b> is for comparing a smile determination predetermined value and a difference with each other, and the smile determination predetermined value is a segment between the nearest black circles. The white circle <b>601</b>, the difference <b>602</b>, and the line segment <b>603</b> are used in the same sense with respect to images other than the first one.</p>
<p id="p-0062" num="0061">Particularly in <figref idref="DRAWINGS">FIG. 5A</figref>, as indicated by the line segment <b>603</b>, a difference between the first image and the second image is twice the smile determination predetermined value, and in this case, the ratio is &#x201c;+2&#x201d;.</p>
<p id="p-0063" num="0062">A difference between the second image and the third image and a difference between the third image and the fourth image are smaller than the smile determination predetermined value, and a difference between the fourth image and the fifth image is a little greater than the smile determination predetermined value.</p>
<p id="p-0064" num="0063">On the other hand, in the graph of <figref idref="DRAWINGS">FIG. 5B</figref>, the vertical axis represents smile evaluation value, and the horizontal axis represents the number of images that are continuously taken.</p>
<p id="p-0065" num="0064">Referring to the graph of <figref idref="DRAWINGS">FIG. 5B</figref>, because there is no image prior to the first image, no difference in smile level can be calculated, and hence the above described basic evaluation value such as &#x201c;10&#x201d; is assigned. Then, when the first image and the second image shown in <figref idref="DRAWINGS">FIG. 5A</figref> are compared in smile level with each other, a difference between them is a plus value greater than the smile determination predetermined value.</p>
<p id="p-0066" num="0065">Because the ratio between the difference and the smile determination predetermined value is &#x201c;+2&#x201d; as described above, the system controller <b>108</b> assigns an evaluation value of &#x201c;+20&#x201d;, and thus the total evaluation value is &#x201c;30&#x201d;. When the second image and the third image are compared in smile level with each other, a difference between them is smaller than the smile determination predetermined value, and the system controller <b>108</b> thus determines that the face has not changed to a smile. However, when it is determined that the face has changed to a smile for the first time in the previous image, and facial expression has not changed, the system controller <b>108</b> assigns a uniform evaluation value only once. Here, the system controller <b>108</b> assigns an evaluation value of &#x201c;+10&#x201d;, and thus the total evaluation value is &#x201c;40&#x201d;.</p>
<p id="p-0067" num="0066">Next, when the third image and the fourth image are compared in smile level with each other, a difference between them is smaller than the smile determination predetermined value, and the system controller <b>108</b> thus determines that the face has not changed to a smile. As a result, no evaluation is assigned, and hence the total evaluation value is still &#x201c;40&#x201d;. Then, when the fourth image and the fifth image are compared in smile level with each other, a difference between them is a minus value greater than the smile determination predetermined value. Because the ratio between the difference and the smile determination predetermined value is not less than 1 and less than 2, an evaluation value of &#x201c;&#x2212;10&#x201d; is assigned, and thus the total evaluation value is &#x201c;30&#x201d;.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIGS. 6A and 6B</figref> are flowcharts showing the procedure of the eye open evaluation process carried out in the step S<b>303</b> in <figref idref="DRAWINGS">FIG. 3</figref>.</p>
<p id="p-0069" num="0068">Referring to <figref idref="DRAWINGS">FIGS. 6A and 6B</figref>, with respect to an evaluation target face, the system controller <b>108</b> causes the eye open detection module <b>117</b> to obtain an eye open level indicative of the degree to which the eyes open (step S<b>501</b>). The system controller <b>108</b> determines whether or not the present evaluated image is the first one of images that are continuously taken (step S<b>502</b>). Upon determining that the present evaluated image is the first one of images that are continuously taken (YES in the step S<b>502</b>), the system controller <b>108</b> assigns the basic evaluation value (step S<b>503</b>) and terminates the present process. On the other hand, upon determining that the present evaluated image is not the first one of images that are continuously taken (NO in the step S<b>502</b>), the system controller <b>108</b> determines whether or not the present evaluation target face is the same as an evaluation target face in the previous image pickup (step S<b>504</b>). The reason why it is determined whether or not the faces are the same is that the persons have to be the same so as to use an eye open change determination predetermined value (second predetermined value) for evaluating changes in the degree to which the eyes open because it varies among different individuals. Thus, eye open levels indicative of the degrees to which the eyes open are obtained with respect to the same faces in the respective images.</p>
<p id="p-0070" num="0069">The eye open change determination predetermined value may be prepared in advance as a predetermined value, or calculated from a measurement value obtained by measuring transitions in the eye open level of each eye of evaluation target faces obtained before the start of image pickup. Also, the eye open change determination predetermined value may be different between right and left eyes.</p>
<p id="p-0071" num="0070">When it is determined in the step S<b>504</b> that the present evaluation target face is the same as an evaluation target face in the previous image pickup (YES in the step S<b>504</b>), the system controller <b>108</b> determines whether or not the face has changed to a smile (step S<b>505</b>).</p>
<p id="p-0072" num="0071">On the other hand, when it is determined in the step S<b>504</b> that the present evaluation target face is not the same as an evaluation target face in the previous image pickup (NO in the step S<b>504</b>), the system controller <b>108</b> terminates the present process without assigning any evaluation value (step S<b>506</b>). The reason why no evaluation value is assigned when the face has changed to a smile is that the face having changed to a smile is likely to close the eyes.</p>
<p id="p-0073" num="0072">When it is determined in the step S<b>504</b> that the face has not changed to a smile (NO in the step S<b>505</b>), the system controller <b>108</b> carries out an eye open change detection process in <figref idref="DRAWINGS">FIGS. 7A and 7B</figref>, to be described later (step S<b>507</b>). The eye open change detection process is a process in which a change in eye open is detected by calculating a difference in the eye open level of each eye between the present image pickup and the previous image pickup. Regarding the previous image pickup subjected to comparison in the step S<b>507</b>, the system controller <b>108</b> compares eye open levels between an image in the present image pickup and an image the previous image pickup to detect a change in facial expression. On this occasion, the eye open level of an image in the present image pickup may be compared to the eye open level of an image taken a predetermined time ago, or may be compared to the eye open level of a taken image which is an image to be recorded.</p>
<p id="p-0074" num="0073">Then, the system controller <b>108</b> determines whether or not the eye open levels of both eyes have changed (step S<b>508</b>). When it is determined that the eye open levels of both eyes have changed (YES in the step S<b>508</b>), the system controller <b>108</b> determines whether or not both eyes have changed in the same direction (step S<b>512</b>). When it is determined that both eyes have changed in the same direction (YES in the step S<b>512</b>), the system controller <b>108</b> determines whether or not both eyes have changed in an eye opening direction (step S<b>514</b>). When it is determined that both eyes have changed in an eye opening direction (YES in the step S<b>514</b>), the system controller <b>108</b> assigns a plus evaluation value (step S<b>515</b>) and terminates the present process. The plus evaluation value assigned here is a value according to the ratio of an eye whose ratio between the difference and the eye open change determination predetermined value is higher.</p>
<p id="p-0075" num="0074">When it is determined in the step S<b>508</b> that the eye open levels of both eyes have not changed (NO in the step S<b>508</b>), the system controller <b>108</b> determines whether or not the eye open level of only one eye has changed (step S<b>509</b>). When it is determined that the eye open level of only one eye has not changed, that is, the eye open level of neither of the eyes has changed (NO in the step S<b>509</b>), the system controller <b>108</b> terminates the present process without assigning any evaluation value (step S<b>506</b>).</p>
<p id="p-0076" num="0075">When it is determined that the eye open level of only one eye has changed (YES in the step S<b>509</b>), the system controller <b>108</b> determines whether or not the eye has change in an eye opening direction (step S<b>510</b>). When it is determined that the eye has changed in an eye opening direction (YES in the step S<b>510</b>), the system controller <b>108</b> assigns a plus evaluation value (step S<b>511</b>) and terminates the present process.</p>
<p id="p-0077" num="0076">When it is determined in the step S<b>510</b> that the eye has not changed in an eye opening direction (NO in the step S<b>510</b>), the system controller <b>108</b> assigns a minus evaluation value (step S<b>513</b>) and terminates the present process. When it is determined in the step S<b>512</b> that both eyes have not changed in the same direction (NO in the step S<b>512</b>), the system controller <b>108</b> assigns a minus evaluation value (step S<b>513</b>) and terminates the present process. When it is determined that the eyes have not changed in an eye opening direction (NO in the step S<b>514</b>), the system controller <b>108</b> assigns a minus evaluation value (step S<b>513</b>) and terminates the present process.</p>
<p id="p-0078" num="0077">The minus evaluation value assigned in the step S<b>513</b> is a value according to the ratio of an eye whose ratio between the difference and the eye open change determination predetermined value is higher.</p>
<p id="p-0079" num="0078">The reason why a minus evaluation value is assigned when it is determined in the step S<b>512</b> that the eyes have not changed in the same direction is that when the facial expression is likely to be unbalanced because one eye has changed in an eye opening direction, and the other eye has changed in an eye closing direction.</p>
<p id="p-0080" num="0079"><figref idref="DRAWINGS">FIGS. 7A and 7B</figref> are flowcharts showing the procedure of the eye open change detection process carried out in the step S<b>507</b> in <figref idref="DRAWINGS">FIG. 6A</figref>.</p>
<p id="p-0081" num="0080">According to the flowcharts of <figref idref="DRAWINGS">FIGS. 7A and 7B</figref>, the process is sequentially carried out for the right eye and the left eye in this order.</p>
<p id="p-0082" num="0081"><figref idref="DRAWINGS">FIGS. 7A and 7B</figref>, the system controller <b>108</b> calculates a difference in the eye open level of each eye between the present image pickup and the previous image pickup (step S<b>701</b>). The system controller <b>108</b> then calculates the ratio between the difference of each eye and the eye open change determination predetermined value (step S<b>702</b>), and determines whether or not the ratio of the right eye is greater than &#x2212;1 and smaller than 1 (that is, whether or not the absolute value of the ratio is smaller than 1) (step S<b>703</b>). When it is determined that the absolute value of the ratio of the right eye is smaller than 1 (YES in the step S<b>703</b>), this means that the right eye is unchanged, and thus the system controller <b>108</b> stores in the DRAM <b>106</b> that the right eye is unchanged (step S<b>704</b>) and proceeds to step S<b>705</b>.</p>
<p id="p-0083" num="0082">When it is determined in the step S<b>703</b> that the absolute value of the ratio of the right eye is not smaller than 1 (NO in the step S<b>703</b>), the system controller <b>108</b> determines whether or not the ratio of the right eye is equal to or greater than 1 (step S<b>707</b>). When it is determined that the ratio of the right eye is equal to or greater than 1 (YES in the step S<b>707</b>), this means that the right eye has changed in an eye opening direction, and thus the system controller <b>108</b> stores in the DRAM <b>106</b> that the right eye has changed in an eye opening direction (step S<b>708</b>) and proceeds to the step S<b>705</b>.</p>
<p id="p-0084" num="0083">When it is determined that the ratio of the right eye is smaller than 1, that is, the ratio of the right eye is not more than &#x2212;1 (NO in the step S<b>707</b>), this means that the right eye has changed in an eye closing direction, and thus the system controller <b>108</b> stores in the DRAM <b>106</b> that the right eye has changed in an eye closing direction (step S<b>709</b>) and proceeds to the step S<b>705</b>.</p>
<p id="p-0085" num="0084">The system controller <b>108</b> then determines whether or not the ratio of the left eye is greater than &#x2212;1 and smaller than 1 (that is, whether or not the absolute value of the ratio is smaller than 1) (step S<b>705</b>). When it is determined that the absolute value of the ratio of the left eye is smaller than 1 (YES in the step S<b>705</b>), this means that the left eye is unchanged, and thus the system controller <b>108</b> stores in the DRAM <b>106</b> that the left eye is unchanged (step S<b>706</b>) and terminates the present process.</p>
<p id="p-0086" num="0085">When it is determined in the step S<b>705</b> that the absolute value of the ratio of the left eye is not smaller than 1 (NO in the step S<b>705</b>), the system controller <b>108</b> determines whether or not the ratio of the left eye is equal to or greater than 1 (step S<b>710</b>). When it is determined that the ratio of the left eye is equal to or greater than 1 (YES in the step S<b>710</b>), this means that the left eye has changed in an eye opening direction. Thus, the system controller <b>108</b> stores in the DRAM <b>106</b> that the left eye has changed in an eye opening direction (step S<b>711</b>) and terminates the present process.</p>
<p id="p-0087" num="0086">When it is determined in the step S<b>710</b> that the ratio of the left eye is smaller than 1, that is, not more than &#x2212;1 (NO in the step S<b>710</b>), this means that the left eye has changed in an eye closing direction. Thus, the system controller <b>108</b> stores in the DRAM <b>106</b> that the left eye has changed in an eye closing direction (step S<b>712</b>) and terminates the present process.</p>
<p id="p-0088" num="0087"><figref idref="DRAWINGS">FIGS. 8A to 8C</figref> are diagrams useful in explaining an eye open evaluation value assigned in the process in <figref idref="DRAWINGS">FIGS. 6A and 6B</figref>, in which <figref idref="DRAWINGS">FIG. 8A</figref> shows the eye open level of the right eye, <figref idref="DRAWINGS">FIG. 8B</figref> shows the eye open level of the left eye, and <figref idref="DRAWINGS">FIG. 8C</figref> shows eye open evaluation value.</p>
<p id="p-0089" num="0088">In the graphs of <figref idref="DRAWINGS">FIGS. 8A and 8B</figref>, the vertical axis represents the eye open level of the right or left eye, and the horizontal axis represents the number of images that are continuously taken. In these graphs, white circles <b>701</b> and <b>704</b> represent eye open levels of the right or left eye in the first image. Differences <b>702</b> and <b>705</b> are differences in the smile level of the right or left eye between the first image and the second image. Line segments <b>703</b> and <b>706</b> are for comparing an eye open determination predetermined value and a difference with each other with respect to the right or left eye, and the eye open determination predetermined value is a segment between the nearest two black circles. The white circles <b>701</b> and <b>704</b> and the line segments <b>703</b> and <b>706</b> are used in the same sense with respect to images other than the first one.</p>
<p id="p-0090" num="0089">Referring to <figref idref="DRAWINGS">FIG. 8A</figref>, an eye open level corresponding to the difference <b>702</b> between the first image and the second image is a minus value not less than the eye open determination predetermined value and less than twice the eye open determination predetermined value, and hence the system controller <b>108</b> determines that the eye has changed in an eye closing direction. However, when it is determined that there is a smile in the second image, it is not determined whether or not the left eye has changed in an eye closing direction. Referring to <figref idref="DRAWINGS">FIG. 8A</figref>, a description will be given of an eye open evaluation value on the assumption that there is a smile in the second image.</p>
<p id="p-0091" num="0090">Next, when the second image and the third image are compared in eye open level with each other, a difference between them is a plus value not less than the eye open determination predetermined value and less than twice the eye open determination predetermined value, and the system controller <b>108</b> thus determines that the eye has changed in an eye opening direction. Further, when the third image and the fourth image are compared in eye open level with each other, a difference between them is a minus value not less than the eye open determination predetermined value and less than twice the eye open determination predetermined value, and the system controller <b>108</b> thus determines that the eye has changed in an eye closing direction.</p>
<p id="p-0092" num="0091">When the fourth image and the fifth image are compared in eye open level with each other, a difference between them is a plus value not less than the eye open determination predetermined value and less than twice the eye open determination predetermined value, and the system controller <b>108</b> thus determines that the eye has changed in an eye opening direction.</p>
<p id="p-0093" num="0092">Referring to <figref idref="DRAWINGS">FIG. 8B</figref>, an eye open level corresponding to the difference <b>705</b> between the first image and the second image is a minus value not less than the eye open determination predetermined value and less than twice the eye open determination predetermined value, and hence the system controller <b>108</b> determines that the eye has changed in an eye closing direction. However, when it is determined that there is a smile in the second image, it is not determined whether or not the eye has not changed in an eye closing direction. Referring to <figref idref="DRAWINGS">FIG. 8B</figref>, a description will be given of an eye open evaluation value on the assumption that there is a smile in the second image.</p>
<p id="p-0094" num="0093">Next, when the second image and the third image are compared in eye open level with each other, a difference between them is a plus value not less than the eye open determination predetermined value and less than twice the eye open determination predetermined value, and the system controller <b>108</b> thus determines that the eye has changed in an eye opening direction. Further, when the third image and the fourth image are compared in eye open level with each other, a difference between them is less than the eye open determination predetermined value, and the system controller <b>108</b> thus determines that the eye open level is unchanged. In addition, when the fourth image and the fifth image are compared in eye open level with each other, a difference between them is less than the eye open determination predetermined value, and the system controller <b>108</b> thus determines that the eye open level is unchanged.</p>
<p id="p-0095" num="0094">In the graph of <figref idref="DRAWINGS">FIG. 8C</figref>, the vertical axis represents eye open evaluation value, and the horizontal axis represents the number of images that are continuously taken. The eye open evaluation value is calculated based on the state of change in both eyes.</p>
<p id="p-0096" num="0095">A difference in smile level cannot be calculated for the first image because there is no previous image, and hence the system controller <b>108</b> assigns a predetermined evaluation value, for example, &#x201c;10&#x201d;. In the second image, the right eye has changed in an eye closing direction, and the left eye has changed in an eye closing direction, and thus the system controller <b>108</b> determines that the eyes have changed in the same direction, i.e. an eye closing direction. However, when it is determined that there is a smile in the second image, no evaluation value for eye open level is assigned, and thus the evaluation value remains to be &#x201c;10&#x201d;. In the third image, the right eye has changed in an eye opening direction, and the left eye has changed in an eye opening direction, and thus the system controller <b>108</b> determines that the eyes have changed in the same direction, i.e. an eye opening direction. The ratios of both the right and left eyes are not less than 1 and less than 2, and thus an evaluation value of &#x201c;+10&#x201d; is assigned, and the total evaluation value is &#x201c;20&#x201d;. In the fourth image, the right eye has changed in an eye closing direction, and the left eye has not changed, and thus the system controller <b>108</b> determines that the eyes have changed in an eye closing direction. The ratio of the right eye is not less than 1 and less than 2, and thus an evaluation value of &#x201c;&#x2212;10&#x201d; is assigned, and the total evaluation value is &#x201c;10&#x201d;. In the fifth image, the right eye has changed in an eye opening direction, and the left eye has not changed, and thus the system controller <b>108</b> determines that the eyes have changed in an eye opening direction. The ratio of the right eye is not less than 1 and less than 2, and thus an evaluation value of &#x201c;+10&#x201d; is assigned, and the total evaluation value is &#x201c;20&#x201d;.</p>
<p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. 9</figref> is a diagram useful in explaining the face evaluation value calculated in the process in <figref idref="DRAWINGS">FIG. 2</figref>.</p>
<p id="p-0098" num="0097">In the graph of <figref idref="DRAWINGS">FIG. 9</figref>, the vertical axis represents face evaluation value, and the horizontal axis represents the number of images that are continuously taken. The face evaluation value is the sum of a smile evaluation value and an eye open evaluation value.</p>
<p id="p-0099" num="0098">The smile evaluation value of the first image is &#x201c;10&#x201d;, the smile evaluation values of the second and fifth images are &#x201c;30&#x201d;, and the smile evaluation values of the third and fourth images are &#x201c;40&#x201d;. On the other hand, the eye open evaluation values of the third and fifth images are &#x201c;20&#x201d;, and the other eye open evaluation values are &#x201c;10&#x201d;.</p>
<p id="p-0100" num="0099">Thus, in the case of smile evaluation values in <figref idref="DRAWINGS">FIG. 5B</figref> and eye open evaluation values in <figref idref="DRAWINGS">FIG. 8C</figref>, the evaluation value of the first image is &#x201c;20&#x201d;, the evaluation value of the second image is &#x201c;40&#x201d;, the evaluation value of the third image is &#x201c;60&#x201d;, the evaluation value of the fourth image is &#x201c;50&#x201d;, and the evaluation value of the fifth image is &#x201c;50&#x201d;.</p>
<p id="p-0101" num="0100">When there is only one evaluation target face, a face evaluation value therefor is an image evaluation value. In the present embodiment, the third image is selected as an image to be recorded due to the conditions that &#x201c;no minus evaluation values are assigned to all the evaluation target faces. Also, a plus evaluation value is assigned to any of the evaluation target faces. Also, a face evaluation value is the highest&#x201d;.</p>
<p id="p-0102" num="0101">It should be noted that as described above, when there are a plurality of images that satisfy the above conditions, an image taken first is selected from the plurality of images with consideration given to the timing in which the SW <b>113</b> is operated by the user.</p>
<p id="p-0103" num="0102">Thus, when it is determined that the degree of change in the smile level or eye open level of an evaluation target face is smaller than a threshold value, the resulting evaluation value is the same value as the previous evaluation value. Thus, when there is only a small change in facial expression, image data can be selected with the timing of operation of the SW <b>113</b> by the user given higher priority than the level of the facial expression. As a result, an image can be selected with consideration given to both the evaluation value of the facial expression and the timing of the operation of the SW <b>113</b> by the user, and image data more suited to the user can be stored from images that have been continuously taken.</p>
<p id="p-0104" num="0103">Although in the present embodiment, face-related degrees are the levels of smile and eye open, evaluation values may be similarly assigned to the composition and focusing of an image depending on the position and size of a face and used for the selection of an image.</p>
<p id="p-0105" num="0104">In the present embodiment, when there are a plurality of images with the highest evaluation value, an image taken first is selected (YES in the step S<b>203</b>). Thus, an image close to the time at which image pickup is started is selected, and hence an image more suited to the user can be selected.</p>
<p id="p-0106" num="0105">In the embodiment described above, control of the system controller <b>108</b> may be exercised by a single piece of hardware, or shared by a plurality of pieces of hardware so as to control the entire apparatus.</p>
<p id="p-0107" num="0106">Moreover, although in the embodiment described above, the present invention is applied to the digital camera <b>100</b>, the present invention is not limited to this. Specifically, the present invention may be applied to any display control apparatus insofar as it can provide control so that so that a plurality of images can be displayed at the same time. Examples of the display control apparatus include a personal computer and a PDA. Moreover, a cellular phone terminal, a portable image viewer, a display provided in a printer unit which allows selection and confirmation of an image to be printed, and a digital photo-frame.</p>
<heading id="h-0005" level="1">Other Embodiments</heading>
<p id="p-0108" num="0107">Aspects of the present invention can also be realized by a computer of a system or apparatus (or devices such as a CPU or MPU) that reads out and executes a program recorded on a memory device to perform the functions of the above-described embodiment(s), and by a method, the steps of which are performed by a computer of a system or apparatus by, for example, reading out and executing a program recorded on a memory device to perform the functions of the above-described embodiment(s). For this purpose, the program is provided to the computer for example via a network or from a recording medium of various types serving as the memory device (e.g., computer-readable medium).</p>
<p id="p-0109" num="0108">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.</p>
<p id="p-0110" num="0109">This application claims the benefit of Japanese Patent Application No. 2010-183297 filed Aug. 18, 2010, which is hereby incorporated by reference herein in its entirety.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. An image pickup apparatus that continuously takes images of a subject to obtain a plurality of images representing the subject, comprising:
<claim-text>a choosing unit configured to choose a face included in each of the plurality of images;</claim-text>
<claim-text>a calculation unit configured to calculate an evaluation value, which is used to select one image from the plurality of images, based on the face chosen in each of the plurality of images by said choosing unit, with respect to each of the plurality of images;</claim-text>
<claim-text>a selection unit configured to select an image with the highest evaluation value calculated by said calculation unit, and when there are a plurality of images with the highest evaluation value calculated by said calculation unit, select an image taken first; and</claim-text>
<claim-text>a recording unit configured to record the image selected by said selection unit.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. An image pickup apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said calculation unit obtains a face evaluation value for the face chosen by said choosing unit, and calculates an evaluation value for each of the plurality of images from the face evaluation value, and
<claim-text>when the face evaluation value in first image data among the plurality of image data is away by more than a predetermined value from the evaluation value obtained for second image data obtained before the first image data among the plurality of image data, determines the face evaluation value in the first image data as an evaluation value for the first image data, and when the face evaluation value in the first image data among the plurality of image data is not away by more than a predetermined value from the evaluation value obtained for the second image data, determines the evaluation value obtained for the second image data as an evaluation value for the first image data.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. An image pickup apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said calculation unit calculates the evaluation value by calculating a smile evaluation value for a smile in the face, and an eye open evaluation value for eyes in the face.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. An image pickup apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein said calculation unit obtains a smile level indicative of a degree of smiling with respect to the same face in each of the images based on contours of parts constituting the face and positions of the parts constituting the face, and calculates the smile evaluation value based on a first predetermined value for evaluating a change in smile, as well as the obtained smile level.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. An image pickup apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein said calculation unit obtains an eye open level indicative of a degree to which eyes open with respect to the same face in each of the images, and calculates the eye open evaluation value based on a second predetermined value for evaluating a change in the degree to which eyes open, as well as the obtained eye open level.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. A control method for an image pickup apparatus that continuously takes images of a subject to obtain a plurality of images representing the subject, comprising:
<claim-text>a choosing step of choosing a face included in each of the plurality of images;</claim-text>
<claim-text>a calculation step of calculating an evaluation value, which is used to select one image from the plurality of images, based on the face chosen in each of the plurality of images in said choosing step, with respect to each of the plurality of images;</claim-text>
<claim-text>a selection step of selecting an image with the highest evaluation value calculated in said calculation step, and when there are a plurality of images with the highest evaluation value calculated in said calculation step, selecting an image taken first; and</claim-text>
<claim-text>a recording step of recording the image selected in said selection step.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A computer-readable non-transitory storage medium storing a program for causing a computer to implement a control method for an image pickup apparatus that continuously takes images of a subject to obtain a plurality of images representing the subject, the control method comprising:
<claim-text>a choosing step of choosing a face included in each of the plurality of images;</claim-text>
<claim-text>a calculation step of calculating an evaluation value, which is used to select one image from the plurality of images, based on the face chosen in each of the plurality of images in the choosing step, with respect to each of the plurality of images;</claim-text>
<claim-text>a selection step of selecting an image with the highest evaluation value calculated in the calculation step, and when there are a plurality of images with the highest evaluation value calculated in the calculation step, selecting an image taken first; and
<claim-text>a recording step of recording the image selected in the selection step. </claim-text>
</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-grant>
