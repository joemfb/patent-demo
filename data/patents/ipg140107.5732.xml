<!DOCTYPE us-patent-grant SYSTEM "us-patent-grant-v44-2013-05-16.dtd" [ ]>
<us-patent-grant lang="EN" dtd-version="v4.4 2013-05-16" file="US08626836-20140107.XML" status="PRODUCTION" id="us-patent-grant" country="US" date-produced="20131224" date-publ="20140107">
<us-bibliographic-data-grant>
<publication-reference>
<document-id>
<country>US</country>
<doc-number>08626836</doc-number>
<kind>B2</kind>
<date>20140107</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>13447689</doc-number>
<date>20120416</date>
</document-id>
</application-reference>
<us-application-series-code>13</us-application-series-code>
<us-term-of-grant>
<disclaimer>
<text>This patent is subject to a terminal disclaimer.</text>
</disclaimer>
</us-term-of-grant>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>15</main-group>
<subgroup>16</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20140107</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classification-national>
<country>US</country>
<main-classification>709204</main-classification>
<further-classification>345419</further-classification>
<further-classification>705 144</further-classification>
<further-classification>705 1473</further-classification>
</classification-national>
<invention-title id="d2e51">Providing context for an automated agent to service multiple avatars within a virtual universe</invention-title>
<us-references-cited>
<us-citation>
<patcit num="00001">
<document-id>
<country>US</country>
<doc-number>6219045</doc-number>
<kind>B1</kind>
<name>Leahy et al.</name>
<date>20010400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715757</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00002">
<document-id>
<country>US</country>
<doc-number>6393461</doc-number>
<kind>B1</kind>
<name>Okada et al.</name>
<date>20020500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00003">
<document-id>
<country>US</country>
<doc-number>6421047</doc-number>
<kind>B1</kind>
<name>de Groot</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00004">
<document-id>
<country>US</country>
<doc-number>6772195</doc-number>
<kind>B1</kind>
<name>Hatlelid et al.</name>
<date>20040800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709204</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00005">
<document-id>
<country>US</country>
<doc-number>6804333</doc-number>
<kind>B1</kind>
<name>Liu et al.</name>
<date>20041000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00006">
<document-id>
<country>US</country>
<doc-number>7036128</doc-number>
<kind>B1</kind>
<name>Julia et al.</name>
<date>20060400</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00007">
<document-id>
<country>US</country>
<doc-number>7073129</doc-number>
<kind>B1</kind>
<name>Robarts et al.</name>
<date>20060700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00008">
<document-id>
<country>US</country>
<doc-number>7260605</doc-number>
<kind>B1</kind>
<name>Okada et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00009">
<document-id>
<country>US</country>
<doc-number>7263526</doc-number>
<kind>B1</kind>
<name>Busey et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>  1  1</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00010">
<document-id>
<country>US</country>
<doc-number>7293169</doc-number>
<kind>B1</kind>
<name>Righi et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00011">
<document-id>
<country>US</country>
<doc-number>7468729</doc-number>
<kind>B1</kind>
<name>Levinson</name>
<date>20081200</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345473</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00012">
<document-id>
<country>US</country>
<doc-number>7774388</doc-number>
<kind>B1</kind>
<name>Runchey</name>
<date>20100800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00013">
<document-id>
<country>US</country>
<doc-number>7901288</doc-number>
<kind>B2</kind>
<name>Barsness et al.</name>
<date>20110300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00014">
<document-id>
<country>US</country>
<doc-number>8046408</doc-number>
<kind>B2</kind>
<name>Torabi</name>
<date>20111000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00015">
<document-id>
<country>US</country>
<doc-number>8051462</doc-number>
<kind>B2</kind>
<name>Hamilton, II et al.</name>
<date>20111100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00016">
<document-id>
<country>US</country>
<doc-number>8296246</doc-number>
<kind>B2</kind>
<name>Hamilton et al.</name>
<date>20121000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705346</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00017">
<document-id>
<country>US</country>
<doc-number>2002/0073154</doc-number>
<kind>A1</kind>
<name>Murakami et al.</name>
<date>20020600</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00018">
<document-id>
<country>US</country>
<doc-number>2002/0087704</doc-number>
<kind>A1</kind>
<name>Chesnais et al.</name>
<date>20020700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00019">
<document-id>
<country>US</country>
<doc-number>2002/0113809</doc-number>
<kind>A1</kind>
<name>Akazawa et al.</name>
<date>20020800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00020">
<document-id>
<country>US</country>
<doc-number>2003/0086437</doc-number>
<kind>A1</kind>
<name>Benveniste</name>
<date>20030500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00021">
<document-id>
<country>US</country>
<doc-number>2005/0179685</doc-number>
<kind>A1</kind>
<name>Kake et al.</name>
<date>20050800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00022">
<document-id>
<country>US</country>
<doc-number>2005/0216848</doc-number>
<kind>A1</kind>
<name>Thompson et al.</name>
<date>20050900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715753</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00023">
<document-id>
<country>US</country>
<doc-number>2007/0073582</doc-number>
<kind>A1</kind>
<name>Jung et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00024">
<document-id>
<country>US</country>
<doc-number>2007/0074114</doc-number>
<kind>A1</kind>
<name>Adjali et al.</name>
<date>20070300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00025">
<document-id>
<country>US</country>
<doc-number>2007/0192413</doc-number>
<kind>A1</kind>
<name>Murakami et al.</name>
<date>20070800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00026">
<document-id>
<country>US</country>
<doc-number>2007/0260984</doc-number>
<kind>A1</kind>
<name>Marks et al.</name>
<date>20071100</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715706</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00027">
<document-id>
<country>US</country>
<doc-number>2008/0177994</doc-number>
<kind>A1</kind>
<name>Mayer</name>
<date>20080700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00028">
<document-id>
<country>US</country>
<doc-number>2008/0204450</doc-number>
<kind>A1</kind>
<name>Dawson et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00029">
<document-id>
<country>US</country>
<doc-number>2008/0207327</doc-number>
<kind>A1</kind>
<name>Van Luchene et al.</name>
<date>20080800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00030">
<document-id>
<country>US</country>
<doc-number>2008/0215679</doc-number>
<kind>A1</kind>
<name>Gillo et al.</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>709204</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00031">
<document-id>
<country>US</country>
<doc-number>2008/0215995</doc-number>
<kind>A1</kind>
<name>Wolf</name>
<date>20080900</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715758</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00032">
<document-id>
<country>US</country>
<doc-number>2008/0262910</doc-number>
<kind>A1</kind>
<name>Altberg et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705 14</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00033">
<document-id>
<country>US</country>
<doc-number>2008/0262911</doc-number>
<kind>A1</kind>
<name>Altberg et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00034">
<document-id>
<country>US</country>
<doc-number>2008/0263446</doc-number>
<kind>A1</kind>
<name>Altberg et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715706</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00035">
<document-id>
<country>US</country>
<doc-number>2008/0263458</doc-number>
<kind>A1</kind>
<name>Altberg et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715757</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00036">
<document-id>
<country>US</country>
<doc-number>2008/0263460</doc-number>
<kind>A1</kind>
<name>Altberg et al.</name>
<date>20081000</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715757</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00037">
<document-id>
<country>US</country>
<doc-number>2009/0017916</doc-number>
<kind>A1</kind>
<name>Blanchard, III et al.</name>
<date>20090100</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00038">
<document-id>
<country>US</country>
<doc-number>2009/0058862</doc-number>
<kind>A1</kind>
<name>Finn et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00039">
<document-id>
<country>US</country>
<doc-number>2009/0082110</doc-number>
<kind>A1</kind>
<name>Relyea et al.</name>
<date>20090300</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 42</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00040">
<document-id>
<country>US</country>
<doc-number>2009/0112906</doc-number>
<kind>A1</kind>
<name>Shuster</name>
<date>20090400</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>707102</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00041">
<document-id>
<country>US</country>
<doc-number>2009/0124349</doc-number>
<kind>A1</kind>
<name>Dawson et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>463 24</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00042">
<document-id>
<country>US</country>
<doc-number>2009/0128567</doc-number>
<kind>A1</kind>
<name>Shuster et al.</name>
<date>20090500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00043">
<document-id>
<country>US</country>
<doc-number>2009/0141023</doc-number>
<kind>A1</kind>
<name>Shuster</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>345419</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00044">
<document-id>
<country>US</country>
<doc-number>2009/0144105</doc-number>
<kind>A1</kind>
<name>Blatchley et al.</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>705  7</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00045">
<document-id>
<country>US</country>
<doc-number>2009/0158161</doc-number>
<kind>A1</kind>
<name>Gibbs et al.</name>
<date>20090600</date>
</document-id>
</patcit>
<category>cited by examiner</category>
<classification-national><country>US</country><main-classification>715733</main-classification></classification-national>
</us-citation>
<us-citation>
<patcit num="00046">
<document-id>
<country>US</country>
<doc-number>2009/0193092</doc-number>
<kind>A1</kind>
<name>Li</name>
<date>20090700</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00047">
<document-id>
<country>US</country>
<doc-number>2009/0201299</doc-number>
<kind>A1</kind>
<name>Bhogal et al.</name>
<date>20090800</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00048">
<document-id>
<country>US</country>
<doc-number>2009/0325138</doc-number>
<kind>A1</kind>
<name>Shuster</name>
<date>20091200</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<patcit num="00049">
<document-id>
<country>US</country>
<doc-number>2010/0115426</doc-number>
<kind>A1</kind>
<name>Liu et al.</name>
<date>20100500</date>
</document-id>
</patcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00050">
<othercit>Barrientos et al., &#x201c;Cursive: Controlling Expressive Avatar Gesture Using Pen Gesture&#x201d;, CVE '02, Sep. 30-Oct. 2, 2002, pp. 113-119.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00051">
<othercit>Vilhj&#xe1;lmsson et al., &#x201c;BodyChat: Autonomous Communicative Behaviors in Avatars&#x201d;, Autonomous Agents, 1998, pp. 269-276.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00052">
<othercit>Gillies et al., &#x201c;Integrating Autonomous Behavior and User Control for Believable Agents&#x201d;, AAMAS '04, Jul. 19-23, 2004, pp. 336-343.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
<us-citation>
<nplcit num="00053">
<othercit>Traum et al., &#x201c;Embodied Agents for Multi-party Dialog in Immersive Virtual Worlds&#x201d;, AAMAS '02, Jul. 15-19, 2002, pp. 766-773.</othercit>
</nplcit>
<category>cited by applicant</category>
</us-citation>
</us-references-cited>
<number-of-claims>16</number-of-claims>
<us-exemplary-claim>1</us-exemplary-claim>
<us-field-of-classification-search>
<classification-national>
<country>US</country>
<main-classification>709204</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>345419</main-classification>
</classification-national>
<classification-national>
<country>US</country>
<main-classification>705 144- 1473</main-classification>
<additional-info>unstructured</additional-info>
</classification-national>
</us-field-of-classification-search>
<figures>
<number-of-drawing-sheets>3</number-of-drawing-sheets>
<number-of-figures>3</number-of-figures>
</figures>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>12334866</doc-number>
<date>20081215</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>8214433</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>13447689</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<related-publication>
<document-id>
<country>US</country>
<doc-number>20120203848</doc-number>
<kind>A1</kind>
<date>20120809</date>
</document-id>
</related-publication>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="001" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Dawson</last-name>
<first-name>Christopher J.</first-name>
<address>
<city>Arlington</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="002" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Osias</last-name>
<first-name>Michael J.</first-name>
<address>
<city>Westtown</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
<us-applicant sequence="003" app-type="applicant" designation="us-only">
<addressbook>
<last-name>Sledge</last-name>
<first-name>Brian W.</first-name>
<address>
<city>Shreveport</city>
<state>LA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="001" designation="us-only">
<addressbook>
<last-name>Dawson</last-name>
<first-name>Christopher J.</first-name>
<address>
<city>Arlington</city>
<state>VA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="002" designation="us-only">
<addressbook>
<last-name>Osias</last-name>
<first-name>Michael J.</first-name>
<address>
<city>Westtown</city>
<state>NY</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="003" designation="us-only">
<addressbook>
<last-name>Sledge</last-name>
<first-name>Brian W.</first-name>
<address>
<city>Shreveport</city>
<state>LA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
<agents>
<agent sequence="01" rep-type="attorney">
<addressbook>
<orgname>Pillsbury Winthrop Shaw Pittman LLP</orgname>
<address>
<country>unknown</country>
</address>
</addressbook>
</agent>
</agents>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>Activision Publishing, Inc.</orgname>
<role>02</role>
<address>
<city>Santa Monica</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
<examiners>
<primary-examiner>
<last-name>Chan</last-name>
<first-name>Wing F</first-name>
<department>2441</department>
</primary-examiner>
<assistant-examiner>
<last-name>Miah</last-name>
<first-name>Razu</first-name>
</assistant-examiner>
</examiners>
</us-bibliographic-data-grant>
<abstract id="abstract">
<p id="p-0001" num="0000">A system and method to interact with business systems through the use of automated agents and provide context for an automated agent to service multiple avatars within a virtual universe. The system comprises a chat engine, a communications subsystem, and a channelizer configured to mediate one or more communications between the chat engine and the communications subsystem. The channelizer is further configured to map at least one channel to an application.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="142.16mm" wi="200.32mm" file="US08626836-20140107-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="223.10mm" wi="202.27mm" file="US08626836-20140107-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="190.75mm" wi="126.15mm" file="US08626836-20140107-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="211.50mm" wi="158.83mm" orientation="landscape" file="US08626836-20140107-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?RELAPP description="Other Patent Relations" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading>
<p id="p-0002" num="0001">This application is a continuation of U.S. application Ser. No. 12/334,866, filed Dec. 15, 2008, the contents of which are incorporated by reference herein in their entirety.</p>
<?RELAPP description="Other Patent Relations" end="tail"?>
<?BRFSUM description="Brief Summary" end="lead"?>
<heading id="h-0002" level="1">FIELD OF THE INVENTION</heading>
<p id="p-0003" num="0002">The invention generally relates to a system and method for interacting with business systems through the use of automated agents and, in particular, to providing context for an automated agent to service multiple avatars within a virtual universe.</p>
<heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading>
<p id="p-0004" num="0003">A virtual universe (VU) is an interactive simulated environment accessed by multiple users through an online interface. Users inhabit and interact in the VU via avatars, which are a user's representation of himself or herself. Typically avatars can be representative of a human user, however, avatars may be used to represent any number of entities. These representations can be in the form of a three-dimensional model, a two-dimensional icon, a text construct, a user screen name, etc. Although there are many different types of VUs, there are several features many VUs generally have in common. These features include, for example,
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0004">Shared Space: the VU allows many users to participate at once;</li>
        <li id="ul0002-0002" num="0005">Graphical User Interface: the VU depicts space visually, ranging in style from 2D &#x201c;cartoon&#x201d; imagery to more immersive 3D environments;</li>
        <li id="ul0002-0003" num="0006">Immediacy: interaction takes place in real time;</li>
        <li id="ul0002-0004" num="0007">Interactivity: the VU allows users to alter, develop, build, or submit customized content. Interactivity may also include the ability to move and to touch and/or manipulate an object in the VU;</li>
        <li id="ul0002-0005" num="0008">Persistence: the VU's existence continues regardless of whether individual users are logged in;</li>
        <li id="ul0002-0006" num="0009">Socialization/Community: the VU allows and encourages the formation of social groups such as teams, guilds, clubs, cliques, housemates, neighborhoods, etc.; and</li>
        <li id="ul0002-0007" num="0010">Communication: the VU allows users to speak in a natural language and be detected by other users within a proximity radius by using proximity based chat.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0005" num="0011">With the proliferation of VUs and 3D Internet, the number of ways that avatars can interact with one another has increased. Now, instead of being limited to communication between one or more users, users have the opportunity to interact with automated machine driven avatars. Automated avatars may be used for a number of purposes such as, e.g., hospitality greeters, sales, service, etc.</p>
<p id="p-0006" num="0012">Automated avatars may be instructed to interact the same way with all users' avatars. For example, a hospitality greeter may be programmed to automatically say hello to every avatar that comes within a predefined proximity of the hospitality greeter. Similarly, a sales avatar may be programmed to automatically initiate a one on one conversation with an avatar that is chatting within a predefined radius of the sales avatar. While this provides some level of automated interaction, the type of interaction is generic and does not engage the user of the avatar.</p>
<heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading>
<p id="p-0007" num="0013">In a first aspect of the invention, a system comprises a chat engine, a communications subsystem, and a channelizer configured to mediate one or more communications between the chat engine and the communications subsystem, the channelizer further configured to map at least one channel to an application.</p>
<p id="p-0008" num="0014">In another aspect of the invention, a computer implemented method comprises receiving a communication from an avatar over a chat framework and checking whether the communication is a new communication or a preexisting communication. The computer implemented method further comprise mapping the communication to a channel and transmitting a response to the communication over the chat framework, wherein the response maintains a context of the communication.</p>
<p id="p-0009" num="0015">In another aspect of the invention, a computer program product comprising a computer usable storage medium having readable program code embodied in the storage medium is provided. The computer program product includes at least one component operable to: identify an incoming communication; map the incoming communication to a channel; send the incoming communication to an application program interface; and receive a communication from the application program interface, wherein the communication is a response in a context of the incoming communication.</p>
<p id="p-0010" num="0016">In yet another aspect of the invention, a method for providing context for interactions, comprises providing a computer infrastructure being operable to: identify an incoming communication; map the incoming communication to a channel; send the incoming communication to an application program interface; and receive a response from the application program interface, wherein the response is in a context of the incoming communication.</p>
<?BRFSUM description="Brief Summary" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0011" num="0017">The present invention is described in the detailed description which follows, in reference to the noted plurality of drawings by way of non-limiting examples of exemplary embodiments of the present invention.</p>
<p id="p-0012" num="0018"><figref idref="DRAWINGS">FIG. 1</figref> shows an illustrative environment for implementing the steps in accordance with the invention;</p>
<p id="p-0013" num="0019"><figref idref="DRAWINGS">FIG. 2</figref> shows a flow chart of an exemplary process in accordance with aspects of the invention; and</p>
<p id="p-0014" num="0020"><figref idref="DRAWINGS">FIG. 3</figref> shows an exemplary embodiment in accordance with aspects of the invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?DETDESC description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0015" num="0021">The invention is directed to a system and method for interacting with business systems through the use of automated agents and, in particular, to providing context for an automated agent to service multiple avatars within a virtual universe. More specifically, the invention describes a system and method to interact with business systems through the use of automated avatars.</p>
<p id="p-0016" num="0022">Artificially intelligent automated avatars (&#x201c;automated avatars&#x201d;) may be used to perform a number of functions in a VU. For example, automated avatars may be used to answer questions, trouble shoot problems, sell products in a VU, etc. In embodiments, automated avatars may be connected to a back end system such as an email or artificially intelligent chat program. These programs may be used to instruct automated avatars on what to say in order to carry on individual natural language conversations with user avatars and/or other automated avatars.</p>
<p id="p-0017" num="0023">The present invention allows a single automated avatar to carry on natural language conversations with a plurality of user avatars or automated avatars at one time, i.e., the present invention is scalable. Moreover, in addition to being scalable, the present invention allows the context of each communication to be maintained by using a multi threaded application, to which the user avatars and/or automated avatars may be connected. This beneficially reduces the number of automated avatars that are needed to maintain coherent interactions with a plurality of avatars. Additionally, this also advantageously reduces the number of automated avatars that need to be located within a given region to interact with user avatars and/or automated avatars.</p>
<p id="p-0018" num="0024">The present invention maintains the context of one or more communications by assigning a separate chat channel to each communication or logical group of communications. This is accomplished, in part, by sending an avatar's communication to an automated avatar via a chat engine. Once received, the automated avatar can determine whether the communication is part of a pre-existing communication, such as a conversation. This determination may be performed using a component that is configured to match information associated with the incoming communication to information about previous communications, which may be stored in a lookup table. If a match occurs, the incoming communication is mapped or otherwise associated with the matching entry in the lookup table. However, if a match does not occur, the incoming communication is mapped to a new channel and information relating to the communication and/or avatar can be stored in the lookup table.</p>
<p id="p-0019" num="0025">Once the incoming communication is mapped or otherwise associated with a channel, the communication and information associated with the communication may be sent to an artificially intelligent application, such as an Application Program Interface (API). In embodiments, a number of instances of the application may be created such that an individual instance may be associated with each communication or a manageable group of communications via a channel. By associating a channel to a communication as well as to an instance of an application, the invention allows the application to generate a response that maintains the context of the communication. This response may be sent from the automated avatar and to the user's avatar and/or automated avatar via the designated channel, thereby facilitating communication between the avatar and the automated avatar.</p>
<p id="p-0020" num="0026">Accordingly, the present invention allows a single automated avatar to communicate with a plurality of avatars while maintaining the context of each communication. This increases user experiences in the VU and also requires fewer resources than traditional automated avatar programs because, e.g., the present invention does not require a separate automated avatar to be created for each avatar. Thus, resources consumed in creating the automated avatar and running the back end programs for each automated avatar may be preserved. Moreover, by centralizing the communications infrastructure and logic, the maintenance and development of applications that use an avatar as a user API may be simplified.</p>
<heading id="h-0007" level="1">System Environment</heading>
<p id="p-0021" num="0027">As will be appreciated by one skilled in the art, the present invention may be embodied as a system, method or computer program product. Accordingly, the present invention may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a &#x201c;circuit,&#x201d; &#x201c;module&#x201d; or &#x201c;system.&#x201d; Furthermore, the present invention may take the form of a computer program product embodied in any tangible medium of expression having computer-usable program code embodied in the medium.</p>
<p id="p-0022" num="0028">Any combination of one or more computer usable or computer readable medium(s) may be utilized. The computer-usable or computer-readable medium may be, for example but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, device, or propagation medium. More specific examples (a non-exhaustive list) of the computer-readable medium would include the following:
<ul id="ul0003" list-style="none">
    <li id="ul0003-0001" num="0000">
    <ul id="ul0004" list-style="none">
        <li id="ul0004-0001" num="0029">a portable computer diskette,</li>
        <li id="ul0004-0002" num="0030">a hard disk,</li>
        <li id="ul0004-0003" num="0031">a random access memory (RAM),</li>
        <li id="ul0004-0004" num="0032">a read-only memory (ROM),</li>
        <li id="ul0004-0005" num="0033">an erasable programmable read-only memory (EPROM or Flash memory),</li>
        <li id="ul0004-0006" num="0034">a portable compact disc read-only memory (CDROM),</li>
        <li id="ul0004-0007" num="0035">an optical storage device, and/or</li>
        <li id="ul0004-0008" num="0036">a transmission media such as those supporting the Internet or an intranet, or a magnetic storage device.
<br/>
The computer-usable or computer-readable medium could even be paper or another suitable medium upon which the program is printed, as the program can be electronically captured, via, for instance, optical scanning of the paper or other medium, then compiled, interpreted, or otherwise processed in a suitable manner, if necessary, and then stored in a computer memory.
</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0023" num="0037">In the context of this document, a computer-usable or computer-readable medium may be any medium that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, or device. The computer usable program code may be transmitted using any appropriate transmission media via a network.</p>
<p id="p-0024" num="0038">Computer program code for carrying out operations of the present invention may be written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++ or the like and conventional procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network. This may include, for example, a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).</p>
<p id="p-0025" num="0039"><figref idref="DRAWINGS">FIG. 1</figref> shows an illustrative environment <b>10</b> for managing the processes in accordance with the invention. To this extent, the environment <b>10</b> includes a computer infrastructure <b>12</b> that can perform the processes described herein. In particular, the computer infrastructure <b>12</b> includes a computing device <b>14</b> that comprises a Chat Engine <b>30</b>, a Channelizer <b>35</b>, a Communications Subsystem <b>40</b>, and an Application Program Interface (API) <b>50</b>. The Chat Engine <b>30</b>, the Channelizer <b>35</b>, the Communications Subsystem <b>40</b>, and the API <b>50</b> are configured to make computing device <b>14</b> operable to provide context for interactions between users and automated agents in accordance with the invention, e.g., process described herein.</p>
<p id="p-0026" num="0040">More specifically, the Chat Engine <b>30</b> is configured to provide a framework that facilitates communications, e.g., chats, between a plurality of user avatars and/or automated avatars. The Chat Engine <b>30</b> is further configured to interact with a plurality of components and/or systems such as the Communications Subsystem <b>40</b>, the API <b>50</b>, the Channelizer <b>35</b>, etc., to provide context for interactions between, e.g., user avatars and automated agents. Any number of chat engines known to those of skill in the art may be used to provide a chat framework in accordance with the present invention.</p>
<p id="p-0027" num="0041">The Channelizer <b>35</b>, which may be embodied as part of an automated avatar, is configured to mediate communications between the Chat Engine <b>30</b> and the Communications Subsystem <b>40</b> by mapping chat channels to network API calls using keys and/or channels defined in a lookup table. In embodiments, the automated avatar may invoke an API <b>50</b> via the Communications Subsystem <b>40</b> over the Internet using an assigned key by passing the key with a network request over the Internet to an application. Embodiments may associate the key with one or more unique API <b>50</b> instances. The key may be predefined and/or loaded into a system memory table at runtime or configured dynamically.</p>
<p id="p-0028" num="0042">More specifically, communications from a user's avatar or another automated avatar may be sent from the Chat Engine <b>30</b> to the Channelizer <b>35</b>. Once obtained, the Channelizer <b>35</b> may map chat channels to network API calls using information in a lookup table, which includes, e.g., keys, channels, etc. This information may be sent from the Channelizer <b>35</b> to the Communications Subsystem <b>40</b> or from the Communications Subsystem <b>40</b> to the Channelizer <b>35</b>, i.e., the Channelizer <b>35</b> can send and receive information. In embodiments, the relayed information may include a communication from a user's avatar and/or an automated avatar.</p>
<p id="p-0029" num="0043">The Communications Subsystem <b>40</b> is configured to facilitate communication between an avatar and an automated avatar. More specifically, the Communications Subsystem <b>40</b> may be embodied as part of an automated avatar and may be used by the automated avatar to interface with user's avatars and/or other automated avatars within a VU. The Communications Subsystem <b>40</b> is also configured to send communications to one or more systems or components, and may be used to invoke the API <b>50</b>.</p>
<p id="p-0030" num="0044">In embodiments the Communications Subsystem <b>40</b> may comprise a number of components, including a Simulator <b>55</b>, which allows the automated avatar to recognize the presence and/or speech of a user's avatar. The Simulator <b>55</b> may be embodied as a middleware component on a server and is configured to run one or more scripts and/or store data in a memory <b>22</b>A. The one or more scripts may be used to, e.g., manage user states, monitor avatar positions, integrate data and/or graphics, provide miscellaneous services, etc. In embodiments, the scripts may be attached to a 3D object within the VU such as, e.g., a magic ring, an inventory item, etc. The 3D object may be attached to an avatar and give the avatar a behavior, as specified herein. One or more of the scripts may be executed by an Interpreter <b>70</b>, which may be included as part of the Simulator <b>55</b>.</p>
<p id="p-0031" num="0045">The Simulator <b>55</b> may allow an automated avatar to recognize the presence and speech of a user's avatar by utilizing Sensors <b>60</b> and/or Chat Facilities <b>65</b>, which may be embodied as components of the Simulator <b>55</b>. More specifically, an automated avatar may use Sensors <b>60</b> to detect the presence of a user's avatar that is located within a predefined proximity of the automated avatar. In embodiments, the detection may be performed by directly sensing the avatar or an avatar's 3D object. Additionally, in embodiments, the detection may be performed by programming an avatar or an avatar's 3D object to directly interface with the automated avatar via the Communications Subsystem <b>40</b>, without first being sensed by the Sensors <b>60</b>.</p>
<p id="p-0032" num="0046">Chat Facilities <b>65</b> are configured to facilitate communication between the automated avatars and the user's avatars. The Chat Facilities <b>65</b> may be provided by the Simulator <b>55</b> as part of a 3D simulator environment. In embodiments, the Chat Facilities <b>65</b> may be proximity based, channel based, or a combination thereof. Proximity based Chat Facilities <b>65</b> may be used to determine, e.g., what user avatars and/or automated avatars are within a specified spatial scope. The spatial scope may be representative of the distance a user avatar or automated avatar can be within while still being able to hear a communication, i.e., a chat.</p>
<p id="p-0033" num="0047">Channel based Chat Facilities <b>65</b> may be used to determine what channel a communication, e.g., a chat, takes place on. For example, channel based Chat Facilities <b>65</b> may allow chats to occur on 65535 different channels. One or more of these channels may be public or non-public. Thus, for example, channel <b>1</b> may be represented as a public channel on which everyone can be heard; whereas other channels may be non-public channels on which a user avatar and/or an automated avatar can only be heard by those listening to the same channel. In embodiments, a group or range of channels may be logically grouped together for a specific purpose to create a channel bank. Channel banks may be used for, e.g., multiplexing and/or channelizing data.</p>
<p id="p-0034" num="0048">An automated avatar may invoke the API <b>50</b> via the Communications Subsystem <b>40</b> over the Internet using an assigned key, which may be stored in the lookup table. In embodiments, the Communications Subsystem <b>40</b> may pass the key with a network request, such as a hypertext transfer protocol (HTTP) request, over the Internet to an API <b>50</b> or similar application. The API <b>50</b> may respond to the network request with an appropriate output result, which may be delivered back to the Communications Subsystem <b>40</b> over the same network. The response may include, e.g., a communication from the automated avatar in response to the communication from the user's avatar.</p>
<p id="p-0035" num="0049">The computing device <b>14</b> also includes a processor <b>20</b>, the memory <b>22</b>A, an I/O interface <b>24</b>, and a bus <b>26</b>. The memory <b>22</b>A can include local memory employed during actual execution of program code, bulk storage, and cache memories which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during execution.</p>
<p id="p-0036" num="0050">The computing device <b>14</b> is in further communication with the external I/O device/resource <b>28</b> and the storage system <b>22</b>B. For example, the I/O device <b>28</b> can comprise any device that enables an individual to interact with the computing device <b>14</b> or any device that enables the computing device <b>14</b> to communicate with one or more other computing devices using any type of communications link. The external I/O device/resource <b>28</b> may be keyboards, displays, pointing devices, microphones, headsets, etc.</p>
<p id="p-0037" num="0051">In general, the processor <b>20</b> executes computer program code, which is stored in the memory <b>22</b>A and/or storage system <b>22</b>B. The computer code may be representative of the functionality of the Chat Engine <b>30</b>, the Channelizer <b>35</b>, the Communications Subsystem <b>40</b>, and the API <b>50</b>. While executing computer program code, the processor <b>20</b> can read and/or write data to/from memory <b>22</b>A, storage system <b>22</b>B, and/or I/O interface <b>24</b>. The program code executes the processes of the invention. The bus <b>26</b> provides a communications link between each of the components in the computing device <b>14</b>.</p>
<p id="p-0038" num="0052">The computing device <b>14</b> can comprise any general purpose computing article of manufacture capable of executing computer program code installed thereon (e.g., a personal computer, server, handheld device, etc.). However, it is understood that the computing device <b>14</b> is only representative of various possible equivalent computing devices that may perform the processes described herein. To this extent, in embodiments, the functionality provided by the computing device <b>14</b> can be implemented by a computing article of manufacture that includes any combination of general and/or specific purpose hardware and/or computer program code. In each embodiment, the program code and hardware can be created using standard programming and engineering techniques, respectively.</p>
<p id="p-0039" num="0053">Similarly, the server <b>12</b> is only illustrative of various types of computer infrastructures for implementing the invention. For example, in embodiments, the server <b>12</b> comprises two or more computing devices (e.g., a server cluster) that communicate over any type of communications link, such as a network, a shared memory, or the like, to perform the processes described herein. Further, while performing the processes described herein, one or more computing devices on the server <b>12</b> can communicate with one or more other computing devices external to the server <b>12</b> using any type of communications link. The communications link can comprise any combination of wired and/or wireless links; any combination of one or more types of networks (e.g., the Internet, a wide area network, a local area network, a virtual private network, etc.); and/or utilize any combination of transmission techniques and protocols.</p>
<p id="p-0040" num="0054">In embodiments, the invention provides a business method that performs the steps of the invention on a subscription, advertising, and/or fee basis. That is, a service provider, such as a Solution Integrator, could offer to perform the processes described herein. In this case, the service provider can create, maintain, deploy, support, etc., a computer infrastructure that performs the process steps of the invention for one or more customers. In return, the service provider can receive payment from the customer(s) under a subscription and/or fee agreement and/or the service provider can receive payment from the sale of advertising content to one or more third parties.</p>
<p id="p-0041" num="0055">As will be appreciated by one skilled in the art, the present invention may be embodied as a system, method or computer program product. Accordingly, the present invention may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a &#x201c;circuit,&#x201d; &#x201c;module&#x201d; or &#x201c;system.&#x201d; Furthermore, the present invention may take the form of a computer program product embodied in any tangible medium of expression having computer-usable program code embodied in the medium.</p>
<p id="p-0042" num="0056">Any combination of one or more computer usable or computer readable medium(s) may be utilized. The computer-usable or computer-readable medium may be, for example but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, device, or propagation medium. More specific examples (a non-exhaustive list) of the computer-readable medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CDROM), an optical storage device, a transmission media such as those supporting the Internet or an intranet, or a magnetic storage device. Note that the computer-usable or computer-readable medium could even be paper or another suitable medium upon which the program is printed, as the program can be electronically captured, via, for instance, optical scanning of the paper or other medium, then compiled, interpreted, or otherwise processed in a suitable manner, if necessary, and then stored in a computer memory. In the context of this document, a computer-usable or computer-readable medium may be any medium that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, or device. The computer-usable medium may include a propagated data signal with the computer-usable program code embodied therewith, either in baseband or as part of a carrier wave. The computer usable program code may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, RF, etc.</p>
<heading id="h-0008" level="1">Exemplary Implementation of the System</heading>
<p id="p-0043" num="0057">The present invention is described below with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0044" num="0058">These computer program instructions may also be stored in a computer-readable medium that can direct a computer or other programmable data processing apparatus to function in a particular manner, such that the instructions stored in the computer-readable medium produce an article of manufacture including instruction means which implement the function/act specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0045" num="0059">The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.</p>
<p id="p-0046" num="0060">The flowchart and block diagram in <figref idref="DRAWINGS">FIG. 2</figref> illustrates the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagram may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figure. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagram and/or flowchart illustration, and combinations of blocks in the block diagram and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.</p>
<p id="p-0047" num="0061"><figref idref="DRAWINGS">FIG. 2</figref> shows a flow chart of an exemplary process according to embodiments of the invention. At step <b>210</b>, a communication may be initiated. The initiation may result from a user's avatar and/or an automated avatar. For example, a communication may be initiated by a user's avatar speaking to an automated avatar. Likewise, a communication may be initiated by an automated avatar speaking to a user's avatar. A communication may also be initiated from the happening of an event. Exemplary events may include, e.g., an avatar selecting a button or option to initiate a communication, an announcement being made that allows an avatar to respond, etc.</p>
<p id="p-0048" num="0062">A communication may be initiated when the automated avatar detects an avatar and/or an object of the avatar's within a predetermined proximity. For example, an automated avatar at a virtual kiosk may detect avatars that come within a predefined distance of the virtual kiosk and initiate a communication, such as a greeting, to the avatar. Similarly, an automated avatar may detect communications between one or more avatars on an existing communication channel. For example, an automated avatar may detect one or more avatars speaking with one another on a public chat channel and initiate a communication with one or more of the avatars. It should be understood that a communication may be initiated between one avatar and/or a plurality of avatars at a time.</p>
<p id="p-0049" num="0063">At step <b>220</b>, the initiated communication may be sent to a chat engine or chat framework (such as the Chat Engine <b>30</b> in <figref idref="DRAWINGS">FIG. 1</figref>). The chat engine may provide a mechanism, such as a handler, to encode the communicated message into a request. In embodiments the request may be encoded as a HTTP request and sent to a channelizer as a package. The package may include, e.g., the communication from the user's avatar, a session identifier, etc.</p>
<p id="p-0050" num="0064">The channelizer may receive the HTTP request and unpackage the HTTP request to parse out the information sent by the avatar via the chat engine. The channelizer may also perform a lookup to determine whether the received communication is part of an existing communication, i.e., a communication that has already been initiated between the avatar and the automated avatar. This may be performed, e.g., using a lookup table having columns for an avatar's name, the channel the communication with the avatar is taking place on, a key, the time and/or date that the last communication took place, etc. In embodiments, the key may be a hexadecimal string, such as a universally unique identifier (UUID), and may be used to uniquely identify an avatar and/or an avatar's object within the VU.</p>
<p id="p-0051" num="0065">At step <b>230</b>, a chat channel may be mapped onto a communication. In those instances where an existing communication between the avatar and the automated avatar already exists in the lookup table, the same chat channel may be used. However, in those instances where an existing communication has lapsed or a new communication is being initiated, a new chat channel may be mapped to the communication.</p>
<p id="p-0052" num="0066">In embodiments, one or more entries in the lookup table may lapse. A lapse may occur, e.g., after a predetermined period of time has expired. Similarly, a lapse may occur when there exists no available chat channels and the communication is the oldest communication in the lookup table. The process of determining whether an entry, e.g., a communication, has lapsed may include, e.g., having an internal timer periodically check the current time against the time stored in the lookup table. In embodiments, the internal timer may be provided by the simulator runtime environment timer, which causes a routine inside of the automated agent script environment to be invoked to check the current time against one or more of the times stored in the table. If the amount of time that has passed between the last conversation in the lookup table and the current time is greater than some predefined value, the entry and the information associated with the entry may be cleared from the lookup table. Contrarily, if the amount of time that has passed between the last conversation in the lookup table and the current time is less than some predefined value, no action need be taken.</p>
<p id="p-0053" num="0067">In embodiments, the mapping process may include identifying an available chat channel. This may be performed by identifying the first available chat channel, randomly selecting a chat channel from the available chat channels, etc. Once an available chat channel is identified, the channelizer may assign the chat channel to the avatar's communication. This may be performed by, e.g., associating the avatar's name with the chat channel and/or associating a key to the chat channel, wherein the key is unique to the avatar and/or an object of the avatar. At any point after the channelizer assigns a chat channel to the communication, the channelizer may communicate the chat channel to the avatar via the chat engine so that the avatar may use the assigned chat channel for future communications.</p>
<p id="p-0054" num="0068">The present invention allows a plurality of avatars to be mapped to the same communication channel at one time, thereby allowing multiple avatars to participate in the same communication at the same time. In embodiments, one or more avatars may automatically be mapped to the same chat channel or may be mapped to the same chat channel upon an avatar's request. The request may include, e.g., an avatar passing a keyword to the automated avatar through the chat engine, wherein the keyword indicates that the avatar would like to join a specified channel or communication. In embodiments, the keyword may be a simple request message, a password, etc.</p>
<p id="p-0055" num="0069">Upon mapping a communication to a chat channel, the unpackaged information may be sent to a communications subsystem (such as the communications subsystem <b>40</b> described in <figref idref="DRAWINGS">FIG. 1</figref>). The communications subsystem may package the communication, as well as additional information about the communication, as an HTTP request. At step <b>240</b>, the package may be sent to an application, which may be embodied as an instance of an API (such as the API <b>50</b> described in <figref idref="DRAWINGS">FIG. 1</figref>). The number of API instances may vary between embodiments and include, e.g., an API instance for each chat channel.</p>
<p id="p-0056" num="0070">One or more artificial intelligence algorithms may be associated with each of the API instances and used to determine what communication should be sent in response to the avatar's communication, which was transmitted with the HTTP request package. Once determined, the response may be packaged and transmitted from the API to the communications subsystem, at step <b>250</b>. In embodiments, the communications subsystem may unpackage the response, and any information sent with the response, and send the response to the channelizer. At step <b>260</b>, the channelizer may package the response and send it to the chat engine through the appropriate channel such that the response from the API is communicated to the avatar on the predefined chat channel.</p>
<p id="p-0057" num="0071">Understandably, by allowing a single automated avatar to utilize a plurality of chat channels, and by associating an API to one or more of the chat channels, the automated avatar may maintain the context of a number of communications at one time. Beneficially this engages avatars participating in a VU and reduces the number of automated avatars needed to communicate with avatars within the VU.</p>
<heading id="h-0009" level="1">Exemplary Embodiments</heading>
<p id="p-0058" num="0072"><figref idref="DRAWINGS">FIG. 3</figref> shows an exemplary embodiment according to the invention. In particular, <figref idref="DRAWINGS">FIG. 3</figref> includes avatars <b>300</b> and <b>305</b>, which may be user avatars, automated avatars, etc. <figref idref="DRAWINGS">FIG. 3</figref> also includes an automated avatar <b>310</b> configured to intelligently communicate with one or more avatars <b>300</b> and <b>305</b> at a time while maintaining the context of each communication.</p>
<p id="p-0059" num="0073">For example, a first avatar <b>300</b> may come within a certain proximity of a virtual kiosk. The avatar <b>300</b> may initiate a communication by saying &#x201c;hello&#x201d; to the automated avatar <b>310</b> at the virtual kiosk. In embodiments, the initial communication may be sent on a public chat channel using a chat engine or chat framework <b>30</b>.</p>
<p id="p-0060" num="0074">The chat framework <b>30</b> may use a handler to encode the communication and send the communication to the automated avatar <b>310</b> as part of an HTTP request. The automated avatar <b>310</b> may recognize the communication and or presence of the avatar <b>300</b> via a simulator (such as the simulator <b>55</b> in <figref idref="DRAWINGS">FIG. 1</figref>). For example, the automated avatar <b>310</b> may utilize a sensor within the simulator to recognize that the avatar <b>300</b> is within a specified proximity of the automated avatar <b>310</b>. Similarly, the automated avatar <b>310</b> may utilize a chat facility within the simulator to recognize that the avatar <b>300</b> is communicating on a specific channel.</p>
<p id="p-0061" num="0075">The automated avatar <b>310</b> may receive the HTTP request from the chat framework <b>30</b> via a channelizer <b>35</b>. The channelizer <b>35</b> may unpackage the HTTP request and determine information about the avatar <b>300</b>. Exemplary information may include the avatar's name, a channel identifier, and/or a key, etc. The channelizer <b>35</b> may lookup one or more of these pieces of information in a chat bank, which may be stored as a lookup table <b>340</b> in a system memory (such as memory <b>22</b>A in <figref idref="DRAWINGS">FIG. 1</figref>). The chat bank may be preconfigured and have predefined chat channels. In embodiments, the chat bank may be assigned prior to the start of the application or API. However, in embodiments, the chat bank may be configured dynamically.</p>
<p id="p-0062" num="0076">If the information in the received HTTP request matches an entry in the lookup table <b>340</b>, the channel corresponding to the entry may be used for communicating between the avatar <b>300</b> and the automated avatar <b>310</b>. However, if there is no match, i.e., the communication is the first communication between the avatar <b>300</b> and the automated avatar <b>310</b>, a new channel and table entry may be allocated in the system memory and may be mapped or otherwise assigned to the communication via the channelizer. Once assigned, additional information relating to the communication and/or avatar <b>300</b> may be added to the lookup table <b>340</b> for future reference. In embodiments, this information will remain in the lookup table <b>340</b> until it lapses.</p>
<p id="p-0063" num="0077">For illustrative purposes, an exemplary lookup table <b>340</b> is displayed in <figref idref="DRAWINGS">FIG. 3</figref>. The lookup table <b>340</b> includes, e.g., an avatar <b>300</b> named &#x201c;AV<b>1</b>&#x201d; that has a UUID &#x201c;key <b>1</b>&#x201d;, and is communicating on &#x201c;channel <b>1</b>&#x201d;. The last communication that was received and/or sent to &#x201c;AV<b>1</b>&#x201d; was at time &#x201c;t&#x201d;. In addition to information existing on existing communications, the lookup table <b>340</b> may also comprise a number of chat channels that are available, i.e., not currently being used for communication.</p>
<p id="p-0064" num="0078">Upon using the lookup table to identify the appropriate channel, the automated avatar <b>310</b> may send a communication to the avatar <b>300</b> on the identified channel. In embodiments, the communication may be automatic, such as saying &#x201c;hello&#x201d;. Alternatively, in embodiments, the communication may be determined using one or more artificially intelligent programs. For example, the channelizer <b>35</b> may send the unpackaged information to a communications subsystem <b>40</b>, which may be embodied as part of the automated avatar <b>310</b>. The communications subsystem <b>40</b> may package the communication as part of a HTTP request and send it to an API on a network. In embodiments, the HTTP request may be the same or different than the HTTP request that was sent from the chat framework <b>30</b> to the channelizer <b>35</b>. For example, the HTTP request being sent to the API may include information on the communication channel being used for the communication and may avoid unneeded or duplicative information that may be found, e.g., in the header.</p>
<p id="p-0065" num="0079">The API may receive the HTTP request and determine what API instance is associated with the communication session. For example, API instance &#x201c;1&#x201d; may be associated with the avatar's communication on channel <b>1</b>. In embodiments, a single API instance may be assigned to each communication, thereby allowing the context of the communication to be easily kept between the avatar <b>300</b> and the automated avatar <b>310</b>. However, embodiments may also group together avatars that are communicating on the same channel provided that the context of the communication can be maintained.</p>
<p id="p-0066" num="0080">The API instance may determine an appropriate response given the context of the communication and send the response to the communications subsystem over the same or a different network as a packaged HTTP request. The communications subsystem <b>40</b> may unpackage the HTTP request and send it to the channelizer <b>35</b>, which identifies the proper communication channel on which to send the response. In embodiments, the communications subsystem may also create a HTTP request package comprising the response and may send the HTTP request package to the chat framework, which may in turn relay the automated avatar's <b>310</b> response to the avatar <b>300</b>.</p>
<p id="p-0067" num="0081">While the above example has been described in terms of a single avatar <b>300</b> communicating with an automated avatar <b>310</b>, it should be understood that a similar process may be used when more than one avatar <b>300</b> and <b>305</b> is communicating with the automated avatar <b>310</b>. For example, a second avatar <b>305</b> may initiate a communication with the automated avatar <b>310</b>. The communication may be sent from the chat framework to the channelizer, as described above. The channelizer may determine whether the communication has already been assigned to the communication or whether a new channel need be assigned to the communication. Once a channel is assigned, the communication may be relayed to an API instance <b>50</b> via the communications subsystem <b>40</b>. The API instance <b>50</b> may determine the appropriate response given the context of the communication and send the response back to the communications subsystem <b>40</b>, which may send the response to the avatar <b>305</b> via an appropriate channel as determined by the channelizer <b>35</b>.</p>
<p id="p-0068" num="0082">It should be understood that the communications between one or more avatars <b>300</b> and <b>305</b> and the automated avatar <b>310</b> may occur simultaneously or at different times. Thus, for example, a single automated avatar <b>310</b> may communicate with any number of avatars <b>300</b> and <b>305</b> at the same time while maintaining the context of each communication.</p>
<p id="p-0069" num="0083">Accordingly, as described herein, the present invention comprises parallel concurrent channels for communication and also provides stateful interactions with parallel instances of network APIs that provide some service by returning data to the avatar based on an input request. In particular, the present invention comprises an initial interaction pattern which recognizes an avatar and assigns a channel to a communication with the avatar. The invention also provides a state management algorithm that maps avatar instances to stateful API instances in the back-end applications. Beneficially this allows the context of interactions, e.g., communications, between avatars and automated avatars to be maintained.</p>
<p id="p-0070" num="0084">While the invention has been described in terms of embodiments, those skilled in the art will recognize that the invention can be practiced with modifications and in the spirit and scope of the appended claims. Additionally, the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein, the singular forms &#x201c;a&#x201d;, &#x201c;an&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms &#x201c;comprises&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.</p>
<p id="p-0071" num="0085">The corresponding structures, materials, acts, and equivalents of all means or step plus function elements, if applicable, in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated. Accordingly, while the invention has been described in terms of embodiments, those of skill in the art will recognize that the invention can be practiced with modifications and in the spirit and scope of the appended claims.</p>
<?DETDESC description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text>1. A system for facilitating conversations between an automated avatar and user avatars, comprising:
<claim-text>one or more physical processors configured to:
<claim-text>receive, from a first user avatar, a communication that is directed to an automated avatar;</claim-text>
<claim-text>map the communication to a first chat channel that is determined based on matching at least one of a key, a username, or the first chat channel to an entry in a lookup table; and</claim-text>
<claim-text>maintain a first conversation between the automated avatar and the first user avatar on the first chat channel while simultaneously maintaining a second conversation between the automated avatar and a second user avatar on a second chat channel such that the first conversation cannot be heard by the second user avatar and the second conversation cannot be heard by the first user avatar, wherein the first user avatar is different than the second user avatar, and the first chat channel is different than the second chat channel, and</claim-text>
<claim-text>wherein the first user avatar is a representation of a first user in a virtual universe, the virtual universe is an interactive simulated environment accessed by a plurality of users through an online interface, and the automated avatar is a machine-driven avatar in the virtual universe.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text>2. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more physical processors are further configured to:
<claim-text>transmit the communication to an application, wherein the application is an instance of an application program interface.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text>3. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the application includes one or more artificially intelligent programs configured to provide a response to the communication while maintaining a context of the communication.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text>4. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the key is a universally unique identifier.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text>5. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more physical processors are further configured to:
<claim-text>track a time at which the communication took place;</claim-text>
<claim-text>remove information from the lookup table after a predetermined period of time has elapsed;</claim-text>
<claim-text>transmit the communication to an application; and</claim-text>
<claim-text>transmit a response to the communication from the application to the first user avatar through the mapped first chat channel.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text>6. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein maintaining the first conversation while simultaneously maintaining the second conversation comprises maintaining the first conversation on the first chat channel while simultaneously maintaining the second conversation on the second chat channel such that the first conversation is perceivable by the second user avatar and the second conversation is perceivable by the first user avatar.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text>7. A computer-implemented method of facilitating conversations between an automated avatar and user avatars, the method being implemented by a computer that includes one or more physical processors, the method comprising:
<claim-text>receiving, from a first user avatar, a first communication that is directed to an automated avatar over a chat framework;</claim-text>
<claim-text>mapping the first communication to a first chat channel that is determined based on matching at least one of a key, a username, or the first chat channel to an entry in a lookup table;</claim-text>
<claim-text>transmitting a response to the first communication over the chat framework on the first chat channel, wherein the response maintains a context of the first communication, and the response is directed from the automated avatar to the first user avatar;</claim-text>
<claim-text>removing the entry from the lookup table after a predetermined period of time has elapsed;</claim-text>
<claim-text>receiving, from a second user avatar, a second communication that is directed to the automated avatar; and</claim-text>
<claim-text>mapping the second communication to a second chat channel, wherein a first conversation between the first user avatar and the automated avatar on the first chat channel is maintained simultaneously with a second conversation between the second user avatar and the automated avatar on the second chat channel, the first conversation and the second conversation being maintained such that the first conversation cannot be heard by the second user avatar and the second conversation cannot be heard by the first user avatar, and</claim-text>
<claim-text>wherein the first user avatar is a representation of a first user in a virtual universe, the virtual universe is an interactive simulated environment accessed by a plurality of users through an online interface, and the automated avatar is a machine-driven avatar in the virtual universe.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text>8. The computer-implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein at least one of receiving the first communication or transmitting the response includes a hypertext transfer protocol (HTTP) message.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text>9. The computer-implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the key is a universally unique identifier.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text>10. The computer-implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:
<claim-text>tracking a time at which the first communication took place; and</claim-text>
<claim-text>transmitting the first communication to an application;</claim-text>
<claim-text>wherein the transmitting of the response comprises transmitting the response from the application to the first user avatar through the mapped first chat channel.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text>11. The computer-implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the method is at least one of: supported, deployed, maintained, or created by a service provider.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text>12. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the first conversation and the second conversation are maintained such that the first conversation is perceivable by the second user avatar and the second conversation is perceivable by the first user avatar.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text>13. A computer program product to facilitate conversations between an automated avatar and user avatars, the computer program product comprising:
<claim-text>one or more computer-readable tangible storage devices;</claim-text>
<claim-text>program instructions, stored on at least one of the one or more computer-readable tangible storage devices, to:
<claim-text>identify a first incoming communication, from a first user avatar, that is directed to an automated avatar;</claim-text>
<claim-text>map the first incoming communication to a first chat channel;</claim-text>
<claim-text>transmit the first incoming communication to an application program interface;</claim-text>
<claim-text>receive a response in a context of the first incoming communication from the application program interface; and</claim-text>
<claim-text>transmit the received response to the first user avatar through the first chat channel;</claim-text>
<claim-text>map a second incoming communication, from a second user avatar, that is directed to the automated avatar to a second chat channel; and</claim-text>
<claim-text>maintain the mapping of the first chat channel and the mapping of the second chat channel while the automated avatar simultaneously carries on a conversation with the first user avatar on the first chat channel and a conversation with the second user avatar on the second chat channel such that the first conversation cannot be heard by the second user avatar and the second conversation cannot be heard by the first user avatar,</claim-text>
<claim-text>wherein the first user avatar is a representation of a first user in a virtual universe, the virtual universe is an interactive simulated environment accessed by a plurality of users through an online interface, and the automated avatar is a machine-driven avatar in the virtual universe.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text>14. The computer program product of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising program instructions, stored on at least one of the one or more computer-readable tangible storage devices, to detect the first user avatar within a proximity of the automated avatar.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text>15. The computer program product of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the automated avatar comprises at least one of a hospitality greeter, a sales avatar, or a service avatar.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text>16. The computer program product of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising program instructions, stored on at least one of the one or more computer-readable tangible storage devices, to detect the first user avatar communicating on the first chat channel. </claim-text>
</claim>
</claims>
</us-patent-grant>
